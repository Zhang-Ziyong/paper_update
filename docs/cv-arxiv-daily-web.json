{
  "SLAM": {
    "2505.01017": "|**2025-05-02**|**Tightly Coupled Range Inertial Odometry and Mapping with Exact Point Cloud Downsampling**|Kenji Koide et.al.|[2505.01017](http://arxiv.org/abs/2505.01017)|null|\n",
    "2505.04095": "|**2025-05-07**|**Scalable Aerial GNSS Localization for Marine Robots**|Shuo Wen et.al.|[2505.04095](http://arxiv.org/abs/2505.04095)|**[link](https://github.com/stevvwen/aerial_gnss)**|\n",
    "2505.03565": "|**2025-05-06**|**Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and Low-Visibility Conditions**|Lukas Schichler et.al.|[2505.03565](http://arxiv.org/abs/2505.03565)|null|\n",
    "2505.03448": "|**2025-05-06**|**AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames**|Yifan Peng et.al.|[2505.03448](http://arxiv.org/abs/2505.03448)|null|\n",
    "2505.03422": "|**2025-05-06**|**LiftFeat: 3D Geometry-Aware Local Feature Matching**|Yepeng Liu et.al.|[2505.03422](http://arxiv.org/abs/2505.03422)|**[link](https://github.com/lyp-deeplearning/liftfeat)**|\n",
    "2505.02598": "|**2025-05-05**|**LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven Control System for Skid-Steer Robots**|Mehdi Heydari Shahna et.al.|[2505.02598](http://arxiv.org/abs/2505.02598)|null|\n",
    "2505.02272": "|**2025-05-04**|**Robust Localization, Mapping, and Navigation for Quadruped Robots**|Dyuman Aditya et.al.|[2505.02272](http://arxiv.org/abs/2505.02272)|null|\n",
    "2505.01956": "|**2025-05-04**|**SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment**|Ganesh Sapkota et.al.|[2505.01956](http://arxiv.org/abs/2505.01956)|null|\n",
    "2505.01934": "|**2025-05-03**|**GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels**|Yongxin Su et.al.|[2505.01934](http://arxiv.org/abs/2505.01934)|null|\n",
    "2505.09024": "|**2025-05-13**|**Automated Meta Prompt Engineering for Alignment with the Theory of Mind**|Aaron Baughman et.al.|[2505.09024](http://arxiv.org/abs/2505.09024)|null|\n",
    "2505.08388": "|**2025-05-13**|**MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for Enhanced Indoor Localization Using LiDAR-SLAM**|Saqi Hussain Kalan et.al.|[2505.08388](http://arxiv.org/abs/2505.08388)|null|\n",
    "2505.08230": "|**2025-05-13**|**SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments**|Hogyun Kim et.al.|[2505.08230](http://arxiv.org/abs/2505.08230)|null|\n",
    "2505.08013": "|**2025-05-12**|**RDD: Robust Feature Detector and Descriptor using Deformable Transformer**|Gonglin Chen et.al.|[2505.08013](http://arxiv.org/abs/2505.08013)|null|\n",
    "2505.07198": "|**2025-05-12**|**Ranking-aware Continual Learning for LiDAR Place Recognition**|Xufei Wang et.al.|[2505.07198](http://arxiv.org/abs/2505.07198)|null|\n",
    "2505.13309": "|**2025-05-19**|**eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks**|Jad Mansour et.al.|[2505.13309](http://arxiv.org/abs/2505.13309)|null|\n",
    "2505.12549": "|**2025-05-23**|**VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold**|Dominic Maggio et.al.|[2505.12549](http://arxiv.org/abs/2505.12549)|null|\n",
    "2505.12384": "|**2025-05-18**|**Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey**|Calvin Galagain et.al.|[2505.12384](http://arxiv.org/abs/2505.12384)|null|\n",
    "2505.12337": "|**2025-05-18**|**Structureless VIO**|Junlin Song et.al.|[2505.12337](http://arxiv.org/abs/2505.12337)|null|\n",
    "2505.11709": "|**2025-05-16**|**EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video**|Ryan Hoque et.al.|[2505.11709](http://arxiv.org/abs/2505.11709)|null|\n",
    "2505.11620": "|**2025-05-16**|**Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization**|Aaron Wilhelm et.al.|[2505.11620](http://arxiv.org/abs/2505.11620)|null|\n",
    "2505.10847": "|**2025-05-16**|**Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS**|Paola Nazate-Burgos et.al.|[2505.10847](http://arxiv.org/abs/2505.10847)|null|\n",
    "2505.10696": "|**2025-05-15**|**TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation**|Manthan Patel et.al.|[2505.10696](http://arxiv.org/abs/2505.10696)|null|\n",
    "2505.10310": "|**2025-05-15**|**A hybrid SLAM-Payne framework for atmospheric parameter and abundance determination of early-type Stars from LAMOST DR9 low-resolution Spectra**|Weijia Sun et.al.|[2505.10310](http://arxiv.org/abs/2505.10310)|null|\n",
    "2505.09915": "|**2025-05-15**|**Large-Scale Gaussian Splatting SLAM**|Zhe Xin et.al.|[2505.09915](http://arxiv.org/abs/2505.09915)|null|\n",
    "2505.16447": "|**2025-05-22**|**TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition**|Oliver Grainge et.al.|[2505.16447](http://arxiv.org/abs/2505.16447)|null|\n",
    "2505.14128": "|**2025-05-20**|**A Methodological Framework for Measuring Spatial Labeling Similarity**|Yihang Du et.al.|[2505.14128](http://arxiv.org/abs/2505.14128)|**[link](https://github.com/yihdu/slam)**|\n",
    "2505.14068": "|**2025-05-22**|**Place Recognition: A Comprehensive Review, Current Challenges and Future Directions**|Zhenyu Li et.al.|[2505.14068](http://arxiv.org/abs/2505.14068)|**[link](https://github.com/cv4ra/sota-place-recognitioner)**|\n",
    "2505.22880": "|**2025-05-28**|**Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera**|Xiaoyang Zhan et.al.|[2505.22880](http://arxiv.org/abs/2505.22880)|null|\n",
    "2505.22859": "|**2025-05-28**|**4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians**|Hidenobu Matsuki et.al.|[2505.22859](http://arxiv.org/abs/2505.22859)|null|\n",
    "2505.22335": "|**2025-05-28**|**UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments**|Wancai Zheng et.al.|[2505.22335](http://arxiv.org/abs/2505.22335)|null|\n",
    "2505.20906": "|**2025-05-27**|**HS-SLAM: A Fast and Hybrid Strategy-Based SLAM Approach for Low-Speed Autonomous Driving**|Bingxiang Kang et.al.|[2505.20906](http://arxiv.org/abs/2505.20906)|null|\n",
    "2505.20858": "|**2025-05-27**|**ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient**|Jason Chui et.al.|[2505.20858](http://arxiv.org/abs/2505.20858)|null|\n",
    "2505.19420": "|**2025-05-26**|**ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting**|Wenhua Wu et.al.|[2505.19420](http://arxiv.org/abs/2505.19420)|null|\n",
    "2505.18992": "|**2025-05-25**|**VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes**|Tianchen Deng et.al.|[2505.18992](http://arxiv.org/abs/2505.18992)|null|\n",
    "2505.17576": "|**2025-05-23**|**CU-Multi: A Dataset for Multi-Robot Data Association**|Doncey Albin et.al.|[2505.17576](http://arxiv.org/abs/2505.17576)|null|\n",
    "2506.04224": "|**2025-06-04**|**Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset**|Zirui Wang et.al.|[2506.04224](http://arxiv.org/abs/2506.04224)|null|\n",
    "2506.03073": "|**2025-06-03**|**LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM**|Roman Titkov et.al.|[2506.03073](http://arxiv.org/abs/2506.03073)|null|\n",
    "2506.02932": "|**2025-06-03**|**Online Performance Assessment of Multi-Source-Localization for Autonomous Driving Systems Using Subjective Logic**|Stefan Orf et.al.|[2506.02932](http://arxiv.org/abs/2506.02932)|null|\n",
    "2506.02741": "|**2025-06-03**|**VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians**|Pengchong Hu et.al.|[2506.02741](http://arxiv.org/abs/2506.02741)|null|\n",
    "2506.02736": "|**2025-06-03**|**GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal**|Shufan Qing et.al.|[2506.02736](http://arxiv.org/abs/2506.02736)|**[link](https://github.com/qingshufan/GeneA-SLAM2)**|\n",
    "2506.02373": "|**2025-06-03**|**Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent**|Kordel K. France et.al.|[2506.02373](http://arxiv.org/abs/2506.02373)|null|\n",
    "2506.00970": "|**2025-06-01**|**Globally Consistent RGB-D SLAM with 2D Gaussian Splatting**|Xingguang Zhong et.al.|[2506.00970](http://arxiv.org/abs/2506.00970)|null|\n",
    "2505.24654": "|**2025-05-30**|**Black-box Adversarial Attacks on CNN-based SLAM Algorithms**|Maria Rafaela Gkeka et.al.|[2505.24654](http://arxiv.org/abs/2505.24654)|null|\n",
    "2506.08005": "|2025-06-09|ZeroVO: Visual Odometry with Minimal Assumptions|Lei Lai\u7b49|[2506.08005](http://arxiv.org/pdf/2506.08005)|\u65e0|ZeroVO\u662f\u4e00\u79cd\u65e0\u9700\u9884\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u81f3\u4e0d\u540c\u76f8\u673a\u548c\u73af\u5883\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7b97\u6cd5\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u4e0e\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u65e0\u9700\u6807\u5b9a\u7684\u51e0\u4f55\u611f\u77e5\u7f51\u7edc\u7ed3\u6784\uff0c\u80fd\u6709\u6548\u5904\u7406\u6df1\u5ea6\u4f30\u8ba1\u548c\u76f8\u673a\u53c2\u6570\u4e2d\u7684\u566a\u58f0\uff0c\u6446\u8131\u4f20\u7edf\u65b9\u6cd5\u5bf9\u56fa\u5b9a\u6807\u5b9a\u914d\u7f6e\u7684\u4f9d\u8d56\u3002\n\n\u25c6 \u5f15\u5165\u57fa\u4e8e\u8bed\u8a00\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u672a\u77e5\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002\n\n\u25c6 \u5f00\u53d1\u534a\u76d1\u7763\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u8fed\u4ee3\u9002\u5e94\u65b0\u573a\u666f\uff0c\u8fdb\u4e00\u6b65\u5f3a\u5316\u6a21\u578b\u5728\u771f\u5b9e\u590d\u6742\u573a\u666f\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002\n\n\u25c6 \u5728KITTI\u3001nuScenes\u548cArgoverse 2\u7b49\u6807\u51c6\u6570\u636e\u96c6\u53caGTA\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u6027\u80fd\uff0c\u76f8\u5bf9\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc730%\u3002\n\n\u25c6 \u65e0\u9700\u5fae\u8c03\u6216\u76f8\u673a\u6807\u5b9a\u7684\u7279\u6027\uff0c\u4f7f\u5f97\u8be5\u6280\u672f\u5177\u5907\u5927\u89c4\u6a21\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u6781\u5927\u6269\u5c55\u4e86\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u5e94\u7528\u8303\u56f4\u3002|\n",
    "2506.07164": "|2025-06-08|Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs|Qiong Chang\u7b49|[2506.07164](http://arxiv.org/pdf/2506.07164)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u901a\u8fc7\u4f18\u5316\u4f4e\u7aef\u5d4c\u5165\u5f0fGPU\u4e0a\u7684Oriented FAST\u7279\u5f81\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9SLAM\u7cfb\u7edf\u7684\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002\u5177\u4f53\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u4e8c\u8fdb\u5236\u7ea7\u7f16\u7801\u7b56\u7565\uff0c\u5feb\u901f\u786e\u5b9aFAST\u7279\u5f81\u70b9\u5019\u9009\u70b9\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u53ef\u5206\u79bb\u7684Harris\u89d2\u70b9\u68c0\u6d4b\u7b56\u7565\uff0c\u7ed3\u5408\u5e95\u5c42GPU\u786c\u4ef6\u6307\u4ee4\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u5728Jetson TX2\u5d4c\u5165\u5f0fGPU\u4e0a\u5b9e\u73b0\u4e86\u5e73\u57477.3\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u8fdc\u8d85\u73b0\u6709OpenCV\u7684GPU\u52a0\u901f\u65b9\u6848\u3002  \n\u25c6 \u901a\u8fc7\u4f18\u5316FAST\u7279\u5f81\u70b9\u68c0\u6d4b\u548cHarris\u89d2\u70b9\u68c0\u6d4b\u8fd9\u4e24\u4e2a\u6700\u8017\u65f6\u7684\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u5e73\u53f0\u5b9e\u65f6\u5904\u7406\u7684\u74f6\u9888\u95ee\u9898\u3002  \n\u25c6 \u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6SLAM\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u79fb\u52a8\u548c\u5d4c\u5165\u5f0f\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2506.07013": "|2025-06-08|UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment|Wentao Zhao\u7b49|[2506.07013](http://arxiv.org/pdf/2506.07013)|\u65e0|\u25c6 \u63d0\u51faUNO\u6846\u67b6\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u3001\u8de8\u73af\u5883\u7684\u7edf\u4e00\u81ea\u76d1\u7763\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u6216\u8bbe\u5907\u8fdb\u884c\u8c03\u4f18\u3002  \n\u25c6 \u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u7b56\u7565\uff08Mixture-of-Experts\uff09\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u7528\u89e3\u7801\u5668\u5206\u522b\u5904\u7406\u4e0d\u540c\u7c7b\u522b\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1\u53ef\u5fae\u5206\u7684Gumbel-Softmax\u6a21\u5757\uff0c\u52a8\u6001\u6784\u5efa\u5e27\u95f4\u5173\u8054\u56fe\u5e76\u9009\u62e9\u6700\u4f18\u89e3\u7801\u5668\uff0c\u540c\u65f6\u5254\u9664\u9519\u8bef\u4f30\u8ba1\u3002  \n\u25c6 \u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u5c3a\u5ea6\u65e0\u5173\u6df1\u5ea6\u5148\u9a8c\u4e0e\u8f7b\u91cf\u7ea7\u6346\u7ed1\u8c03\u6574\uff08bundling adjustment\uff09\uff0c\u540e\u7aef\u7edf\u4e00\u4f18\u5316\u51e0\u4f55\u4e00\u81f4\u6027\u3002  \n\u25c6 \u5728KITTI\uff08\u81ea\u52a8\u9a7e\u9a76\uff09\u3001EuRoC-MAV\uff08\u65e0\u4eba\u673a\uff09\u548cTUM-RGBD\uff08\u624b\u6301\u8bbe\u5907\uff09\u4e09\u5927\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u8fbe\u5230SOTA\u3002|\n",
    "2506.06517": "|2025-06-06|GS4: Generalizable Sparse Splatting Semantic SLAM|Mingqi Jiang\u7b49|[2506.06517](http://arxiv.org/pdf/2506.06517)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u53ef\u6cdb\u5316\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u7684\u8bed\u4e49SLAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7f51\u7edc\u5b9e\u73b0\u8de8\u573a\u666f\u76843D\u5730\u56fe\u6784\u5efa\uff0c\u6446\u8131\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5355\u573a\u666f\u4f18\u5316\u7684\u9650\u5236\u3002  \n\u25c6 \u91c7\u7528RGB-D\u56fe\u50cf\u8bc6\u522b\u4e3b\u5e72\u7f51\u7edc\uff0c\u76f4\u63a5\u4ece\u964d\u91c7\u6837\u548c\u53cd\u5411\u6295\u5f71\u7684\u56fe\u50cf\u4f4d\u7f6e\u9884\u6d4b\u9ad8\u65af\u53c2\u6570\uff0c\u5b9e\u73b0\u9ad8\u6548\u589e\u91cf\u5f0f\u5730\u56fe\u66f4\u65b0\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c063D\u8bed\u4e49\u5206\u5272\u96c6\u6210\u5230GS\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u5171\u4eab\u4e3b\u5e72\u7f51\u7edc\u7edf\u4e003D\u5efa\u56fe\u4e0e\u8bc6\u522b\u4efb\u52a1\uff0c\u63d0\u5347\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002  \n\u25c6 \u63d0\u51fa\u4ec5\u97001\u6b21\u8fed\u4ee3\u7684\u5168\u5c40\u5b9a\u4f4d\u540e\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u5b9a\u4f4d\u6f02\u79fb\u548c\u6f02\u6d6e\u7269\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728ScanNet\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u6240\u7528\u9ad8\u65af\u6570\u91cf\u6bd4\u540c\u7c7b\u65b9\u6cd5\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u5728NYUv2\u548cTUM RGB-D\u4e0a\u5c55\u793a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2506.06476": "|2025-06-06|Enhancing Situational Awareness in Underwater Robotics with Multi-modal Spatial Perception|Pushyami Kaveti\u7b49|[2506.06476](http://arxiv.org/pdf/2506.06476)|\u65e0|\u25c6 \u63d0\u51fa\u591a\u6a21\u6001\u611f\u77e5\u878d\u5408\u6846\u67b6\uff0c\u6574\u5408\u6444\u50cf\u5934\u3001IMU\u548c\u58f0\u5b66\u8bbe\u5907\u6570\u636e\uff0c\u89e3\u51b3\u6c34\u4e0b\u89c6\u89c9SLAM\u56e0\u5149\u7ebf\u8870\u51cf\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u5bfc\u81f4\u7684\u5931\u6548\u95ee\u9898\u3002  \n\u25c6 \u7a81\u7834\u4f20\u7edf\u5355\u76ee/\u53cc\u76ee\u89c6\u89c9\u9650\u5236\uff0c\u652f\u6301\u591a\u6444\u50cf\u5934\u914d\u7f6e\uff0c\u63d0\u5347\u7cfb\u7edf\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002  \n\u25c6 \u7ed3\u5408\u51e0\u4f55\u65b9\u6cd5\u4e0e\u5b66\u4e60\u6280\u672f\uff0c\u5f15\u5165\u8bed\u4e49\u5206\u6790\u589e\u5f3a\u573a\u666f\u7406\u89e3\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u72b6\u6001\u4f30\u8ba1\u548c3D\u91cd\u5efa\u3002  \n\u25c6 \u901a\u8fc7\u771f\u5b9e\u6d77\u57df\u5b9e\u9a8c\u9a8c\u8bc1\uff08\u7279\u9686\u8d6b\u59c6\u5ce1\u6e7e\uff09\uff0c\u9996\u6b21\u5c55\u793a\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u6076\u52a3\u6c34\u4e0b\u6761\u4ef6\u4e0b\u7684\u5b9e\u65f6\u53ef\u9760\u6027\u80fd\u3002  \n\u25c6 \u7cfb\u7edf\u5206\u6790\u4f20\u611f\u5668\u6807\u5b9a\u7b49\u5de5\u7a0b\u6311\u6218\uff0c\u6307\u51fa\u57fa\u4e8e\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u6c34\u4e0b\u4f5c\u4e1a\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002|\n",
    "2506.05965": "|2025-06-06|Dy3DGS-SLAM: Monocular 3D Gaussian Splatti...|Mingrui Li\u7b49|[2506.05965](http://arxiv.org/pdf/2506.05965)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u5355\u76eeRGB\u8f93\u5165\u7684\u52a8\u6001\u573a\u666f3D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edfDy3DGS-SLAM\u3002  \n\u25c6 \u901a\u8fc7\u878d\u5408\u5149\u6d41\u63a9\u7801\u548c\u6df1\u5ea6\u63a9\u7801\u7684\u6982\u7387\u6a21\u578b\u751f\u6210\u52a8\u6001\u63a9\u7801\uff0c\u4ec5\u9700\u5355\u6b21\u7f51\u7edc\u8fed\u4ee3\u5373\u53ef\u4f18\u5316\u8ddf\u8e2a\u5c3a\u5ea6\u548c\u51e0\u4f55\u6e32\u67d3\u3002...|\n",
    "2506.05866": "|2025-06-06|Analysis of points outcome in ATP Grand Slam Tenni...|Martin Illum\u7b49|[2506.05866](http://arxiv.org/pdf/2506.05866)|\u65e0|\u25c6 \u8be5\u8bba\u6587\u521b\u65b0\u5730\u5229\u7528\u5927\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u7b49\uff09\u9884\u6d4b\u7f51\u7403\u5927\u6ee1\u8d2f\u8d5b\u4e8b\u4e2d\u6bcf\u4e00\u5206\u7684\u80dc\u8d1f\uff0c\u5e76\u7ed3\u5408\u7403\u5458\u6392\u540d\u3001\u5386\u53f2\u6570\u636e\u7b49\u56e0\u7d20\u5206\u6790\u5f71\u54cd\u5f97\u5206\u7684\u5173\u952e\u6218\u7565\u56e0\u7d20\u3002  \n\u25c6 \u7814\u7a76\u57fa\u4e8e2016-2020...|\n",
    "2506.05558": "|2025-06-05|On-the-fly Reconstruction for Large-Scale Novel Vi...|Andreas Meuleman\u7b49|[2506.05558](http://arxiv.org/pdf/2506.05558)|\u65e0|\u25c6\u63d0\u51fa\u5b9e\u65f6\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728\u62cd\u6444\u540e\u7acb\u5373\u751f\u6210\u76f8\u673a\u4f4d\u59ff\u548c\u8bad\u7ec3\u597d\u76843D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u8fc7\u957f\u7684\u95ee\u9898\u3002  \n\u25c6\u7ed3\u5408\u5feb\u901f\u521d\u59cb\u4f4d\u59ff\u4f30\u8ba1\u548c\u76f4\u63a5\u91c7\u6837\u9ad8\u65af\u57fa\u5143\u4f4d\u7f6e/\u5f62\u72b6\u7684\u6280\u672f\uff0c\u663e\u8457\u52a0\u901f\u8054\u5408\u4f18\u5316\u8fc7\u7a0b\u3002  \n...|\n",
    "2506.04619": "|**2025-06-05**|**Deep Learning Reforms Image Matching: A Survey and Outlook**|Shihua Zhang et.al.|[2506.04619](http://arxiv.org/abs/2506.04619)|null|\n",
    "2506.04359": "|**2025-06-04**|**cuVSLAM: CUDA accelerated visual odometry**|Alexander Korovko et.al.|[2506.04359](http://arxiv.org/abs/2506.04359)|null|\n",
    "2506.09035": "|2025-06-10|Princeton365: A Diverse Dataset with Accurate Camera Pose|Karhan Kayan\u7b49|[2506.09035](http://arxiv.org/pdf/2506.09035)|\u65e0|\u25c6 \u63d0\u51fa\u4e86Princeton365\u6570\u636e\u96c6\uff0c\u5305\u542b365\u4e2a\u591a\u6837\u5316\u89c6\u9891\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u76f8\u673a\u4f4d\u59ff\uff0c\u586b\u8865\u4e86\u5f53\u524dSLAM\u57fa\u51c6\u5728\u7cbe\u5ea6\u548c\u6570\u636e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u7a7a\u767d\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5730\u9762\u771f\u503c\u91c7\u96c6\u6846\u67b6\uff0c\u7ed3\u5408\u6821\u51c6\u677f\u548c360\u5ea6\u76f8\u673a\uff0c\u5b9e\u73b0\u4e86\u5ba4\u5185\u3001\u5ba4\u5916\u548c\u7269\u4f53\u626b\u63cf\u89c6\u9891\u7684\u591a\u6a21\u6001\u540c\u6b65\u91c7\u96c6\uff08\u5355\u76ee/\u7acb\u4f53RGB\u89c6\u9891\u548cIMU\uff09\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5149\u6d41\u7684\u573a\u666f\u5c3a\u5ea6\u611f\u77e5SLAM\u8bc4\u4f30\u6307\u6807\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6307\u6807\uff08\u5982ATE\uff09\u65e0\u6cd5\u8de8\u573a\u666f\u6bd4\u8f83\u7684\u5c40\u9650\u6027\uff0c\u4fbf\u4e8e\u5206\u6790\u7b97\u6cd5\u5931\u8d25\u6a21\u5f0f\u3002  \n\u25c6 \u6784\u5efa\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u57fa\u51c6\uff0c\u6db5\u76d6\u5f53\u524d\u57fa\u51c6\u672a\u6d89\u53ca\u7684\u573a\u666f\uff08\u5982\u975e\u6717\u4f2f\u8868\u9762\u548c360\u5ea6\u76f8\u673a\u8f68\u8ff9\uff09\u3002  \n\u25c6 \u516c\u5f00\u4e86\u5b8c\u6574\u7684\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u63d0\u4ea4\u5e73\u53f0\uff08https://princeton365.cs.princeton.edu\uff09\uff0c\u63a8\u52a8SLAM\u548cNVS\u9886\u57df\u7684\u6807\u51c6\u5316\u7814\u7a76\u3002|\n",
    "2506.08384": "|2025-06-10|Planar Collisionless Shock Simulations with Semi-Implicit Particle-in-Cell Model FLEKS|Hongyang Zhou\u7b49|[2506.08384](http://arxiv.org/pdf/2506.08384)|\u65e0|\u25c6 \u9a8c\u8bc1\u4e86\u534a\u9690\u5f0f\u7c92\u5b50\u7f51\u683c\u4ee3\u7801FLEKS\u5728\u65e0\u78b0\u649e\u6fc0\u6ce2\u6a21\u62df\u4e2d\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u9488\u5bf9\u5168\u7403\u78c1\u5c42\u5efa\u6a21\u76f8\u5173\u53c2\u6570\u8303\u56f4\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u7cbe\u7ec6\u5316\u7b97\u6cd5\uff0c\u4f7fFLEKS\u80fd\u591f\u5728\u7535\u5b50\u60ef\u6027\u957f\u5ea6\u91cf\u7ea7\u7684\u7f51\u683c\u5206\u8fa8\u7387\u4e0b\u7cbe\u786e\u6a21\u62df\u6fc0\u6ce2\u7ed3\u6784\u3002  \n\u25c6 \u6210\u529f\u6355\u6349\u4e86\u6fc0\u6ce2\u5173\u952e\u7279\u5f81\uff0c\u5305\u62ec\u6fc0\u6ce2\u7ed3\u6784\uff08\u811a\u90e8\u3001\u9661\u5761\u3001\u8fc7\u51b2\u548c\u6b20\u51b2\uff09\u3001\u4e0a\u4e0b\u6e38\u6ce2\u52a8\uff08\u5feb\u78c1\u58f0\u6ce2\u3001\u54e8\u58f0\u6ce2\u3001\u963f\u5c14\u82ac\u79bb\u5b50\u56de\u65cb\u6ce2\u548c\u955c\u50cf\u6a21\uff09\u4ee5\u53ca\u975e\u9ea6\u514b\u65af\u97e6\u7c92\u5b50\u5206\u5e03\u3002  \n\u25c6 \u63ed\u793a\u4e86\u4e8c\u7ef4\u6a21\u62df\u5bf9\u51c6\u786e\u91cd\u73b0\u51c6\u5782\u76f4\u6fc0\u6ce2\u4e0b\u6e38\u6ce2\u52a8\u7269\u7406\u548c\u51c6\u5e73\u884c\u6fc0\u6ce2\u590d\u6742\u52a8\u529b\u5b66\uff08\u5982\u8868\u9762\u6ce2\u7eb9\u3001\u6fc0\u6ce2\u5b50\u3001SLAMS\u548c\u55b7\u6d41\uff09\u7684\u5fc5\u8981\u6027\u3002  \n\u25c6 \u901a\u8fc7\u53c2\u6570\u7814\u7a76\u9610\u660e\u4e86\u8d28\u91cf\u6bd4\u548c\u7f51\u683c\u5206\u8fa8\u7387\u5bf9\u6fc0\u6ce2\u7269\u7406\u7684\u5f71\u54cd\uff0c\u4e3a\u534a\u9690\u5f0fPIC\u4ee3\u7801\u7684\u7269\u7406\u548c\u6570\u503c\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002  \n\u25c6 \u4e3a\u5c06\u52a8\u529b\u5b66\u6fc0\u6ce2\u8fc7\u7a0b\u6574\u5408\u5230MHD-AEPIC\u6a21\u578b\u7684\u5927\u5c3a\u5ea6\u7a7a\u95f4\u7b49\u79bb\u5b50\u4f53\u6a21\u62df\u4e2d\u5960\u5b9a\u4e86\u57fa\u7840\u3002|\n",
    "2506.09583": "|2025-06-11|VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots|Miguel \u00c1. Gonz\u00e1lez-Santamarta\u7b49|[2506.09583](http://arxiv.org/pdf/2506.09583)|\u65e0|\u25c6 \u63d0\u51faVAULT\u539f\u578b\u7cfb\u7edf\uff0c\u57fa\u4e8eROS 2\u6846\u67b6\uff0c\u4e13\u4e3a\u6237\u5916\u81ea\u4e3b\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u89e3\u51b3\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u96be\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u878d\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\uff08GNSS\u3001VIO\u3001IMU\uff09\u4e0e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08EKF\uff09\uff0c\u751f\u6210\u9ad8\u53ef\u9760\u60273D\u91cc\u7a0b\u8ba1\uff0c\u63d0\u5347\u6237\u5916\u5b9a\u4f4d\u9c81\u68d2\u6027\u3002  \n\u25c6 \u7ed3\u5408\u89c6\u89c9SLAM\uff08VSLAM\uff09\u6280\u672f\uff0c\u6784\u5efa\u7cbe\u7ec63D\u70b9\u4e91\u5730\u56fe\uff0c\u5f25\u8865\u4f20\u7edf2D LiDAR\u5728\u6237\u5916\u573a\u666f\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u5b9e\u73b0\u5ba4\u5185\u5916\u73af\u5883\u901a\u7528\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u534f\u540c\uff0c\u9002\u5e94\u519c\u4e1a\u3001\u6797\u4e1a\u7b49\u65e0\u7ed3\u6784\u5316\u6237\u5916\u573a\u666f\u9700\u6c42\u3002  \n\u25c6 \u63d0\u4f9b\u5f00\u6e90ROS 2\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u793e\u533a\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u7684\u79fb\u52a8\u6d4b\u7ed8\u7cfb\u7edf\uff08MMS\uff09\u53c2\u8003\u6846\u67b6\u3002|\n",
    "2506.09278": "|2025-06-10|UFM: A Simple Path towards Unified Dense Correspondence with Flow|Yuchen Zhang\u7b49|[2506.09278](http://arxiv.org/pdf/2506.09278)|\u65e0|\u25c6 \u63d0\u51fa\u7edf\u4e00\u6d41\u4e0e\u5339\u914d\u6a21\u578b\uff08UFM\uff09\uff0c\u9996\u6b21\u5b9e\u73b0\u5bbd\u57fa\u7ebf\u573a\u666f\u548c\u5149\u6d41\u4f30\u8ba1\u7684\u7edf\u4e00\u8bad\u7ec3\uff0c\u7a81\u7834\u4f20\u7edf\u5206\u800c\u6cbb\u4e4b\u7684\u5c40\u9650\u3002  \n\u25c6 \u91c7\u7528\u7b80\u5355\u901a\u7528\u7684Transformer\u67b6\u6784\u76f4\u63a5\u56de\u5f52(u,v)\u6d41\uff0c\u907f\u514d\u4f20\u7edf coarse-to-fine \u4ee3\u4ef7\u4f53\u79ef\u7684\u590d\u6742\u6027\uff0c\u8bad\u7ec3\u66f4\u9ad8\u6548\u4e14\u5bf9\u5927\u4f4d\u79fb\u66f4\u7cbe\u51c6\u3002  \n\u25c6 \u5728\u5149\u6d41\u4efb\u52a1\u4e0a\u7cbe\u5ea6\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff08Unimatch\uff0928%\uff0c\u5728\u5bbd\u57fa\u7ebf\u5339\u914d\u4efb\u52a1\u4e0a\u8bef\u5dee\u964d\u4f4e62%\u4e14\u901f\u5ea6\u63d0\u53476.7\u500d\uff08\u5bf9\u6bd4RoMa\uff09\u3002  \n\u25c6 \u9996\u6b21\u8bc1\u660e\u7edf\u4e00\u8bad\u7ec3\u6a21\u578b\u53ef\u540c\u65f6\u5728\u5149\u6d41\u548c\u5bbd\u57fa\u7ebf\u5339\u914d\u4e24\u4e2a\u9886\u57df\u8d85\u8d8a\u4e13\u7528\u65b9\u6cd5\uff0c\u4e3a\u901a\u7528\u7a20\u5bc6\u5bf9\u5e94\u5f00\u8f9f\u65b0\u8def\u5f84\u3002  \n\u25c6 \u901a\u8fc7\u5171\u53ef\u89c1\u50cf\u7d20\u7684\u7edf\u4e00\u6570\u636e\u8bad\u7ec3\uff0c\u4e3a\u591a\u6a21\u6001\u3001\u957f\u8ddd\u79bb\u548c\u5b9e\u65f6\u5bf9\u5e94\u4efb\u52a1\u63d0\u4f9b\u65b0\u601d\u8def\u3002|\n",
    "2506.10567": "|2025-06-12|LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System|Hongbeen Park\u7b49|[2506.10567](http://arxiv.org/pdf/2506.10567)|\u65e0|\u25c6 \u63d0\u51faLRSLAM\u6a21\u578b\uff0c\u91c7\u7528\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\uff08Six-axis\u548cCP\u5206\u89e3\uff09\u4f18\u5316\u7a20\u5bc6\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u5185\u5b58\u5229\u7528\u7387\u3002  \n\u25c6 \u901a\u8fc7\u4f4e\u79e9\u8868\u793a\u6709\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\u95ee\u9898\uff0c\u9002\u5408\u5927\u89c4\u6a21\u573a\u666f\u3002  \n\u25c6 \u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982ESLAM\u7684\u5e73\u9762\u5f20\u91cf\u5206\u89e3\uff09\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u5185\u5b58\u589e\u957f\u538b\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u4e0e\u5b9a\u4f4d\u80fd\u529b\u3002  \n\u25c6 \u5728\u591a\u79cd\u5ba4\u5185RGB-D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cLRSLAM\u5728\u53c2\u6570\u6548\u7387\u3001\u5904\u7406\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u3001\u79fb\u52a8\u673a\u5668\u4eba\u7b49\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u4ee3\u7801\u5c06\u5f00\u6e90\uff0c\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7814\u7a76\u53d1\u5c55\u3002|\n",
    "2506.13664": "|2025-06-16|Slanted light-sheet array microscopy for large volume imaging at rates exceeding 100 Hz|Kai Long\u7b49|[2506.13664](http://arxiv.org/pdf/2506.13664)|\u65e0|\u25c6 \u5f00\u53d1\u4e86\u503e\u659c\u5149\u7247\u9635\u5217\u663e\u5fae\u955c\uff08SLAM\uff09\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc7100 Hz\u7684\u8d85\u5feb\u901f\u5927\u4f53\u79ef\u6210\u50cf\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6210\u50cf\u901f\u5ea6\u9650\u5236\u3002  \n\u25c6 \u57fa\u4e8e\u6807\u51c6\u5bbd\u573a\u590d\u5408\u663e\u5fae\u955c\u8fdb\u884c\u7b80\u5355\u6539\u9020\uff0c\u4ec5\u9700\u5bf9\u7167\u660e\u5149\u8def\u8fdb\u884c\u6700\u5c0f\u5316\u4fee\u6539\uff0c\u4fbf\u4e8e\u96c6\u6210\u548c\u63a8\u5e7f\u3002  \n\u25c6 \u652f\u6301\u5927\u8303\u56f4\u591a\u7ef4\u5ea6\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\uff08\u6a2a\u5411\u8d85\u8fc7500\u50cf\u7d20\uff0c\u6df1\u5ea6\u8d85\u8fc7200\u5c42\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5149\u5b66\u5207\u7247\u548c\u5c40\u90e8\u5149\u5316\u5b66\u80fd\u529b\u3002  \n\u25c6 \u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\uff08\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff09\uff0c\u5b9e\u73b0\u4e86\u5404\u5411\u540c\u6027\u5206\u8fa8\u7387\u63d0\u5347\uff0c\u4f18\u5316\u4e86\u56fe\u50cf\u8d28\u91cf\u3002  \n\u25c6 \u517c\u5bb9\u5e38\u89c4\u751f\u7269\u6837\u672c\u5236\u5907\u534f\u8bae\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u751f\u7269\u533b\u5b66\u7814\u7a76\u573a\u666f\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002  \n\u25c6 \u5728\u9ad8\u901f\u6210\u50cf\u7684\u540c\u65f6\u517c\u987e\u4e86\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u4fe1\u566a\u6bd4\u548c\u5927\u89c6\u573a\u9700\u6c42\uff0c\u4e3a\u52a8\u6001\u751f\u7269\u8fc7\u7a0b\u89c2\u6d4b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002|\n",
    "2506.13149": "|2025-06-16|Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots|Jaehong Oh|[2506.13149](http://arxiv.org/pdf/2506.13149)|\u65e0|\u25c6 \u63d0\u51faSEGO\uff08\u8bed\u4e49\u56fe\u8c31\u672c\u4f53\uff09\u8ba4\u77e5\u6620\u5c04\u67b6\u6784\uff0c\u9996\u6b21\u5c06\u51e0\u4f55\u611f\u77e5\u3001\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u751f\u6210\u6574\u5408\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u673a\u5668\u4eba\u7684\u8ba4\u77e5\u534f\u540c\u3002  \n\u25c6 \u6784\u5efa\u52a8\u6001\u8ba4\u77e5\u573a\u666f\u56fe\uff0c\u7a81\u7834\u4f20\u7edfSLAM\u4ec5\u5173\u6ce8\u7a7a\u95f4\u51e0\u4f55\u7684\u5c40\u9650\uff0c\u540c\u65f6\u8868\u5f81\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u672c\u4f53\u4e00\u81f4\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u878d\u5408\u57fa\u4e8eSLAM\u7684\u5b9a\u4f4d\u3001\u6df1\u5ea6\u5b66\u4e60\u7269\u4f53\u68c0\u6d4b\u8ddf\u8e2a\u4e0e\u672c\u4f53\u9a71\u52a8\u63a8\u7406\u4e09\u5927\u6a21\u5757\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8bed\u4e49\u8fde\u8d2f\u7684\u73af\u5883\u5efa\u6a21\u3002  \n\u25c6 \u901a\u8fc7\u672c\u4f53\u8bba\u7ea6\u675f\u786e\u4fdd\u8bed\u4e49\u63a8\u7406\u7684\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u7406\u89e3\"\u684c\u5b50\u4e0a\u7684\u676f\u5b50\"\u7b49\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u3002  \n\u25c6 \u652f\u6301\u53ef\u89e3\u91ca\u6027\u8f93\u51fa\uff0c\u673a\u5668\u4eba\u53ef\u751f\u6210\u5bf9\u4eba\u7c7b\u53cb\u597d\u7684\u573a\u666f\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002  \n\u25c6 \u8be5\u67b6\u6784\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u534f\u4f5c\u673a\u5668\u4eba\u63d0\u4f9b\u6807\u51c6\u5316\u8ba4\u77e5\u5904\u7406\u6d41\u7a0b\uff0c\u5728\u5de5\u4e1a\u88c5\u914d\u3001\u5bb6\u5ead\u670d\u52a1\u7b49\u573a\u666f\u5c55\u73b0\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2506.13100": "|2025-06-16|A Novel ViDAR Device With Visual Inertial Encoder Odometry and Reinforcement Learning-Based Active SLAM Method|Zhanhua Xin\u7b49|[2506.13100](http://arxiv.org/pdf/2506.13100)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bViDAR\u8bbe\u5907\uff0c\u7ed3\u5408\u89c6\u89c9\u3001\u60ef\u6027\u548c\u7535\u673a\u7f16\u7801\u5668\uff0c\u6784\u5efa\u7d27\u8026\u5408\u7684\u89c6\u89c9-\u60ef\u6027-\u7f16\u7801\u5668\u91cc\u7a0b\u8ba1\uff08VIEO\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86SLAM\u7cfb\u7edf\u7684\u4e3b\u52a8\u80fd\u529b\u548c\u89c6\u91ce\u8303\u56f4\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86ViDAR\u6821\u51c6\u65b9\u6cd5\uff0c\u786e\u4fddVIEO\u7b97\u6cd5\u7684\u7cbe\u786e\u521d\u59cb\u5316\uff0c\u89e3\u51b3\u4e86\u591a\u4f20\u611f\u5668\u878d\u5408\u4e2d\u7684\u6807\u5b9a\u96be\u9898\u3002  \n\u25c6 \u9996\u6b21\u5c06\u7535\u673a\u7f16\u7801\u5668\u5f15\u5165SLAM\u7cfb\u7edf\uff0c\u4ee5\u6781\u4f4e\u7684\u6210\u672c\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u589e\u5f3a\u4e86\u8de8\u5e27\u5171\u89c6\u5173\u7cfb\uff0c\u63d0\u9ad8\u4e86\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u5e73\u53f0\u8fd0\u52a8\u89e3\u8026\u4e3b\u52a8SLAM\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u4e3b\u4f18\u5316\u8fd0\u52a8\u7b56\u7565\u4ee5\u589e\u52a0\u7279\u5f81\u70b9\u591a\u6837\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0VIEO\u7b97\u6cd5\u76f8\u6bd4\u4f20\u7edfVIO\u7b97\u6cd5\u5728\u5171\u89c6\u5173\u7cfb\u548c\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e14DRL\u4e3b\u52a8SLAM\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u7cfb\u7edf\u6027\u80fd\u3002  \n\u25c6 \u4e3a\u590d\u6742\u73af\u5883\u4e0b\u4e3b\u52a8SLAM\u7cfb\u7edf\u7684\u5e73\u53f0\u8bbe\u8ba1\u548c\u8fd0\u52a8\u89e3\u8026\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u517c\u5177\u7406\u8bba\u521b\u65b0\u548c\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2506.13089": "|2025-06-16|SuperPoint-SLAM3: Augmenting ORB-SLAM3 with Deep Features, Adaptive NMS, and Learning-Based Loop Closure|Shahram Najam Syed\u7b49|[2506.13089](http://arxiv.org/pdf/2506.13089)|[\u4ee3\u7801](https://github.com/shahram95/superpointslam3)|\u25c6 \u7528\u81ea\u76d1\u7763\u7684SuperPoint\u7279\u5f81\u68c0\u6d4b-\u63cf\u8ff0\u5b50\u66ff\u4ee3\u4f20\u7edfORB\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u5728\u6781\u7aef\u89c6\u89d2\u3001\u5c3a\u5ea6\u548c\u5149\u7167\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5f15\u5165\u81ea\u9002\u5e94\u975e\u6781\u5927\u503c\u6291\u5236(ANMS)\u6280\u672f\uff0c\u5b9e\u73b0\u7a7a\u95f4\u5206\u5e03\u66f4\u5747\u5300\u7684\u5173\u952e\u70b9\u63d0\u53d6\uff0c\u589e\u5f3a\u573a\u666f\u8986\u76d6\u5ea6\u3002  \n\u25c6 \u96c6\u6210\u8f7b\u91cf\u7ea7NetVLAD\u6a21\u5757\u4f5c\u4e3a\u5b66\u4e60\u5f0f\u56de\u73af\u68c0\u6d4b\u5668\uff0c\u663e\u8457\u6539\u5584\u4e86\u4f20\u7edf\u8bcd\u888b\u6a21\u578b\u7684\u8bc6\u522b\u80fd\u529b\u3002  \n\u25c6 \u5728KITTI\u6570\u636e\u96c6\u4e0a\u5c06\u5e73\u5747\u5e73\u79fb\u8bef\u5dee\u4ece4.15%\u964d\u81f30.34%\uff0c\u65cb\u8f6c\u8bef\u5dee\u4ece0.0027\u5ea6/\u7c73\u964d\u81f30.0010\u5ea6/\u7c73\u3002  \n\u25c6 \u5728EuRoC MAV\u6570\u636e\u96c6\u4e0a\u6240\u6709\u5e8f\u5217\u8bef\u5dee\u964d\u4f4e\u7ea650%\uff08\u5982V2_03\u4ece1.58%\u964d\u81f30.79%\uff09\u3002  \n\u25c6 \u4fdd\u6301ORB-SLAM3\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u4e0e\u5b66\u4e60\u5f0f\u56de\u73af\u7684\u878d\u5408\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u7a81\u7834\u3002|\n",
    "2506.15402": "|2025-06-18|MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System|Miaoxin Pan\u7b49|[2506.15402](http://arxiv.org/pdf/2506.15402)|\u65e0|\u25c6 \u63d0\u51faMCOO-SLAM\u7cfb\u7edf\uff0c\u9996\u6b21\u5c06\u591a\u76f8\u673a\u5168\u666f\u914d\u7f6e\u5f15\u5165\u7269\u4f53\u7ea7SLAM\uff0c\u89e3\u51b3\u4f20\u7edf\u5355\u76ee\u6216RGB-D\u7cfb\u7edf\u89c6\u573a\u7a84\u3001\u906e\u6321\u654f\u611f\u548c\u6df1\u5ea6\u611f\u77e5\u53d7\u9650\u7684\u95ee\u9898\u3002  \n\u25c6 \u878d\u5408\u70b9\u7279\u5f81\u4e0e\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u589e\u5f3a\u7684\u7269\u4f53\u7ea7\u5730\u6807\uff0c\u5b9e\u73b0\u590d\u6742\u6237\u5916\u573a\u666f\u4e2d\u66f4\u9c81\u68d2\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u5efa\u56fe\u3002  \n\u25c6 \u8bbe\u8ba1\u8bed\u4e49-\u51e0\u4f55-\u65f6\u5e8f\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8de8\u89c6\u89d2\u7269\u4f53\u5173\u8054\u7684\u51c6\u786e\u6027\uff0c\u6539\u5584\u7269\u4f53\u5efa\u6a21\u4e00\u81f4\u6027\u3002  \n\u25c6 \u521b\u65b0\u5168\u666f\u95ed\u73af\u68c0\u6d4b\u6a21\u5757\uff0c\u901a\u8fc7\u573a\u666f\u7ea7\u63cf\u8ff0\u7b26\u5b9e\u73b0\u89c6\u89d2\u65e0\u5173\u7684\u5730\u70b9\u8bc6\u522b\uff0c\u589e\u5f3a\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u6784\u5efa\u5206\u5c423D\u573a\u666f\u56fe\u8c31\u62bd\u8c61\u5730\u56fe\uff0c\u4e3a\u673a\u5668\u4eba\u9ad8\u5c42\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u7ed3\u6784\u5316\u8bed\u4e49\u652f\u6301\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7cfb\u7edf\u5728\u906e\u6321\u3001\u4f4d\u59ff\u53d8\u5316\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|\n",
    "2506.15242": "|2025-06-24|RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories|Qingsong Yan\u7b49|[2506.15242](http://arxiv.org/pdf/2506.15242)|\u65e0|\u25c6 \u63d0\u51faRA-NeRF\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfNeRF\u548c3DGS\u4f9d\u8d56\u51c6\u786e\u4f4d\u59ff\u5148\u9a8c\u7684\u95ee\u9898\u3002  \n\u25c6 \u91c7\u7528\u589e\u91cf\u5f0f\u91cd\u5efa\u6d41\u7a0b\uff0c\u7ed3\u5408\u5149\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u5149\u6d41\u9a71\u52a8\u7684\u4f4d\u59ff\u8c03\u8282\u673a\u5236\uff0c\u63d0\u5347\u4e86\u521d\u59cb\u5316\u548c\u5b9a\u4f4d\u9636\u6bb5\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u9690\u5f0f\u4f4d\u59ff\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u6355\u6349\u76f8\u673a\u8fd0\u52a8\u6a21\u5f0f\u6709\u6548\u6d88\u9664\u4f4d\u59ff\u4f30\u8ba1\u4e2d\u7684\u566a\u58f0\u5e72\u6270\u3002  \n\u25c6 \u5728Tanks&Temple\u548cNeRFBuster\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5176\u4e2dNeRFBuster\u5305\u542b\u6781\u5177\u6311\u6218\u6027\u7684\u76f8\u673a\u8f68\u8ff9\u573a\u666f\u3002  \n\u25c6 \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRA-NeRF\u5728\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u548c\u573a\u666f\u91cd\u5efa\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u590d\u6742\u8f68\u8ff9\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002|\n",
    "2506.15175": "|2025-06-18|SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization|Hanjun Kim\u7b49|[2506.15175](http://arxiv.org/pdf/2506.15175)|\u65e0|\u25c6 \u63d0\u51faSHeRLoc\uff0c\u9996\u4e2a\u4e13\u4e3a\u5f02\u6784\u96f7\u8fbe\u8bbe\u8ba1\u7684\u6df1\u5ea6\u7f51\u7edc\uff0c\u586b\u8865\u4e86\u8de8\u6a21\u6001\u96f7\u8fbe\u5b9a\u4f4d\u7814\u7a76\u7684\u7a7a\u767d\u3002  \n\u25c6 \u91c7\u7528RCS\u6781\u5750\u6807\u5339\u914d\u6280\u672f\uff0c\u6709\u6548\u5bf9\u9f50\u591a\u6a21\u6001\u96f7\u8fbe\u6570\u636e\uff0c\u89e3\u51b3\u5f02\u6784\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u96be\u9898\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u6700\u4f18\u4f20\u8f93\u7684\u7279\u5f81\u805a\u5408\u65b9\u6cd5\uff0c\u751f\u6210\u5177\u6709\u65cb\u8f6c\u9c81\u68d2\u6027\u7684\u591a\u5c3a\u5ea6\u63cf\u8ff0\u7b26\u3002  \n\u25c6 \u7ed3\u5408FFT\u76f8\u4f3c\u6027\u6570\u636e\u6316\u6398\u548c\u81ea\u9002\u5e94\u8fb9\u754c\u4e09\u5143\u7ec4\u635f\u5931\uff0c\u5b9e\u73b0\u89c6\u573a\u611f\u77e5\u7684\u5ea6\u91cf\u5b66\u4e60\u3002  \n\u25c6 \u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u53ec\u56de\u7387@1\u4ece\u4e0d\u8db30.1\u63d0\u5347\u81f30.9\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002  \n\u25c6 \u6269\u5c55\u6027\u5f3a\uff0c\u53ef\u5e94\u7528\u4e8eLiDAR\u7b49\u4f20\u611f\u5668\uff0c\u4e3a\u8de8\u6a21\u6001\u5730\u70b9\u8bc6\u522b\u548c\u5f02\u6784SLAM\u5f00\u8f9f\u65b0\u9014\u5f84\u3002|\n",
    "2506.15126": "|2025-06-18|VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments|Bingbing Zhang\u7b49|[2506.15126](http://arxiv.org/pdf/2506.15126)|\u65e0|\u25c6 \u63d0\u51faVIMS\u7cfb\u7edf\uff0c\u9996\u6b21\u5c06\u89c6\u89c9-\u60ef\u6027-\u78c1\u529b-\u58f0\u7eb3\u591a\u6a21\u6001\u878d\u5408\u7528\u4e8e\u6c34\u4e0bSLAM\uff0c\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9-\u60ef\u6027\u65b9\u6cd5\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u5c3a\u5ea6\u4f30\u8ba1\u548c\u95ed\u73af\u96be\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165\u4f4e\u6210\u672c\u5355\u6ce2\u675f\u58f0\u7eb3\uff0c\u6709\u6548\u63d0\u5347\u6c34\u4e0b\u5c3a\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u514b\u670d\u7eaf\u89c6\u89c9\u65b9\u6cd5\u56e0\u6c34\u4f53\u6298\u5c04\u5bfc\u81f4\u7684\u5c3a\u5ea6\u6f02\u79fb\u95ee\u9898\u3002  \n\u25c6 \u5229\u7528\u9ad8\u91c7\u6837\u7387\u78c1\u529b\u8ba1\u914d\u5408\u7ecf\u6d4e\u578b\u78c1\u573a\u7ebf\u5708\u751f\u6210\u78c1\u7279\u5f81\uff0c\u5b9e\u73b0\u57fa\u4e8e\u78c1\u573a\u6307\u7eb9\u7684\u573a\u6240\u8bc6\u522b\uff0c\u586b\u8865\u6c34\u4e0b\u65e0\u7eb9\u7406\u533a\u57df\u7684\u611f\u77e5\u7a7a\u767d\u3002  \n\u25c6 \u8bbe\u8ba1\u5206\u5c42\u5f0f\u89c6\u89c9-\u78c1\u529b\u6df7\u5408\u95ed\u73af\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u4e92\u8865\u589e\u5f3a\u95ed\u73af\u9c81\u68d2\u6027\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u5339\u914d\u7387\u3002  \n\u25c6 \u4f18\u5316\u524d\u7aef\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u5e73\u8861\u5c40\u90e8\u7279\u5f81\u8ddf\u8e2a\u4e0e\u5168\u5c40\u63cf\u8ff0\u5b50\u5339\u914d\uff0c\u5728\u4e0d\u589e\u52a0\u524d\u7aef\u8d1f\u62c5\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u6548\u95ed\u73af\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5b9a\u4f4d\u7cbe\u5ea6\u63d0\u534730%\u4ee5\u4e0a\uff0c\u4e3a\u4f4e\u6210\u672c\u6c34\u4e0b\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u65b0\u65b9\u6848\u3002|\n",
    "2506.18885": "|2025-06-23|GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM|Annika Thomas\u7b49|[2506.18885](http://arxiv.org/pdf/2506.18885)|\u65e0|\u25c6 \u63d0\u51fa\u4e86GRAND-SLAM\u65b9\u6cd5\uff0c\u9996\u6b21\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6237\u5916\u591a\u667a\u80fd\u4f53SLAM\u573a\u666f\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5c0f\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u7684\u9650\u5236\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5c40\u90e8\u5b50\u5730\u56fe\u4f18\u5316\u7684\u9690\u5f0f\u8ddf\u8e2a\u6a21\u5757\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u673a\u5668\u4eba\u5185/\u95f4\u95ed\u73af\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u4e00\u81f4\u6027\u7684\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u3002  \n\u25c6 \u5728Replica\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u8ddf\u8e2a\u6027\u80fd\uff0cPSNR\u6307\u6807\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534728%\u3002  \n\u25c6 \u5728\u5927\u578b\u6237\u5916Kimera-Multi\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u667a\u80fd\u4f53\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e91%\uff0c\u6e32\u67d3\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002  \n\u25c6 \u901a\u8fc7\u53ef\u6269\u5c55\u7684\u73af\u5883\u8868\u793a\u65b9\u6cd5\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u540c\u5feb\u901f\u63a2\u7d22\u4e0e\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2506.18678": "|2025-06-23|MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation|Tianchen Deng\u7b49|[2506.18678](http://arxiv.org/pdf/2506.18678)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u795e\u7ecfSLAM\u6846\u67b6MCN-SLAM\uff0c\u7ed3\u5408\u6df7\u5408\u9690\u5f0f\u795e\u7ecf\u573a\u666f\u8868\u793a\uff0c\u89e3\u51b3\u4f20\u7edf\u5355\u667a\u80fd\u4f53SLAM\u5728\u5927\u573a\u666f\u548c\u957f\u5e8f\u5217\u4e2d\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u521b\u65b0\u8bbe\u8ba1\u4e09\u5e73\u9762-\u7f51\u683c\u8054\u5408\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u573a\u666f\u91cd\u5efa\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u65b9\u6848\u3002  \n\u25c6 \u5f00\u53d1\u65b0\u578b\"\u5185\u90e8-\u8de8\u667a\u80fd\u4f53\"\u95ed\u73af\u68c0\u6d4b\u673a\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5c40\u90e8\u4e0e\u591a\u667a\u80fd\u4f53\u5168\u5c40\u4e00\u81f4\u6027\u534f\u540c\u4f18\u5316\u3002  \n\u25c6 \u63d0\u51fa\u5728\u7ebf\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u591a\u5b50\u5730\u56fe\u878d\u5408\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u901a\u4fe1\u4f18\u5316\u89e3\u51b3NeRF\u7c7b\u7cfb\u7edf\u5e26\u5bbd\u53d7\u9650\u95ee\u9898\u3002  \n\u25c6 \u53d1\u5e03\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u5bc6\u96c6SLAM\u6570\u636e\u96c6DES\uff0c\u6db5\u76d6\u5355/\u591a\u667a\u80fd\u4f53\u573a\u666f\uff0c\u63d0\u4f9b\u8fde\u7eed\u8f68\u8ff9\u4e0e\u9ad8\u7cbe\u5ea63D\u7f51\u683c\u771f\u503c\uff0c\u586b\u8865\u9886\u57df\u7a7a\u767d\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5efa\u56fe\u3001\u5b9a\u4f4d\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u63a8\u52a8SLAM\u4e0e\u4e09\u7ef4\u91cd\u5efa\u7814\u7a76\u53d1\u5c55\u3002|\n",
    "2506.18204": "|2025-06-24|Multimodal Fusion SLAM with Fourier Attention|Youjie Zhou\u7b49|[2506.18204](http://arxiv.org/pdf/2506.18204)|\u65e0|\u25c6 \u63d0\u51faFMF-SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u63d0\u5347\u591a\u6a21\u6001SLAM\u7684\u7b97\u6cd5\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u5149\u6d41SLAM\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u8bbe\u8ba1\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u81ea\u6ce8\u610f\u529b\u4e0e\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u53f7\u7684\u7279\u5f81\u63d0\u53d6\u3002  \n\u25c6 \u5f15\u5165\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\u95f4\u7684\u4ea4\u4e92\u4e0e\u4e92\u8865\u6027\u3002  \n\u25c6 \u7ed3\u5408GNSS-RTK\u5168\u5c40\u5b9a\u4f4d\u6a21\u5757\u4e0e\u5168\u5c40Bundle Adjustment\uff0c\u5b9e\u73b0\u5b89\u5168\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u5e94\u7528\u9a8c\u8bc1\u3002  \n\u25c6 \u5728TUM\u3001TartanAir\u53ca\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6027\u80fd\uff0c\u5728\u566a\u58f0\u3001\u5149\u7167\u53d8\u5316\u548c\u9ed1\u6697\u6761\u4ef6\u4e0b\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002  \n\u25c6 \u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u591a\u6a21\u6001SLAM\u9886\u57df\u7684\u53ef\u590d\u73b0\u7814\u7a76\u3002|\n",
    "2506.18016": "|2025-06-22|ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM|Yongxin Shao\u7b49|[2506.18016](http://arxiv.org/pdf/2506.18016)|\u65e0|\u25c6 \u63d0\u51faADA-DPM\u81ea\u9002\u5e94\u566a\u58f0\u8fc7\u6ee4\u7b56\u7565\uff0c\u5728\u52a8\u6001\u7269\u4f53\u5e72\u6270\u548c\u566a\u58f0\u73af\u5883\u4e0b\u540c\u65f6\u63d0\u5347SLAM\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u7cfb\u7edf\u9c81\u68d2\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u52a8\u6001\u5206\u5272\u5934\uff08Dynamic Segmentation Head\uff09\uff0c\u901a\u8fc7\u9884\u6d4b\u7279\u5f81\u70b9\u7c7b\u522b\u4e3b\u52a8\u5254\u9664\u52a8\u6001\u7279\u5f81\u70b9\uff0c\u51cf\u5c11\u52a8\u6001\u5e72\u6270\u3002  \n\u25c6 \u5f15\u5165\u5168\u5c40\u91cd\u8981\u6027\u8bc4\u5206\u5934\uff08Global Importance Scoring Head\uff09\uff0c\u81ea\u9002\u5e94\u7b5b\u9009\u9ad8\u8d21\u732e\u7279\u5f81\u70b9\u5e76\u6291\u5236\u566a\u58f0\u5e72\u6270\uff0c\u4f18\u5316\u7279\u5f81\u9009\u62e9\u3002  \n\u25c6 \u6784\u5efa\u8de8\u5c42\u56fe\u5185\u5377\u79ef\u6a21\u5757\uff08GLI-GCN\uff09\uff0c\u878d\u5408\u591a\u5c3a\u5ea6\u90bb\u57df\u7ed3\u6784\uff0c\u589e\u5f3a\u91cd\u53e0\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u3002  \n\u25c6 \u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002|\n",
    "2506.17775": "|2025-06-21|Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems|Sebastian Sansoni\u7b49|[2506.17775](http://arxiv.org/pdf/2506.17775)|\u65e0|\u25c6\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\uff08UM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u5206\u5e03\u91cf\u5316\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u4e3b\u52a8SLAM\u7cfb\u7edf\u5efa\u7acb\u65b0\u578b\u73af\u5883\u5efa\u6a21\u65b9\u6cd5\u3002  \n\u25c6\u5b9a\u4e49\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff08UF\uff09\u4f5c\u4e3a\u63a2\u7d22-\u5f00\u53d1\u7684\u5173\u952e\u76ee\u6807\u4e0e\u505c\u6b62\u51c6\u5219\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u63a2\u7d22\u7ec8\u6b62\u6761\u4ef6\u6a21\u7cca\u7684\u95ee\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5f15\u5165\u57fa\u4e8eKL\u6563\u5ea6\u7684\u7b26\u53f7\u76f8\u5bf9\u71b5\uff08SiREn\uff09\uff0c\u9996\u6b21\u5b9e\u73b0\u8986\u76d6\u5ea6\u4e0e\u4e0d\u786e\u5b9a\u6027\u7684\u8054\u5408\u5ea6\u91cf\uff0c\u4ec5\u9700\u5355\u4e00\u53c2\u6570\u5373\u53ef\u5e73\u8861\u63a2\u7d22\u4e0e\u5f00\u53d1\u3002  \n\u25c6\u8bbe\u8ba1\u4f20\u611f\u5668\u65e0\u5173\u7684\u901a\u7528\u67b6\u6784\uff0c\u517c\u5bb9\u76f8\u673a\u3001\u6fc0\u5149\u96f7\u8fbe\u53ca\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u5bf9\u7279\u5b9aSLAM\u914d\u7f6e\u7684\u4f9d\u8d56\u3002  \n\u25c6\u7ed3\u5408UF\u7684\u8def\u5f84\u89c4\u5212\u7cfb\u7edf\u9996\u6b21\u5b9e\u73b0\u5f00\u653e\u7a7a\u95f4\u7684\u81ea\u4e3b\u63a2\u7d22\u80fd\u529b\uff0c\u586b\u8865\u4e86\u4e3b\u52a8SLAM\u6587\u732e\u4e2d\u8be5\u884c\u4e3a\u7684\u7a7a\u767d\u3002  \n\u25c6\u5f00\u6e90ROS\u8282\u70b9\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u65b9\u6cd5\u9a8c\u8bc1\u4e0e\u793e\u533a\u5e94\u7528\uff0c\u589e\u5f3a\u7814\u7a76\u53ef\u590d\u73b0\u6027\u3002|\n",
    "2506.20394": "|2025-06-25|SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning|Mimo Shirasaka\u7b49|[2506.20394](http://arxiv.org/pdf/2506.20394)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u5728\u7ebf\u8bed\u4e49\u4fe1\u606f\u66f4\u65b0\u6846\u67b6SPARK\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u4e2d\u8bed\u4e49\u4fe1\u606f\u5b9e\u65f6\u66f4\u65b0\u7684\u7a7a\u767d\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u79bb\u7ebf\u573a\u666f\u56fe\u8868\u793a\u6269\u5c55\u5230\u5728\u7ebf\u573a\u666f\uff0c\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u4fe1\u606f\u5904\u7406\u80fd\u529b\u3002  \n\u25c6 \u901a\u8fc7\u73af\u5883\u5d4c\u5165\u7ebf\u7d22\uff08\u5982\u624b\u52bf\u7b49\u975e\u4f20\u7edf\u7a7a\u95f4\u63d0\u793a\uff09\u5b9e\u65f6\u66f4\u65b0\u573a\u666f\u56fe\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u5bf9\u52a8\u6001\u73af\u5883\u7684\u9002\u5e94\u6027\u3002  \n\u25c6 \u9a8c\u8bc1\u4e86\u57fa\u4e8e\u56fe\u7684\u7a7a\u95f4\u5173\u7cfb\u8868\u793a\u80fd\u663e\u8457\u63d0\u5347\u4efb\u52a1\u89c4\u5212\u6548\u7387\uff0c\u5c24\u5176\u5728\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002  \n\u25c6 \u7cfb\u7edf\u6574\u5408\u51e0\u4f55\u4e0e\u8bed\u4e49\u6570\u636e\uff0c\u4e3a\u901a\u7528\u670d\u52a1\u673a\u5668\u4eba\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5728\u7ebf\u4fe1\u606f\u66f4\u65b0\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u53d8\u5316\uff0c\u4e3a\u540e\u7eed\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u53ef\u9760\u652f\u6301\u3002|\n",
    "2506.20311": "|2025-06-25|Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles|Jingwen Wei|[2506.20311](http://arxiv.org/pdf/2506.20311)|\u65e0|\u25c6 \u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u590d\u67423D\u73af\u5883\u7684\u5b9e\u65f6\u907f\u969c\u7b97\u6cd5\uff0c\u7279\u522b\u9488\u5bf9\u68ee\u6797\u706b\u707e\u7b49\u707e\u5bb3\u573a\u666f\u4e2d\u7684\u65e0\u4eba\u673a\u5b89\u5168\u5bfc\u822a\u9700\u6c42\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u76842D\u878d\u5408\u5bfc\u822a\u7b56\u7565\uff0c\u6700\u521d\u4e3a\u5730\u9762\u79fb\u52a8\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u5177\u5907\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u79fb\u52a8\u80fd\u529b\uff0c\u5e76\u652f\u6301\u81ea\u9002\u5e94\u969c\u788d\u5904\u7406\u4e0e\u51b3\u7b56\u4f18\u5316\u3002  \n\u25c6 \u9996\u6b21\u8bbe\u8ba1\u4e86\u9488\u5bf9\u68ee\u6797\u706b\u707e\u6a21\u62df\u76843D\u53cd\u5e94\u5f0f\u5bfc\u822a\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5728\u6b64\u7c7b\u7279\u6b8a\u573a\u666f\u4e2d\u7684\u907f\u969c\u96be\u9898\u3002  \n\u25c6 \u63d0\u51fa\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u65e0\u4eba\u8f66\uff08UGV\uff09\u7684\u534f\u540c\u63a7\u5236\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u7a7a\u5730\u8f66\u8f86\u5728\u68ee\u6797\u6551\u63f4\u4efb\u52a1\u4e2d\u7684\u7edf\u4e00\u534f\u8c03\u4f5c\u4e1a\u3002  \n\u25c6 \u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u4e0e\u4eff\u771f\u9a8c\u8bc1\u4e86\u5404\u9636\u6bb5\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u81ea\u7136\u707e\u5bb3\u6551\u63f4\u4e2d\u65e0\u4eba\u7cfb\u7edf\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u517c\u5177\u5b9e\u7528\u4ef7\u503c\u4e0e\u5b66\u672f\u610f\u4e49\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2506.19957": "|2025-06-24|Posterior Cram\u00e9r-Rao Bounds on Localization and Mapping Errors in Distributed MIMO SLAM|Benjamin J. B. Deutschmann\u7b49|[2506.19957](http://arxiv.org/pdf/2506.19957)|\u65e0|\u25c6 \u9996\u6b21\u63d0\u51fa\u4e86\u9488\u5bf9\u5206\u5e03\u5f0fMIMO SLAM\u7cfb\u7edf\u4e2d\u955c\u9762\u53cd\u5c04\u9762\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u8bef\u5dee\u7684\u540e\u9a8c\u514b\u62c9\u7f8e\u7f57\u4e0b\u754c\uff08MEB\uff09\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u6027\u80fd\u8bc4\u4f30\u7684\u7406\u8bba\u7a7a\u767d\u3002  \n\u25c6 \u8003\u8651\u4e86\u5355\u6b21\u53cd\u5c04\u548c\u53cc\u6b21\u53cd\u5c04\u7684\u590d\u6742\u4f20\u64ad\u573a\u666f\uff0c\u5e76\u652f\u6301\u5206\u5e03\u5f0f\u951a\u70b9\u914d\u7f6e\uff0c\u6269\u5c55\u4e86\u4f20\u7edfSLAM\u6027\u80fd\u8fb9\u754c\u7684\u9002\u7528\u8303\u56f4\u3002  \n\u25c6 \u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u73b0\u6709\u5148\u8fdbRF-SLAM\u7b97\u6cd5\u7684\u5efa\u56fe\u8bef\u5dee\u80fd\u6e10\u8fdb\u6536\u655b\u81f3MEB\uff0c\u4e3a\u7b97\u6cd5\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u51c6\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u6620\u5c04\u6027\u80fd\uff08\u955c\u9762\u4f4d\u7f6e/\u671d\u5411\uff09\u4e0e\u7528\u6237\u5b9a\u4f4d\u6027\u80fd\u7edf\u4e00\u7eb3\u5165\u5168\u5c40\u7279\u5f81\u8bc4\u4f30\u6846\u67b6\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u4ec5\u5173\u6ce8\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5c40\u9650\u3002  \n\u25c6 \u6240\u63d0\u8fb9\u754c\u7406\u8bba\u53ef\u63d0\u5347\u591a\u5f84\u4fe1\u9053\u4e2d\u975e\u89c6\u8ddd\u4fe1\u53f7\u7684\u5229\u7528\u6548\u7387\uff0c\u4e3a\u901a\u4fe1-\u5b9a\u4f4d\u4e00\u4f53\u5316\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u3002|\n",
    "2506.21420": "|2025-06-26|EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting|Taoyu Wu\u7b49|[2506.21420](http://arxiv.org/pdf/2506.21420)|\u65e0|\u25c6 \u63d0\u51faEndoFlow-SLAM\u7cfb\u7edf\uff0c\u9996\u6b21\u5c06\u5149\u6d41\u635f\u5931\u4f5c\u4e3a\u51e0\u4f55\u7ea6\u675f\u5f15\u5165\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684SLAM\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u975e\u6717\u4f2f\u8868\u9762\u548c\u547c\u5438\u8fd0\u52a8\u5bfc\u81f4\u7684\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u6df1\u5ea6\u6b63\u5219\u5316\u7b56\u7565\uff0c\u7f13\u89e3\u5185\u7aa5\u955c\u573a\u666f\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u786e\u4fdd3DGS\u6df1\u5ea6\u6e32\u67d3\u7684\u53ef\u9760\u6027\u3002  \n\u25c6 \u6539\u8fdb3DGS\u4f18\u5316\u7b56\u7565\uff0c\u9488\u5bf9\u5173\u952e\u5e27\u4e2d\u6e32\u67d3\u8d28\u91cf\u8f83\u5dee\u7684\u89c6\u89d2\u8fdb\u884c\u91cd\u70b9\u4f18\u5316\uff0c\u63d0\u5347\u573a\u666f\u8868\u793a\u7cbe\u5ea6\u3002  \n\u25c6 \u5728\u9759\u6001\uff08C3VD\u6570\u636e\u96c6\uff09\u548c\u52a8\u6001\uff08StereoMIS\u6570\u636e\u96c6\uff09\u624b\u672f\u573a\u666f\u4e2d\u5747\u5b9e\u73b0\u9886\u5148\u6027\u80fd\uff0c\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002  \n\u25c6 \u7cfb\u7edf\u652f\u6301\u5b9e\u65f6\u8fd0\u884c\uff0c\u4e3a\u5185\u7aa5\u955c\u624b\u672f\u63d0\u4f9b\u9ad8\u6548\u7684\u4e09\u7ef4\u91cd\u5efa\u4e0e\u53ef\u89c6\u5316\u80fd\u529b\u3002|\n",
    "2506.21077": "|2025-06-26|CURL-SLAM: Continuous and Compact LiDAR Mapping|Kaicheng Zhang\u7b49|[2506.21077](http://arxiv.org/pdf/2506.21077)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bLiDAR SLAM\u8303\u5f0fCURL-SLAM\uff0c\u5229\u7528\u8fde\u7eed\u8d85\u7d27\u51d1\u8868\u793a\uff08CURL\uff09\u5b9e\u73b0\u53ef\u66f4\u65b0\u3001\u53ef\u5b9a\u4f4d\u7684\u5730\u56fe\u8868\u793a\u3002  \n\u25c6 \u91c7\u7528\u7403\u8c10\u51fd\u6570\u9690\u5f0f\u7f16\u7801\u6280\u672f\uff0c\u751f\u6210\u652f\u6301\u53ef\u53d8\u5bc6\u5ea6\u8fde\u7eed\u91cd\u5efa\u7684\u7d27\u51d13D\u5730\u56fe\uff0c\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u9700\u6c42\u3002  \n\u25c6 \u901a\u8fc7\u72ec\u7279\u7684CURL\u5b9a\u5236\u4f18\u5316\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49LiDAR\u4f4d\u59ff\u4f30\u8ba1\uff0c\u66ff\u4ee3\u4f20\u7edfICP\u65b9\u6cd5\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u6269\u5c55\u5c40\u90e8\u5149\u675f\u6cd5\u5e73\u5dee\uff08BA\uff09\u6280\u672f\uff0c\u5b9e\u73b0\u4f4d\u59ff\u7cbe\u4fee\u4e0e\u5730\u56fe\u6821\u6b63\u540c\u6b65\u8fdb\u884c\uff0c\u786e\u4fdd\u95ed\u73af\u540e\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002  \n\u25c6 \u5728CPU\u4e0a\u8fbe\u523010Hz\u5b9e\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9886\u5148\u76843D\u5efa\u56fe\u8d28\u91cf\u548c\u8f68\u8ff9\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u3002  \n\u25c6 \u5f00\u6e90CURL-SLAM\u5b9e\u73b0\uff0c\u63a8\u52a8\u8fde\u7eed\u7d27\u51d1\u5730\u56fe\u8868\u793a\u9886\u57df\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u3002|\n",
    "2506.21798": "|2025-06-26|Adaptive Multipath-Based SLAM for Distributed MIMO Systems|Xuhong Li\u7b49|[2506.21798](http://arxiv.org/pdf/2506.21798)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5206\u5e03\u5f0fMIMO\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u591a\u8def\u5f84SLAM\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u51f8\u51e0\u4f55\u73af\u5883\u4e2d\u65e0\u6cd5\u8fdb\u884c\u5149\u7ebf\u8ffd\u8e2a\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u5229\u7528\u632f\u5e45\u7edf\u8ba1\u91cf\u5efa\u7acb\u81ea\u9002\u5e94\u65f6\u53d8\u68c0\u6d4b\u6982\u7387\uff0c\u5b9e\u73b0\u4e86\"\u8f6f\"\u5149\u7ebf\u8ffd\u8e2a\u7b56\u7565\uff0c\u80fd\u591f\u5728\u975e\u51f8\u51e0\u4f55\u7684\u5c04\u9891\u73af\u5883\u4e2d\u8de8\u4f20\u64ad\u8def\u5f84\u878d\u5408\u4fe1\u606f\u3002  \n\u25c6 \u901a\u8fc7\u5c06\u548c\u79ef\u7b97\u6cd5(SPA)\u7684\u6d88\u606f\u4f20\u9012\u89c4\u5219\u5e94\u7528\u4e8e\u6240\u63d0\u51fa\u7684\u7edf\u8ba1\u6a21\u578b\u56e0\u5b50\u56fe\uff0c\u5efa\u7acb\u4e86\u5730\u56fe\u7279\u5f81\u548c\u667a\u80fd\u4f53\u4f4d\u7f6e\u8054\u5408\u4f30\u8ba1\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5efa\u8bae\u6982\u7387\u5bc6\u5ea6\u51fd\u6570(PDF)\uff0c\u7528\u4e8e\u57fa\u4e8e\u7c92\u5b50\u7684SPA\u6d88\u606f\u8ba1\u7b97\uff0c\u80fd\u591f\u65e9\u671f\u68c0\u6d4b\u4ec5\u7531\u53cc\u8df3\u8def\u5f84\u652f\u6301\u7684\u65b0\u8868\u9762\u3002  \n\u25c6 \u5728\u5177\u6709\u975e\u51f8\u51e0\u4f55\u5f62\u72b6\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u4f7f\u7528\u5408\u6210\u5c04\u9891\u6d4b\u91cf\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u5b9a\u4f4d\u548c\u5efa\u56fe\u4f30\u8ba1\uff0c\u5e76\u8fbe\u5230\u540e\u9a8cCRLB\u754c\u3002|\n",
    "2506.21628": "|2025-06-24|Ark: An Open-source Python-based Framework for Robot Learning|Magnus Dierking\u7b49|[2506.21628](http://arxiv.org/pdf/2506.21628)|\u65e0|\u25c6 \u63d0\u51faARK\u6846\u67b6\uff0c\u9996\u4e2a\u4ee5Python\u4e3a\u6838\u5fc3\u7684\u673a\u5668\u4eba\u5b66\u4e60\u5f00\u6e90\u5e73\u53f0\uff0c\u5f25\u5408\u673a\u5668\u4eba\u6280\u672f\u4e0e\u73b0\u4ee3AI\u5de5\u5177\u94fe\u7684\u9e3f\u6c9f\u3002  \n\u25c6 \u91c7\u7528Gym\u98ce\u683c\u63a5\u53e3\u8bbe\u8ba1\uff0c\u652f\u6301\u6570\u636e\u91c7\u96c6\u3001\u9884\u5904\u7406\u5230\u7b56\u7565\u8bad\u7ec3\u7684\u5168\u6d41\u7a0b\uff0c\u517c\u5bb9\u4eff\u771f\u4e0e\u5b9e\u4f53\u673a\u5668\u4eba\u65e0\u7f1d\u5207\u6362\u3002  \n\u25c6 \u72ec\u521b\u8f7b\u91cf\u7ea7\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\uff0c\u5b9e\u73b0\u7f51\u7edc\u5316\u53d1\u5e03-\u8ba2\u9605\u901a\u4fe1\uff0c\u5e76\u4fdd\u7559C/C++\u7ed1\u5b9a\u9009\u9879\u4fdd\u969c\u5b9e\u65f6\u6027\u80fd\u9700\u6c42\u3002  \n\u25c6 \u5185\u7f6e\u63a7\u5236\u3001SLAM\u3001\u8fd0\u52a8\u89c4\u5212\u7b49\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u539f\u751f\u652f\u6301ROS\u4ea4\u4e92\uff0c\u63d0\u4f9b\u5f00\u7bb1\u5373\u7528\u7684\u673a\u5668\u4eba\u529f\u80fd\u5957\u4ef6\u3002  \n\u25c6 \u901a\u8fc7\u8be6\u5b9e\u6587\u6863\u548c\u6848\u4f8b\uff08\u5982\u64cd\u4f5c\u4e0e\u5bfc\u822a\u4efb\u52a1\uff09\uff0c\u9a8c\u8bc1\u5176\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3001\u786c\u4ef6\u7075\u6d3b\u5207\u6362\u53ca\u7aef\u5230\u7aef\u6d41\u6c34\u7ebf\u4f18\u52bf\u3002  \n\u25c6 \u7edf\u4e00Python\u751f\u6001\u4e0e\u673a\u5668\u4eba\u5f00\u53d1\uff0c\u663e\u8457\u964d\u4f4e\u5b66\u4e60\u95e8\u69db\uff0c\u52a0\u901f\u5b66\u672f\u7814\u7a76\u4e0e\u5546\u4e1a\u573a\u666f\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u843d\u5730\u3002|\n",
    "2506.23207": "|2025-06-29|TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints|Zhen Tan\u7b49|[2506.23207](http://arxiv.org/pdf/2506.23207)|\u65e0|TVG-SLAM\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684RGB-only SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u89c6\u56fe\u51e0\u4f55\u7ea6\u675f\u63d0\u5347\u9c81\u68d2\u6027\u548c\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u3002\u5176\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u4e09\u89c6\u56fe\u51e0\u4f55\u8303\u5f0f\uff0c\u901a\u8fc7\u5bc6\u96c6\u4e09\u89c6\u56fe\u5339\u914d\u6a21\u5757\u805a\u5408\u53ef\u9760\u7684\u5e27\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u5f62\u6210\u8de8\u5e27\u7684\u9c81\u68d2\u51e0\u4f55\u7ea6\u675f\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u7684\u5c40\u9650\u6027\u3002\n\n\u25c6 \u8bbe\u8ba1\u6df7\u5408\u51e0\u4f55\u7ea6\u675f\uff08Hybrid Geometric Constraints\uff09\uff0c\u7ed3\u5408\u4e09\u89c6\u56fe\u5339\u914d\u7684\u51e0\u4f55\u7ebf\u7d22\u4e0e\u5149\u5ea6\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u5728\u89c6\u89d2\u7a81\u53d8\u548c\u5149\u7167\u53d8\u5316\u573a\u666f\u3002\n\n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u6982\u7387\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5c06\u4e09\u89c6\u56fe\u5bf9\u5e94\u5173\u7cfb\u7684\u51e0\u4f55\u4e0d\u786e\u5b9a\u6027\u7f16\u7801\u5230\u65b0\u521d\u59cb\u5316\u7684\u9ad8\u65af\u6a21\u578b\u4e2d\uff0c\u63d0\u5347\u6620\u5c04\u8d28\u91cf\u3002\n\n\u25c6 \u5f15\u5165\u52a8\u6001\u6e32\u67d3\u4fe1\u4efb\u8870\u51cf\u673a\u5236\uff08Dynamic Attenuation of Rendering Trust\uff09\uff0c\u6709\u6548\u7f13\u89e3\u56e0\u5efa\u56fe\u5ef6\u8fdf\u5bfc\u81f4\u7684\u8ddf\u8e2a\u6f02\u79fb\u95ee\u9898\u3002\n\n\u5b9e\u9a8c\u8868\u660e\uff0cTVG-SLAM\u5728\u6237\u5916\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709RGB-only 3DGS SLAM\u7cfb\u7edf\uff0c\u5728\u6700\u6311\u6218\u6027\u6570\u636e\u96c6\u4e2d\u5c06\u8f68\u8ff9\u8bef\u5dee\uff08ATE\uff09\u964d\u4f4e69.0%\uff0c\u540c\u65f6\u4fdd\u6301\u9876\u5c16\u7684\u6e32\u67d3\u8d28\u91cf\u3002|\n",
    "2506.23078": "|2025-06-29|Event-based Stereo Visual-Inertial Odometry with Voxel Map|Zhaoxing Zhang\u7b49|[2506.23078](http://arxiv.org/pdf/2506.23078)|\u65e0|\u25c6 \u63d0\u51faVoxel-ESVIO\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u7acb\u4f53\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff0c\u5229\u7528\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u70b9\u9009\u62e9\u65b9\u6cd5\uff0c\u6709\u6548\u8fc7\u6ee4\u4e8b\u4ef6\u6d41\u4e2d\u7684\u566a\u58f0\uff0c\u7b5b\u9009\u9ad8\u8d28\u91cf3D\u5730\u56fe\u70b9\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u4f53\u7d20\u611f\u77e5\u7684\u70b9\u7ba1\u7406\u673a\u5236\uff0c\u52a8\u6001\u4f18\u5316\u6bcf\u4e2a\u4f53\u7d20\u5185\u5730\u56fe\u70b9\u7684\u66f4\u65b0\u548c\u9009\u62e9\u3002  \n\u25c6 \u901a\u8fc7\u534f\u540c\u7b56\u7565\u9ad8\u6548\u63d0\u53d6\u6297\u566a\u58f0\u4e14\u89c2\u6d4b\u6982\u7387\u6700\u9ad8\u7684\u5730\u56fe\u70b9\uff0c\u786e\u4fdd\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002  \n\u25c6 \u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|\n",
    "2507.00937": "|2025-07-01|RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles|David Hunt\u7b49|[2507.00937](http://arxiv.org/pdf/2507.00937)|\u65e0|\u25c6\u63d0\u51faRaGNNarok\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u96f7\u8fbe\u70b9\u4e91\u6570\u636e\uff0c\u89e3\u51b3\u73b0\u6709\u96f7\u8fbe\u5b9a\u4f4d\u4e2d\u7a00\u758f\u70b9\u4e91\u3001\u566a\u58f0\u548c\u8bef\u68c0\u6d4b\u95ee\u9898\u3002  \n\u25c6\u8be5\u6846\u67b6\u5728\u4f4e\u6210\u672c\u8bbe\u5907\uff08\u5982\u6811\u8393\u6d3e5\uff09\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\uff0c\u63a8\u7406\u65f6\u95f4\u4ec57.3\u6beb\u79d2\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u673a\u5668\u4eba\u3002  \n\u25c6\u901a\u8fc7GNN\u6a21\u578b\u4f18\u5316\u96f7\u8fbe\u70b9\u4e91\uff0c\u663e\u8457\u63d0\u5347\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u5728\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002  \n\u25c6\u5728\u5b9a\u4f4d\u3001SLAM\u548c\u81ea\u4e3b\u5bfc\u822a\u7b49\u5173\u952e\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u591a\u73af\u5883\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6\u4e3a\u4f4e\u6210\u672c\u5ba4\u5185\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u4f4e\u6210\u672c\u4f18\u52bf\uff0c\u63a8\u52a8\u81ea\u52a8\u5316\u5728\u5bb6\u5ead\u548c\u5546\u4e1a\u7a7a\u95f4\u7684\u5e94\u7528\u3002|\n",
    "2507.00552": "|2025-07-01|Generation of Indoor Open Street Maps for Robot Navigation from CAD Files|Jiajie Zhang\u7b49|[2507.00552](http://arxiv.org/pdf/2507.00552)|\u65e0|\u25c6 \u63d0\u51fa\u5168\u81ea\u52a8\u7cfb\u7edf\uff0c\u5c06\u5efa\u7b51CAD\u6587\u4ef6\u8f6c\u6362\u4e3a\u5206\u5c42\u62d3\u6251OpenStreetMap\uff08OSM\uff09\u8868\u793a\uff0c\u4e13\u4e3a\u673a\u5668\u4eba\u7ec8\u8eab\u5bfc\u822a\u8bbe\u8ba1\uff0c\u89e3\u51b3SLAM\u5728\u52a8\u6001\u5927\u5c3a\u5ea6\u5ba4\u5185\u73af\u5883\u4e2d\u8017\u65f6\u3001\u8106\u5f31\u4e14\u6613\u8fc7\u65f6\u7684\u95ee\u9898\u3002  \n\u25c6 \u5f00\u53d1\u591a\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff0c\u4ece\u539f\u59cbCAD\u6570\u636e\u4e2d\u63d0\u53d6\u5173\u952e\u7ed3\u6784\u5c42\uff0c\u5e76\u57fa\u4e8eAreaGraph\u8fdb\u884c\u62d3\u6251\u5206\u5272\uff0c\u751f\u6210\u5c42\u6b21\u5316\u53ef\u5bfc\u822a\u7a7a\u95f4\u56fe\uff0c\u5b9e\u73b0\u8bed\u4e49\u4e30\u5bcc\u7684\u73af\u5883\u5efa\u6a21\u3002  \n\u25c6 \u81ea\u52a8\u5173\u8054CAD\u6e90\u6587\u4ef6\u4e2d\u7684\u6587\u672c\u6807\u7b7e\uff0c\u589e\u5f3a\u5730\u56fe\u8bed\u4e49\u4fe1\u606f\uff0c\u540c\u65f6\u652f\u6301\u591a\u697c\u5c42\u65e0\u7f1d\u5408\u5e76\uff0c\u6784\u5efa\u62d3\u6251\u6b63\u786e\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u63d0\u5347\u5bfc\u822a\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5229\u7528CAD\u6587\u4ef6\u56fa\u6709\u7684\u6c38\u4e45\u7ed3\u6784\u4fe1\u606f\uff0c\u89c4\u907fSLAM\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u4e3a\u590d\u6742\u5ba4\u5185\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u96c6\u6210\u76f4\u89c2\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u5c01\u88c5\u8f6f\u4ef6\uff0c\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4fc3\u8fdb\u793e\u533a\u5e94\u7528\u4e0e\u7814\u7a76\u3002|\n",
    "2507.00243": "|2025-06-30|VOCAL: Visual Odometry via ContrAstive Learning|Chi-Yao Huang\u7b49|[2507.00243](http://arxiv.org/pdf/2507.00243)|\u65e0|\u25c6 VOCAL\u5c06\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff08VO\uff09\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6807\u7b7e\u6392\u5e8f\u95ee\u9898\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u57fa\u4e8e\u51e0\u4f55\u5047\u8bbe\u7684\u5c40\u9650\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002  \n\u25c6 \u901a\u8fc7\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u4e0e\u8868\u5f81\u5b66\u4e60\uff0c\u8be5\u6846\u67b6\u4f7f\u89c6\u89c9\u7279\u5f81\u4e0e\u76f8\u673a\u72b6\u6001\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u3002  \n\u25c6 \u63d0\u51fa\u7684\u6392\u5e8f\u673a\u5236\u8feb\u4f7f\u76f8\u4f3c\u76f8\u673a\u72b6\u6001\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f62\u6210\u4e00\u81f4\u4e14\u7a7a\u95f4\u8fde\u8d2f\u7684\u8868\u5f81\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u6846\u67b6\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684VO\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3002  \n\u25c6 \u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86VOCAL\u5728\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u7684\u663e\u8457\u4f18\u52bf\uff0c\u63a8\u52a8\u4e86\u7a7a\u95f4\u667a\u80fd\u5411\u66f4\u901a\u7528\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u5411\u53d1\u5c55\u3002|\n",
    "2507.04662": "|2025-07-07|Simultaneous Localization and Mapping Using Active mmWave Sensing in 5G NR|Tao Du\u7b49|[2507.04662](http://arxiv.org/pdf/2507.04662)|\u65e0|\u25c6 \u63d0\u51fa\u5229\u7528\u6beb\u7c73\u6ce25G NR\u7cfb\u7edf\u8fdb\u884c\u4e3b\u52a8\u611f\u77e5\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u6fc0\u5149\u96f7\u8fbe\u7684\u70b9\u4e91\u751f\u6210\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u88ab\u52a8\u611f\u77e5SLAM\u6280\u672f\u5bf9\u955c\u9762\u53cd\u5c04\u5047\u8bbe\u548c\u7b80\u5316\u5730\u56fe\u8868\u793a\u7684\u4f9d\u8d56\u3002  \n\u25c6 \u91c7\u7528\u4e8c\u8fdb\u5236\u641c\u7d22\u65b9\u6cd5\u4ece\u6bcf\u4e2a\u6ce2\u675f\u65b9\u5411\u7684\u529f\u7387\u5ef6\u8fdf\u5256\u9762\u4e2d\u63d0\u53d6\u70b9\u4e91\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u73af\u5883\u611f\u77e5\u7684\u7cbe\u5ea6\u548c\u7ec6\u8282\u3002  \n\u25c6 \u901a\u8fc7\u591a\u4e2a\u9884\u5b9a\u4e49\u76ee\u6807\u70b9\u6821\u51c6\u786c\u4ef6\u5ef6\u8fdf\uff0c\u786e\u4fdd\u70b9\u4e91\u6570\u636e\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u786c\u4ef6\u8bef\u5dee\u5bf9\u5b9a\u4f4d\u7684\u5f71\u54cd\u3002  \n\u25c6 \u5229\u7528\u70b9\u4e91\u914d\u51c6\u7b97\u6cd5\u4ece\u8fde\u7eed\u8f68\u8ff9\u89c6\u89d2\u7684\u70b9\u4e91\u6570\u636e\u4e2d\u4f30\u8ba1\u7ec8\u7aef\u4f4d\u59ff\u53d8\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u7ec8\u7aef\u5b9a\u4f4d\u3002  \n\u25c6 \u5f15\u5165\u95ed\u73af\u68c0\u6d4b\u548c\u4f4d\u59ff\u56fe\u4f18\u5316\u6280\u672f\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u611f\u77e5\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7ec8\u7aef\u5b9a\u4f4d\u548c\u8be6\u7ec6\u7684\u65e0\u7ebf\u7535\u5730\u56fe\u91cd\u5efa\u3002  \n\u25c6 \u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4e3a5G NR\u5728SLAM\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8df5\u652f\u6301\u3002|\n",
    "2507.04321": "|2025-07-06|Lidar Variability: A Novel Dataset and Comparative Study of Solid-State and Spinning Lidars|Doumegna Mawuto Koudjo Felix\u7b49|[2507.04321](http://arxiv.org/pdf/2507.04321)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u5305\u542b\u7a79\u9876\u5f0f\u56fa\u6001\u6fc0\u5149\u96f7\u8fbe\uff08\u5982Livox Mid-360\uff09\u4e0e\u5176\u4ed6\u56fa\u6001\u53ca\u65cb\u8f6c\u5f0f\u6fc0\u5149\u96f7\u8fbe\uff08\u5982Ouster\u7cfb\u5217\uff09\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u591a\u7c7b\u578b\u6fc0\u5149\u96f7\u8fbe\u5bf9\u6bd4\u7814\u7a76\u7684\u7a7a\u767d\u3002  \n\u25c6 \u9996\u6b21\u5728\u65e0IMU\u652f\u6301\u7684\u91cc\u7a0b\u8ba1\u573a\u666f\u4e0b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4f4e\u6210\u672c\u56fa\u6001\u6fc0\u5149\u96f7\u8fbe\uff08Livox Avia/Mid-360\uff09\u4e0e\u9ad8\u7aef\u65cb\u8f6c\u5f0f\u6fc0\u5149\u96f7\u8fbe\u7684\u6027\u80fd\u5dee\u5f02\u3002  \n\u25c6 \u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u5bf9\u4e3b\u6d41SLAM\u7b97\u6cd5\u8fdb\u884c\u4e86\u8de8\u4f20\u611f\u5668\u5e73\u53f0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u5f02\u6784\u6fc0\u5149\u96f7\u8fbe\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u53c2\u8003\u3002  \n\u25c6 \u9488\u5bf9\u70b9\u4e91\u914d\u51c6\u6280\u672f\uff0c\u901a\u8fc7\u5ba4\u5185\u5916\u5b9e\u6d4b\u6570\u636e\u5b9a\u91cf\u6bd4\u8f83\u4e86\u70b9\u5bf9\u70b9\u3001\u70b9\u5bf9\u5e73\u9762\u53ca\u6df7\u5408\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\u3002  \n\u25c6 \u7814\u7a76\u7ed3\u679c\u4e3aSLAM\u548c3D\u91cd\u5efa\u9886\u57df\u5728\u4f4e\u6210\u672c\u56fa\u6001\u6fc0\u5149\u96f7\u8fbe\uff08\u5c24\u5176\u662f\u7a79\u9876\u5f0f\u8bbe\u8ba1\uff09\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u4e0e\u65b9\u6cd5\u6307\u5bfc\u3002  \n\u25c6 \u6570\u636e\u96c6\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u4e3a\u672a\u6765\u5f02\u6784\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u7684\u7b97\u6cd5\u5f00\u53d1\u4e0e\u6027\u80fd\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002|\n",
    "2507.04004": "|2025-07-09|Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM|Xiaolei Lang\u7b49|[2507.04004](http://arxiv.org/pdf/2507.04004)|\u65e0|\u25c6 \u9996\u6b21\u63d0\u51fa\u7ed3\u5408LiDAR-\u60ef\u6027-\u76f8\u673a\u76843D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff0c\u540c\u6b65\u4f18\u5316\u89c6\u89c9\u8d28\u91cf\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f3D\u9ad8\u65af\u5730\u56fe\u7684\u5b9e\u65f6\u6784\u5efa\u4e0eRGB/\u6df1\u5ea6\u6e32\u67d3\u3002  \n\u25c6 \u9488\u5bf9LiDAR\u8986\u76d6\u4e0d\u8db3\u533a\u57df\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u96f6\u6837\u672c\u6df1\u5ea6\u6a21\u578b\uff0c\u878d\u5408RGB\u5916\u89c2\u7ebf\u7d22\u4e0e\u7a00\u758fLiDAR\u6570\u636e\u751f\u6210\u7a20\u5bc6\u6df1\u5ea6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758fLiDAR\u4f20\u611f\u5668\u7684\u573a\u666f\u9002\u7528\u6027\u3002  \n\u25c6 \u5229\u7528\u9ad8\u7cbe\u5ea6\u7a00\u758fLiDAR\u6df1\u5ea6\u76d1\u7763\u9ad8\u65af\u5730\u56fe\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236CUDA\u52a0\u901f\u7b56\u7565\u63d0\u5347\u6548\u7387\uff0c\u589e\u5f3a\u51e0\u4f55\u51c6\u786e\u6027\u3002  \n\u25c6 \u521b\u65b0\u5730\u5c06\u589e\u91cf\u91cd\u5efa\u7684\u9ad8\u65af\u5730\u56fe\u5149\u5ea6\u7ea6\u675f\u878d\u5165\u8fde\u7eed\u65f6\u95f4\u56e0\u5b50\u56fe\u4f18\u5316\uff0c\u5728LiDAR\u6027\u80fd\u9000\u5316\u65f6\u63d0\u5347\u4f4d\u59ff\u4f30\u8ba1\u9c81\u68d2\u6027\u3002  \n\u25c6 \u6269\u5c55\u7cfb\u7edf\u529f\u80fd\u81f3\u4e0b\u6e38\u5e94\u7528\uff08\u5982\u89c6\u9891\u5e27\u63d2\u503c\u4e0e\u5feb\u901f3D\u7f51\u683c\u63d0\u53d6\uff09\uff0c\u5e76\u6784\u5efa\u5305\u542b\u771f\u503c\u4f4d\u59ff\u3001\u6df1\u5ea6\u56fe\u548c\u5916\u63a8\u8f68\u8ff9\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u652f\u6301\u4e25\u683c\u8bc4\u4f30\u3002  \n\u25c6 \u5728\u516c\u5f00\u4e0e\u81ea\u91c7\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u7cfb\u7edf\u5bf9\u591a\u79cd\u5bc6\u5ea6LiDAR\u7684\u4f18\u8d8a\u6027\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u3002|\n",
    "2507.03737": "|2025-07-04|Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps|Chong Cheng\u7b49|[2507.03737](http://arxiv.org/pdf/2507.03737)|\u65e0|\u25c6 \u63d0\u51faS3PO-GS\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8eRGB\u5355\u76ee\u76f8\u673a\u7684\u6237\u5916\u5168\u5c40\u5c3a\u5ea6\u4e00\u81f43D\u9ad8\u65af\u70b9\u5efa\u56feSLAM\u7cfb\u7edf\u3002  \n\u25c6 \u8bbe\u8ba1\u81ea\u4e00\u81f4\u8ddf\u8e2a\u6a21\u5757\uff0c\u4ee53D\u9ad8\u65af\u70b9\u56fe\u4e3a\u951a\u70b9\uff0c\u907f\u514d\u7d2f\u79ef\u5c3a\u5ea6\u6f02\u79fb\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u9c81\u68d2\u7684\u76f8\u673a\u8ddf\u8e2a\uff08\u8fed\u4ee3\u6b21\u6570\u66f4\u5c11\uff09\u3002  \n\u25c6 \u521b\u65b0\u6027\u63d0\u51fa\u57fa\u4e8e\u5206\u5757\u7684\u52a8\u6001\u70b9\u56fe\u5efa\u56fe\u6a21\u5757\uff0c\u5f15\u5165\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u7684\u540c\u65f6\u89c4\u907f\u5c3a\u5ea6\u6b67\u4e49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u6237\u5916\u573a\u666f\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6 \u901a\u8fc7\u878d\u5408\u51e0\u4f55\u5148\u9a8c\u4e0e3DGS\u6e32\u67d3\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6237\u5916\u573a\u666f\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\u6216\u4f9d\u8d56\u72ec\u7acb\u8ddf\u8e2a\u6a21\u5757\u5bfc\u81f4\u7684\u5c3a\u5ea6\u6f02\u79fb\u95ee\u9898\u3002  \n\u25c6 \u5728Waymo\u3001KITTI\u548cDL3DV\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u4e0a\u5747\u8d85\u8d8a\u73b0\u67093DGS SLAM\u65b9\u6cd5\u3002|\n",
    "2507.05718": "|2025-07-08|Cooperative Mapping, Localization, and Beam Management via Multi-Modal SLAM in ISAC Systems|Hang Que\u7b49|[2507.05718](http://arxiv.org/pdf/2507.05718)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001SLAM\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u534f\u540c\u591a\u7528\u6237SLAM\u5728ISAC\u7cfb\u7edf\u4e2d\u7684\u7406\u8bba\u5efa\u6a21\u548c\u901a\u4fe1\u5c42\u96c6\u6210\u4e0d\u8db3\u7684\u95ee\u9898\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f30\u8ba1\u7684\u534f\u540c\u591a\u7528\u6237SLAM\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u7b97\u6cd5\uff0c\u5728\u52a8\u6001\u5f02\u6784\u611f\u77e5\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u65e0\u7ebf\u7535\u5730\u56fe\u6784\u5efa\u3002  \n\u25c6 \u5f15\u5165\u591a\u6a21\u6001\u5b9a\u4f4d\u7b56\u7565\uff0c\u901a\u8fc7\u8bef\u5dee\u611f\u77e5\u6a21\u578b\u878d\u5408SLAM\u7ed3\u679c\u3001\u6444\u50cf\u5934\u591a\u76ee\u6807\u8ddf\u8e2a\u548cIMU\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u7528\u6237\u573a\u666f\u4e0b\u7684UE\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u611f\u77e5\u8f85\u52a9\u7684\u6ce2\u675f\u7ba1\u7406\u65b9\u6848\uff0c\u5229\u7528\u5168\u5c40\u65e0\u7ebf\u7535\u5730\u56fe\u548c\u5b9a\u4f4d\u6570\u636e\u751f\u6210UE\u7279\u5b9a\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u4f18\u5316\u6ce2\u675f\u9009\u62e9\uff0c\u964d\u4f4e\u7528\u6237\u95f4\u5e72\u6270\u5e76\u63d0\u5347\u4e0b\u884c\u9891\u8c31\u6548\u7387\u3002  \n\u25c6 \u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5c06\u65e0\u7ebf\u7535\u5730\u56fe\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe60%\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u63d0\u9ad837.5%\uff0c\u5728\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002|\n",
    "2507.06397": "|2025-07-08|Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System|Michalis Chatzispyrou\u7b49|[2507.06397](http://arxiv.org/pdf/2507.06397)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u6c34\u4e0b\u6d1e\u7a74\u6d4b\u7ed8\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u4e8e\u4f5b\u7f57\u91cc\u8fbe\u5ddeGinnie Springs\u7684Devil's Eye\u6d1e\u7a74\u7cfb\u7edf\u3002  \n\n\u25c6 \u4f7f\u7528\u5ec9\u4ef7\u8fd0\u52a8\u76f8\u673a\u7ed3\u5408\u6f5c\u6c34\u7535\u8111\uff0c\u5b9e\u73b0\u4e86\u6c34\u4e0b\u6d1e\u7a74\u8f68\u8ff9\u4f30\u8ba1\u548c\u7a00\u758f\u70b9\u4e91\u91cd\u5efa\uff0c\u964d\u4f4e\u4e86\u6d4b\u7ed8\u6210\u672c\u3002  \n\u25c6 \u901a\u8fc7\u6f5c\u6c34\u7535\u8111\u6570\u636e\u589e\u5f3a\u4e86\u89c6\u89c9/\u60ef\u6027\u6846\u67b6\uff08SVIn2\uff09\uff0c\u5b9e\u73b0\u4e86Z\u8f74\u7ef4\u5ea6\u53ca\u6a2a\u6eda/\u4fef\u4ef0\u89d2\u5ea6\u7684\u89c2\u6d4b\uff0c\u5f25\u8865\u4e86\u7eaf\u89c6\u89c9\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u5c06SVIn2\u751f\u6210\u7684\u5173\u952e\u5e27\u4e0e\u76f8\u673a\u4f4d\u59ff\u4f5c\u4e3a\u8f93\u5165\uff0c\u7ed3\u5408\u5168\u5c40\u4f18\u5316\u6846\u67b6COLMAP\uff0c\u91cd\u5efa\u4e86\u5c40\u90e8\u533a\u57df\u7684\u9ad8\u5bc6\u5ea6\u4e09\u7ef4\u6a21\u578b\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u6d1e\u7a74\u901a\u9053\u7684\u4e00\u7ef4\u62bd\u8c61\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u5747\u8f68\u8ff9\u4e0e\u8fb9\u754c\uff08\u4e0a\u4e0b\u5de6\u53f3\uff09\u63cf\u8ff0\u6d1e\u7a74\u8f6e\u5ed3\u3002  \n\u25c6 \u91c7\u7528MNemo V2\u4eea\u5668\u8fdb\u884c\u4eba\u5de5\u6d4b\u7ed8\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u8868\u660e\u8fd0\u52a8\u76f8\u673a\u8db3\u4ee5\u6784\u5efa\u6d1e\u7a74\u5730\u56fe\u7684\u57fa\u672c\u8981\u7d20\u3002  \n\u25c6 \u901a\u8fc7VI-SLAM\u4e0e\u5168\u5c40\u4f18\u5316\u6846\u67b6\u7684\u534f\u540c\uff0c\u5b9e\u73b0\u4e86\u9009\u5b9a\u533a\u57df\u7684\u903c\u771f\u5bc6\u96c6\u4e09\u7ef4\u91cd\u5efa\uff0c\u4e3a\u6c34\u6587\u5730\u8d28\u548c\u8003\u53e4\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002|\n",
    "2507.07903": "|2025-07-10|Hardware-Aware Feature Extraction Quantisation for Real-Time Visual Odometry on FPGA Platforms|Mateusz Wasala\u7b49|[2507.07903](http://arxiv.org/pdf/2507.07903)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5316SuperPoint\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5d4c\u5165\u5f0f\u65e0\u76d1\u7763\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u7684\u7279\u5f81\u70b9\u68c0\u6d4b\u4e0e\u63cf\u8ff0\u3002  \n\u25c6 \u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u6a21\u578b\u91cf\u5316\u6280\u672f\uff08\u4f7f\u7528Brevitas\u5e93\u548cFINN\u6846\u67b6\uff09\uff0c\u5728\u4fdd\u8bc1\u9ad8\u68c0\u6d4b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002  \n\u25c6 \u5728AMD/Xilinx Zynq UltraScale+ FPGA\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u5355\u5143\uff08DPU\uff09\u4f18\u5316\u6027\u80fd\u3002  \n\u25c6 \u5b9e\u73b0\u4e86640\u00d7480\u5206\u8fa8\u7387\u56fe\u50cf54\u5e27/\u79d2\u7684\u5904\u7406\u901f\u5ea6\uff0c\u4f18\u4e8e\u5f53\u524d\u540c\u7c7b\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u5728TUM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4e0d\u540c\u91cf\u5316\u6280\u672f\u5bf9\u6a21\u578b\u7cbe\u5ea6\u4e0e\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\uff08\u5982\u79fb\u52a8/\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff09\u63d0\u4f9b\u4e86\u5b9e\u7528\u4f18\u5316\u65b9\u6848\u3002|\n",
    "2507.07752": "|2025-07-10|IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End for Visual SLAM in Challenging Environments|Thanh Nguyen Canh\u7b49|[2507.07752](http://arxiv.org/pdf/2507.07752)|\u65e0|\u25c6 IRAF-SLAM\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u590d\u6742\u5149\u7167\u73af\u5883\u7684\u89c6\u89c9SLAM\u524d\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u56fe\u50cf\u589e\u5f3a\u65b9\u6848\uff0c\u52a8\u6001\u9884\u5904\u7406\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u6539\u5584\u7279\u5f81\u63d0\u53d6\u57fa\u7840\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u57fa\u4e8e\u56fe\u50cf\u71b5\u3001\u50cf\u7d20\u5f3a\u5ea6\u548c\u68af\u5ea6\u5206\u6790\u7684\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u673a\u5236\uff0c\u6839\u636e\u73af\u5883\u52a8\u6001\u8c03\u6574\u68c0\u6d4b\u7075\u654f\u5ea6\u3002  \n\u25c6 \u63d0\u51fa\u65b0\u578b\u7279\u5f81\u7b5b\u9009\u7b56\u7565\uff0c\u7ed3\u5408\u5bc6\u5ea6\u5206\u5e03\u5206\u6790\u548c\u5149\u7167\u5f71\u54cd\u56e0\u5b50\uff0c\u6709\u6548\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7279\u5f81\u70b9\u3002  \n\u25c6 \u5728TUM-VI\u548cEuRoC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ddf\u8e2a\u5931\u8d25\u7387\uff0c\u5e76\u5728\u6076\u52a3\u5149\u7167\u4e0b\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8f68\u8ff9\u7cbe\u5ea6\u3002  \n\u25c6 \u6574\u4e2a\u7cfb\u7edf\u5728\u63d0\u5347\u9c81\u68d2\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.07142": "|2025-07-09|g2o vs. Ceres: Optimizing Scan Matching in Cartographer SLAM|Quanjie Qiu\u7b49|[2507.07142](http://arxiv.org/pdf/2507.07142)|\u65e0|\u25c6 \u9996\u6b21\u5728Cartographer\u6846\u67b6\u4e2d\u5bf9g2o\u548cCeres\u4e24\u79cd\u4f18\u5316\u5668\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u626b\u63cf\u5339\u914d\u6027\u80fd\u5bf9\u6bd4\u5206\u6790\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Ceres\u4f5c\u4e3aCartographer\u9ed8\u8ba4\u6c42\u89e3\u5668\u5728\u901f\u5ea6\u3001\u6536\u655b\u6548\u7387\u548c\u5730\u56fe\u6e05\u6670\u5ea6\u4e0a\u7684\u5168\u9762\u4f18\u52bf\u3002  \n\u25c6 \u53d1\u73b0Ceres\u5728\u771f\u5b9e\u573a\u666f\uff08AgileX LIMO\u673a\u5668\u4eba\uff09\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u6240\u9700\u8fed\u4ee3\u6b21\u6570\u66f4\u5c11\u4e14\u6536\u655b\u66f4\u5feb\u3002  \n\u25c6 \u63ed\u793a\u4e86g2o\u5728\u5c40\u90e8\u969c\u788d\u7269\u68c0\u6d4b\u65b9\u9762\u7684\u7279\u6b8a\u4f18\u52bf\uff0c\u4e3a\u5176\u5728\u7279\u5b9a\u573a\u666f\u7684\u5e94\u7528\u4ef7\u503c\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002  \n\u25c6 \u4e3aSLAM\u7cfb\u7edf\u4f18\u5316\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u53c2\u8003\uff0c\u6307\u51fa\u4e0d\u540c\u4f18\u5316\u5668\u7684\u9002\u7528\u573a\u666f\u5dee\u5f02\u3002  \n\u25c6 \u901a\u8fc7\u5b9a\u91cf\u5316\u6307\u6807\uff08\u5982\u8fed\u4ee3\u6b21\u6570\u3001\u6536\u655b\u65f6\u95f4\uff09\u5bf9\u6bd4\uff0c\u6df1\u5316\u4e86\u5bf9\u4e24\u79cd\u4f18\u5316\u5668\u6027\u80fd\u7279\u5f81\u7684\u7406\u89e3\u3002|\n",
    "2507.08364": "|2025-07-11|Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework|Deteng Zhang\u7b49|[2507.08364](http://arxiv.org/pdf/2507.08364)|\u65e0|\u25c6 \u63d0\u51faM3DGR\u6570\u636e\u96c6\uff1a\u9996\u4e2a\u5305\u542b\u89c6\u89c9\u5e72\u6270\u3001\u6fc0\u5149\u96f7\u8fbe\u9000\u5316\u3001\u8f6e\u5f0f\u6253\u6ed1\u548cGNSS\u5931\u6548\u7b49\u7cfb\u7edf\u6027\u9000\u5316\u573a\u666f\u7684\u591a\u4f20\u611f\u5668\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\u3002  \n\u25c6 \u5bf940\u79cdSLAM\u7cfb\u7edf\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u6d4b\uff1a\u9996\u6b21\u5728\u591a\u6837\u5316\u9000\u5316\u6761\u4ef6\u4e0b\u5168\u9762\u5206\u6790\u73b0\u6709\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6027\u80fd\u74f6\u9888\u3002  \n\u25c6 \u5f00\u53d1Ground-Fusion++\u6846\u67b6\uff1a\u521b\u65b0\u6027\u5730\u878d\u5408GNSS\u3001RGB-D\u3001\u6fc0\u5149\u96f7\u8fbe\u3001IMU\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u591a\u4f20\u611f\u5668\u81ea\u9002\u5e94\u9009\u62e9\u3002  \n\u25c6 \u89e3\u51b3\u4f20\u611f\u5668\u52a8\u6001\u9002\u914d\u95ee\u9898\uff1a\u63d0\u51fa\u73af\u5883\u53d8\u5316\u4e0b\u7684\u4f20\u611f\u5668\u4f18\u9009\u7b56\u7565\uff0c\u7a81\u7834\u4f20\u7edf\u6846\u67b6\u4ec5\u56fa\u5b9a\u878d\u5408\u5c11\u6570\u4f20\u611f\u5668\u7684\u5c40\u9650\u3002  \n\u25c6 \u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\uff1a\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u5e73\u53f0\u548c\u6027\u80fd\u5bf9\u6bd4\u57fa\u51c6\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002|\n",
    "2507.12273": "|2025-07-17|Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot|Luca Garello\u7b49|[2507.12273](http://arxiv.org/pdf/2507.12273)|\u65e0|\u25c6 \u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u6b3e\u540d\u4e3aAlter-Ego\u7684\u81ea\u4e3b\u535a\u7269\u9986\u5bfc\u89c8\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u5148\u8fdb\u5bfc\u822a\u4e0e\u4ea4\u4e92\u529f\u80fd\uff0c\u63d0\u5347\u53c2\u89c2\u4f53\u9a8c\u3002  \n\u25c6 \u9996\u6b21\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u535a\u7269\u9986\u573a\u666f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u60c5\u5883\u611f\u77e5\u7684\u95ee\u7b54\u4ea4\u4e92\uff0c\u652f\u6301\u6e38\u5ba2\u4e0e\u673a\u5668\u4eba\u5c31\u5c55\u54c1\u5c55\u5f00\u5bf9\u8bdd\u3002  \n\u25c6 \u91c7\u7528\u9c81\u68d2\u7684SLAM\u6280\u672f\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u535a\u7269\u9986\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\uff0c\u5e76\u6839\u636e\u7528\u6237\u9700\u6c42\u52a8\u6001\u8c03\u6574\u5bfc\u89c8\u8def\u7ebf\u3002  \n\u25c6 \u901a\u8fc7\u771f\u5b9e\u535a\u7269\u9986\u73af\u5883\u4e0b\u7684\u7528\u6237\u7814\u7a76\uff0834\u540d\u53c2\u4e0e\u8005\uff09\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\uff0c\u7ed3\u5408\u5bf9\u8bdd\u8d28\u91cf\u5206\u6790\u4e0e\u95ee\u5377\u8c03\u67e5\u91cf\u5316\u8bc4\u4f30\u6548\u679c\u3002  \n\u25c6 \u63ed\u793a\u4e86AI\u9a71\u52a8\u673a\u5668\u4eba\u5728\u6587\u5316\u7a7a\u95f4\u4e2d\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5305\u62ec\u77e5\u8bc6\u83b7\u53d6\u7684\u4fc3\u8fdb\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6280\u672f\u5c40\u9650\u6027\u3002  \n\u25c6 \u4e3a\u516c\u5171\u670d\u52a1\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u517c\u5177\u6280\u672f\u521b\u65b0\u4e0e\u5b9e\u8bc1\u7814\u7a76\u7684\u8303\u4f8b\uff0c\u63a8\u52a8\u590d\u6742\u573a\u666f\u4e0b\u81ea\u4e3b\u7cfb\u7edf\u7684\u5e94\u7528\u63a2\u7d22\u3002|\n",
    "2507.12093": "|2025-07-16|Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards|David Rapado-Rincon\u7b49|[2507.12093](http://arxiv.org/pdf/2507.12093)|\u65e0|\u25c6 \u63d0\u51faTree-SLAM\uff0c\u4e00\u79cd\u4e13\u4e3a\u679c\u56ed\u73af\u5883\u8bbe\u8ba1\u7684\u8bed\u4e49SLAM\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edfSLAM\u5728\u6811\u6728\u91cd\u590d\u5916\u89c2\u4e0b\u6613\u6df7\u6dc6\u7684\u95ee\u9898\u3002  \n\u25c6 \u7ed3\u5408RGB-D\u56fe\u50cf\u548c\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\uff0c\u5b9e\u73b0\u6811\u5e72\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u7ea7\u8054\u56fe\u6570\u636e\u5173\u8054\u7b97\u6cd5\u8fdb\u884c\u6811\u5e72\u91cd\u8bc6\u522b\u3002  \n\u25c6 \u5c06\u91cd\u8bc6\u522b\u7684\u6811\u5e72\u4f5c\u4e3a\u5730\u6807\uff0c\u878d\u5408\u566a\u58f0GPS\u4fe1\u53f7\u3001\u91cc\u7a0b\u8ba1\u548c\u6811\u5e72\u89c2\u6d4b\u6570\u636e\uff0c\u6784\u5efa\u56e0\u5b50\u56fe\u6846\u67b6\uff0c\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u7cfb\u7edf\u5728GPS\u4fe1\u53f7\u4e0d\u53ef\u9760\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5355\u68f5\u6811\u7684\u5730\u7406\u5b9a\u4f4d\u8bef\u5dee\u4f4e\u81f318\u5398\u7c73\uff0c\u4f4e\u4e8e\u79cd\u690d\u95f4\u8ddd\u768420%\u3002  \n\u25c6 \u5728\u82f9\u679c\u56ed\u548c\u68a8\u56ed\u7684\u4e0d\u540c\u5b63\u8282\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u76ee\u6807\u64cd\u4f5c\u548c\u5355\u6811\u76d1\u6d4b\u4efb\u52a1\u3002|\n",
    "2507.13145": "|2025-07-17|DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model|Maulana Bisyir Azhari\u7b49|[2507.13145](http://arxiv.org/pdf/2507.13145)|\u65e0|\u25c6 \u63d0\u51faDINO-VO\u7cfb\u7edf\uff0c\u9996\u6b21\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578bDINOv2\u7684\u9c81\u68d2\u8bed\u4e49\u7279\u5f81\u5e94\u7528\u4e8e\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff08VO\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5b66\u4e60\u578bVO\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u9488\u5bf9DINOv2\u7c97\u7c92\u5ea6\u7279\u5f81\u7684\u663e\u8457\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u514b\u670d\u4e86\u57fa\u7840\u6a21\u578b\u7279\u5f81\u5728VO\u4efb\u52a1\u4e2d\u7c92\u5ea6\u4e0d\u8db3\u7684\u96c6\u6210\u96be\u9898\u3002  \n\u25c6 \u7ed3\u5408DINOv2\u7684\u8bed\u4e49\u7279\u5f81\u4e0e\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7279\u5f81\uff0c\u751f\u6210\u66f4\u5177\u5c40\u90e8\u5316\u80fd\u529b\u7684\u6df7\u5408\u7279\u5f81\u8868\u793a\uff0c\u63d0\u5347\u4e86\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u3002  \n\u25c6 \u91c7\u7528\u57fa\u4e8eTransformer\u7684\u5339\u914d\u5668\u548c\u53ef\u5fae\u5206\u4f4d\u59ff\u4f30\u8ba1\u5c42\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u4f18\u5316\u7279\u5f81\u5339\u914d\u4e0e\u8fd0\u52a8\u4f30\u8ba1\u6027\u80fd\u3002  \n\u25c6 \u5728TartanAir\u3001KITTI\u7b49\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4f20\u7edf\u5e27\u95f4VO\u65b9\u6cd5\uff08\u5982SuperPoint\uff09\uff0c\u5e76\u5728\u5ba4\u5916\u9a7e\u9a76\u573a\u666f\u4e2d\u4e0e\u89c6\u89c9SLAM\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u630172 FPS\u7684\u9ad8\u6548\u5b9e\u65f6\u6027\uff08GPU\u5185\u5b58<1GB\uff09\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eDINOv2\u7279\u5f81\u7ecf\u6539\u8fdb\u540e\uff0c\u5176\u63cf\u8ff0\u5b50\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u7c97\u7c92\u5ea6\u7279\u5f81\uff0c\u5c24\u5176\u5728\u5149\u7167\u53d8\u5316\u3001\u52a8\u6001\u7269\u4f53\u7b49\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002|\n",
    "2507.12920": "|2025-07-17|MoCap2GT: A High-Precision Ground Truth Estimator for SLAM Benchmarking Based on Motion Capture and IMU Fusion|Zichao Shu\u7b49|[2507.12920](http://arxiv.org/pdf/2507.12920)|\u65e0|MoCap2GT\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u4f5c\u6355\u6349\u548cIMU\u878d\u5408\u7684\u9ad8\u7cbe\u5ea6SLAM\u57fa\u51c6\u6d4b\u8bd5\u771f\u503c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6838\u5fc3\u8d21\u732e\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u878d\u5408MoCap\u6570\u636e\u548c\u8bbe\u5907IMU\u6d4b\u91cf\u503c\uff0c\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u771f\u503c\u7684\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u65f6\u7a7a\u6807\u5b9a\u8bef\u5dee\u548cMoCap\u6296\u52a8\u5bfc\u81f4\u7684\u7cbe\u5ea6\u9650\u5236\u95ee\u9898\u3002\n\n\u25c6 \u8bbe\u8ba1\u9c81\u68d2\u7684\u72b6\u6001\u521d\u59cb\u5316\u5668\uff0c\u786e\u4fdd\u5168\u5c40\u6536\u655b\u6027\uff0c\u907f\u514d\u4e86\u4f18\u5316\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u3002\n\n\u25c6 \u5f15\u5165SE(3)\u6d41\u5f62\u4e0a\u7684\u9ad8\u9636B\u6837\u6761\u4f4d\u59ff\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u53ef\u53d8\u65f6\u95f4\u504f\u79fb\u5efa\u6a21\uff0c\u6709\u6548\u5904\u7406MoCap\u56e0\u7d20\uff0c\u63d0\u9ad8\u4e86\u8f68\u8ff9\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002\n\n\u25c6 \u63d0\u51fa\u9000\u5316\u611f\u77e5\u7684\u6d4b\u91cf\u5254\u9664\u7b56\u7565\uff0c\u80fd\u591f\u8bc6\u522b\u5e76\u5254\u9664\u4e0d\u53ef\u9760\u7684\u6d4b\u91cf\u6570\u636e\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u3002\n\n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoCap2GT\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aSLAM\u7b97\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u771f\u503c\u3002\u8be5\u65b9\u6cd5\u5f00\u6e90\u53ef\u7528\uff0c\u5c06\u4fc3\u8fdbSLAM\u7814\u7a76\u793e\u533a\u7684\u53d1\u5c55\u3002|\n",
    "2507.15716": "|2025-07-21|DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models|Ziyu Wan\u7b49|[2507.15716](http://arxiv.org/pdf/2507.15716)|\u65e0|\u25c6 DiffPF\u9996\u6b21\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u878d\u5165\u7c92\u5b50\u6ee4\u6ce2\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u540e\u9a8c\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002  \n\u25c6 \u76f8\u6bd4\u4f20\u7edf\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\u4f9d\u8d56\u9884\u5b9a\u4e49\u6216\u4f4e\u5bb9\u91cf\u63d0\u8bae\u5206\u5e03\uff0cDiffPF\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7075\u6d3b\u7684\u91c7\u6837\u5668\uff0c\u76f4\u63a5\u751f\u6210\u7b49\u6743\u7c92\u5b50\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u590d\u6742\u3001\u9ad8\u7ef4\u3001\u591a\u6a21\u6001\u7684\u6ee4\u6ce2\u5206\u5e03\u4e2d\u8fdb\u884c\u7cbe\u786e\u91c7\u6837\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u5206\u5e03\u4e0b\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u5728\u5168\u5c40\u5b9a\u4f4d\u548cKITTI\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7b49\u4efb\u52a1\u4e2d\uff0cDiffPF\u5206\u522b\u4ee582.8%\u548c26%\u7684\u7cbe\u5ea6\u4f18\u52bf\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u53ef\u5fae\u5206\u6ee4\u6ce2\u5668\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cDiffPF\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u573a\u666f\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u52a8\u6001\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002|\n",
    "2507.15496": "|2025-07-21|Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images|JunYing Huang\u7b49|[2507.15496](http://arxiv.org/pdf/2507.15496)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684LiDAR-\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u878d\u5408\u7a00\u758fLiDAR\u70b9\u4e91\u548c\u56fe\u50cf\u6570\u636e\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u6df1\u5ea6\u8865\u5168\u6280\u672f\u751f\u6210\u7a20\u5bc6\u6df1\u5ea6\u56fe\uff0c\u4e3a\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u51e0\u4f55\u7ea6\u675f\u4fe1\u606f\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u5e26\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u751f\u6210\u6df1\u5ea6\u611f\u77e5\u7684\u7279\u5f81\u8868\u793a\u3002  \n\u25c6 \u91c7\u7528\u7a20\u5bc6\u6df1\u5ea6\u4fe1\u606f\u4f18\u5316\u5149\u6d41\u4f30\u8ba1\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u906e\u6321\u533a\u57df\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u5206\u5c42\u4f4d\u59ff\u4f18\u5316\u6a21\u5757\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8fd0\u52a8\u4f30\u8ba1\u63d0\u5347\u52a8\u6001\u73af\u5883\u548c\u5c3a\u5ea6\u6a21\u7cca\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728KITTI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u4e0e\u5f53\u524d\u6700\u4f18\u89c6\u89c9/LiDAR\u91cc\u7a0b\u8ba1\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002|\n",
    "2507.15474": "|2025-07-21|All-UWB SLAM Using UWB Radar and UWB AOA|Charith Premachandra\u7b49|[2507.15474](http://arxiv.org/pdf/2507.15474)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408UWB\u96f7\u8fbe\u548cUWB\u5230\u8fbe\u89d2\uff08AOA\uff09\u6d4b\u91cf\u7684\u65b0\u578bSLAM\u65b9\u6cd5\uff0c\u7528\u4e8e\u89c6\u89c9\u53d7\u9650\u4e14\u7279\u5f81\u7a00\u7f3a\u7684\u73af\u5883\u3002  \n\u25c6 \u901a\u8fc7\u52a8\u6001\u90e8\u7f72UWB\u951a\u70b9-\u6807\u7b7e\u5355\u5143\uff0c\u5728\u73af\u5883\u7279\u5f81\u4e0d\u8db3\u7684\u533a\u57df\u8865\u5145AOA\u6d4b\u91cf\u6570\u636e\uff0c\u63d0\u5347\u4e86SLAM\u7684\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u73b0\u6709UWB\u96f7\u8fbeSLAM\u65b9\u6cd5\u4f9d\u8d56\u73af\u5883\u7279\u5f81\u6570\u91cf\u7684\u5c40\u9650\u6027\uff0c\u6269\u5c55\u4e86\u5176\u5728\u65e0\u7279\u5f81\u73af\u5883\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002  \n\u25c6 \u8be6\u7ec6\u5206\u6790\u4e86UWB AOA\u6d4b\u91cf\u5355\u5143\u7684\u5e38\u89c1\u7ea6\u675f\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u53d7\u9650\u4e14\u7279\u5f81\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u4ecd\u80fd\u6709\u6548\u5b9e\u73b0SLAM\uff0c\u4e3a\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2507.15321": "|2025-07-21|BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?|Zhenyu Li\u7b49|[2507.15321](http://arxiv.org/pdf/2507.15321)|\u65e0|\u25c6\u63d0\u51faBenchDepth\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e0b\u6e38\u4ee3\u7406\u4efb\u52a1\uff08\u6df1\u5ea6\u8865\u5168\u3001\u7acb\u4f53\u5339\u914d\u3001\u5355\u76ee3D\u573a\u666f\u91cd\u5efa\u3001SLAM\u548c\u89c6\u89c9\u8bed\u8a00\u7a7a\u95f4\u7406\u89e3\uff09\u8bc4\u4f30\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff08DFMs\uff09\uff0c\u7a81\u7834\u4f20\u7edf\u8bc4\u4f30\u5c40\u9650\u3002  \n\u25c6\u6452\u5f03\u4f9d\u8d56\u5bf9\u9f50\u6307\u6807\u7684\u56fa\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u8bc4\u4f30\u4e2d\u56e0\u5bf9\u9f50\u504f\u5dee\u3001\u6df1\u5ea6\u8868\u793a\u504f\u597d\u5bfc\u81f4\u7684\u4e0d\u516c\u5e73\u6bd4\u8f83\u95ee\u9898\u3002  \n\u25c6\u9996\u6b21\u4ece\u5b9e\u9645\u5e94\u7528\u6548\u7528\u89d2\u5ea6\u8bc4\u4f30DFMs\uff0c\u5f3a\u8c03\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u800c\u975e\u5355\u7eaf\u6307\u6807\u5206\u6570\u3002  \n\u25c6\u7cfb\u7edf\u5730\u5bf98\u79cd\u524d\u6cbfDFMs\u8fdb\u884c\u6a2a\u5411\u5bf9\u6bd4\uff0c\u63ed\u793a\u5173\u952e\u53d1\u73b0\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002  \n\u25c6\u63a8\u52a8\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u8bc4\u4f30\u6807\u51c6\u9769\u65b0\uff0c\u5f15\u53d1\u793e\u533a\u5bf9\u8bc4\u4f30\u6700\u4f73\u5b9e\u8df5\u7684\u8ba8\u8bba\uff0c\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u53d1\u5c55\u3002|\n",
    "2507.15109": "|2025-07-20|LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM|Mohammad-Maher Nakshbandi\u7b49|[2507.15109](http://arxiv.org/pdf/2507.15109)|\u65e0|\u25c6 \u63d0\u51faLoopNet\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u5927\u89c4\u6a21SLAM\u4e2d\u7684\u95ed\u73af\u68c0\u6d4b\u95ee\u9898\uff0c\u517c\u987e\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u6027\u9700\u6c42\u3002  \n\u25c6 \u91c7\u7528\u6539\u8fdb\u7684ResNet\u591a\u4efb\u52a1\u67b6\u6784\uff0c\u652f\u6301\u52a8\u6001\u89c6\u89c9\u6570\u636e\u96c6\u7684\u5728\u7ebf\u91cd\u8bad\u7ec3\uff0c\u5e76\u9488\u5bf9\u5d4c\u5165\u5f0f\u8bbe\u5907\u8fdb\u884c\u4f18\u5316\uff0c\u9002\u5e94\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u3002  \n\u25c6 \u521b\u65b0\u6027\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u5728\u7ebf\u8bad\u7ec3\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u65b0\u73af\u5883\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u3002  \n\u25c6 \u9996\u6b21\u5728\u95ed\u73af\u68c0\u6d4b\u4e2d\u540c\u65f6\u8f93\u51fa\u573a\u666f\u7d22\u5f15\u548c\u9884\u6d4b\u8d28\u91cf\u8bc4\u4f30\uff0c\u589e\u5f3a\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u907f\u514d\u8bef\u5339\u914d\u3002  \n\u25c6 \u5229\u7528DISK\u63cf\u8ff0\u7b26\u66ff\u4ee3\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u6216\u7eaf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u5149\u7167\u3001\u89c6\u89d2\u53d8\u5316\u7b49\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002  \n\u25c6 \u5f00\u6e90\u4e86\u65b0\u578b\u95ed\u73af\u68c0\u6d4b\u6570\u636e\u96c6LoopDB\uff0c\u586b\u8865\u9886\u57df\u5185\u6807\u51c6\u5316\u8bc4\u4f30\u6570\u636e\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002|\n",
    "2507.14501": "|2025-07-19|Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey|Jiahui Zhang\u7b49|[2507.14501](http://arxiv.org/pdf/2507.14501)|\u65e0|\u25c6 \u7cfb\u7edf\u68b3\u7406\u4e86\u57fa\u4e8e\u524d\u9988\u5f0f\u6df1\u5ea6\u5b66\u4e60\u76843D\u91cd\u5efa\u4e0e\u89c6\u56fe\u5408\u6210\u6280\u672f\uff0c\u9996\u6b21\u63d0\u51fa\u6309\u8868\u793a\u67b6\u6784\uff08\u5982\u70b9\u4e91\u30013D\u9ad8\u65af\u6cfc\u6e85\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u7b49\uff09\u7684\u5206\u7c7b\u4f53\u7cfb\u3002  \n\u25c6 \u91cd\u70b9\u5206\u6790\u4e86\u65e0\u59ff\u6001\u91cd\u5efa\u3001\u52a8\u60013D\u91cd\u5efa\u30013D\u611f\u77e5\u56fe\u50cf/\u89c6\u9891\u5408\u6210\u7b49\u5173\u952e\u4efb\u52a1\uff0c\u62d3\u5c55\u4e86\u5728\u6570\u5b57\u4eba\u3001SLAM\u7b49\u9886\u57df\u7684\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u5bf9\u6bd4\u4f20\u7edf\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u7a81\u663e\u524d\u9988\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4e3aAR/VR\u7b49\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002  \n\u25c6 \u5168\u9762\u6c47\u603b\u4e86\u4e3b\u6d41\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u534f\u8bae\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u6807\u51c6\u5316\u8bc4\u6d4b\u5de5\u5177\u7684\u7efc\u8ff0\u7a7a\u767d\u3002  \n\u25c6 \u6307\u51fa\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3001\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u793a\u80fd\u529b\u5e73\u8861\u7b49\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002|\n",
    "2507.17406": "|2025-07-23|Physics-based Human Pose Estimation from a Single Moving RGB Camera|Ayce Idil Aytekin\u7b49|[2507.17406](http://arxiv.org/pdf/2507.17406)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u975e\u5408\u6210\u7684\u771f\u5b9e\u6570\u636e\u96c6MoviCam\uff0c\u5305\u542b\u52a8\u6001\u79fb\u52a8\u7684\u5355\u76eeRGB\u76f8\u673a\u8f68\u8ff9\u3001\u573a\u666f\u51e0\u4f55\u548c3D\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\uff0c\u5e76\u6807\u6ce8\u4e86\u4eba-\u573a\u666f\u63a5\u89e6\u4fe1\u606f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002  \n\u25c6 \u5f00\u53d1\u4e86PhysDynPose\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u573a\u666f\u51e0\u4f55\u548c\u7269\u7406\u7ea6\u675f\u6574\u5408\u5230\u57fa\u4e8e\u7269\u7406\u7684\u4eba\u4f53\u59ff\u6001\u8ddf\u8e2a\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u76f8\u673a\u548c\u975e\u5e73\u9762\u573a\u666f\u4e0b\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u3002  \n\u25c6 \u7ed3\u5408\u4e86\u5148\u8fdb\u8fd0\u52a8\u5b66\u4f30\u8ba1\u5668\u548c\u9c81\u68d2SLAM\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4e16\u754c\u5750\u6807\u7cfb\u4e0b\u4eba\u4f53\u59ff\u6001\u4e0e\u76f8\u673a\u8f68\u8ff9\u7684\u540c\u6b65\u6062\u590d\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u76f8\u673a\u5e26\u6765\u7684\u53c2\u8003\u7cfb\u6f02\u79fb\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u573a\u666f\u611f\u77e5\u7684\u7269\u7406\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u4fee\u6b63\u8fd0\u52a8\u5b66\u4f30\u8ba1\u7ed3\u679c\uff0c\u4f7f\u59ff\u6001\u4f30\u8ba1\u66f4\u7b26\u5408\u771f\u5b9e\u7269\u7406\u89c4\u5f8b\u3002  \n\u25c6 \u901a\u8fc7\u65b0\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u79fb\u52a8\u76f8\u673a\u548c\u975e\u5e73\u9762\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u672c\u65b9\u6cd5\u5728\u6b64\u6311\u6218\u6027\u573a\u666f\u4e2d\u4ecd\u80fd\u7a33\u5b9a\u8f93\u51fa\u4eba\u4f53\u4e0e\u76f8\u673a\u4f4d\u59ff\u3002  \n\u25c6 \u4e3a\u590d\u6742\u771f\u5b9e\u573a\u666f\uff08\u5982\u4e0d\u5e73\u5730\u9762\u3001\u52a8\u6001\u89c6\u89d2\uff09\u7684\u7269\u7406\u53ef\u4fe1\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.17312": "|2025-07-23|CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance|Peiqi Chen\u7b49|[2507.17312](http://arxiv.org/pdf/2507.17312)|\u65e0|\u25c6 \u63d0\u51faCasP\u65b0\u6d41\u7a0b\uff0c\u901a\u8fc7\u7ea7\u8054\u5bf9\u5e94\u5148\u9a8c\u5f15\u5bfc\u534a\u7a20\u5bc6\u7279\u5f81\u5339\u914d\uff0c\u6539\u8fdb\u4f20\u7edf\u5168\u5c40\u641c\u7d22\u65b9\u5f0f\uff0c\u63d0\u5347\u7cbe\u5ea6\u548c\u6548\u7387\u3002  \n\u25c6 \u5c06\u5339\u914d\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u6e10\u8fdb\u9636\u6bb5\uff0c\u4e2d\u95f4\u5f15\u5165\u57fa\u4e8e\u533a\u57df\u7684\u9009\u62e9\u6027\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u7279\u5f81\u533a\u5206\u5ea6\u3002  \n\u25c6 \u5728\u7b2c\u4e8c\u9636\u6bb5\u5c06\u641c\u7d22\u8303\u56f4\u9650\u5236\u5728\u7b2c\u4e00\u9636\u6bb5\u8bc6\u522b\u7684\u4e00\u5bf9\u591a\u5148\u9a8c\u533a\u57df\uff0c\u5b9e\u73b0\u4e00\u5bf9\u4e00\u5339\u914d\u7684\u7cbe\u786e\u5b9a\u4f4d\u3002  \n\u25c6 \u7ed3\u5408\u9ad8\u5c42\u7279\u5f81\u964d\u4f4e\u4f4e\u5c42\u7279\u5f81\u63d0\u53d6\u8ba1\u7b97\u6210\u672c\uff0c\u5206\u8fa8\u7387\u8d8a\u9ad8\u52a0\u901f\u6548\u679c\u8d8a\u663e\u8457\uff081152\u5206\u8fa8\u7387\u4e0b\u6bd4ELoFTR\u5feb2.2\u500d\uff09\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u51e0\u4f55\u4f30\u8ba1\uff08\u5c24\u5176\u662f\u8de8\u57df\u6cdb\u5316\uff09\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u9002\u7528\u4e8eSLAM\u3001\u65e0\u4eba\u673a\u7b49\u9ad8\u5b9e\u65f6\u6027\u9ad8\u9c81\u68d2\u6027\u573a\u666f\u3002|\n",
    "2507.18344": "|2025-07-24|G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM|Gyuhyeon Pak\u7b49|[2507.18344](http://arxiv.org/pdf/2507.18344)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u611f\u77e5\u7684\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff08G2S-ICP SLAM\uff09\uff0c\u901a\u8fc7\u5c06\u573a\u666f\u5143\u7d20\u8868\u793a\u4e3a\u5c40\u90e8\u5207\u5e73\u9762\u7ea6\u675f\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u548c\u5b9e\u65f6\u76f8\u673a\u4f4d\u59ff\u8ddf\u8e2a\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u5c40\u90e8\u8868\u9762\u5efa\u6a21\u4e3a\u4e0e\u51e0\u4f55\u5bf9\u9f50\u76842D\u9ad8\u65af\u5706\u76d8\uff0c\u76f8\u6bd4\u4f20\u7edf\u5404\u5411\u540c\u60273D\u692d\u7403\u8868\u793a\uff0c\u80fd\u66f4\u4e00\u81f4\u5730\u5904\u7406\u591a\u89c6\u89d2\u6df1\u5ea6\u4fe1\u606f\u3002  \n\u25c6 \u5c06\u8868\u9762\u5bf9\u9f50\u7684\u9ad8\u65af\u5706\u76d8\u5d4c\u5165\u5e7f\u4e49ICP\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5404\u5411\u5f02\u6027\u534f\u65b9\u5dee\u5148\u9a8c\uff0c\u5728\u4e0d\u6539\u53d8\u914d\u51c6\u516c\u5f0f\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\u3002  \n\u25c6 \u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316\u5149\u5ea6\u3001\u6df1\u5ea6\u548c\u6cd5\u5411\u4e00\u81f4\u6027\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u91cd\u5efa\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u3002  \n\u25c6 \u7cfb\u7edf\u5728Replica\u548cTUM-RGBD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u91cd\u5efa\u5b8c\u6574\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709SLAM\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u3002|\n",
    "2507.19474": "|2025-07-25|DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations|Ziren Gong\u7b49|[2507.19474](http://arxiv.org/pdf/2507.19474)|\u65e0|\u25c6 \u63d0\u51faDINO-SLAM\uff0c\u4e00\u79cd\u57fa\u4e8eDINO\u7279\u5f81\u7684\u8bbe\u8ba1\u7b56\u7565\uff0c\u7528\u4e8e\u589e\u5f3aSLAM\u7cfb\u7edf\u4e2d\u795e\u7ecf\u9690\u5f0f\uff08NeRF\uff09\u548c\u663e\u5f0f\uff083DGS\uff09\u8868\u793a\u7684\u573a\u666f\u5efa\u6a21\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1\u573a\u666f\u7ed3\u6784\u7f16\u7801\u5668\uff08SSE\uff09\uff0c\u5c06DINO\u7279\u5f81\u5347\u7ea7\u4e3a\u589e\u5f3a\u7248EDINO\uff0c\u4ee5\u6355\u6349\u573a\u666f\u7684\u5c42\u6b21\u5316\u5143\u7d20\u53ca\u5176\u7ed3\u6784\u5173\u7cfb\u3002  \n\u25c6 \u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eEDINO\u7279\u5f81\u7684\u57fa\u7840\u8303\u5f0f\uff0c\u5206\u522b\u96c6\u6210\u5230NeRF\u548c3DGS\u7684SLAM\u7cfb\u7edf\u4e2d\uff0c\u63d0\u5347\u573a\u666f\u8868\u793a\u7684\u5168\u9762\u6027\u3002  \n\u25c6 \u5728Replica\u3001ScanNet\u548cTUM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u3002  \n\u25c6 \u901a\u8fc7\u878d\u5408DINO\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSLAM\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e2d\u7ec6\u8282\u6355\u6349\u548c\u7ed3\u6784\u5173\u7cfb\u5efa\u6a21\u7684\u4e0d\u8db3\u3002|\n",
    "2507.19308": "|2025-07-25|The Eloquence team submission for task 1 of MLC-SLM challenge|Lorenzo Concina\u7b49|[2507.19308](http://arxiv.org/pdf/2507.19308)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u9488\u5bf9MLC-SLM\u6311\u6218\u8d5b\u4efb\u52a11\uff0c\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u4f1a\u8bdd\u8bed\u97f3\u8bc6\u522b\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u6838\u5fc3\u8d21\u732e\u5982\u4e0b\uff1a\n\n\u25c6 \u8bc4\u4f30\u4e86\u5b98\u65b9\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u8bad\u7ec3\u7ebf\u6027\u6295\u5f71\u5668\u548cqformer\u4e24\u79cd\u6295\u5f71\u5668\uff0c\u7ed3\u5408\u4e0d\u540c\u57fa\u7840\u6a21\u578b\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u7ebf\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002\n\n\u25c6 \u5229\u7528SLAM-ASR\u6846\u67b6\u8bad\u7ec3\u4e86\u81ea\u5b9a\u4e49\u7684\u591a\u8bed\u8a00\u7ebf\u6027\u6295\u5f71\u5668\uff0c\u4f18\u5316\u4e86\u6a21\u578b\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u3002\n\n\u25c6 \u63a2\u7d22\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5728\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u9c81\u68d2\u6027\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u591a\u6837\u5316\u8bed\u97f3\u8f93\u5165\u7684\u8bc6\u522b\u80fd\u529b\u3002\n\n\u25c6 \u7814\u7a76\u4e86\u6269\u5c55\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u5bf9\u8bc6\u522b\u6548\u679c\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5728\u6539\u5584\u4f1a\u8bdd\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\n\n\u25c6 \u7efc\u5408\u4e09\u79cd\u65b9\u6cd5\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u4f1a\u8bdd\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u548c\u7406\u8bba\u652f\u6301\u3002|\n",
    "2507.19079": "|2025-07-25|SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research|Feng Zhu\u7b49|[2507.19079](http://arxiv.org/pdf/2507.19079)|\u65e0|\u25c6 \u63d0\u51faSmartPNT-MSF\u591a\u6e90\u878d\u5408\u6570\u636e\u96c6\uff0c\u6574\u5408GNSS\u3001IMU\u3001\u5149\u5b66\u76f8\u673a\u548c\u6fc0\u5149\u96f7\u8fbe\u7b49\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5f25\u8865\u73b0\u6709\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u8be6\u7ec6\u8bb0\u5f55\u6570\u636e\u96c6\u6784\u5efa\u8fc7\u7a0b\uff0c\u5305\u62ec\u4f20\u611f\u5668\u914d\u7f6e\u3001\u5750\u6807\u7cfb\u5b9a\u4e49\u53ca\u76f8\u673a\u4e0e\u6fc0\u5149\u96f7\u8fbe\u6807\u5b9a\u6d41\u7a0b\uff0c\u786e\u4fdd\u6570\u636e\u7684\u4e00\u81f4\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u6807\u51c6\u5316\u6570\u636e\u91c7\u96c6\u4e0e\u5904\u7406\u6846\u67b6\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5206\u6790\u5e76\u5177\u5907\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u591a\u4f20\u611f\u5668\u878d\u5408\u7814\u7a76\u63d0\u4f9b\u7ed3\u6784\u5316\u57fa\u7840\u3002  \n\u25c6 \u901a\u8fc7VINS-Mono\u3001LIO-SAM\u7b49\u5148\u8fdbSLAM\u7b97\u6cd5\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u5176\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u5bfc\u822a\u4e0e\u5b9a\u4f4d\u7b97\u6cd5\u5f00\u53d1\u3002  \n\u25c6 \u8986\u76d6\u57ce\u5e02\u3001\u6821\u56ed\u3001\u96a7\u9053\u53ca\u90ca\u533a\u7b49\u591a\u79cd\u771f\u5b9e\u573a\u666f\uff0c\u589e\u5f3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u6280\u672f\u7814\u7a76\u80fd\u529b\uff0c\u586b\u8865\u73af\u5883\u591a\u6837\u6027\u7a7a\u767d\u3002  \n\u25c6 \u516c\u5f00\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u5bfc\u822a\u9886\u57df\u7b97\u6cd5\u6d4b\u8bd5\u4e0e\u6bd4\u8f83\uff0c\u63a8\u52a8\u591a\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u7684\u521b\u65b0\u4e0e\u53d1\u5c55\u3002|\n",
    "2507.18886": "|2025-07-25|A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras|Zheng Yang\u7b49|[2507.18886](http://arxiv.org/pdf/2507.18886)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u975e\u8fed\u4ee3\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u5c066\u81ea\u7531\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u5206\u4e3a\u65cb\u8f6c\u548c\u5e73\u79fb\u4e24\u6b65\u8ba1\u7b97\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8fed\u4ee3\u4f18\u5316\u5e26\u6765\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002  \n\u25c6 \u5229\u7528\u573a\u666f\u4e2d\u7684\u91cd\u53e0\u5e73\u9762\u7279\u5f81\u76f4\u63a5\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\uff0c\u7b80\u5316\u4e86\u65cb\u8f6c\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u91c7\u7528\u6838\u4e92\u76f8\u5173\u5668(KCC)\u8ba1\u7b97\u5e73\u79fb\u91cf\uff0c\u7701\u53bb\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7279\u5f81\u63d0\u53d6\u4e0e\u5339\u914d\u7684\u8017\u65f6\u6b65\u9aa4\u3002  \n\u25c6 \u6574\u4e2a\u6d41\u7a0b\u65e0\u9700\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u4f4e\u7aefi5 CPU\u4e0a\u5b9e\u73b0\u4e8671Hz\u7684\u9ad8\u5b9e\u65f6\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u4e0d\u4f9d\u8d56\u7279\u5f81\u70b9\u7684\u7279\u6027\u4f7f\u7b97\u6cd5\u5728\u4f4e\u7eb9\u7406\u6216\u9000\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u9c81\u68d2\u6027\u66f4\u5f3a\u3002  \n\u25c6 \u901a\u8fc7\u5206\u79bb\u65cb\u8f6c\u4e0e\u5e73\u79fb\u4f30\u8ba1\u5e76\u5229\u7528\u5e73\u9762\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5feb\u901f\u4f4d\u59ff\u4f30\u8ba1\u3002|\n",
    "2507.20854": "|2025-07-28|$S^3$LAM: Surfel Splatting SLAM for Geometrically Accurate Tracking and Mapping|Ruoyu Fan\u7b49|[2507.20854](http://arxiv.org/pdf/2507.20854)|\u65e0|\u25c6 \u63d0\u51faS\u00b3LAM\u7cfb\u7edf\uff0c\u91c7\u75282D\u9762\u5143\uff08surfel\uff09\u4f5c\u4e3a\u57fa\u672c\u8868\u793a\u5355\u5143\uff0c\u66ff\u4ee3\u4f20\u7edf3D\u9ad8\u65af\u692d\u7403\u4f53\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u573a\u666f\u51e0\u4f55\u5efa\u6a21\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u75282D\u9ad8\u65af\u9762\u5143\u8fdb\u884c\u573a\u666f\u8868\u9762\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u51e0\u4f55\u7cbe\u5ea6\uff0c\u540c\u65f6\u4f18\u5316\u8ddf\u8e2a\u4e0e\u5efa\u56fe\u6027\u80fd\u3002  \n\u25c6 \u8bbe\u8ba1\u81ea\u9002\u5e94\u8868\u9762\u6e32\u67d3\u7b56\u7565\uff0c\u89e3\u51b3SLAM\u5728\u6709\u9650\u89c6\u89d2\u4e0b\u7684\u5b9e\u65f6\u4f18\u5316\u95ee\u9898\uff0c\u517c\u987e\u8ba1\u7b97\u6548\u7387\u4e0e\u5efa\u56fe\u51c6\u786e\u6027\u3002  \n\u25c6 \u76f4\u63a5\u4ece2D\u9762\u5143\u6e32\u67d3\u516c\u5f0f\u63a8\u5bfc\u76f8\u673a\u4f4d\u59ff\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u51f8\u663e\u51e0\u4f55\u7cbe\u786e\u8868\u793a\u5bf9\u8ddf\u8e2a\u6536\u655b\u6027\u7684\u5173\u952e\u4f5c\u7528\u3002  \n\u25c6 \u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86S\u00b3LAM\u7684\u4f18\u8d8a\u6027\uff0c\u6027\u80fd\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002|\n",
    "2507.20516": "|2025-07-28|Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping|Xiaofeng Jin\u7b49|[2507.20516](http://arxiv.org/pdf/2507.20516)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u7cbe\u5ea6\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u6570\u636e\u96c6\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e0d\u8db3\u7684\u7a7a\u767d\u3002  \n\u25c6 \u6570\u636e\u96c6\u8986\u76d6\u56db\u79cd\u591a\u6837\u5316\u771f\u5b9e\u73af\u5883\uff086\u4e07\u81f375\u4e07\u5e73\u65b9\u7c73\uff09\uff0c\u901a\u8fc7\u5b9a\u5236\u80cc\u5305\u5f0f\u5e73\u53f0\u91c7\u96c6\uff0c\u96c6\u6210\u591a\u7ebf\u6fc0\u5149\u96f7\u8fbe\u3001\u5de5\u4e1a\u7ea7IMU\u548cRTK-GNSS\u6a21\u5757\u3002  \n\u25c6 \u63d0\u4f9b\u957f\u8f68\u8ff9\u3001\u590d\u6742\u573a\u666f\u548c\u9ad8\u7cbe\u5ea6\u771f\u503c\uff0c\u7ed3\u5408SLAM\u4f18\u5316\u4e0eRTK-GNSS\u951a\u5b9a\u6280\u672f\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u503e\u659c\u6444\u5f71\u4e0eRTK-GNSS\u878d\u5408\u9a8c\u8bc1\u8f68\u8ff9\u7cbe\u5ea6\u3002  \n\u25c6 \u9996\u6b21\u5728\u6570\u636e\u96c6\u4e2d\u878d\u5408\u591a\u4f20\u611f\u5668\u5197\u4f59\u6570\u636e\uff08\u5982LiDAR-IMU-RTK\uff09\uff0c\u652f\u6301\u9000\u5316\u573a\u666f\uff08\u5982\u96a7\u9053\u3001\u690d\u88ab\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002  \n\u25c6 \u4e3a\u9ad8\u7cbe\u5ea6\u5730\u56fe\u6784\u5efa\u4efb\u52a1\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u91cd\u70b9\u9a8c\u8bc1LIO\u7cfb\u7edf\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u9000\u5316\u9002\u5e94\u6027\u3002|\n",
    "2507.19742": "|2025-07-26|DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning|Yanbin Li\u7b49|[2507.19742](http://arxiv.org/pdf/2507.19742)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u81ea\u9002\u5e94\u9000\u5316\u4f18\u5316\u667a\u80fd\u4f53\uff08DOA\uff09\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3SLAM\u5728\u957f\u76f4\u8d70\u5eca\u7b49\u9000\u5316\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u7cfb\u7edf\u6027\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u7684\u4e09\u5927\u6311\u6218\uff1a\u9000\u5316\u6570\u636e\u96c6\u83b7\u53d6\u74f6\u9888\u3001\u8bad\u7ec3\u6837\u672c\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u4ee5\u53ca\u6807\u6ce8\u534f\u8bae\u8bbe\u8ba1\u7684\u6a21\u7cca\u6027\u3002  \n\u25c6 \u5f00\u53d1\u4e13\u7528\u5956\u52b1\u51fd\u6570\uff0c\u5f15\u5bfc\u667a\u80fd\u4f53\u5b66\u4e60\u9000\u5316\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u57fa\u4e8e\u9000\u5316\u56e0\u5b50\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u4f20\u611f\u5668\u5bf9\u4f4d\u59ff\u4f18\u5316\u7684\u8d21\u732e\u6743\u91cd\u3002  \n\u25c6 \u63d0\u51fa\u7ebf\u6027\u63d2\u503c\u516c\u5f0f\u63a7\u5236\u89c2\u6d4b\u5206\u5e03\u5411\u8fd0\u52a8\u6a21\u578b\u5206\u5e03\u7684\u504f\u79fb\u6b65\u957f\uff0c\u5b9e\u73b0\u4f4d\u59ff\u8865\u507f\u7684\u81ea\u9002\u5e94\u8c03\u6574\u3002  \n\u25c6 \u5f15\u5165\u8fc1\u79fb\u5b66\u4e60\u6a21\u5757\u63d0\u5347\u667a\u80fd\u4f53\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u9000\u5316\u73af\u5883\u4e2d\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u8bbe\u8ba1\u5408\u7406\u6027\uff0c\u5e76\u8bc1\u660eDOA\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9000\u5316\u68c0\u6d4b\u4e0e\u4f18\u5316\u6027\u80fd\u3002|\n",
    "2507.21715": "|2025-07-29|Impact of Underwater Image Enhancement on Feature Matching|Jason M. Summers\u7b49|[2507.21715](http://arxiv.org/pdf/2507.21715)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u5c40\u90e8\u5339\u914d\u7a33\u5b9a\u6027\u548c\u6700\u8fdc\u53ef\u5339\u914d\u5e27\u6570\u4e24\u9879\u91cf\u5316\u6307\u6807\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u6548\u679c\u3002  \n\u25c6 \u9488\u5bf9\u6c34\u4e0b\u73af\u5883\u7279\u6709\u7684\u5149\u5438\u6536\u3001\u6563\u5c04\u3001\u751f\u7269\u9644\u7740\u7b49\u9000\u5316\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u9762\u5411\u589e\u5f3a\u6280\u672f\u7684\u5e27\u5339\u914d\u6027\u80fd\u8bc4\u4f30\u6846\u67b6\u3002  \n\u25c6 \u901a\u8fc7\u5ea6\u91cf\u5206\u6790\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u9996\u6b21\u6307\u51fa\u5176\u5728\u771f\u5b9e\u573a\u666f\u9002\u7528\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u5b9e\u9645\u5339\u914d\u7b56\u7565\u878d\u5165\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5efa\u7acb\u4e86\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u589e\u5f3a\u65b9\u6cd5\u5bf9\u6bd4\u57fa\u51c6\u3002  \n\u25c6 \u901a\u8fc7SLAM\u7cfb\u7edf\u7684\u5168\u6d41\u7a0b\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u4e86\u89c6\u89c9\u8d28\u91cf\u63d0\u5347\u5bf9\u6c34\u4e0b\u81ea\u4e3b\u5bfc\u822a\u7b97\u6cd5\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u5f3a\u5316\u4e86\u6846\u67b6\u7684\u5de5\u7a0b\u4ef7\u503c\u3002  \n\u25c6 \u8be5\u7814\u7a76\u586b\u8865\u4e86\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5173\u8054\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u7b97\u6cd5\u4f18\u5316\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002|\n",
    "2507.21709": "|2025-07-29|Adaptive Prior Scene-Object SLAM for Dynamic Environments|Haolan Zhang\u7b49|[2507.21709](http://arxiv.org/pdf/2507.21709)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u573a\u666f-\u7269\u4f53\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5f53\u524d\u5e27\u8d28\u91cf\u6307\u6807\u548c\u76f8\u5bf9\u4e8e\u53ef\u9760\u53c2\u8003\u5e27\u7684\u573a\u666f\u53d8\u5316\uff0c\u5168\u9762\u8bc4\u4f30SLAM\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u9488\u5bf9\u73b0\u6709\u7cfb\u7edf\u5728\u59ff\u6001\u4f30\u8ba1\u4e0d\u53ef\u9760\u65f6\u7f3a\u4e4f\u7ea0\u9519\u673a\u5236\u7684\u95ee\u9898\uff0c\u91c7\u7528\u59ff\u6001\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u53ef\u9760\u5e27\u4fe1\u606f\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u6709\u6548\u51cf\u5c11\u52a8\u6001\u5e72\u6270\u7684\u8d1f\u9762\u5f71\u54cd\u3002  \n\u25c6 \u5728\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89d2\u7a81\u53d8\u548c\u8fd0\u52a8\u7269\u4f53\u7279\u5f81\u4e0d\u660e\u786e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002  \n\u25c6 \u901a\u8fc7TUM RGB-D\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u52a8\u6001\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002  \n\u25c6 \u7ed3\u5408\u5f53\u524d\u5e27\u8d28\u91cf\u4e0e\u573a\u666f\u53d8\u5316\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684\u52a8\u6001\u73af\u5883SLAM\u7a33\u5b9a\u6027\u5224\u65ad\u65b9\u6cd5\u3002  \n\u25c6 \u63d0\u51fa\u7684\u59ff\u6001\u4f18\u5316\u7b56\u7565\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684SLAM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bef\u5dee\u6821\u6b63\u673a\u5236\u3002|\n",
    "2507.21553": "|2025-08-01|Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments|Federica Di Lauro\u7b49|[2507.21553](http://arxiv.org/pdf/2507.21553)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5730\u4e0b\u96a7\u9053\u73af\u5883\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u673a\u5668\u4ebaLiDAR SLAM\u7cfb\u7edf\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u53d1\u73b0\u5f53\u524d\u95ed\u73af\u68c0\u6d4b\u5b58\u5728\u5927\u91cf\u8bef\u62a5\u95ee\u9898\uff0c\u8fd9\u662f\u5bfc\u81f4\u7cfb\u7edf\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u95ed\u73af\u68c0\u6d4b\u4e2d\u7684\u8bef\u62a5\u60c5\u51b5\u3002  \n\u25c6 \u5728\u5730\u4e0b\u96a7\u9053\u8fd9\u4e00\u6781\u5177\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u6307\u51fa\u4e86\u8be5\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6f5c\u5728\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u53c2\u8003\u3002|\n",
    "2507.22791": "|2025-07-30|Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques|Weide Liu\u7b49|[2507.22791](http://arxiv.org/pdf/2507.22791)|\u65e0|\u25c6 \u5168\u9762\u7efc\u8ff0\u4e86\u5355\u6a21\u6001\u4e0e\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u6280\u672f\uff0c\u6db5\u76d6RGB\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u50cf\u30013D\u70b9\u4e91\u3001LiDAR\u626b\u63cf\u3001\u533b\u5b66\u56fe\u50cf\u53ca\u89c6\u89c9-\u8bed\u8a00\u4ea4\u4e92\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7cfb\u7edf\u6027\u603b\u7ed3\u7684\u7a7a\u767d\u3002  \n\u25c6 \u5bf9\u6bd4\u5206\u6790\u4e86\u4f20\u7edf\u624b\u5de5\u65b9\u6cd5\uff08\u5982Harris\u89d2\u70b9\u3001SIFT\u548cORB\u63cf\u8ff0\u5b50\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982SuperPoint\u548cLoFTR\uff09\uff0c\u6307\u51fa\u540e\u8005\u5728\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u4e0a\u7684\u663e\u8457\u4f18\u52bf\u3002  \n\u25c6 \u91cd\u70b9\u4ecb\u7ecd\u4e86\u6a21\u6001\u611f\u77e5\u6280\u672f\u8fdb\u5c55\uff0c\u5305\u62ec\u9488\u5bf9\u6df1\u5ea6\u56fe\u50cf\u7684\u51e0\u4f55\u4e0e\u6df1\u5ea6\u4e13\u7528\u63cf\u8ff0\u5b50\u30013D\u70b9\u4e91\u7684\u7a00\u758f\u4e0e\u7a20\u5bc6\u5b66\u4e60\u65b9\u6cd5\u3001LiDAR\u626b\u63cf\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u53ca\u533b\u5b66\u56fe\u50cf\u5339\u914d\u7684MIND\u63cf\u8ff0\u5b50\u7b49\u521b\u65b0\u65b9\u6848\u3002  \n\u25c6 \u6df1\u5165\u63a2\u8ba8\u8de8\u6a21\u6001\u5e94\u7528\u573a\u666f\uff0c\u5982\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u548c\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u5339\u914d\u6280\u672f\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u4ea4\u4e92\u7684\u6700\u65b0\u53d1\u5c55\u8d8b\u52bf\u3002  \n\u25c6 \u5f3a\u8c03\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u57fa\u4e8eCNN\u548cTransformer\u7684\u6a21\u578b\uff09\u5bf9\u8de8\u6a21\u6001\u5339\u914d\u6027\u80fd\u7684\u63d0\u5347\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002|\n",
    "2507.22412": "|2025-07-30|UAVScenes: A Multi-Modal Dataset for UAVs|Sijie Wang\u7b49|[2507.22412](http://arxiv.org/pdf/2507.22412)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u652f\u6301\u591a\u6a21\u6001\uff08\u76f8\u673a\u56fe\u50cf\u548cLiDAR\u70b9\u4e91\uff09\u5e27\u7ea7\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u6570\u636e\u96c6UAVScenes\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u652f\u6301\u5b9a\u4f4d\u6216\u5730\u56fe\u7ea7\u8bed\u4e49\u5206\u5272\u7684\u7a7a\u767d\u3002  \n\u25c6 \u57fa\u4e8eMARS-LVIG\u6570\u636e\u96c6\u8fdb\u884c\u5347\u7ea7\uff0c\u65b0\u589e\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u9010\u5e27\u56fe\u50cf\u548c\u70b9\u4e91\u8bed\u4e49\u6807\u7b7e\uff0c\u4ee5\u53ca\u9ad8\u7cbe\u5ea66\u81ea\u7531\u5ea6\u4f4d\u59ff\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u5b9e\u7528\u6027\u3002  \n\u25c6 \u9996\u6b21\u652f\u6301\u65e0\u4eba\u673a\u573a\u666f\u4e0b\u7684\u591a\u4efb\u52a1\u8054\u5408\u8bc4\u6d4b\uff0c\u5305\u62ec\u5206\u5272\u3001\u6df1\u5ea6\u4f30\u8ba1\u30016-DoF\u5b9a\u4f4d\u3001\u5730\u70b9\u8bc6\u522b\u548c\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u7b49\u9ad8\u7ea7\u611f\u77e5\u4efb\u52a1\u3002  \n\u25c6 \u901a\u8fc7\u4e25\u683c\u7684\u4f20\u611f\u5668\u6807\u5b9a\u548c\u540c\u6b65\uff0c\u786e\u4fdd\u591a\u6a21\u6001\u6570\u636e\u7684\u65f6\u95f4-\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4e3a\u8de8\u6a21\u6001\u878d\u5408\u7814\u7a76\u63d0\u4f9b\u53ef\u9760\u57fa\u51c6\u3002  \n\u25c6 \u5f00\u6e90\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u6807\u51c6\u5316\u8bc4\u6d4b\u534f\u8bae\uff0c\u63a8\u52a8\u65e0\u4eba\u673a\u591a\u6a21\u6001\u611f\u77e5\u9886\u57df\u7684\u7b97\u6cd5\u53d1\u5c55\u548c\u516c\u5e73\u6bd4\u8f83\u3002|\n",
    "2507.23677": "|2025-07-31|Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes|Xiaohan Li\u7b49|[2507.23677](http://arxiv.org/pdf/2507.23677)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u6237\u5916\u573a\u666f\u7684\u53cc\u76ee3D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff08BGS-SLAM\uff09\uff0c\u586b\u8865\u4e86\u73b0\u67093DGS-SLAM\u4e3b\u8981\u9488\u5bf9\u5ba4\u5185\u73af\u5883\u4e14\u4f9d\u8d56\u4e3b\u52a8\u6df1\u5ea6\u4f20\u611f\u5668\u7684\u7a7a\u767d\u3002  \n\u25c6 \u4ec5\u4f7f\u7528RGB\u7acb\u4f53\u56fe\u50cf\u5bf9\uff0c\u65e0\u9700LiDAR\u6216\u4e3b\u52a8\u4f20\u611f\u5668\uff0c\u964d\u4f4e\u4e86\u786c\u4ef6\u6210\u672c\u5e76\u63d0\u5347\u4e86\u7cfb\u7edf\u9002\u7528\u6027\u3002  \n\u25c6 \u5229\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u7acb\u4f53\u7f51\u7edc\u7684\u6df1\u5ea6\u4f30\u8ba1\u6307\u5bfc3D\u9ad8\u65af\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u635f\u5931\u7b56\u7565\u540c\u65f6\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002  \n\u25c6 \u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e3DGS\u65b9\u6848\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5efa\u56fe\u6027\u80fd\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u6237\u5916\u5927\u89c4\u6a21\u573a\u666f\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771fSLAM\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.23629": "|2025-07-31|DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching|Yewei Huang\u7b49|[2507.23629](http://arxiv.org/pdf/2507.23629)|\u65e0|\u25c6 \u63d0\u51faDRACo-SLAM2\u6846\u67b6\uff0c\u4e3a\u914d\u5907\u591a\u6ce2\u675f\u6210\u50cf\u58f0\u7eb3\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u56e2\u961f\u8bbe\u8ba1\u5206\u5e03\u5f0fSLAM\u7cfb\u7edf\uff0c\u6539\u8fdb\u539f\u6709DRACo-SLAM\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u58f0\u7eb3\u5730\u56fe\u8868\u793a\u4e3a\u5bf9\u8c61\u56fe\uff0c\u5229\u7528\u5bf9\u8c61\u56fe\u5339\u914d\u5b9e\u73b0\u9ad8\u6548\u8de8\u673a\u5668\u4eba\u56de\u73af\u68c0\u6d4b\uff0c\u65e0\u9700\u4f9d\u8d56\u5148\u9a8c\u51e0\u4f55\u4fe1\u606f\u3002  \n\u25c6 \u9488\u5bf9\u6c34\u4e0b\u626b\u63cf\u5339\u914d\u7279\u70b9\uff0c\u63d0\u51fa\u589e\u91cf\u5f0f\u7ec4\u95f4\u4e00\u81f4\u6d4b\u91cf\u96c6\u6700\u5927\u5316\uff08GCM\uff09\u65b9\u6cd5\uff0c\u6539\u8fdb\u539f\u6709\u7684PCM\u7b97\u6cd5\u3002  \n\u25c6 GCM\u65b9\u6cd5\u6709\u6548\u5904\u7406\u76f8\u90bb\u8de8\u673a\u5668\u4eba\u56de\u73af\u5171\u4eab\u76f8\u4f3c\u914d\u51c6\u8bef\u5dee\u7684\u573a\u666f\uff0c\u63d0\u5347\u5339\u914d\u9c81\u68d2\u6027\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002|\n",
    "2507.23273": "|2025-07-31|GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting|Jaeseok Park\u7b49|[2507.23273](http://arxiv.org/pdf/2507.23273)|\u65e0|\u25c6 \u63d0\u51faGSFusion\u7cfb\u7edf\uff0c\u9996\u6b21\u5c06\u6fc0\u5149\u96f7\u8fbe\uff08LiDAR\uff09\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u4e0e\u89c6\u89c9\u4f20\u611f\u5668\u878d\u5408\uff0c\u5b9e\u73b0\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u5728\u7ebf\u5efa\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7eaf\u89c6\u89c9\u65b9\u6cd5\u5728\u5f31\u7eb9\u7406\u3001\u5149\u7167\u4e0d\u8db3\u548c\u8fdc\u8ddd\u79bb\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u5f15\u5165\u5168\u5c40\u4f4d\u59ff\u56fe\u4f18\u5316\u4e2d\u7684\u9762\u5143\u5230\u9762\u5143\uff08surfel-to-surfel\uff09\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u5730\u56fe\u7684\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u9ad8\u7cbe\u5ea6\u5efa\u56fe\u8d28\u91cf\u3002  \n\u25c6 \u8bbe\u8ba1\u50cf\u7d20\u611f\u77e5\u7684\u9ad8\u65af\u521d\u59cb\u5316\u7b56\u7565\uff0c\u6709\u6548\u5229\u7528\u7a00\u758f\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u5feb\u901f\u751f\u6210\u9ad8\u65af\u8868\u793a\uff0c\u5927\u5e45\u7f29\u77ed\u4f18\u5316\u65f6\u95f4\u3002  \n\u25c6 \u63d0\u51fa\u6709\u754cSigmoid\u7ea6\u675f\u673a\u5236\uff0c\u9632\u6b62\u9ad8\u65af\u5206\u5e03\u65e0\u9650\u5236\u6269\u5f20\uff0c\u63d0\u5347\u573a\u666f\u8868\u793a\u7684\u7a33\u5b9a\u6027\u548c\u6e32\u67d3\u6548\u7387\u3002  \n\u25c6 \u5728\u516c\u5f00\u548c\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u5efa\u56fe\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u67093DGS SLAM\u65b9\u6848\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002|\n",
    "2508.00568": "|2025-08-01|CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry|Jingchao Xie\u7b49|[2508.00568](http://arxiv.org/pdf/2508.00568)|\u65e0|\u25c6 \u63d0\u51faCoProU-VO\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u4e0e\u878d\u5408\u5f15\u5165\u65e0\u76d1\u7763\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff0c\u901a\u8fc7\u6982\u7387\u5316\u5efa\u6a21\u7ed3\u5408\u5f53\u524d\u5e27\u4e0e\u53c2\u8003\u5e27\u7684\u4e0d\u786e\u5b9a\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u57fa\u4e8e\u89c6\u89c9Transformer\u4e3b\u5e72\u7f51\u7edc\uff0c\u540c\u6b65\u5b66\u4e60\u6df1\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u76f8\u673a\u4f4d\u59ff\uff0c\u65e0\u9700\u52a8\u6001\u7269\u4f53\u663e\u5f0f\u6807\u6ce8\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u6295\u5f71\u673a\u5236\u5c06\u53c2\u8003\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u9012\u81f3\u76ee\u6807\u5e27\uff0c\u6709\u6548\u8bc6\u522b\u52a8\u6001\u573a\u666f\u4e2d\u7684\u4e0d\u53ef\u9760\u533a\u57df\uff0c\u7a81\u7834\u4f20\u7edf\u5355\u5e27\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7684\u5c40\u9650\u3002  \n\u25c6 \u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65e0\u76d1\u7763\u5355\u76ee\u4e24\u5e27\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u52a8\u6001\u7269\u4f53\u5bc6\u96c6\u7684\u9ad8\u901f\u516c\u8def\u573a\u666f\u8868\u73b0\u7a81\u51fa\u3002  \n\u25c6 \u901a\u8fc7\u8be6\u5b9e\u7684\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u8de8\u5e27\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u7684\u6709\u6548\u6027\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u4f4d\u59ff\u4f30\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\u3002|\n",
    "2508.00088": "|2025-07-31|The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking|Mateo de Mayo\u7b49|[2508.00088](http://arxiv.org/pdf/2508.00088)|\u65e0|\u25c6 \u63d0\u51fa\u4e86Monado SLAM\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u9488\u5bf9\u5934\u6234\u5f0f\u8bbe\u5907\u7684\u89c6\u89c9-\u60ef\u6027\u8ddf\u8e2a\u6311\u6218\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u6570\u636e\u96c6\u5305\u542b\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u9ad8\u52a8\u6001\u8fd0\u52a8\u3001\u52a8\u6001\u906e\u6321\u3001\u957f\u65f6\u95f4\u8ddf\u8e2a\u7b49\u590d\u6742\u60c5\u51b5\uff0c\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002  \n\u25c6 \u8986\u76d6\u4e86\u4f4e\u7eb9\u7406\u533a\u57df\u3001\u6076\u52a3\u5149\u7167\u6761\u4ef6\u548c\u4f20\u611f\u5668\u9971\u548c\u7b49\u73b0\u6709\u6570\u636e\u96c6\u8f83\u5c11\u6d89\u53ca\u7684\u96be\u70b9\u573a\u666f\u3002  \n\u25c6 \u6570\u636e\u6765\u81ea\u591a\u6b3e\u865a\u62df\u73b0\u5b9e\u5934\u663e\u8bbe\u5907\uff0c\u5177\u6709\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\uff0c\u9002\u7528\u4e8e\u5934\u6234\u5f0f\u4f20\u611f\u5668\u7814\u7a76\u3002  \n\u25c6 \u91c7\u7528CC BY 4.0\u8bb8\u53ef\u534f\u8bae\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u548cSLAM\u6280\u672f\u7684\u7814\u53d1\u8fdb\u6b65\u3002  \n\u25c6 \u901a\u8fc7\u771f\u5b9e\u573a\u666f\u6570\u636e\u66b4\u9732\u73b0\u6709VIO/SLAM\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002|\n",
    "2508.02187": "|2025-08-04|A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration|Xingyi Li\u7b49|[2508.02187](http://arxiv.org/pdf/2508.02187)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u77e9\u5339\u914d\u7684\u70b9\u4e91\u914d\u51c6\u6846\u67b6\uff0c\u5c06\u70b9\u4e91\u89c6\u4e3a\u540c\u5206\u5e03\u72ec\u7acb\u6837\u672c\uff0c\u901a\u8fc7\u5339\u914d\u5e7f\u4e49\u9ad8\u65af\u5f84\u5411\u57fa\u77e9\u4f30\u8ba1\u521a\u4f53\u53d8\u6362\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u5bf9\u663e\u5f0f\u70b9\u5bf9\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u4f9d\u8d56\u3002  \n\u25c6 \u5728\u7406\u8bba\u5c42\u9762\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6570\u5b66\u4e00\u81f4\u6027\uff0c\u4e3a\u7b97\u6cd5\u6709\u6548\u6027\u63d0\u4f9b\u7406\u8bba\u652f\u6491\u3002  \n\u25c6 \u9488\u5bf9\u7a00\u758f\u548c\u5f3a\u566a\u58f0\u573a\u666f\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86ICP\u3001NDT\u7b49\u4f20\u7edf\u65b9\u6cd5\u5728\u6b64\u7c7b\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u914d\u51c6\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\uff0c\u5c24\u5176\u57284D\u96f7\u8fbeSLAM\u7cfb\u7edf\u4e2d\u8fbe\u5230\u4e0e\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u76f8\u5f53\u7684\u5b9a\u4f4d\u6027\u80fd\u3002  \n\u25c6 \u9996\u6b21\u5c06\u77e9\u5339\u914d\u6280\u672f\u7cfb\u7edf\u6027\u5730\u5e94\u7528\u4e8e\u70b9\u4e91\u914d\u51c6\u9886\u57df\uff0c\u4e3a\u7a00\u758f\u566a\u58f0\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u5f00\u8f9f\u65b0\u601d\u8def\u3002|\n",
    "2508.02140": "|2025-08-04|AID4AD: Aerial Image Data for Automated Driving Perception|Daniel Lengerer\u7b49|[2508.02140](http://arxiv.org/pdf/2508.02140)|\u65e0|\u25c6 \u63d0\u51faAID4AD\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5c06\u9ad8\u5206\u8fa8\u7387\u822a\u62cd\u56fe\u50cf\u4e0enuScenes\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u7a7a\u95f4\u5750\u6807\u7cfb\u7cbe\u786e\u5bf9\u9f50\uff0c\u586b\u8865\u4e86\u822a\u62cd\u6570\u636e\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u7a7a\u767d\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u4e00\u5957\u57fa\u4e8eSLAM\u70b9\u4e91\u5730\u56fe\u7684\u5bf9\u9f50\u6d41\u7a0b\uff0c\u901a\u8fc7\u5b9a\u4f4d\u548c\u6295\u5f71\u5931\u771f\u6821\u6b63\u6280\u672f\u786e\u4fdd\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff0c\u5e76\u4eba\u5de5\u7b5b\u9009\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6837\u672c\u4f5c\u4e3a\u57fa\u51c6\u771f\u503c\u3002  \n\u25c6 \u9a8c\u8bc1\u4e86\u822a\u62cd\u56fe\u50cf\u5728\u81ea\u52a8\u9a7e\u9a76\u4e24\u5927\u6838\u5fc3\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\uff1a\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u4e2d\u4f5c\u4e3a\u8865\u5145\u8f93\u5165\u63d0\u534715-23%\u7cbe\u5ea6\uff0c\u8fd0\u52a8\u9884\u6d4b\u4e2d\u66ff\u4ee3\u9ad8\u7cbe\u5730\u56fe\u5b9e\u73b02%\u6027\u80fd\u63d0\u5347\u3002  \n\u25c6 \u63ed\u793a\u4e86\u822a\u62cd\u56fe\u50cf\u4f5c\u4e3a\u53ef\u6269\u5c55\u73af\u5883\u4e0a\u4e0b\u6587\u6e90\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u7cbe\u5730\u56fe\u7f3a\u5931\u3001\u8fc7\u65f6\u6216\u7ef4\u62a4\u6210\u672c\u9ad8\u7684\u573a\u666f\u3002  \n\u25c6 \u5f00\u6e90\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u4ee3\u7801\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\uff08https://github.com/DriverlessMobility/AID4AD\uff09\u3002|\n",
    "2508.03672": "|2025-08-05|Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways|Zhongbi Luo\u7b49|[2508.03672](http://arxiv.org/pdf/2508.03672)|\u65e0|\u25c6 \u63d0\u51faInland-LOAM\u6846\u67b6\uff0c\u9488\u5bf9\u5185\u6cb3\u822a\u9053\u73af\u5883\u4f18\u5316LiDAR SLAM\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5782\u76f4\u6f02\u79fb\u548c\u8bed\u4e49\u7f3a\u5931\u65b9\u9762\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u5e76\u5f15\u5165\u6c34\u9762\u5e73\u9762\u7ea6\u675f\uff0c\u6709\u6548\u6291\u5236SLAM\u7cfb\u7edf\u7684\u5782\u76f4\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u901a\u8fc7\u4f53\u7d20\u5316\u51e0\u4f55\u5206\u6790\u5c063D\u70b9\u4e91\u8f6c\u5316\u4e3a\u7ed3\u6784\u53162D\u8bed\u4e49\u5730\u56fe\uff0c\u5b9e\u65f6\u8ba1\u7b97\u6865\u6881\u51c0\u7a7a\u7b49\u5173\u952e\u5bfc\u822a\u53c2\u6570\u3002  \n\u25c6 \u5f00\u53d1\u81ea\u52a8\u5316\u6a21\u5757\u63d0\u53d6\u5cb8\u7ebf\u8f6e\u5ed3\uff0c\u5e76\u8f93\u51fa\u8f7b\u91cf\u5316\u3001\u517c\u5bb9\u56fd\u9645\u7535\u5b50\u822a\u9053\u56fe\uff08IENC\uff09\u7684\u6807\u51c6\u683c\u5f0f\u3002  \n\u25c6 \u5728\u771f\u5b9e\u822a\u9053\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u8bed\u4e49\u5730\u56fe\u4e0e\u5cb8\u7ebf\u6570\u636e\u7b26\u5408\u5b9e\u9645\u573a\u666f\u9700\u6c42\u3002  \n\u25c6 \u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\uff0c\u4e3a\u5185\u6cb3\u81ea\u4e3b\u822a\u884c\u63d0\u4f9b\u53ef\u9760\u7684\u73af\u5883\u611f\u77e5\u4e0e\u5730\u7406\u4fe1\u606f\u652f\u6301\u3002|\n",
    "2508.05149": "|2025-08-07|Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages|Seraphina Fong\u7b49|[2508.05149](http://arxiv.org/pdf/2508.05149)|\u65e0|\u25c6 \u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u8bed\u97f3\u5927\u6a21\u578b(Speech LLMs)\u5728\u4f4e\u8d44\u6e90\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u573a\u666f\u4e0b\u7684\u6570\u636e\u91cf\u9700\u6c42\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7814\u7a76\u7a7a\u767d\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8eSLAM-ASR\u6846\u67b6\u7684\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u6295\u5f71\u5668\u65b9\u6848\uff0c\u6709\u6548\u8fde\u63a5\u8bed\u97f3\u7f16\u7801\u5668\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9002\u914d\u4f4e\u8d44\u6e90\u6761\u4ef6\u3002  \n\u25c6 \u91cf\u5316\u5206\u6790\u4e86\u8fbe\u5230Whisper\u6a21\u578b\u6027\u80fd\u6240\u9700\u7684\u6700\u4f4e\u8bad\u7ec3\u6570\u636e\u91cf\uff0c\u5b9e\u8bc1\u63ed\u793a\u4e86\u6570\u636e\u7a00\u7f3a\u5e26\u6765\u7684\u6838\u5fc3\u6311\u6218\u3002  \n\u25c6 \u521b\u65b0\u6027\u53d1\u73b0\uff1a\u5229\u7528\u9ad8\u8d44\u6e90\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u5355\u4e00/\u591a\u8bed\u8a00\u6295\u5f71\u5668\u80fd\u663e\u8457\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u7279\u522b\u5728\u5c0f\u89c4\u6a21\u8bad\u7ec3\u96c6\u65f6\u6548\u679c\u7a81\u51fa\u3002  \n\u25c6 \u901a\u8fc7EuroLLM\u548cSalamandra\u7b49\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e0ewhisper-large-v3-turbo\u7684\u7ec4\u5408\u5b9e\u9a8c\uff0c\u4e3a\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u4f18\u5316\u601d\u8def\u3002  \n\u25c6 \u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4e3a\u672a\u6765\u4f4e\u8d44\u6e90\u8bed\u97f3\u5927\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8bbe\u8ba1\u53c2\u8003\u548c\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002|\n",
    "2508.04597": "|2025-08-06|Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline|Linqing Zhao\u7b49|[2508.04597](http://arxiv.org/pdf/2508.04597)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6620\u5c04\u7684RGB SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u5668\u548c3D\u9ad8\u65af\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002  \n\u25c6 \u5f15\u5165\u524d\u9988\u5faa\u73af\u9884\u6d4b\u6a21\u5757\uff0c\u76f4\u63a5\u4ece\u5149\u6d41\u63a8\u65ad\u76f8\u673a\u4f4d\u59ff\uff0c\u66ff\u4ee3\u4e86\u8017\u65f6\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\uff0c\u5c06\u8ddf\u8e2a\u901f\u5ea6\u63d0\u534790%\u4ee5\u4e0a\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u5c40\u90e8\u56fe\u6e32\u67d3\u6280\u672f\uff0c\u589e\u5f3a\u4e86\u524d\u9988\u4f4d\u59ff\u9884\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u5728Replica\u548cTUM-RGBD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u5f53\u524d\u6700\u4f18\u7684SplaTAM\u76f8\u5f53\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u65f63D\u91cd\u5efa\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002|\n",
    "2508.07003": "|2025-08-09|EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events|Siyu Chen\u7b49|[2508.07003](http://arxiv.org/pdf/2508.07003)|\u65e0|EGS-SLAM\u7684\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u878d\u5408\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e0eRGB-D\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u6a21\u7cca\u573a\u666f\u4e0b\u7684SLAM\u6027\u80fd\u3002\u5177\u4f53\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\n\u25c6 \u63d0\u51fa\u9996\u4e2a\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\u4e0eRGB-D\u8f93\u5165\u7684GS-SLAM\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u4e25\u91cd\u8fd0\u52a8\u6a21\u7cca\u4e0b\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002\n\n\u25c6 \u521b\u65b0\u6027\u5730\u5efa\u6a21\u76f8\u673a\u66dd\u5149\u671f\u95f4\u7684\u8fde\u7eed\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u611f\u77e5\u4e0e\u6a21\u7cca\u611f\u77e5\u7684\u8054\u5408\u8ddf\u8e2a\u4e0e\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u5efa\u56fe\u3002\n\n\u25c6 \u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u76f8\u673a\u54cd\u5e94\u51fd\u6570\uff0c\u52a8\u6001\u5bf9\u9f50\u4e8b\u4ef6\u6d41\u4e0eRGB\u56fe\u50cf\u7684\u4eae\u5ea6\u8303\u56f4\u5dee\u5f02\u3002\n\n\u25c6 \u5f15\u5165\u65e0\u4e8b\u4ef6\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u6291\u5236\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u7684\u632f\u94c3\u4f2a\u5f71\u3002\n\n\u25c6 \u6784\u5efa\u5305\u542b\u5408\u6210\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u65b0\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u5728\u6781\u7aef\u8fd0\u52a8\u6a21\u7cca\u6761\u4ef6\u4e0b\u7684\u4f18\u8d8a\u6027\u3002\n\n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8f68\u8ff9\u7cbe\u5ea6\u548c\u4e09\u7ef4\u91cd\u5efa\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709GS-SLAM\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u52a8\u6001\u573a\u666f\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2508.08890": "|2025-08-12|Transient Noise Removal via Diffusion-based Speech Inpainting|Mordehay Moradi\u7b49|[2508.08890](http://arxiv.org/pdf/2508.08890)|\u65e0|\u25c6 \u63d0\u51faPGDI\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u8bed\u97f3\u4fee\u590d\u4efb\u52a1\uff0c\u80fd\u7cbe\u51c6\u91cd\u5efa\u957f\u8fbe1\u79d2\u7684\u7f3a\u5931\u6216\u4e25\u91cd\u635f\u574f\u8bed\u97f3\u6bb5\u3002  \n\u25c6 \u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u9650\u5236\uff0c\u5728\u4fdd\u6301\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u97f5\u5f8b\u548c\u73af\u5883\u7279\u5f81\uff08\u5982\u6df7\u54cd\uff09\u7684\u540c\u65f6\uff0c\u6709\u6548\u5904\u7406\u8bf4\u8bdd\u4eba\u5dee\u5f02\u548c\u957f\u95f4\u9699\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165\u5206\u7c7b\u5668\u5f15\u5bfc\u673a\u5236\uff0c\u7279\u522b\u662f\u97f3\u7d20\u7ea7\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002  \n\u25c6 \u5b9e\u73b0\u4e0e\u8bf4\u8bdd\u4eba\u65e0\u5173\u7684\u9c81\u68d2\u4fee\u590d\uff0c\u5373\u4f7f\u8bed\u97f3\u6bb5\u88ab\u5f3a\u70c8\u77ac\u6001\u566a\u58f0\uff08\u5982\u70df\u82b1\u3001\u6454\u95e8\u58f0\uff09\u5b8c\u5168\u906e\u853d\u4ecd\u80fd\u7a33\u5b9a\u5de5\u4f5c\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u5176\u5728\u65e0\u6587\u672c\u8f6c\u5f55\u6761\u4ef6\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6548\uff0c\u6709\u6587\u672c\u8f85\u52a9\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002  \n\u25c6 \u9488\u5bf9\u5b9e\u9645\u566a\u58f0\u573a\u666f\uff08\u65bd\u5de5\u566a\u97f3\u3001\u6572\u51fb\u58f0\u7b49\uff09\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u8bed\u97f3\u4fee\u590d\u6280\u672f\u7684\u73b0\u5b9e\u5e94\u7528\u3002|\n",
    "2508.10398": "|2025-08-14|Super LiDAR Reflectance for Robotic Perception|Wei Gao\u7b49|[2508.10398](http://arxiv.org/pdf/2508.10398)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u626b\u63cf\u6570\u636e\u751f\u6210\u9ad8\u5bc6\u5ea6\u7684LiDAR\u53cd\u5c04\u7387\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f4e\u6210\u672cLiDAR\u56e0\u6570\u636e\u7a00\u758f\u6027\u5bfc\u81f4\u7684\u5e94\u7528\u53d7\u9650\u95ee\u9898\u3002  \n\u25c6 \u9488\u5bf9\u975e\u91cd\u590d\u626b\u63cfLiDAR\uff08NRS-LiDAR\uff09\u7684\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e13\u7528\u7684\u53cd\u5c04\u7387\u56fe\u50cf\u7a20\u5bc6\u5316\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u6570\u636e\u7684\u5229\u7528\u7387\u3002  \n\u25c6 \u653b\u514b\u4e86\u53cd\u5c04\u7387\u6821\u51c6\u548c\u4ece\u9759\u6001\u573a\u666f\u5230\u52a8\u6001\u573a\u666f\u8fc1\u79fb\u7684\u5173\u952e\u6280\u672f\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u7a20\u5bc6\u53cd\u5c04\u7387\u56fe\u50cf\u91cd\u5efa\u3002  \n\u25c6 \u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684LiDAR\u53cd\u5c04\u7387\u56fe\u50cf\u7a20\u5bc6\u5316\u6570\u636e\u96c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002  \n\u25c6 \u5c55\u793a\u4e86\u7a20\u5bc6\u53cd\u5c04\u7387\u56fe\u50cf\u5728\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u591a\u6837\u5316\u5e94\u7528\uff0c\u5982\u95ed\u73af\u68c0\u6d4b\u548c\u4ea4\u901a\u8f66\u9053\u8bc6\u522b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u4ef7\u503c\u3002  \n\u25c6 \u901a\u8fc7\u4e3b\u52a8\u5149\u5b66\u4f20\u611f\u91cd\u65b0\u5b9a\u4e49\u89c6\u89c9\u8fb9\u754c\uff0c\u63a8\u52a8\u4e86\u4e3b\u52a8\u89c6\u89c9\u65b0\u8303\u5f0f\u7684\u53d1\u5c55\u3002|\n",
    "2508.14014": "|2025-08-19|Online 3D Gaussian Splatting Modeling with Novel View Selection|Byeonggwon Lee\u7b49|[2508.14014](http://arxiv.org/pdf/2508.14014)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u4ec5\u4f7f\u7528RGB\u56fe\u50cf\u8fdb\u884c\u5728\u7ebf3D\u9ad8\u65af\u6cfc\u6e85\u5efa\u6a21\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u81ea\u9002\u5e94\u89c6\u56fe\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u91cd\u5efa\u6a21\u578b\u7684\u5b8c\u6574\u6027\u3002\n\n\u25c6 \u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u65b0\u9896\u89c6\u56fe\u9009\u62e9\u673a\u5236\uff0c\u5728\u7ebf\u5206\u6790\u91cd\u5efa\u8d28\u91cf\u5e76\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684\u975e\u5173\u952e\u5e27\u8fdb\u884c\u8865\u5145\u8bad\u7ec3\u3002\n\u25c6 \u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5173\u952e\u5e27\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u878d\u5408\u5173\u952e\u5e27\u548c\u7cbe\u9009\u7684\u975e\u5173\u952e\u5e27\uff0c\u4ece\u591a\u6837\u5316\u89c6\u89d2\u7ec6\u5316\u4e0d\u5b8c\u6574\u533a\u57df\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u96c6\u6210\u5728\u7ebf\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\u7684\u6846\u67b6\uff0c\u786e\u4fdd\u6574\u4e2a3D\u9ad8\u65af\u6cfc\u6e85\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u4e09\u7ef4\u4fe1\u606f\u7684\u4e00\u81f4\u6027\u3002\n\u25c6 \u5b9e\u73b0\u4e86\u5728\u5728\u7ebf\u5904\u7406\u7684\u4e25\u683c\u9650\u5236\u4e0b\uff08\u65e0\u6cd5\u4f7f\u7528\u5927\u91cf\u5e27\u6216\u8fc7\u591a\u8bad\u7ec3\u8fed\u4ee3\uff09\uff0c\u4ecd\u80fd\u6784\u5efa\u9ad8\u8d28\u91cf\u901a\u7528\u6a21\u578b\u7684\u76ee\u6807\u3002\n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6237\u5916\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002|\n",
    "2508.13488": "|2025-08-19|ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments|Jingwen Yu\u7b49|[2508.13488](http://arxiv.org/pdf/2508.13488)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91cd\u590d\u73af\u5883\u4e0b\u56de\u73af\u95ed\u5408\u9a8c\u8bc1\u7684\u9c81\u68d2\u65b9\u6cd5ROVER\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u5148\u9a8c\u7ea6\u675f\u6765\u8bc6\u522b\u9519\u8bef\u56de\u73af\u3002  \n\u25c6 \u9996\u6b21\u5c06\u673a\u5668\u4eba\u65f6\u7a7a\u8fd0\u52a8\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u5f15\u5165\u56de\u73af\u9a8c\u8bc1\u6846\u67b6\uff0c\u7a81\u7834\u4f20\u7edf\u4ec5\u4f9d\u8d56\u5916\u89c2\u7279\u5f81\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u63d0\u51fa\u901a\u8fc7\u4f4d\u59ff\u56fe\u4f18\u5316\u751f\u6210\u5019\u9009\u56de\u73af\u7684\u8f68\u8ff9\u5047\u8bbe\uff0c\u5e76\u8bbe\u8ba1\u8bc4\u5206\u673a\u5236\u8bc4\u4f30\u5176\u4e0e\u539f\u59cb\u8f68\u8ff9\u5148\u9a8c\u7684\u4e00\u81f4\u6027\u3002  \n\u25c6 \u5728\u9ad8\u5ea6\u91cd\u590d\u73af\u5883\u4e2d\u80fd\u6709\u6548\u62d2\u7edd\u865a\u5047\u56de\u73af\uff0c\u89e3\u51b3\u4e86\u5916\u89c2\u76f8\u4f3c\u6027\u5bfc\u81f4\u7684\u8bef\u68c0\u6d4b\u96be\u9898\u3002  \n\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u663e\u8457\u63d0\u5347\u9a8c\u8bc1\u51c6\u786e\u6027\uff0c\u4e14\u53ef\u65e0\u7f1d\u96c6\u6210\u81f3\u73b0\u6709SLAM\u7cfb\u7edf\u589e\u5f3a\u9c81\u68d2\u6027\u3002|\n",
    "2508.14235": "|2025-08-19|SLAM-based Safe Indoor Exploration Strategy|Omar Mostafa\u7b49|[2508.14235](http://arxiv.org/pdf/2508.14235)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSLAM\u7684\u5ba4\u5185\u5b89\u5168\u63a2\u7d22\u7b56\u7565\uff0c\u4e3b\u8981\u9762\u5411\u5177\u6709\u5706\u5f62\u8f6e\u5ed3\u7684\u975e\u5b8c\u6574\u79fb\u52a8\u673a\u5668\u4eba\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5c06\u5b89\u5168\u6027\u4f5c\u4e3a\u6700\u9ad8\u4f18\u5148\u7ea7\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u63a2\u7d22\u4e0e\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002\n\n\u25c6 \u9488\u5bf9\u975e\u5b8c\u6574\u5706\u5f62\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u53cc\u8f6e\u5dee\u901f\u9a71\u52a8\uff09\uff0c\u63d0\u51fa\u4e13\u7528\u63a2\u7d22\u7b56\u7565\uff0c\u800c\u975e\u5047\u8bbe\u7406\u60f3\u70b9\u673a\u5668\u4eba\u6a21\u578b\u3002\n\u25c6 \u91c7\u7528\u591a\u4f20\u611f\u5668\u878d\u5408\u65b9\u6848\uff0c\u7ed3\u5408IMU\u30013D-LiDAR\u8fdb\u884cRTAB-SLAM\uff0c\u5e76\u7528RGB-D\u76f8\u673a\u8fdb\u884c\u56de\u73af\u68c0\u6d4b\uff0c\u63d0\u9ad8\u4e86\u5efa\u56fe\u4e0e\u5b9a\u4f4d\u7684\u7a33\u5b9a\u6027\u3002\n\u25c6 \u63d0\u51fa\u201c\u5b89\u5168\u9aa8\u67b6\u201d\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u59cb\u7ec8\u5c3d\u53ef\u80fd\u8fdc\u79bb\u9759\u6001\u969c\u788d\u7269\uff0c\u6781\u5927\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3002\n\u25c6 \u63a2\u7d22\u7b56\u7565\u4ee5\u5b89\u5168\u907f\u969c\u4e3a\u9996\u8981\u76ee\u6807\uff0c\u5176\u6b21\u624d\u662f\u672a\u77e5\u533a\u57df\u63a2\u7d22\uff0c\u5bfc\u5411\u7a7a\u95f4\u4e2d\u7684\u5f00\u653e\u533a\u57df\u8fdb\u884c\u524d\u8fdb\u3002\n\u25c6 \u901a\u8fc7ROS\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u4e0e\u63a2\u7d22\u8fc7\u7a0b\u3002|\n",
    "2508.16459": "|2025-08-22|GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks|Ali Emre Balc\u0131\u7b49|[2508.16459](http://arxiv.org/pdf/2508.16459)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u5730\u6807\u7684\u65b0\u578b\u6fc0\u5149SLAM\u6846\u67b6GPL-SLAM\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u5bf9\u73af\u5883\u4e2d\u7269\u4f53\u7684\u8f6e\u5ed3\u8fdb\u884c\u5efa\u6a21\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u6805\u683c\u5730\u56fe\u6216\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\u3002\n\u25c6 \u63d0\u51fa\u5728\u7ebf\u9012\u5f52\u66f4\u65b0\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u66f4\u65b0\u5730\u6807\u8f6e\u5ed3\u5e76\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002\n\u25c6 \u5728\u5b8c\u5168\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u5f62\u5f0f\u5316SLAM\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u4f4d\u59ff\u4e0e\u7269\u4f53\u5730\u56fe\u7684\u8054\u5408\u63a8\u7406\u3002\n\u25c6 \u63d0\u4f9b\u8bed\u4e49\u4fe1\u606f\u8f93\u51fa\uff0c\u5982\u7269\u4f53\u6570\u91cf\u548c\u9762\u79ef\uff0c\u5e76\u652f\u6301\u6982\u7387\u6d4b\u91cf\u7684\u7269\u4f53\u5173\u8054\u3002\n\u25c6 \u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u751f\u6210\u7269\u4f53\u5f62\u72b6\u7684\u7f6e\u4fe1\u8fb9\u754c\uff0c\u4e3a\u5b89\u5168\u5bfc\u822a\u548c\u63a2\u7d22\u7b49\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u80fd\u5b9e\u73b0\u7cbe\u786e\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002|\n",
    "2508.15990": "|2025-08-21|GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System|Hung-Jui Huang\u7b49|[2508.15990](http://arxiv.org/pdf/2508.15990)|\u65e0|GelSLAM\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u9760\u89e6\u89c9\u611f\u77e5\u5373\u53ef\u5b9e\u73b0\u5b9e\u65f6\u4e09\u7ef4SLAM\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u957f\u65f6\u95f4\u4f30\u8ba1\u7269\u4f53\u4f4d\u59ff\u5e76\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u7269\u4f53\u5f62\u72b6\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u89e6\u89c9\u884d\u751f\u7684\u8868\u9762\u6cd5\u7ebf\u548c\u66f2\u7387\u4fe1\u606f\u8fdb\u884c\u4f4d\u59ff\u8ddf\u8e2a\u4e0e\u56de\u73af\u68c0\u6d4b\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u70b9\u4e91\u65b9\u6cd5\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u5b9e\u65f6\u4f4e\u8bef\u5dee\u3001\u4f4e\u6f02\u79fb\u7684\u8fd0\u52a8\u8ddf\u8e2a\u80fd\u529b\uff0c\u5373\u4f7f\u5bf9\u4e8e\u6728\u8d28\u5de5\u5177\u7b49\u4f4e\u7eb9\u7406\u7269\u4f53\u4e5f\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002  \n\u25c6 \u80fd\u591f\u4ee5\u4e9a\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u91cd\u5efa\u7269\u4f53\u5f62\u72b6\uff0c\u8fbe\u5230\u9ad8\u4fdd\u771f\u5ea6\u7684\u51e0\u4f55\u590d\u539f\u6548\u679c\u3002  \n\u25c6 \u5c06\u89e6\u89c9\u611f\u77e5\u4ece\u5c40\u90e8\u63a5\u89e6\u6269\u5c55\u81f3\u5168\u5c40\u957f\u65f6\u5e8f\u7a7a\u95f4\u611f\u77e5\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\u3002  \n\u8be5\u7cfb\u7edf\u5728\u906e\u6321\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5f25\u8865\u4e86\u89c6\u89c9\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u9002\u7528\u4e8e\u624b\u5185\u64cd\u4f5c\u7b49\u7cbe\u5bc6\u4ea4\u4e92\u573a\u666f\u3002|\n",
    "2508.17255": "|2025-08-24|SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality|Yuzhi Lai\u7b49|[2508.17255](http://arxiv.org/pdf/2508.17255)|\u65e0|SEER-VAR\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8f66\u8f86\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7684\u8bed\u4e49\u81ea\u4e2d\u5fc3\u73af\u5883\u63a8\u7406\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u4e0e\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u91c7\u7528\u6df1\u5ea6\u5f15\u5bfc\u7684\u89c6\u89c9-\u8bed\u8a00 grounding \u6280\u672f\uff0c\u52a8\u6001\u5206\u79bb\u8f66\u5185\u4e0e\u8f66\u5916\u573a\u666f\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u9759\u6001\u6216\u5355\u89c6\u89d2\u8bbe\u5b9a\u7684\u9650\u5236\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684SLAM\u5206\u652f\uff08CASB\uff09\uff0c\u901a\u8fc7\u53cc\u8defSLAM\u7cfb\u7edf\u5206\u522b\u7a33\u5065\u5730\u8ddf\u8e2a\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u81ea\u4e2d\u5fc3\u8fd0\u52a8\u3002  \n\u25c6 \u5f15\u5165\u57fa\u4e8eGPT\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u5757\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684AR\u53e0\u52a0\u5185\u5bb9\uff0c\u5982\u4eea\u8868\u76d8\u63d0\u793a\u548c\u5371\u9669\u9884\u8b66\u3002  \n\u25c6 \u6784\u5efa\u5e76\u5f00\u6e90EgoSLAM-Drive\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u591a\u573a\u666f\u540c\u6b65\u7684\u81ea\u4e2d\u5fc3\u89c6\u56fe\u30016DoF\u771f\u503c\u4f4d\u59ff\u4e0eAR\u6807\u6ce8\uff0c\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u591a\u79cd\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7a7a\u95f4\u5bf9\u9f50\u4e0e\u611f\u77e5\u4e00\u81f4\u7684AR\u6e32\u67d3\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u573a\u666f\u7406\u89e3\u3001\u4fe1\u606f\u76f8\u5173\u6027\u548c\u9a7e\u9a76\u4f53\u9a8c\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002|\n",
    "2508.17172": "|2025-08-24|VROOM - Visual Reconstruction over Onboard Multiview|Yajat Yadav\u7b49|[2508.17172](http://arxiv.org/pdf/2508.17172)|\u65e0|VROOM\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u9760F1\u8d5b\u8f66\u7684\u8f66\u8f7d\u6444\u50cf\u5934\u89c6\u9891\u6765\u91cd\u5efa\u8d5b\u90533D\u6a21\u578b\u7684\u7cfb\u7edf\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u89e3\u51b3\u4e86\u6781\u7aef\u9ad8\u901f\u8fd0\u52a8\u548c\u89c6\u9891\u5e27\u5267\u70c8\u5207\u6362\u5e26\u6765\u7684\u6311\u6218\u3002\n\u25c6 \u9996\u521b\u4e86\u57fa\u4e8eF1\u8d5b\u8f66\u8f66\u8f7d\u5355\u76ee\u89c6\u9891\u8fdb\u884c\u5927\u89c4\u6a21\u8d5b\u90534D\u91cd\u5efa\u7684\u53ef\u884c\u65b9\u6848\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u5957\u7ed3\u5408DROID-SLAM\u3001AnyCam\u548cMonst3r\u7b49\u591a\u79cd\u65b9\u6cd5\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u9488\u5bf9\u52a8\u6001\u573a\u666f\u8fdb\u884c\u4f18\u5316\u3002\n\u25c6 \u91c7\u7528\u4e86\u5305\u62ec\u63a9\u7801\u3001\u65f6\u95f4\u5206\u5757\u548c\u5206\u8fa8\u7387\u7f29\u653e\u7b49\u9884\u5904\u7406\u6280\u672f\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u6a21\u7cca\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u3002\n\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u5728\u6469\u7eb3\u54e5\u7ad9\u7b49\u590d\u6742\u73af\u5883\u4e2d\u90e8\u5206\u6062\u590d\u8d5b\u9053\u548c\u8f66\u8f86\u8f68\u8ff9\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u666f scalable 4D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2508.17034": "|2025-08-23|DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration|Jiayi Li\u7b49|[2508.17034](http://arxiv.org/pdf/2508.17034)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u7a7a\u95f4\u521a\u6027\u914d\u51c6\u65b9\u6cd5DualReg\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u57fa\u4e8e\u7279\u5f81\u5339\u914d\u548c\u5c40\u90e8\u51e0\u4f55\u5339\u914d\u7684\u4f18\u52bf\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u53cc\u7a7a\u95f4\u8303\u5f0f\uff0c\u5206\u522b\u5904\u7406\u5927\u53d8\u6362\u5dee\u5f02\u7684\u521d\u59cb\u5bf9\u9f50\u548c\u7cbe\u7ec6\u5c40\u90e8\u914d\u51c6\uff0c\u514b\u670d\u4e86\u5355\u4e00\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u9ad8\u6548\u8fc7\u6ee4\u673a\u5236\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u5355\u70b9RANSAC\u7b97\u6cd5\u548c\u7ec6\u5316\u6a21\u5757\u5feb\u901f\u5254\u9664\u4e0d\u53ef\u9760\u7684\u7279\u5f81\u5bf9\u5e94\u70b9\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u63d0\u51fa\u5c06\u8fc7\u6ee4\u540e\u7684\u5bf9\u5e94\u70b9\u4f5c\u4e3a\u951a\u70b9\uff0c\u63d0\u53d6\u51e0\u4f55\u4ee3\u7406\u5e76\u6784\u5efa\u4f18\u5316\u76ee\u6807\u51fd\u6570\uff0c\u914d\u5408\u5b9a\u5236\u6c42\u89e3\u5668\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u53d8\u6362\u4f30\u8ba1\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728KITTI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0eMAC\u76f8\u5f53\u7cbe\u5ea6\u7684\u540c\u65f6\uff0cCPU\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe32\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|\n",
    "2508.16856": "|2025-08-23|A Workflow for Map Creation in Autonomous Vehicle Simulations|Zubair Islam\u7b49|[2508.16856](http://arxiv.org/pdf/2508.16856)|\u65e0|\u672c\u6587\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u4e2d\u9ad8\u7cbe\u5ea6\u5730\u56fe\u521b\u5efa\u56f0\u96be\u4e14\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5730\u56fe\u5236\u4f5c\u5de5\u4f5c\u6d41\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9a\u5236\u5316\u6d41\u7a0b\uff0c\u663e\u8457\u7b80\u5316\u4e86\u4eff\u771f\u5c31\u7eea\u5730\u56fe\u7684\u521b\u5efa\uff0c\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002  \n\u25c6 \u4ee5CARLA\u7b49\u4e3b\u6d41\u4eff\u771f\u5668\u4e3a\u80cc\u666f\uff0c\u4f46\u907f\u514d\u4e86\u5bf9\u5176\u7279\u5b9a\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002  \n\u25c6 \u901a\u8fc7\u751f\u6210\u52a0\u62ff\u5927\u5b89\u5927\u7565\u7406\u5de5\u5927\u5b66\u505c\u8f66\u573a\u76843D\u5730\u56fe\u5b9e\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u5de5\u4f5c\u6d41\u7684\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002  \n\u672a\u6765\u5de5\u4f5c\u5c06\u96c6\u6210SLAM\u6280\u672f\u5e76\u4f18\u5316\u7ecf\u7eac\u5ea6\u5904\u7406\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u7cbe\u5ea6\u548c\u517c\u5bb9\u6027\u3002|\n",
    "2508.16731": "|2025-08-22|COSMO-Bench: A Benchmark for Collaborative SLAM Optimization|Daniel McGann\u7b49|[2508.16731](http://arxiv.org/pdf/2508.16731)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u8bbe\u8ba1\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\uff08C-SLAM\uff09\u4f18\u5316\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6COSMO-Bench\uff0c\u4ee5\u89e3\u51b3\u8be5\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u8bc4\u4f30\u5de5\u5177\u7684\u95ee\u9898\u3002\n\u25c6 \u9996\u6b21\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\u540e\u7aef\u4f18\u5316\u7b97\u6cd5\u7684\u6807\u51c6\u5316\u8bc4\u6d4b\u57fa\u51c6\u3002\n\u25c6 \u63d0\u4f9b\u4e8624\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u6e90\u81ea\u771f\u5b9e\u4e16\u754c\u7684LiDAR\u6570\u636e\u548c\u6700\u5148\u8fdb\u7684C-SLAM\u524d\u7aef\u5904\u7406\u7ed3\u679c\uff0c\u786e\u4fdd\u4e86\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u6311\u6218\u6027\u3002\n\u25c6 \u586b\u8865\u4e86\u591a\u673a\u5668\u4ebaSLAM\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u3001\u516c\u8ba4\u8bc4\u4f30\u6807\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u4e0d\u540c\u7b97\u6cd5\u7684\u516c\u5e73\u6bd4\u8f83\u63d0\u4f9b\u4e86\u5e73\u53f0\u3002\n\u25c6 \u6240\u6709\u6570\u636e\u96c6\u5747\u5df2\u5f00\u6e90\uff0c\u65e8\u5728\u4fc3\u8fdb\u8be5\u7814\u7a76\u9886\u57df\u7684\u534f\u4f5c\u3001\u590d\u73b0\u4e0e\u53d1\u5c55\u3002|\n",
    "2508.20526": "|2025-08-28|Adam SLAM - the last mile of camera calibration with 3DGS|Matthieu Gendrin\u7b49|[2508.20526](http://arxiv.org/pdf/2508.20526)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283D\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09\u4f18\u5316\u76f8\u673a\u6807\u5b9a\u7684\u65b0\u65b9\u6cd5\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u901a\u8fc73DGS\u6a21\u578b\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u5229\u7528\u65b0\u89c6\u89d2\u989c\u8272\u635f\u5931\u5bf9\u76f8\u673a\u53c2\u6570\u8fdb\u884c\u7aef\u5230\u7aef\u7cbe\u7ec6\u4f18\u5316\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u76f8\u673a\u6807\u5b9a\u4e2d1\u50cf\u7d20\u8bef\u5dee\u5bf9\u91cd\u5efa\u8d28\u91cf\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u7684\u5173\u952e\u95ee\u9898\uff0c\u76f4\u63a5\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002  \n\u25c6 \u57283DGS\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u5e26\u67650.4 dB PSNR\u7684\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6807\u5b9a\u65b9\u6cd5\u3002  \n\u25c6 \u867d\u7136\u4f18\u5316\u8fc7\u7a0b\u8017\u65f6\u8f83\u957f\uff0c\u4f46\u4e3a\u53c2\u8003\u573a\u666f\uff08\u5982Mip-NeRF 360\uff09\u7684\u6807\u5b9a\u63d0\u4f9b\u4e86\u4ee5\u8d28\u91cf\u4e3a\u4f18\u5148\u7684\u89e3\u51b3\u65b9\u6848\u3002  \n\u8be5\u65b9\u6cd5\u5c24\u5176\u9002\u7528\u4e8e\u5bf9\u6807\u5b9a\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\u7684\u9ad8\u8d28\u91cf\u65b0\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u3002|\n",
    "2508.21635": "|2025-08-29|The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics|Nicolas Soncini\u7b49|[2508.21635](http://arxiv.org/pdf/2508.21635)|\u65e0|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e13\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u8bbe\u8ba1\u7684Rosario v2\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u4e3a\u590d\u6742\u519c\u7530\u73af\u5883\u4e0b\u7684\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\u3002\u4e3b\u8981\u521b\u65b0\u70b9\u5305\u62ec\uff1a  \n\u25c6 \u63d0\u4f9b\u8d85\u8fc7\u4e24\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u6db5\u76d6\u7ea2\u5916/\u5f69\u8272\u76f8\u673a\u3001IMU\u3001\u591a\u6a21\u5f0fGNSS\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\uff0c\u786c\u4ef6\u7ea7\u540c\u6b65\u786e\u4fdd\u6570\u636e\u4e00\u81f4\u6027\u3002  \n\u25c6 \u7cbe\u51c6\u6355\u6349\u519c\u4e1a\u573a\u666f\u5178\u578b\u6311\u6218\uff1a\u5149\u7167\u7a81\u53d8\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u98a0\u7c38\u5730\u5f62\u53ca\u957f\u8ddd\u79bb\u89c6\u89c9\u76f8\u4f3c\u8def\u5f84\uff0c\u9ad8\u5ea6\u8fd8\u539f\u771f\u5b9e\u4f5c\u4e1a\u56f0\u96be\u3002  \n\u25c6 \u63d0\u4f9b\u516d\u81ea\u7531\u5ea6\u771f\u503c\u8f68\u8ff9\u548c\u95ed\u73af\u68c0\u6d4b\u6240\u9700\u7684\u957f\u8def\u5f84\u5faa\u73af\uff0c\u6ee1\u8db3\u591a\u6a21\u6001SLAM\u7cfb\u7edf\u4e25\u683c\u8bc4\u6d4b\u9700\u6c42\u3002  \n\u25c6 \u901a\u8fc7\u8fd0\u884c\u4e3b\u6d41SLAM\u7b97\u6cd5\u5e76\u63ed\u793a\u5176\u5728\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u548c\u6311\u6218\u6027\u3002  \n\u8be5\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u519c\u4e1a\u673a\u5668\u4eba\u5b9a\u4f4d\u3001\u5efa\u56fe\u4e0e\u5bfc\u822a\u7b97\u6cd5\u7684\u7814\u53d1\u548c\u6027\u80fd\u8bc4\u4f30\u3002|\n",
    "2509.02972": "|2025-09-03|IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments|Haolan Zhang\u7b49|[2509.02972](http://arxiv.org/pdf/2509.02972)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u611f\u77e5\u7684\u667a\u80fd\u7ebf\u8f85\u52a9\u52a8\u6001SLAM\u7cfb\u7edfIL-SLAM\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7279\u5f81\u7ba1\u7406\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u7279\u5f81\u611f\u77e5\u673a\u5236\uff0c\u52a8\u6001\u8bc4\u4f30\u73b0\u6709\u70b9\u7279\u5f81\u7684\u5145\u8db3\u6027\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u6fc0\u6d3b\u7ebf\u7279\u5f81\u8865\u5145\uff0c\u907f\u514d\u76f2\u76ee\u5f15\u5165\u989d\u5916\u7279\u5f81\u3002  \n\u25c6 \u901a\u8fc7\u9009\u62e9\u6027\u5f15\u5165\u7ebf\u7279\u5f81\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u4f4e\u8d28\u91cf\u7279\u5f81\u548c\u566a\u58f0\u7684\u7d2f\u79ef\u3002  \n\u25c6 \u5728\u7ebf\u7279\u5f81\u4f7f\u7528\u7b56\u7565\u4e0a\uff0c\u5141\u8bb8\u5176\u5728\u8ddf\u8e2a\u3001\u5c40\u90e8\u5efa\u56fe\u4e0e\u56de\u73af\u68c0\u6d4b\u4e2d\u8f85\u52a9\u63d0\u5347\u521d\u59cb\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4f46\u5c06\u5176\u6392\u9664\u5728\u5168\u5c40\u4f18\u5316\u4e4b\u5916\uff0c\u907f\u514d\u957f\u671f\u8fc7\u7a0b\u4e2d\u7684\u8d1f\u9762\u5e72\u6270\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u5728TUM\u6570\u636e\u96c6\u4e0a\u7684ATE\u548cRPE\u6307\u6807\u5747\u4f18\u4e8eORB-SLAM3\u57fa\u7ebf\u53ca\u5176\u4ed6\u52a8\u6001SLAM\u4e0e\u591a\u7279\u5f81\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002|\n",
    "2509.02453": "|2025-09-02|Coral: A Unifying Abstraction Layer for Composable Robotics Software|Steven Swanbeck\u7b49|[2509.02453](http://arxiv.org/pdf/2509.02453)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Coral\uff0c\u4e00\u4e2a\u7528\u4e8e\u7ec4\u5408\u5f0f\u673a\u5668\u4eba\u8f6f\u4ef6\u7684\u7edf\u4e00\u62bd\u8c61\u5c42\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u7cfb\u7edf\u96c6\u6210\u56f0\u96be\u7684\u6838\u5fc3\u95ee\u9898\u3002  \n\u25c6\u5f15\u5165\u4e86\u4e00\u4e2a\u66f4\u9ad8\u5c42\u6b21\u7684\u62bd\u8c61\u5c42\uff0c\u5728\u4e0d\u4fee\u6539\u5e95\u5c42\u4ee3\u7801\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8f6f\u4ef6\u7ec4\u4ef6\u7684\u5feb\u901f\u96c6\u6210\u4e0e\u534f\u8c03\u3002  \n\u25c6\u901a\u8fc7\u8bed\u4e49\u5316\u7ea6\u675f\u96c6\u6210\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u914d\u7f6e\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u4e0d\u540c\u9886\u57df\u548c\u4efb\u52a1\u7684\u5e7f\u6cdb\u9002\u5e94\u6027\u3002  \n\u25c6\u4e0e\u73b0\u6709\u5de5\u5177\u517c\u5bb9\u800c\u975e\u66ff\u4ee3\uff0c\u589e\u5f3a\u4e86\u7ec4\u4ef6\u590d\u7528\u6027\u548c\u7cfb\u7edf\u53ef\u91cd\u6784\u6027\u3002  \n\u25c6\u5728\u590d\u6742\u573a\u666f\uff08\u5982LiDAR SLAM\u548c\u591a\u673a\u5668\u4eba\u534f\u4f5c\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u89e3\u51b3\u96c6\u6210\u6311\u6218\u7684\u5b9e\u7528\u4ef7\u503c\u3002  \n\u25c6\u5f00\u6e90\u53d1\u5e03Coral\uff0c\u63d0\u5347\u4e86\u4e13\u5bb6\u4e0e\u975e\u4e13\u5bb6\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u8f6f\u4ef6\u5f00\u53d1\u7684\u6807\u51c6\u5316\u4e0e\u666e\u53ca\u3002|\n",
    "2509.01873": "|2025-09-02|Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction|Xueyang Kang|[2509.01873](http://arxiv.org/pdf/2509.01873)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4e09\u7ef4\u89c6\u89c9\u4e2d\u7684\u5173\u952e\u4efb\u52a1\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u9488\u5bf9\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u3001\u70b9\u4e91\u914d\u51c6\u3001\u6df1\u5ea6\u9884\u6d4b\u548c\u4e09\u7ef4\u91cd\u5efa\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7279\u5f81\u6a21\u7cca\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u901a\u8fc7\u5c06\u6df1\u5ea6\u4fe1\u606f\u3001\u8868\u9762\u6cd5\u7ebf\u548c\u7b49\u53d8\u7ea6\u675f\u7b49\u51e0\u4f55\u5148\u9a8c\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u589e\u5f3a\u4e86\u51e0\u4f55\u8868\u793a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002  \n\u25c6 \u7cfb\u7edf\u7814\u7a76\u4e86\u4e09\u7ef4\u89c6\u89c9\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6570\u5b57\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u548c\u6c89\u6d78\u5f0fVR/AR\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u4e09\u7ef4\u6570\u636e\u9ad8\u7ef4\u6027\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u5e26\u6765\u7684\u8bad\u7ec3\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u4f20\u7edf\u51e0\u4f55\u6280\u672f\u4e0e\u6df1\u5ea6\u5b66\u4e60\u80fd\u529b\u7684\u878d\u5408\u3002  \n\u8be5\u7814\u7a76\u4e3a\u751f\u6210\u5177\u6709\u51e0\u4f55\u611f\u77e5\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u4e09\u7ef4\u6620\u5c04\u6280\u672f\u548c\u573a\u666f\u91cd\u5efa\u7ba1\u9053\u7684\u53d1\u5c55\u3002|\n",
    "2509.01584": "|2025-09-01|ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association|Ganlin Zhang\u7b49|[2509.01584](http://arxiv.org/pdf/2509.01584)|\u65e0|ViSTA-SLAM\u662f\u4e00\u4e2a\u65e0\u9700\u5df2\u77e5\u76f8\u673a\u5185\u53c2\u5373\u53ef\u5b9e\u65f6\u8fd0\u884c\u7684\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u5bf9\u79f0\u5f0f\u53cc\u89c6\u56fe\u5173\u8054\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u4e0e\u9002\u7528\u6027\u3002\n\n\u25c6 \u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5bf9\u79f0\u53cc\u89c6\u56fe\u5173\u8054\uff08STA\uff09\u524d\u7aef\u6a21\u578b\uff0c\u4ec5\u9700\u4e24\u5f20RGB\u56fe\u50cf\u5373\u53ef\u540c\u65f6\u4f30\u8ba1\u76f8\u5bf9\u76f8\u673a\u4f4d\u59ff\u5e76\u56de\u5f52\u5c40\u90e8\u70b9\u4e91\u5730\u56fe\u3002\n\u25c6 \u8be5\u524d\u7aef\u8bbe\u8ba1\u6781\u5927\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5176\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a\u540c\u7c7b\u5148\u8fdb\u65b9\u6cd5\u768435%\uff0c\u540c\u65f6\u751f\u6210\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u53cc\u89c6\u56fe\u7ea6\u675f\u7528\u4e8e\u540e\u7eed\u4f18\u5316\u3002\n\u25c6 \u540e\u7aef\u6784\u5efa\u4e86\u4e00\u4e2a\u7279\u6b8a\u7684Sim(3)\u4f4d\u59ff\u56fe\uff0c\u901a\u8fc7\u878d\u5165\u56de\u73af\u68c0\u6d4b\u6765\u6709\u6548\u5904\u7406\u7d2f\u79ef\u7684\u5c3a\u5ea6\u6f02\u79fb\u95ee\u9898\u3002\n\u25c6 \u6574\u4e2a\u7cfb\u7edf\u4e0d\u4f9d\u8d56\u76f8\u673a\u5185\u53c2\uff0c\u4f7f\u5176\u80fd\u591f\u5e7f\u6cdb\u9002\u7528\u4e8e\u5404\u79cd\u4e0d\u540c\u7684\u76f8\u673a\u8bbe\u7f6e\uff0c\u5177\u5907\u4e86\u5f88\u5f3a\u7684\u901a\u7528\u6027\u3002\n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u76f8\u673a\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u7a20\u5bc6\u4e09\u7ef4\u91cd\u5efa\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002|\n",
    "2509.01547": "|2025-09-01|FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field|Fan Zhu\u7b49|[2509.01547](http://arxiv.org/pdf/2509.01547)|\u65e0|FGO-SLAM\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u5c40\u4e00\u81f4\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u7684\u65b0\u578b\u9ad8\u65afSLAM\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u51e0\u4f55\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002\u5176\u521b\u65b0\u70b9\u4e3b\u8981\u4f53\u73b0\u5728\uff1a\n\n\u25c6\u91c7\u7528\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u4f5c\u4e3a\u573a\u666f\u8868\u793a\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u51e0\u4f55\u5efa\u56fe\u6027\u80fd\u3002\n\u25c6\u5728\u521d\u59cb\u4f4d\u59ff\u4f30\u8ba1\u540e\uff0c\u5f15\u5165\u5168\u5c40\u8c03\u6574\u4f18\u5316\u7b56\u7565\uff0c\u540c\u65f6\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u548c\u7a00\u758f\u70b9\u4e91\uff0c\u786e\u4fdd\u4e86\u9c81\u68d2\u4e14\u7cbe\u786e\u7684\u8ddf\u8e2a\u3002\n\u25c6\u7ef4\u62a4\u4e86\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u4e14\u5168\u5c40\u4e00\u81f4\u7684\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\uff0c\u5e76\u5f15\u5165\u4e86\u6df1\u5ea6\u5931\u771f\u548c\u6cd5\u5411\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u7cbe\u7ec6\u5316\u573a\u666f\u8868\u793a\u3002\n\u25c6\u901a\u8fc7\u6784\u5efa\u56db\u9762\u4f53\u7f51\u683c\u5e76\u63d0\u53d6\u7b49\u503c\u9762\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece3D\u9ad8\u65af\u4e2d\u63d0\u53d6\u8868\u9762\uff0c\u7b80\u5316\u4e86\u8868\u9762\u91cd\u5efa\u6d41\u7a0b\u3002\n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u771f\u5b9e\u548c\u5927\u578b\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5efa\u56fe\u6027\u80fd\u3002|\n",
    "2509.01111": "|2025-09-01|SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments|Haolan Zhang\u7b49|[2509.01111](http://arxiv.org/pdf/2509.01111)|\u65e0|SR-SLAM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u53ef\u9760\u6027\u7684RGB-D SLAM\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u89c9SLAM\u7cfb\u7edf\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5f15\u5165\u7edf\u4e00\u7684\u73af\u5883\u611f\u77e5\u4e0e\u53ef\u9760\u6027\u8bc4\u4f30\u673a\u5236\uff0c\u901a\u8fc7\u591a\u6307\u6807\u548c\u5386\u53f2\u89c2\u6d4b\u52a8\u6001\u6307\u5bfc\u7cfb\u7edf\u884c\u4e3a\u3002\u5177\u4f53\u8d21\u732e\u5305\u62ec\uff1a\n\u25c6\u63d0\u51fa\u81ea\u9002\u5e94\u52a8\u6001\u533a\u57df\u9009\u62e9\u65b9\u6cd5\uff0c\u7ed3\u5408\u7075\u6d3b\u51e0\u4f55\u7ea6\u675f\u4ee5\u63d0\u5347\u52a8\u6001\u76ee\u6807\u8bc6\u522b\u80fd\u529b\u3002\n\u25c6\u5f00\u53d1\u6df1\u5ea6\u8f85\u52a9\u7684\u81ea\u9002\u5e94\u805a\u7c7b\u7b97\u6cd5\uff0c\u5728\u9ad8\u7ef4\u73af\u5883\u4e0b\u9ad8\u6548\u5254\u9664\u52a8\u6001\u7279\u5f81\u70b9\u3002\n\u25c6\u8bbe\u8ba1\u53ef\u9760\u6027\u611f\u77e5\u7684\u4f4d\u59ff\u4f18\u5316\u7b56\u7565\uff0c\u5728\u7279\u5f81\u4e0d\u8db3\u65f6\u52a8\u6001\u878d\u5408\u76f4\u63a5\u6cd5\u4ee5\u63d0\u5347\u4f30\u8ba1\u7a33\u5b9a\u6027\u3002\n\u25c6\u63d0\u51fa\u57fa\u4e8e\u53ef\u9760\u6027\u7684\u5173\u952e\u5e27\u9009\u62e9\u4e0e\u52a0\u6743\u4f18\u5316\u65b9\u6848\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002\n\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7cfb\u7edf\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u52a8\u6001SLAM\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u63d0\u5347\u6700\u9ad8\u8fbe90%\u3002|\n",
    "2509.00741": "|2025-08-31|DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments|Yi Liu\u7b49|[2509.00741](http://arxiv.org/pdf/2509.00741)|\u65e0|DyPho-SLAM\u662f\u4e00\u79cd\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u8fd0\u884c\u7684\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u662f\u89e3\u51b3\u4e86\u52a8\u6001\u7269\u4f53\u5e72\u6270\u5bfc\u81f4\u7684\u76f8\u673a\u8ddf\u8e2a\u6f02\u79fb\u548c\u5730\u56fe\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u5bc6\u96c6\u5730\u56fe\u91cd\u5efa\u3002\n\n\u25c6 \u7387\u5148\u5c06\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff08Gaussian Splatting\uff09\u7528\u4e8e\u52a8\u6001\u73af\u5883SLAM\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u5149\u7535\u771f\u5b9e\u611f\u5efa\u56fe\u3002\n\u25c6 \u521b\u65b0\u5730\u6574\u5408\u5148\u9a8c\u56fe\u50cf\u4fe1\u606f\u6765\u751f\u6210\u7cbe\u7ec6\u5316\u63a9\u7801\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u56e0\u52a8\u6001\u7269\u4f53\u8bef\u5224\u800c\u4ea7\u751f\u7684\u566a\u58f0\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u7684\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u5728\u79fb\u9664\u52a8\u6001\u969c\u788d\u7269\u540e\u4e3a\u7cfb\u7edf\u4f18\u5316\u589e\u5f3a\u4e86\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6574\u4e2a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002\n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u516c\u5f00\u52a8\u6001RGB-D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u5730\u56fe\u91cd\u5efa\u7cbe\u5ea6\u3002|\n",
    "2509.00433": "|2025-08-30|AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection|Houshu He\u7b49|[2509.00433](http://arxiv.org/pdf/2509.00433)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AGS\uff0c\u4e00\u4e2a\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u65e8\u5728\u663e\u8457\u52a0\u901f\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684SLAM\u7cfb\u7edf\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5728\u4e8e\u5145\u5206\u5229\u7528\u4e86SLAM\u6d41\u5f0f\u5904\u7406\u4e2d\u76f8\u90bb\u5e27\u7684\u9ad8\u76f8\u4f3c\u6027\u3002  \n\u25c6 \u5728\u8f6f\u4ef6\u5c42\u9762\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6839\u636e\u673a\u5668\u4eba\u8fd0\u52a8\u8fdb\u884c\u5148\u7c97\u540e\u7cbe\u7684\u4f4d\u59ff\u8ddf\u8e2a\u65b9\u6cd5\u3002  \n\u25c6 \u901a\u8fc7\u5728\u5e27\u95f4\u5171\u4eab\u9ad8\u65af\u70b9\u7684\u8d21\u732e\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\u3002  \n\u25c6 \u5728\u786c\u4ef6\u5c42\u9762\uff0c\u521b\u65b0\u5730\u5229\u7528\u89c6\u9891\u7f16\u89e3\u7801\u5668\u63d0\u53d6\u4e2d\u95f4\u6570\u636e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5e27\u5171\u89c6\u6027\u68c0\u6d4b\u5f15\u64ce\u3002  \n\u25c6 \u8fd8\u5b9e\u73b0\u4e86\u914d\u5907\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u7684\u4f4d\u59ff\u8ddf\u8e2a\u5f15\u64ce\u548c\u5efa\u56fe\u5f15\u64ce\uff0c\u4ee5\u9ad8\u6548\u90e8\u7f72\u6574\u4e2aAGS\u7b97\u6cd5\u3002  \n\u6700\u7ec8\uff0cAGS\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u5728\u79fb\u52a8GPU\u3001\u9ad8\u7aefGPU\u53ca\u4e13\u7528\u52a0\u901f\u5668\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad817.12\u500d\u30016.71\u500d\u548c5.41\u500d\u7684\u52a0\u901f\u3002|\n",
    "2509.04370": "|2025-09-04|Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage|Dor Cohen\u7b49|[2509.04370](http://arxiv.org/pdf/2509.04370)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u8ba1\u7b97\u673a\u89c6\u89c9\u6d41\u7a0b\uff0c\u5c06\u6267\u6cd5\u6216\u6025\u6551\u4eba\u5458\u4f69\u6234\u7684\u4f53\u6234\u76f8\u673a\u89c6\u9891\u8f6c\u5316\u4e3a\u7b80\u6d01\u7684\u5168\u666f\u4e8b\u4ef6\u6458\u8981\u56fe\u50cf\u3002  \n\u25c6 \u63d0\u51fa\u5229\u7528\u5355\u76eeSLAM\u6280\u672f\u4ece\u89c6\u9891\u4e2d\u4f30\u8ba1\u76f8\u673a\u8fd0\u52a8\u8f68\u8ff9\u5e76\u91cd\u5efa\u573a\u666f\u7a7a\u95f4\u5e03\u5c40\uff0c\u4e3a\u521b\u5efa\u7a7a\u95f4\u8fde\u8d2f\u7684\u6458\u8981\u5960\u5b9a\u57fa\u7840\u3002  \n\u25c6 \u901a\u8fc7\u6cbf\u8f68\u8ff9\u5bf9\u76f8\u673a\u4f4d\u59ff\u8fdb\u884c\u805a\u7c7b\u6765\u8bc6\u522b\u5173\u952e\u89c6\u89d2\uff0c\u786e\u4fdd\u6458\u8981\u80fd\u8986\u76d6\u573a\u666f\u7684\u91cd\u8981\u90e8\u5206\u3002  \n\u25c6 \u91c7\u7528\u591a\u5e27\u56fe\u50cf\u62fc\u63a5\u6280\u672f\uff0c\u5c06\u9009\u5b9a\u4ee3\u8868\u5e27\u878d\u5408\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u56fe\uff0c\u4fdd\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u5b8c\u6574\u6027\u3002  \n\u25c6 \u6700\u7ec8\u751f\u6210\u7684\u5168\u666f\u6458\u8981\u56fe\u50cf\u652f\u6301\u5feb\u901f\u73af\u5883\u7406\u89e3\u548c\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u5197\u957f\u89c6\u9891\u56de\u987e\u8017\u65f6\u4f4e\u6548\u7684\u75db\u70b9\u3002  \n\u8be5\u6280\u672f\u63d0\u5347\u4e86\u4f53\u6234\u76f8\u673a\u6570\u636e\u5728\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u9002\u7528\u4e8e\u4e8b\u540e\u5206\u6790\u548c\u5e94\u6025\u54cd\u5e94\u3002|\n",
    "2509.04016": "|2025-09-04|Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot|Branimir \u0106aran\u7b49|[2509.04016](http://arxiv.org/pdf/2509.04016)|\u65e0|\u672c\u6587\u9488\u5bf9\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u72ec\u7acb\u9a71\u52a8\uff084WIS4WID\uff09\u722c\u58c1\u673a\u5668\u4eba\u5728\u590d\u6742\u5efa\u7b51\u7acb\u9762\u4e0a\u7684\u5b9a\u4f4d\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u4f4d\u59ff\u4f30\u8ba1\u5668\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u3001\u89c6\u89c9\u91cc\u7a0b\u8ba1\u548cIMU\u6570\u636e\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08EKF\uff09\u548c\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\u8fdb\u884c\u878d\u5408\uff0c\u6784\u5efa\u4e86\u9c81\u68d2\u7684\u4f4d\u59ff\u4f30\u8ba1\u7cfb\u7edf\u3002  \n\u25c6 \u9488\u5bf9\u91cc\u7a0b\u8ba1\u7684\u7cfb\u7edf\u8bef\u5dee\uff0c\u7ed3\u5408\u4f7f\u7528\u4e86\u975e\u7ebf\u6027\u4f18\u5316\u3001Levenberg-Marquardt\u7b49\u786e\u5b9a\u6027\u65b9\u6cd5\u4ee5\u53ca\u9057\u4f20\u7b97\u6cd5\u3001\u7c92\u5b50\u7fa4\u7b49\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u8fd0\u52a8\u5b66\u6821\u51c6\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u7cfb\u7edf\u8bef\u5dee\u548c\u6f02\u79fb\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u5728\u65e0\u6cd5\u4f7f\u7528GPS\u3001\u6fc0\u5149\u96f7\u8fbe\u7b49\u4f20\u7edf\u4f20\u611f\u5668\u7684\u7279\u6b8a\u5de5\u4f5c\u73af\u5883\uff08\u5982\u94a2\u7b4b\u6df7\u51dd\u571f\u7acb\u9762\uff09\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u95ee\u9898\u3002  \n\u8be5\u65b9\u6848\u901a\u8fc7\u5b9e\u7269\u673a\u5668\u4eba\u5b9e\u9a8c\u8be6\u7ec6\u9a8c\u8bc1\u4e86\u5176\u6821\u51c6\u65b9\u6cd5\u548c\u4f4d\u4f30\u8ba1\u7b97\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u6027\u80fd\uff0c\u4e3a\u722c\u58c1\u673a\u5668\u4eba\u5728\u5b9e\u9645\u6d4b\u91cf\u548c\u7ef4\u62a4\u4efb\u52a1\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u57fa\u7840\u3002|\n",
    "2509.06582": "|2025-09-08|Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization|Carlos A. Pinheiro de Sousa\u7b49|[2509.06582](http://arxiv.org/pdf/2509.06582)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7528\u6237\u534f\u540c\u5b9a\u4f4dVR\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5916\u90e8\u52a8\u6355\u7cfb\u7edf\u548c\u5934\u663e\u5185\u7f6eSLAM\u8ddf\u8e2a\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u7684\u5171\u4eab\u865a\u62df\u7a7a\u95f4\u540c\u6b65\u3002  \n\u25c6 \u7ed3\u5408\u57fa\u4e8eSLAM\u7684\u5934\u663e\u5185\u5411\u5916\u8ffd\u8e2a\u4e0e\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\uff0c\u517c\u987e\u9ad8\u5e27\u7387\u3001\u4f4e\u5ef6\u8fdf\u6027\u80fd\u548c\u957f\u671f\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u7a81\u7834\u4f20\u7edf\u4f9d\u8d56\u6301\u7eed\u5916\u90e8\u8ffd\u8e2a\uff08\u6613\u5ef6\u8fdf\u6296\u52a8\uff09\u6216\u4e00\u6b21\u6027\u6821\u51c6\uff08\u65e0\u6cd5\u6821\u6b63\u6f02\u79fb\uff09\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u52a8\u6001\u6309\u9700\u91cd\u5bf9\u9f50\u3002  \n\u25c6 \u652f\u6301\u8de8\u8bbe\u5907\u5b9e\u65f6\u59ff\u6001\u5171\u4eab\uff0c\u786e\u4fdd\u591a\u7528\u6237\u95f4\u7a7a\u95f4\u4e00\u81f4\u6027\u53ca\u4ea4\u4e92\u6c89\u6d78\u611f\u3002  \n\u25c6 \u5728\u4fdd\u6301\u9ad8\u7a7a\u95f4\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u8212\u9002\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002|\n",
    "2509.06433": "|2025-09-08|Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation|Ian Page\u7b49|[2509.06433](http://arxiv.org/pdf/2509.06433)|\u65e0|\u8be5\u8bba\u6587\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u903c\u771f\u5730\u56fe\u6784\u5efa\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u9ad8\u65af\u6e85\u5c04SLAM\u6280\u672f\u4e0e\u5728\u7ebf\u5730\u56fe\u9065\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u884c\u6a21\u5757\u5316\u96c6\u6210  \n\u25c6 \u91c7\u7528\u57fa\u4e8eGPU\u7684\u9ad8\u6548\u8ba1\u7b97\u67b6\u6784\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u7cbe\u5ea6\u4e09\u7ef4\u5730\u56fe\u91cd\u5efa  \n\u25c6 \u7a81\u7834\u4f20\u7edf\u7cfb\u7edf\u56e0\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u5bfc\u81f4\u7684\u89c6\u89c9\u5730\u56fe\u8d28\u91cf\u4e0e\u5b9e\u65f6\u6027\u74f6\u9888  \n\u25c6 \u901a\u8fc7\u771f\u5b9e\u73af\u5883\u65e0\u4eba\u673a\u5b9e\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\u80fd\u5927\u5e45\u63d0\u5347\u64cd\u4f5c\u8005\u51b3\u7b56\u901f\u5ea6\u548c\u73af\u5883\u4ea4\u4e92\u7cbe\u5ea6  \n\u8be5\u7cfb\u7edf\u9996\u6b21\u5b9e\u73b0\u5b9e\u65f6\u903c\u771f\u5730\u56fe\u751f\u6210\u4e0e\u9065\u64cd\u4f5c\u4efb\u52a1\u7684\u65e0\u7f1d\u878d\u5408\uff0c\u4e3a\u964c\u751f\u73af\u5883\u4e0b\u7684\u8fdc\u7a0b\u64cd\u63a7\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.05728": "|2025-09-06|LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction|Niels Balemans\u7b49|[2509.05728](http://arxiv.org/pdf/2509.05728)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86LiDAR-BIND-T\uff0c\u901a\u8fc7\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u6539\u8fdb\u4e86\u591a\u6a21\u6001\u878d\u5408SLAM\u7cfb\u7edf\u3002\u5176\u6838\u5fc3\u8d21\u732e\u4e0e\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u5f15\u5165\u65f6\u95f4\u5d4c\u5165\u76f8\u4f3c\u6027\u635f\u5931\uff0c\u663e\u5f0f\u5bf9\u9f50\u8fde\u7eed\u65f6\u523b\u7684\u6f5c\u5728\u7279\u5f81\u4ee5\u4fdd\u6301\u65f6\u5e8f\u8fde\u8d2f\u3002\n\u25c6 \u63d0\u51fa\u8fd0\u52a8\u5bf9\u9f50\u53d8\u6362\u635f\u5931\uff0c\u786e\u4fdd\u9884\u6d4b\u70b9\u4e91\u4e0e\u771f\u5b9eLiDAR\u70b9\u4e91\u4e4b\u95f4\u7684\u4f4d\u79fb\u4e00\u81f4\u6027\u3002\n\u25c6 \u8bbe\u8ba1\u4e13\u7528\u65f6\u5e8f\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u6ed1\u7a97\u65b9\u5f0f\u878d\u5408\u591a\u5e27\u4fe1\u606f\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\u3002\n\u25c6 \u4f18\u5316\u6a21\u578b\u67b6\u6784\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u7a7a\u95f4\u7ed3\u6784\uff0c\u63d0\u5347\u8de8\u6a21\u6001\uff08\u96f7\u8fbe/\u58f0\u7eb3\u5230LiDAR\uff09\u91cd\u5efa\u8d28\u91cf\u3002\n\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86SLAM\u7684\u8f68\u8ff9\u7cbe\u5ea6\u548c\u5730\u56fe\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eFVMD\u548c\u76f8\u5173\u5cf0\u503c\u8ddd\u79bb\u7684\u5b9e\u7528\u65f6\u5e8f\u8bc4\u4f30\u6307\u6807\u3002|\n",
    "2509.07775": "|2025-09-09|Sensing with Mobile Devices through Radio SLAM: Models, Methods, Opportunities, and Challenges|Yu Ge\u7b49|[2509.07775](http://arxiv.org/pdf/2509.07775)|\u65e0|\u672c\u6587\u63a2\u8ba8\u4e86\u65e0\u7ebf\u7535SLAM\u4f5c\u4e3a6G\u901a\u611f\u4e00\u4f53\u5316\uff08ISAC\uff09\u7684\u5173\u952e\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u7ebf\u4fe1\u53f7\u5b9e\u73b0\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002  \n\u25c6 \u63d0\u51fa\u5c06\u65e0\u7ebf\u7535SLAM\u4f5c\u4e3a6G\u901a\u611f\u4e00\u4f53\u5316\u7684\u6838\u5fc3\u5b9e\u73b0\u8def\u5f84\uff0c\u901a\u8fc7\u5355\u4e00\u65e0\u7ebf\u4fe1\u53f7\u540c\u65f6\u652f\u6301\u901a\u4fe1\u4e0e\u73af\u5883\u611f\u77e5\u3002  \n\u25c6 \u7cfb\u7edf\u5206\u6790\u4e86\u4e0d\u540c\u9891\u6bb5\u4e0b\u65e0\u7ebf\u7535SLAM\u7684\u6027\u80fd\u6743\u8861\uff0c\u5305\u62ec\u8986\u76d6\u8303\u56f4\u3001\u5206\u8fa8\u7387\u548c\u786c\u4ef6\u9700\u6c42\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002  \n\u25c6 \u5f3a\u8c03\u4e86\u4e0e\u4f20\u611f\u3001\u5b9a\u4f4d\u53ca\u534f\u540c\u7f51\u7edc\u878d\u5408\u7684\u673a\u9047\uff0c\u63a8\u52a8\u591a\u6280\u672f\u534f\u540c\u53d1\u5c55\u3002  \n\u25c6 \u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u548c\u5de5\u4e1a\u673a\u5668\u4eba\u7b496G\u5e94\u7528\u9886\u57df\u7684\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u5960\u5b9a\u57fa\u7840\uff0c\u4fc3\u8fdb\u6280\u672f\u843d\u5730\u3002  \n\u8bba\u6587\u901a\u8fc7\u6a21\u578b\u548c\u65b9\u6cd5\u521b\u65b0\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u73af\u5883\u611f\u77e5\u4e0e\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002|\n",
    "2509.07683": "|2025-09-10|Robust Radar SLAM for Vehicle Parking Applications|Luis Diener\u7b49|[2509.07683](http://arxiv.org/pdf/2509.07683)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u81ea\u52a8\u6cca\u8f66\u573a\u666f\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f7\u8fbe\u7684\u9c81\u68d2SLAM\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4f20\u611f\u5668\u6807\u5b9a\u6210\u672c\u9ad8\u548c\u5bf9\u6076\u52a3\u5929\u6c14\u654f\u611f\u7684\u95ee\u9898\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\n\u25c6 \u63d0\u51fa\u4e00\u79cd\u591a\u666e\u52d2\u589e\u5f3a\u7684\u96f7\u8fbeSLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u666e\u52d2\u901f\u5ea6\u548c\u7279\u5f81\u4f4d\u7f6e\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u6570\u636e\u5173\u8054\u4e0e\u6ee4\u6ce2\u5668\u6536\u655b  \n\u25c6 \u652f\u6301\u591a\u96f7\u8fbe\u4f20\u611f\u5668\u878d\u5408\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u611f\u77e5\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027  \n\u25c6 \u8bbe\u8ba1\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u7279\u5f81\u7b5b\u9009\u7b56\u7565\uff0c\u4f18\u5316\u5730\u56fe\u7279\u5f81\u7ba1\u7406\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u591f\u6ee1\u8db3\u81ea\u52a8\u6cca\u8f66\u5bf9\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u4e25\u683c\u8981\u6c42\u3002|\n",
    "2509.07362": "|2025-09-09|Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark|Yandi Yang\u7b49|[2509.07362](http://arxiv.org/pdf/2509.07362)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u6784\u5efa\u4e86\u4e00\u4e2a\u89e3\u51b3\u7a7a\u5730\u8de8\u6a21\u6001\u5b9a\u4f4d\u6311\u6218\u7684\u7efc\u5408\u57fa\u51c6\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u7a7a\u5730\u8de8\u6a21\u6001\u6570\u636e\u96c6\uff0c\u96c6\u6210\u4e86\u6765\u81ea\u79fb\u52a8\u6d4b\u91cf\u7cfb\u7edf\u7684\u5730\u9762\u56fe\u50cf\u548c\u4e09\u4e2a\u57ce\u5e02\uff08\u6b66\u6c49\u3001\u9999\u6e2f\u3001\u65e7\u91d1\u5c71\uff09\u7684\u673a\u8f7d\u6fc0\u5149\u626b\u63cf\uff08ALS\uff09\u70b9\u4e91\u3002\n\u25c6 \u89e3\u51b3\u4e86\u5e73\u53f0\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u4e14\u771f\u5b9e\u7684\u6570\u636e\u57fa\u7840\u3002\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u7684\u53ef\u9760\u5730\u9762\u771f\u503c\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u957f\u671f\u7f3a\u4e4f\u7cbe\u786e\u57fa\u51c6\u7684\u96be\u9898\u3002\n\u25c6 \u9996\u6b21\u5728\u7a7a\u5730\u8de8\u5e73\u53f0\u8bbe\u7f6e\u4e0b\u5bf9\u73b0\u6709\u7684\u56fe\u50cf\u5230\u70b9\u4e91\uff08I2P\uff09\u5b9a\u4f4d\u7b97\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u9a8c\u8bc1\u3002\n\u901a\u8fc7\u63d0\u4f9b\u6570\u636e\u96c6\u3001\u5730\u9762\u771f\u503c\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u5de5\u4f5c\u6781\u5927\u5730\u63a8\u52a8\u548c\u4fc3\u8fdb\u4e86\u57fa\u4e8eALS\u5148\u9a8c\u5730\u56fe\u7684\u3001\u53ef\u6269\u5c55\u4e14\u7cbe\u786e\u7684\u89c6\u89c9\u5b9a\u4f4d\u6280\u672f\u7684\u53d1\u5c55\u3002|\n",
    "2509.08333": "|2025-09-10|Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry|Sai Puneeth Reddy Gottam\u7b49|[2509.08333](http://arxiv.org/pdf/2509.08333)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u56e0\u5149\u7167\u53d8\u5316\u3001\u52a8\u6001\u573a\u666f\u7b49\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0e\u8ddf\u8e2a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\u3002  \n\u25c6 \u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u53cd\u9988\uff0c\u589e\u5f3a\u6df1\u5ea6\u7279\u5f81\u7684\u7a33\u5b9a\u6027\u548c\u4fe1\u606f\u91cf\u3002  \n\u25c6 \u63d0\u5347\u4e86\u5728\u6311\u6218\u6027\u73af\u5883\uff08\u5982\u5927\u5c3a\u5ea6\u6237\u5916\u573a\u666f\u548c\u957f\u671f\u8fd0\u884c\uff09\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u53ef\u9760\u6027\u3002  \n\u25c6 \u514b\u670d\u4e86\u5df2\u6709\u5b66\u4e60\u65b9\u6cd5\uff08\u5982SuperPoint\u548cSuperGlue\uff09\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u5c40\u9650\u3002  \n\u25c6 \u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u673a\u5236\u4f18\u5316\u7279\u5f81\u8d28\u91cf\u3002  \n\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u3002|\n",
    "2509.08242": "|2025-09-10|Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed Task Allocation|Nirabhra Mandal\u7b49|[2509.08242](http://arxiv.org/pdf/2509.08242)|\u65e0|\u672c\u6587\u9488\u5bf9\u884c\u4e3a\u5f02\u6784\u591a\u673a\u5668\u4eba\u534f\u540c\u63a2\u7d22\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5e03\u5f0f\u4efb\u52a1\u5206\u914d\u4e0e\u535a\u5f08\u8bba\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u5f15\u5165\u884c\u4e3a\u71b5\uff08BE\uff09\u4f5c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u8bc4\u4f30\u63a2\u7d22\u6548\u7528\u7684\u6838\u5fc3\u6307\u6807\uff0c\u91cf\u5316\u4e0d\u540c\u884c\u4e3a\u7279\u6027\u5bf9\u4efb\u52a1\u9009\u62e9\u7684\u5f71\u54cd\u3002  \n\u25c6 \u5c06\u4efb\u52a1\u5206\u914d\u95ee\u9898\u8f6c\u5316\u4e3a\u975e\u5408\u4f5c\u535a\u5f08\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u5206\u5e03\u5f0f\u7b97\u6cd5d-PBRAG\u6536\u655b\u81f3\u7eb3\u4ec0\u5747\u8861\uff0c\u8bc1\u660e\u8be5\u5747\u8861\u5373\u6700\u4f18\u5206\u914d\u65b9\u6848\u3002  \n\u25c6 \u9488\u5bf9\u6548\u7528\u672a\u77e5\u573a\u666f\u63d0\u51fa\u8fd1\u4f3c\u5956\u52b1\u65b9\u6cd5\uff0c\u63d0\u4f9b\u5177\u6709\u9c81\u68d2\u6027\u7684\u6027\u80fd\u8fb9\u754c\u4fdd\u8bc1\u3002  \n\u25c6 \u7b97\u6cd5\u5177\u5907\u901a\u4fe1\u6210\u672c\u4f4e\u548c\u6536\u655b\u901f\u5ea6\u5feb\u7684\u7279\u70b9\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u884c\u4e3a\u5f02\u6784\u56e2\u961f\u5728\u63a2\u7d22\u6548\u7387\u548c\u8def\u5f84\u89c4\u5212\u4e0a\u7684\u4f18\u52bf\u3002  \n\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u884c\u4e3a\u5f02\u6784\u7684\u673a\u5668\u4eba\u56e2\u961f\u80fd\u663e\u8457\u63d0\u5347\u6574\u4f53\u63a2\u7d22\u6027\u80fd\u3002|\n",
    "2509.08235": "|2025-09-10|Deep Visual Odometry for Stereo Event Cameras|Sheng Zhong\u7b49|[2509.08235](http://arxiv.org/pdf/2509.08235)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7cfb\u7edfStereo-DEVO\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u4f4d\u59ff\u4f30\u8ba1\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u9759\u6001-\u7acb\u4f53\u5173\u8054\u7b56\u7565\uff0c\u7528\u4e8e\u7a00\u758f\u6df1\u5ea6\u4f30\u8ba1\uff0c\u51e0\u4e4e\u4e0d\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u3002  \n\u25c6 \u5c06\u6df1\u5ea6\u4f30\u8ba1\u4e0e\u7d27\u8026\u5408\u7684\u675f\u8c03\u6574\u4f18\u5316\u65b9\u6848\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5229\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u4e8b\u4ef6\u8868\u793a\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5149\u6d41\u4f30\u8ba1\u548c\u53ef\u9760\u7684\u56fe\u50cf\u5757\u5173\u8054\u3002  \n\u25c6 \u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u5904\u7406VGA\u5206\u8fa8\u7387\u7684\u4e8b\u4ef6\u6d41\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u79bb\u7ebf\u65b9\u6848\u5b9e\u73b0\u4e86\u91cd\u5927\u7a81\u7834\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u591c\u95f4\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u4e2d\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002|\n",
    "2509.08197": "|2025-09-10|Online Dynamic SLAM with Incremental Smoothing and Mapping|Jesse Morris\u7b49|[2509.08197](http://arxiv.org/pdf/2509.08197)|\u65e0|\u672c\u6587\u9996\u6b21\u5c06\u589e\u91cf\u4f18\u5316\u6280\u672f\u5e94\u7528\u4e8e\u52a8\u6001SLAM\uff0c\u5b9e\u73b0\u4e86\u5728\u7ebf\u5b9e\u65f6\u4f30\u8ba1\u80fd\u529b\u3002\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u5b50\u56fe\u6a21\u578b\uff0c\u80fd\u591f\u8054\u5408\u4f30\u8ba1\u9759\u6001\u573a\u666f\u548c\u52a8\u6001\u7269\u4f53\u7684\u72b6\u6001\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u5168\u65b0\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5145\u5206\u5229\u7528\u4e86\u73b0\u6709\u589e\u91cf\u4f18\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u5728\u7ebf\u9ad8\u6548\u6c42\u89e3\u3002\n\u25c6 \u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5176\u76f8\u673a\u4f4d\u59ff\u548c\u7269\u4f53\u8fd0\u52a8\u4f30\u8ba1\u7cbe\u5ea6\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002\n\u25c6 \u901a\u8fc7\u95ee\u9898\u7ed3\u6784\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u589e\u91cf\u5f0f\u6c42\u89e3\u52a8\u6001SLAM\u7684\u5173\u952e\u6311\u6218\u3002\n\u6700\u7ec8\uff0c\u8be5\u7cfb\u7edf\u67b6\u6784\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e865\u500d\u7684\u52a0\u901f\u3002|\n",
    "2509.09509": "|2025-09-11|SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking|Pedro Miguel Bastos Soares\u7b49|[2509.09509](http://arxiv.org/pdf/2509.09509)|\u65e0|SMapper\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3aSLAM\u7814\u7a76\u8bbe\u8ba1\u7684\u5f00\u6e90\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\u5e73\u53f0\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u591a\u6837\u6027\u3001\u73af\u5883\u8986\u76d6\u548c\u5b9e\u9a8c\u53ef\u590d\u73b0\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u91c7\u7528\u5f00\u6e90\u786c\u4ef6\u8bbe\u8ba1\uff0c\u96c6\u6210\u4e86\u540c\u6b65\u7684LiDAR\u3001\u591a\u76f8\u673a\u548c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\u3002\n\u25c6 \u63d0\u4f9b\u4e86\u4e00\u5957\u53ef\u9760\u7684\u6807\u5b9a\u4e0e\u65f6\u95f4\u540c\u6b65\u6d41\u7a0b\uff0c\u786e\u4fdd\u4e86\u8de8\u6a21\u6001\u6570\u636e\u5728\u65f6\u7a7a\u4e0a\u7684\u7cbe\u786e\u5bf9\u9f50\u3002\n\u25c6 \u5177\u5907\u53ef\u6269\u5c55\u548c\u53ef\u590d\u7528\u7684\u7ed3\u6784\uff0c\u652f\u6301\u624b\u6301\u548c\u673a\u5668\u4eba\u642d\u8f7d\u4e24\u79cd\u4f7f\u7528\u573a\u666f\uff0c\u589e\u5f3a\u4e86\u5e73\u53f0\u7684\u9002\u5e94\u6027\u3002\n\u25c6 \u516c\u5f00\u53d1\u5e03\u4e86\u540d\u4e3aSMapper-light\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u5ba4\u5185\u5916\u5178\u578b\u573a\u666f\u7684\u9ad8\u7cbe\u5ea6\u771f\u503c\u8f68\u8ff9\u4e0e\u7a20\u5bc6\u4e09\u7ef4\u91cd\u5efa\u7ed3\u679c\u3002\n\u25c6 \u57fa\u4e8e\u6240\u91c7\u96c6\u6570\u636e\u5bf9\u4e3b\u6d41LiDAR\u4e0e\u89c6\u89c9SLAM\u7b97\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u80fd\u8bc4\u6d4b\uff0c\u4e3a\u7b97\u6cd5\u6bd4\u8f83\u4e0e\u590d\u73b0\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002\n\u901a\u8fc7\u786c\u4ef6\u5f00\u653e\u3001\u6570\u636e\u540c\u6b65\u548c\u8bc4\u6d4b\u4e00\u4f53\u5316\uff0cSMapper\u663e\u8457\u63d0\u5347\u4e86SLAM\u7814\u7a76\u7684\u53ef\u9760\u6027\u3001\u53ef\u6bd4\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002|\n",
    "2509.09110": "|2025-09-11|S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization|Chenghao Zhang\u7b49|[2509.09110](http://arxiv.org/pdf/2509.09110)|\u65e0|S-BEVLoc\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7684\u81ea\u76d1\u7763\u6fc0\u5149\u96f7\u8fbe\u5168\u5c40\u5b9a\u4f4d\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u65e0\u9700\u5730\u9762\u771f\u503c\u59ff\u6001\u5373\u53ef\u5b9e\u73b0\u5927\u89c4\u6a21\u5b9a\u4f4d\u3002  \n\u25c6 \u9996\u6b21\u6784\u5efa\u4e86\u57fa\u4e8eBEV\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5229\u7528\u5173\u952e\u70b9\u4e4b\u95f4\u7684\u5df2\u77e5\u5730\u7406\u8ddd\u79bb\u6784\u5efa\u8bad\u7ec3\u4e09\u5143\u7ec4\uff0c\u5b8c\u5168\u6446\u8131\u4e86\u5bf9GPS\u6216SLAM\u771f\u503c\u6570\u636e\u7684\u4f9d\u8d56\u3002  \n\u25c6 \u63d0\u51faSoftCos\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u589e\u5f3a\u4ece\u751f\u6210\u7684\u4e09\u5143\u7ec4\u4e2d\u5b66\u4e60\u7279\u5f81\u8868\u793a\u7684\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u56f0\u96be\u6837\u672c\u4e0a\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u7ed3\u5408CNN\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u4e0eNetVLAD\u5168\u5c40\u63cf\u8ff0\u7b26\u805a\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5224\u522b\u6027\u5f3a\u7684\u573a\u666f\u8868\u793a\u3002  \n\u5728KITTI\u548cNCLT\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4d\u7f6e\u8bc6\u522b\u3001\u56de\u73af\u68c0\u6d4b\u548c\u5168\u5c40\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8fbe\u5230\u9886\u5148\u6027\u80fd\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u8fdc\u8d85\u76d1\u7763\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002|\n",
    "2509.10433": "|2025-09-12|Robust Localization in Modern Cellular Networks using Global Map Features|Junshi Chen\u7b49|[2509.10433](http://arxiv.org/pdf/2509.10433)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u591a\u8def\u5f84\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff08MP-SLAM\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u73b0\u4ee3\u8702\u7a9d\u7f51\u7edc\u4e2d\u7684\u9c81\u68d2\u5b9a\u4f4d\u3002  \n\u25c6\u5f15\u5165\u5168\u5c40\u5730\u56fe\u7279\u5f81\uff08GMF\uff09\u5b58\u50a8\u5e93\uff0c\u6574\u5408\u5386\u53f2\u904d\u5386\u4e2d\u6536\u96c6\u7684\u9ad8\u8d28\u91cf\u5730\u56fe\u7279\u5f81\uff08\u5982\u865a\u62df\u951a\u70b9\uff09\uff0c\u63d0\u5347\u7cfb\u7edf\u5bf9\u5148\u9a8c\u77e5\u8bc6\u7684\u5229\u7528\u80fd\u529b\u3002  \n\u25c6\u901a\u8fc7\u6982\u7387\u5047\u8bbe\u5bc6\u5ea6\uff08PHD\uff09\u6ee4\u6ce2\u5668\u52a8\u6001\u96c6\u6210\u5168\u5c40\u5730\u56fe\u7279\u5f81\uff0c\u5b9e\u73b0\u7279\u5f81\u5f3a\u5ea6\u51fd\u6570\u7684\u65f6\u5e8f\u4f20\u64ad\u4e0e\u878d\u5408\u3002  \n\u25c6\u5728\u4e25\u91cd\u591a\u5f84\u4f20\u64ad\u548c\u5c0f\u533a\u95f4\u5e72\u6270\u7684\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u57fa\u4e8eLTE\u4fe1\u53f7\u8fdb\u884c\u4e86\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5176\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002  \n\u25c6\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e5G/6G\u7b49\u73b0\u4ee3\u8702\u7a9d\u7f51\u7edc\uff0c\u5373\u4f7f\u5728\u975e\u89c6\u8ddd\uff08OLoS\uff09\u7b49\u6076\u52a3\u4fe1\u53f7\u6761\u4ef6\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u53ef\u9760\u5b9a\u4f4d\u3002  \n\u25c6\u7a81\u7834\u4e86\u4f20\u7edfMP-SLAM\u548c\u57fa\u4e8e\u81ea\u611f\u77e5\u4f20\u611f\u5668\u7684\u5b9a\u4f4d\u5c40\u9650\uff0c\u4e3a\u590d\u6742\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.11653": "|2025-09-15|See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays|Gerlinde Emsenhuber\u7b49|[2509.11653](http://arxiv.org/pdf/2509.11653)|\u65e0|\u672c\u6587\u9488\u5bf9\u5149\u5b66\u900f\u89c6\u5934\u663e\u4e2d\u4e16\u754c\u76f8\u673a\u4e0e\u7528\u6237\u89c6\u89d2\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u773c\u89c6\u89d2\u6e32\u67d3\u6280\u672f\u4ee5\u63d0\u5347\u589e\u5f3a\u73b0\u5b9e\u89c6\u89c9\u5f15\u5bfc\u7684\u51c6\u786e\u6027\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u57fa\u4e8e\u5e73\u9762\u6295\u5f71\u7684Plane-Proxy EPR\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u5e73\u9762\u4e0a\u8fd1\u4f3c\u773c\u89c6\u89d2\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u57fa\u4e8eSLAM\u573a\u666f\u91cd\u5efa\u7684Mesh-Proxy EPR\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u7f51\u683c\u63d0\u9ad8\u6295\u5f71\u7cbe\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u63d0\u51faGaze-Proxy EPR\u65b9\u6cd5\uff0c\u5229\u7528\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u5c06\u6295\u5f71\u4e0e\u7528\u6237\u6ce8\u89c6\u6df1\u5ea6\u52a8\u6001\u5bf9\u9f50\u3002  \n\u901a\u8fc7\u771f\u5b9e\u4efb\u52a1\u7528\u6237\u7814\u7a76\u8bc1\u660e\uff0c\u7cbe\u786e\u7684\u773c\u89c6\u89d2\u6e32\u67d3\u5bf9\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u6240\u63d0\u6ce8\u89c6\u4ee3\u7406\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002  \n\u7814\u7a76\u6700\u7ec8\u5f00\u6e90\u4e86\u5b8c\u6574\u7684\u773c\u89c6\u89d2\u6e32\u67d3\u6846\u67b6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u652f\u6301\u3002|\n",
    "2509.11574": "|2025-09-15|Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps|Zhexi Peng\u7b49|[2509.11574](http://arxiv.org/pdf/2509.11574)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GPS-SLAM\uff0c\u4e00\u79cd\u80fd\u5b9e\u73b0\u8d85\u8fc7150 fps\u5b9e\u65f6\u9ad8\u4fdd\u771f\u4e09\u7ef4\u91cd\u5efa\u7684\u65b0\u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u4e00\u79cd\u6df7\u5408\u8868\u793a\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u9ad8\u65afSLAM\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u74f6\u9888\u95ee\u9898\u3002\n\n\u25c6 \u521b\u65b0\u6027\u5730\u7ed3\u5408\u4e86\u5f69\u8272\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u548c3D\u9ad8\u65af\u4e24\u79cd\u8868\u793a\uff0cSDF\u8d1f\u8d23\u9ad8\u6548\u6784\u5efa\u5e73\u6ed1\u7684\u51e0\u4f55\u4e0e\u5916\u89c2\uff0c\u800c\u9ad8\u65af\u5219\u4e13\u95e8\u7528\u4e8e\u8865\u8db3\u7ec6\u8282\u3002\n\u25c6 \u8be5\u6df7\u5408\u8868\u793a\u907f\u514d\u4e86\u5bf9\u6574\u4e2a\u573a\u666f\u90fd\u7528\u9ad8\u65af\u5efa\u6a21\uff0c\u4ece\u800c\u5c06\u6240\u9700\u7684\u9ad8\u65af\u6570\u91cf\u51cf\u5c11\u4e8650%\u3002\n\u25c6 \u901a\u8fc7\u8ba9SDF\u627f\u62c5\u5927\u90e8\u5206\u57fa\u7840\u8868\u793a\u5de5\u4f5c\uff0c\u9ad8\u65af\u4f18\u5316\u53ea\u9700\u8fdb\u884c\u9488\u5bf9\u6027\u7684\u5916\u89c2\u7ec6\u5316\uff0c\u4f7f\u5f97\u4f18\u5316\u8fed\u4ee3\u6b21\u6570\u5927\u5e45\u51cf\u5c1175%\u3002\n\u25c6 \u6700\u7ec8\u6784\u5efa\u7684GPS-SLAM\u7cfb\u7edf\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u901f\u5ea6\u8d85\u8fc7150\u5e27/\u79d2\u3002|\n",
    "2509.10757": "|2025-09-13|FastTrack: GPU-Accelerated Tracking for Visual SLAM|Kimia Khabiri\u7b49|[2509.10757](http://arxiv.org/pdf/2509.10757)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GPU\u52a0\u901f\u6765\u663e\u8457\u63d0\u5347\u89c6\u89c9-\u60ef\u6027SLAM\u7cfb\u7edf\u8ddf\u8e2a\u6a21\u5757\u6027\u80fd\u7684\u65b0\u65b9\u6cd5FastTrack\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\u6765\u52a0\u901f\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u6700\u8017\u65f6\u7684\u7ec4\u4ef6\uff0c\u5305\u62ec\u7acb\u4f53\u7279\u5f81\u5339\u914d\u548c\u5c40\u90e8\u5730\u56fe\u8ddf\u8e2a\u3002  \n\u25c6 \u5c06\u52a0\u901f\u8bbe\u8ba1\u5177\u4f53\u5b9e\u73b0\u5728\u4e3b\u6d41\u7684ORB-SLAM3\u6846\u67b6\u7684\u8ddf\u8e2a\u6d41\u7a0b\u4e2d\uff0c\u5e76\u4f7f\u7528CUDA\u8fdb\u884c\u5f00\u53d1\u3002  \n\u25c6 \u5728\u516c\u5f00\u6570\u636e\u96c6EuRoC\u548cTUM-VI\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u7acb\u4f53-\u60ef\u6027\u6a21\u5f0f\u4e0b\u8fd0\u884c\uff0c\u8ddf\u8e2a\u6027\u80fd\u6574\u4f53\u63d0\u5347\u6700\u9ad8\u8fbe2.8\u500d\u3002  \n\u25c6 \u9a8c\u8bc1\u4e86\u65b9\u6848\u5728\u4e24\u79cd\u786c\u4ef6\u5e73\u53f0\uff08\u684c\u9762GPU\u548c\u5d4c\u5165\u5f0fJetson Xavier NX\uff09\u4e0a\u7684\u6709\u6548\u6027\u4e0e\u901a\u7528\u6027\uff0c\u786e\u4fdd\u4e86\u5b9e\u65f6\u6027\u5e76\u964d\u4f4e\u4e86\u8ddf\u8e2a\u4e22\u5931\u7684\u98ce\u9669\u3002|\n",
    "2509.12924": "|2025-09-18|MATTER: Multiscale Attention for Registration Error Regression|Shipeng Liu\u7b49|[2509.12924](http://arxiv.org/pdf/2509.12924)|\u65e0|\u25c6 Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking.\n\u25c6 This makes detecting and quantifying registration misalignment, i.e.,~{\\it PCR quality validation}, an important task.\n\u25c6 All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes.|\n",
    "2509.12592": "|2025-09-16|Match Chat: Real Time Generative AI and Generative Computing for Tennis|Aaron Baughman\u7b49|[2509.12592](http://arxiv.org/pdf/2509.12592)|\u65e0|\u25c6 We present Match Chat, a real-time, agent-driven assistant designed to enhance the tennis fan experience by delivering instant, accurate responses to match-related queries.\n\u25c6 Match Chat integrates Generative Artificial Intelligence (GenAI) with Generative Computing (GenComp) techniques to synthesize key insights during live tennis singles matches.\n\u25c6 The system debuted at the 2025 Wimbledon Championships and the 2025 US Open, where it provided about 1 million users with seamless access to streaming and static data through natural language queries.|\n",
    "2509.14191": "|2025-09-17|MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping|Zhihao Cao\u7b49|[2509.14191](http://arxiv.org/pdf/2509.14191)|\u65e0|\u25c6 Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage.\n\u25c6 We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS).\n\u25c6 Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map.|\n",
    "2509.13972": "|2025-09-17|BIM Informed Visual SLAM for Construction Monitoring|Asier Bikandi\u7b49|[2509.13972](http://arxiv.org/pdf/2509.13972)|\u65e0|\u25c6 Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring construction sites, where aligning the evolving as-built state with the as-planned design enables early error detection and reduces costly rework.\n\u25c6 LiDAR-based SLAM achieves high geometric precision, but its sensors are typically large and power-demanding, limiting their use on portable platforms.\n\u25c6 Visual SLAM offers a practical alternative with lightweight cameras already embedded in most mobile devices.|\n",
    "2509.13713": "|2025-09-17|UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry|Tae-Wook Um\u7b49|[2509.13713](http://arxiv.org/pdf/2509.13713)|\u65e0|\u25c6 Monocular depth estimation has been increasingly adopted in robotics and autonomous driving for its ability to infer scene geometry from a single camera.\n\u25c6 In self-supervised monocular depth estimation frameworks, the network jointly generates and exploits depth and pose estimates during training, thereby eliminating the need for depth labels.\n\u25c6 However, these methods remain challenged by uncertainty in the input data, such as low-texture or dynamic regions, which can cause reduced depth accuracy.|\n",
    "2509.13649": "|2025-09-17|Barometer-Aided Attitude Estimation|M\u00e9lon\u00e9 Nyoba Tchonkeu\u7b49|[2509.13649](http://arxiv.org/pdf/2509.13649)|\u65e0|\u25c6 Accurate and robust attitude estimation is a central challenge for autonomous vehicles operating in GNSS-denied or highly dynamic environments.\n\u25c6 In such cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable tilt estimation due to the ambiguity between gravitational and inertial accelerations.\n\u25c6 While auxiliary velocity sensors, such as GNSS, Pitot tubes, Doppler radar, or visual odometry, are often used, they can be unavailable, intermittent, or costly.|\n",
    "2509.13541": "|2025-09-16|Semantic 3D Reconstructions with SLAM for Central Airway Obstruction|Ayberk Acar\u7b49|[2509.13541](http://arxiv.org/pdf/2509.13541)|\u65e0|\u25c6 Central airway obstruction (CAO) is a life-threatening condition with increasing incidence, caused by tumors in and outside of the airway.\n\u25c6 Traditional treatment methods such as bronchoscopy and electrocautery can be used to remove the tumor completely; however, these methods carry a high risk of complications.\n\u25c6 Recent advances allow robotic interventions with lesser risk.|\n",
    "2509.13536": "|2025-09-16|MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM|Yinlong Bai\u7b49|[2509.13536](http://arxiv.org/pdf/2509.13536)|\u65e0|\u25c6 Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques.\n\u25c6 Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs).\n\u25c6 These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality.|\n",
    "2509.14949": "|2025-09-18|Human Interaction for Collaborative Semantic SLAM using Extended Reality|Laura Ribeiro\u7b49|[2509.14949](http://arxiv.org/pdf/2509.14949)|\u65e0|\u25c6 Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robot maps with structural and semantic information, enabling robots to operate more effectively in complex environments.\n\u25c6 However, these systems struggle in real-world scenarios with occlusions, incomplete data, or ambiguous geometries, as they cannot fully leverage the higher-level spatial and semantic knowledge humans naturally apply.\n\u25c6 We introduce HICS-SLAM, a Human-in-the-Loop semantic SLAM framework that uses a shared extended reality environment for real-time collaboration.|\n",
    "2509.14636": "|2025-09-18|BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots|Yufei Wei\u7b49|[2509.14636](http://arxiv.org/pdf/2509.14636)|\u65e0|\u25c6 Bird's-Eye-View (BEV) representation offers a metric-scaled planar workspace, facilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF model for monocular visual odometry (MVO) in intelligent transportation systems.\n\u25c6 However, existing BEV methods suffer from sparse supervision signals and information loss during perspective-to-BEV projection.\n\u25c6 We present BEV-ODOM2, an enhanced framework addressing both limitations without additional annotations.|\n",
    "2509.14516": "|2025-09-18|Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods|Adam D. Hines\u7b49|[2509.14516](http://arxiv.org/pdf/2509.14516)|\u65e0|\u25c6 Event-based localization research and datasets are a rapidly growing area of interest, with a tenfold increase in the cumulative total number of published papers on this topic over the past 10 years.\n\u25c6 Whilst the rapid expansion in the field is exciting, it brings with it an associated challenge: a growth in the variety of required code and package dependencies as well as data formats, making comparisons difficult and cumbersome for researchers to implement reliably.\n\u25c6 To address this challenge, we present Event-LAB: a new and unified framework for running several event-based localization methodologies across multiple datasets.|\n",
    "2509.16019": "|2025-09-19|SLaM-DiMM: Shared Latent Modeling for Diffusion Based Missing Modality Synthesis in MRI|Bhavesh Sandbhor\u7b49|[2509.16019](http://arxiv.org/pdf/2509.16019)|\u65e0|\u25c6 Brain MRI scans are often found in four modalities, consisting of T1-weighted with and without contrast enhancement (T1ce and T1w), T2-weighted imaging (T2w), and Flair.\n\u25c6 Leveraging complementary information from these different modalities enables models to learn richer, more discriminative features for understanding brain anatomy, which could be used in downstream tasks such as anomaly detection.\n\u25c6 However, in clinical practice, not all MRI modalities are always available due to various reasons.|\n",
    "2509.15673": "|2025-09-19|Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion|Yinong Cao\u7b49|[2509.15673](http://arxiv.org/pdf/2509.15673)|\u65e0|\u25c6 Wide field-of-view (FoV) LiDAR sensors provide dense geometry across large environments, but most existing LiDAR-inertial-visual odometry (LIVO) systems rely on a single camera, leading to limited spatial coverage and degraded robustness.\n\u25c6 We present Omni-LIVO, the first tightly coupled multi-camera LIVO system that bridges the FoV mismatch between wide-angle LiDAR and conventional cameras.\n\u25c6 Omni-LIVO introduces a Cross-View direct tracking strategy that maintains photometric consistency across non-overlapping views, and extends the Error-State Iterated Kalman Filter (ESIKF) with multi-view updates and adaptive covariance weighting.|\n",
    "2509.17864": "|2025-09-22|ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos|Shi Chen\u7b49|[2509.17864](http://arxiv.org/pdf/2509.17864)|\u65e0|\u25c6 Achieving truly practical dynamic 3D reconstruction requires online operation, global pose and map consistency, detailed appearance modeling, and the flexibility to handle both RGB and RGB-D inputs.\n\u25c6 However, existing SLAM methods typically merely remove the dynamic parts or require RGB-D input, while offline methods are not scalable to long video sequences, and current transformer-based feedforward methods lack global consistency and appearance details.\n\u25c6 To this end, we achieve online dynamic scene reconstruction by disentangling the static and dynamic parts within a SLAM system.|\n",
    "2509.16909": "|2025-09-23|SLAM-Former: Putting SLAM into One Transformer|Yijun Yuan\u7b49|[2509.16909](http://arxiv.org/pdf/2509.16909)|\u65e0|\u25c6 We present SLAM-Former, a novel neural approach that integrates full SLAM capabilities into a single transformer.\n\u25c6 Similar to traditional SLAM systems, SLAM-Former comprises both a frontend and a backend that operate in tandem.\n\u25c6 The frontend processes sequential monocular images in real-time for incremental mapping and tracking, while the backend performs global refinement to ensure a geometrically consistent result.|\n",
    "2509.16863": "|2025-09-21|ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM|Amanuel T. Dufera\u7b49|[2509.16863](http://arxiv.org/pdf/2509.16863)|\u65e0|\u25c6 We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM system for robust, highfidelity RGB-only reconstruction.\n\u25c6 Addressing geometric inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable depth estimation, ConfidentSplat incorporates a core innovation: a confidence-weighted fusion mechanism.\n\u25c6 This mechanism adaptively integrates depth cues from multiview geometry with learned monocular priors (Omnidata ViT), dynamically weighting their contributions based on explicit reliability estimates-derived predominantly from multi-view geometric consistency-to generate high-fidelity proxy depth for map supervision.|\n",
    "2509.18954": "|2025-09-23|Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation|Minoo Dolatabadi\u7b49|[2509.18954](http://arxiv.org/pdf/2509.18954)|\u65e0|\u25c6 LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans.\n\u25c6 However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation.\n\u25c6 Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions.|\n",
    "2509.18342": "|2025-09-22|Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation|Rajitha de Silva\u7b49|[2509.18342](http://arxiv.org/pdf/2509.18342)|\u65e0|\u25c6 Accurate localisation is critical for mobile robots in structured outdoor environments, yet LiDAR-based methods often fail in vineyards due to repetitive row geometry and perceptual aliasing.\n\u25c6 We propose a semantic particle filter that incorporates stable object-level detections, specifically vine trunks and support poles into the likelihood estimation process.\n\u25c6 Detected landmarks are projected into a birds eye view and fused with LiDAR scans to generate semantic observations.|\n",
    "2509.20171": "|2025-09-24|Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research|Patricia Sch\u00f6ntag\u7b49|[2509.20171](http://arxiv.org/pdf/2509.20171)|\u65e0|\u25c6 The development and evaluation of machine vision in underwater environments remains challenging, often relying on trial-and-error-based testing tailored to specific applications.\n\u25c6 This is partly due to the lack of controlled, ground-truthed testing environments that account for the optical challenges, such as color distortion from spectrally variant light attenuation, reduced contrast and blur from backscatter and volume scattering, and dynamic light patterns from natural or artificial illumination.\n\u25c6 Additionally, the appearance of ocean water in images varies significantly across regions, depths, and seasons.|\n",
    "2509.19522": "|2025-09-23|Bioinspired SLAM Approach for Unmanned Surface Vehicle|Fabio Coelho\u7b49|[2509.19522](http://arxiv.org/pdf/2509.19522)|\u65e0|\u25c6 This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a bioinspired SLAM framework based on computational models of the rodent hippocampus.\n\u25c6 OpenRatSLAM2 delivers low-computation-cost visual-inertial based SLAM, suitable for GPS-denied environments.\n\u25c6 Our contributions include a ROS2-based architecture, experimental results on new waterway datasets, and insights into system parameter tuning.|\n",
    "2509.19463": "|2025-09-23|CU-Multi: A Dataset for Multi-Robot Collaborative Perception|Doncey Albin\u7b49|[2509.19463](http://arxiv.org/pdf/2509.19463)|\u65e0|\u25c6 A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation.\n\u25c6 Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets.\n\u25c6 Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies.|\n",
    "2509.21006": "|2025-09-25|AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation|Konstantin Gubernatorov\u7b49|[2509.21006](http://arxiv.org/pdf/2509.21006)|\u65e0|\u25c6 We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation.\n\u25c6 A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy.\n\u25c6 An approach planner then selects visibility and reachability aware pre grasp base poses.|\n",
    "2509.20757": "|2025-09-29|MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM|Yuxuan Zhou\u7b49|[2509.20757](http://arxiv.org/pdf/2509.20757)|\u65e0|\u25c6 Visual SLAM is a cornerstone technique in robotics, autonomous driving and extended reality (XR), yet classical systems often struggle with low-texture environments, scale ambiguity, and degraded performance under challenging visual conditions.\n\u25c6 Recent advancements in feed-forward neural network-based pointmap regression have demonstrated the potential to recover high-fidelity 3D scene geometry directly from images, leveraging learned spatial priors to overcome limitations of traditional multi-view geometry methods.\n\u25c6 However, the widely validated advantages of probabilistic multi-sensor information fusion are often discarded in these pipelines.|\n",
    "2509.20739": "|2025-09-25|SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning|Guoyang Zhao\u7b49|[2509.20739](http://arxiv.org/pdf/2509.20739)|\u65e0|\u25c6 Conventional SLAM pipelines for legged robot navigation are fragile under rapid motion, calibration demands, and sensor drift, while offering limited semantic reasoning for task-driven exploration.\n\u25c6 To deal with these issues, we propose a vision-only, SLAM-free navigation framework that replaces dense geometry with semantic reasoning and lightweight topological representations.\n\u25c6 A hierarchical vision-language perception module fuses scene-level context with object-level cues for robust semantic inference.|\n",
    "2509.22288": "|2025-09-26|IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM|Johan Hatleskog\u7b49|[2509.22288](http://arxiv.org/pdf/2509.22288)|\u65e0|\u25c6 Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor graph node per measurement to compensate for the lack of time synchronization between radar and LiDAR.\n\u25c6 For a radar-LiDAR sensor pair with equal rates, this strategy results in a state creation rate of twice the individual sensor frequencies.\n\u25c6 This doubling of the number of states per second yields high optimization costs, inhibiting real-time performance on resource-constrained hardware.|\n",
    "2509.21602": "|2025-09-25|Real-Time Indoor Object SLAM with LLM-Enhanced Priors|Yang Jiao\u7b49|[2509.21602](http://arxiv.org/pdf/2509.21602)|\u65e0|\u25c6 Object-level Simultaneous Localization and Mapping (SLAM), which incorporates semantic information for high-level scene understanding, faces challenges of under-constrained optimization due to sparse observations.\n\u25c6 Prior work has introduced additional constraints using commonsense knowledge, but obtaining such priors has traditionally been labor-intensive and lacks generalizability across diverse object categories.\n\u25c6 We address this limitation by leveraging large language models (LLMs) to provide commonsense knowledge of object geometric attributes, specifically size and orientation, as prior factors in a graph-based SLAM framework.|\n",
    "2509.24236": "|2025-09-29|PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization|Siyan Dong\u7b49|[2509.24236](http://arxiv.org/pdf/2509.24236)|\u65e0|\u25c6 Real-time dense scene reconstruction during unstable camera motions is crucial for robotics, yet current RGB-D SLAM systems fail when cameras experience large viewpoint changes, fast motions, or sudden shaking.\n\u25c6 Classical optimization-based methods deliver high accuracy but fail with poor initialization during large motions, while learning-based approaches provide robustness but lack sufficient accuracy for dense reconstruction.\n\u25c6 We address this challenge through a combination of learning-based initialization with optimization-based refinement.|\n",
    "2509.23737": "|2025-09-28|GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State|Guole Shen\u7b49|[2509.23737](http://arxiv.org/pdf/2509.23737)|\u65e0|\u25c6 DUSt3R-based end-to-end scene reconstruction has recently shown promising results in dense visual SLAM.\n\u25c6 However, most existing methods only use image pairs to estimate pointmaps, overlooking spatial memory and global consistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework for dense scene reconstruction and pose estimation from RGB images without any prior knowledge of the scene or camera parameters.\n\u25c6 Unlike existing DUSt3R-based frameworks, which operate on all image pairs and predict per-pair point maps in local coordinate frames, our method supports sequentialized input and incrementally estimates metric-scale point clouds in the global coordinate.|\n",
    "2509.23555": "|2025-09-28|From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations|Javed Ahmad\u7b49|[2509.23555](http://arxiv.org/pdf/2509.23555)|\u65e0|\u25c6 Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted.\n\u25c6 NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding.\n\u25c6 This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation.|\n",
    "2509.23118": "|2025-09-27|EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation|Zeyi Li\u7b49|[2509.23118](http://arxiv.org/pdf/2509.23118)|\u65e0|\u25c6 Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting cannot meet the growing demand for accurate indoor localization and navigation due to its lower accuracy, while solutions based on light detection and ranging (LiDAR) can provide better localization performance but is limited by their higher deployment cost and complexity.\n\u25c6 To address these issues, we propose a novel indoor localization and navigation framework integrating Wi-Fi RSSI fingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and inertial measurement unit (IMU) navigation based on an extended Kalman filter (EKF).\n\u25c6 Specifically, coarse localization by deep neural network (DNN)-based Wi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a Gmapping-based SLAM to generate an occupancy grid map and output high-frequency attitude estimates, which is followed by EKF prediction-update integrating sensor information while effectively suppressing Wi-Fi-induced noise and IMU drift errors.|\n",
    "2509.22910": "|2025-09-26|Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM|Yanwei Du\u7b49|[2509.22910](http://arxiv.org/pdf/2509.22910)|\u65e0|\u25c6 Given that Visual SLAM relies on appearance cues for localization and scene understanding, texture-less or visually degraded environments (e.g., plain walls or low lighting) lead to poor pose estimation and track loss.\n\u25c6 However, robots are typically equipped with sensors that provide some form of dead reckoning odometry with reasonable short-time performance but unreliable long-time performance.\n\u25c6 The Good Weights (GW) algorithm described here provides a framework to adaptively integrate dead reckoning (DR) with passive visual SLAM for continuous and accurate frame-level pose estimation.|\n",
    "2509.26639": "|2025-09-30|Benchmarking Egocentric Visual-Inertial SLAM at City Scale|Anusha Krishnan\u7b49|[2509.26639](http://arxiv.org/pdf/2509.26639)|\u65e0|\u25c6 Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration.\n\u25c6 While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses.\n\u25c6 In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data.|\n",
    "2509.26581": "|2025-09-30|Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework|Shishir Gopinath\u7b49|[2509.26581](http://arxiv.org/pdf/2509.26581)|\u65e0|\u25c6 We present Graphite, a GPU-accelerated nonlinear graph optimization framework.\n\u25c6 It provides a CUDA C++ interface to enable the sharing of code between a realtime application, such as a SLAM system, and its optimization tasks.\n\u25c6 The framework supports techniques to reduce memory usage, including in-place optimization, support for multiple floating point types and mixed-precision modes, and dynamically computed Jacobians.|\n",
    "2509.26558": "|2025-09-30|Radio-based Multi-Robot Odometry and Relative Localization|Andr\u00e9s Mart\u00ednez-Silva\u7b49|[2509.26558](http://arxiv.org/pdf/2509.26558)|\u65e0|\u25c6 Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And Ranging (radar), which have traditionally seen limited adoption in robotics, are experiencing a boost in popularity thanks to their robustness to harsh environmental conditions and cluttered environments.\n\u25c6 This work proposes a multi-robot UGV-UAV localization system that leverages the two technologies with inexpensive and readily-available sensors, such as Inertial Measurement Units (IMUs) and wheel encoders, to estimate the relative position of an aerial robot with respect to a ground robot.\n\u25c6 The first stage of the system pipeline includes a nonlinear optimization framework to trilaterate the location of the aerial platform based on UWB range data, and a radar pre-processing module with loosely coupled ego-motion estimation which has been adapted for a multi-robot scenario.|\n",
    "2509.26498": "|2025-09-30|DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance|Jijun Xiang\u7b49|[2509.26498](http://arxiv.org/pdf/2509.26498)|\u65e0|\u25c6 Depth enhancement, which converts raw dToF signals into dense depth maps using RGB guidance, is crucial for improving depth perception in high-precision tasks such as 3D reconstruction and SLAM.\n\u25c6 However, existing methods often assume ideal dToF inputs and perfect dToF-RGB alignment, overlooking calibration errors and anomalies, thus limiting real-world applicability.\n\u25c6 This work systematically analyzes the noise characteristics of real-world lightweight dToF sensors and proposes a practical and novel depth completion framework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three key aspects.|\n",
    "2509.26121": "|2025-09-30|Side Scan Sonar-based SLAM for Autonomous Algae Farm Monitoring|Julian Valdez\u7b49|[2509.26121](http://arxiv.org/pdf/2509.26121)|\u65e0|\u25c6 The transition of seaweed farming to an alternative food source on an industrial scale relies on automating its processes through smart farming, equivalent to land agriculture.\n\u25c6 Key to this process are autonomous underwater vehicles (AUVs) via their capacity to automate crop and structural inspections.\n\u25c6 However, the current bottleneck for their deployment is ensuring safe navigation within farms, which requires an accurate, online estimate of the AUV pose and map of the infrastructure.|\n",
    "2509.25905": "|2025-09-30|User-Centric Communication Service Provision for Edge-Assisted Mobile Augmented Reality|Conghao Zhou\u7b49|[2509.25905](http://arxiv.org/pdf/2509.25905)|\u65e0|\u25c6 Future 6G networks are envisioned to facilitate edge-assisted mobile augmented reality (MAR) via strengthening the collaboration between MAR devices and edge servers.\n\u25c6 In order to provide immersive user experiences, MAR devices must timely upload camera frames to an edge server for simultaneous localization and mapping (SLAM)-based device pose tracking.\n\u25c6 In this paper, to cope with user-specific and non-stationary uplink data traffic, we develop a digital twin (DT)-based approach for user-centric communication service provision for MAR.|\n",
    "2510.02080": "|2025-10-02|EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction|Lingxiang Hu\u7b49|[2510.02080](http://arxiv.org/pdf/2510.02080)|\u65e0|\u25c6 The application of monocular dense Simultaneous Localization and Mapping (SLAM) is often hindered by high latency, large GPU memory consumption, and reliance on camera calibration.\n\u25c6 To relax this constraint, we propose EC3R-SLAM, a novel calibration-free monocular dense SLAM framework that jointly achieves high localization and mapping accuracy, low latency, and low GPU memory consumption.\n\u25c6 This enables the framework to achieve efficiency through the coupling of a tracking module, which maintains a sparse map of feature points, and a mapping module based on a feed-forward 3D reconstruction model that simultaneously estimates camera intrinsics.|\n",
    "2510.01665": "|2025-10-02|Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale|Yongbo Chen\u7b49|[2510.01665](http://arxiv.org/pdf/2510.01665)|\u65e0|\u25c6 Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention.\n\u25c6 We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset.\n\u25c6 Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework.|\n",
    "2510.01119": "|2025-10-01|Instant4D: 4D Gaussian Splatting in Minutes|Zhanpeng Luo\u7b49|[2510.01119](http://arxiv.org/pdf/2510.01119)|\u65e0|\u25c6 Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation.\n\u25c6 In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors.\n\u25c6 Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation.|\n",
    "2510.00783": "|2025-10-01|Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions|Thanh Nguyen Canh\u7b49|[2510.00783](http://arxiv.org/pdf/2510.00783)|\u65e0|\u25c6 Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of research within robotics and computer vision, focusing on the simultaneous localization of robotic systems and associating semantic information to construct the most accurate and complete comprehensive model of the surrounding environment.\n\u25c6 Since the first foundational work in Semantic SLAM appeared more than two decades ago, this field has received increasing attention across various scientific communities.\n\u25c6 Despite its significance, the field lacks comprehensive surveys encompassing recent advances and persistent challenges.|\n",
    "2510.02616": "|2025-10-02|RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments|Mobin Habibpour\u7b49|[2510.02616](http://arxiv.org/pdf/2510.02616)|\u65e0|\u25c6 Simultaneous Localization and Mapping (SLAM) plays an important role in many robotics fields, including social robots.\n\u25c6 Many of the available visual SLAM methods are based on the assumption of a static world and struggle in dynamic environments.\n\u25c6 In the current study, we introduce a real-time semantic RGBD SLAM approach designed specifically for dynamic environments.|\n",
    "2510.04612": "|2025-10-06|OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS|Simon Boche\u7b49|[2510.04612](http://arxiv.org/pdf/2510.04612)|\u65e0|\u25c6 To empower mobile robots with usable maps as well as highest state estimation accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor Simultaneous Localization and Mapping (SLAM) system building dense volumetric occupancy maps, while scalable to large environments and operating in realtime.\n\u25c6 Our unified SLAM framework seamlessly integrates different sensor modalities: visual, inertial, measured or learned depth, LiDAR and Global Navigation Satellite System (GNSS) measurements.\n\u25c6 Unlike most state-of-the-art SLAM systems, we advocate using dense volumetric map representations when leveraging depth or range-sensing capabilities.|\n",
    "2510.03348": "|2025-10-02|Visual Odometry with Transformers|Vlardimir Yugay\u7b49|[2510.03348](http://arxiv.org/pdf/2510.03348)|\u65e0|\u25c6 Modern monocular visual odometry methods typically combine pre-trained deep learning components with optimization modules, resulting in complex pipelines that rely heavily on camera calibration and hyperparameter tuning, and often struggle in unseen real-world scenarios.\n\u25c6 Recent large-scale 3D models trained on massive amounts of multi-modal data have partially alleviated these challenges, providing generalizable dense reconstruction and camera pose estimation.\n\u25c6 Still, they remain limited in handling long videos and providing accurate per-frame estimates, which are required for visual odometry.|\n",
    "2510.06219": "|2025-10-07|Human3R: Everyone Everywhere All at Once|Yue Chen\u7b49|[2510.06219](http://arxiv.org/pdf/2510.06219)|\u65e0|\u25c6 We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos.\n\u25c6 Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies (\"everyone\"), dense 3D scene (\"everywhere\"), and camera trajectories in a single forward pass (\"all-at-once\").\n\u25c6 Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies.|\n",
    "2510.06216": "|2025-10-07|Dropping the D: RGB-D SLAM Without the Depth Sensor|Mert Kiray\u7b49|[2510.06216](http://arxiv.org/pdf/2510.06216)|\u65e0|\u25c6 We present DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors.\n\u25c6 The system replaces active depth input with three pretrained vision modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation network.\n\u25c6 Dynamic objects are suppressed using dilated instance masks, while static keypoints are assigned predicted depth values and backprojected into 3D to form metrically scaled features.|\n",
    "2510.05992": "|2025-10-07|Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations|Tien-Dat Nguyen\u7b49|[2510.05992](http://arxiv.org/pdf/2510.05992)|\u65e0|\u25c6 Onboard simultaneous localization and mapping (SLAM) methods are commonly used to provide accurate localization information for autonomous robots.\n\u25c6 However, the coordinate origin of SLAM estimate often resets for each run.\n\u25c6 On the other hand, UWB-based localization with fixed anchors can ensure a consistent coordinate reference across sessions; however, it requires an accurate assignment of the anchor nodes' coordinates.|\n",
    "2510.06644": "|2025-10-09|RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction|Leshu Li\u7b49|[2510.06644](http://arxiv.org/pdf/2510.06644)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping (SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering efficiency and accuracy, but have not yet been adopted in resource-constrained edge devices due to insufficient speed.\n\u25c6 Addressing this, we identify notable redundancies across the SLAM pipeline for acceleration.\n\u25c6 While conceptually straightforward, practical approaches are required to minimize the overhead associated with identifying and eliminating these redundancies.|\n",
    "2510.08551": "|2025-10-09|ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation|Guanghao Li\u7b49|[2510.08551](http://arxiv.org/pdf/2510.08551)|\u65e0|\u25c6 On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics.\n\u25c6 Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness.\n\u25c6 In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines.|\n",
    "2510.12749": "|2025-10-14|SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding|Zhiliu Yang\u7b49|[2510.12749](http://arxiv.org/pdf/2510.12749)|\u65e0|\u25c6 The scene perception, understanding, and simulation are fundamental techniques for embodied-AI agents, while existing solutions are still prone to segmentation deficiency, dynamic objects' interference, sensor data sparsity, and view-limitation problems.\n\u25c6 This paper proposes a novel framework, named SPORTS, for holistic scene understanding via tightly integrating Video Panoptic Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into an iterative and unified perspective.\n\u25c6 Firstly, VPS designs an adaptive attention-based geometric fusion mechanism to align cross-frame features via enrolling the pose, depth, and optical flow modality, which automatically adjust feature maps for different decoding stages.|\n",
    "2510.12346": "|2025-10-14|PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing|Bingquan Li\u7b49|[2510.12346](http://arxiv.org/pdf/2510.12346)|\u65e0|\u25c6 Recently, biped robot walking technology has been significantly developed, mainly in the context of a bland walking scheme.\n\u25c6 To emulate human walking, robots need to step on the positions they see in unknown spaces accurately.\n\u25c6 In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs.|\n",
    "2510.13546": "|2025-10-15|Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU|Ruiqi Ye\u7b49|[2510.13546](http://arxiv.org/pdf/2510.13546)|\u65e0|\u25c6 Feature detection is a common yet time-consuming module in Simultaneous Localization and Mapping (SLAM) implementations, which are increasingly deployed on power-constrained platforms, such as drones.\n\u25c6 Graphics Processing Units (GPUs) have been a popular accelerator for computer vision in general, and feature detection and SLAM in particular.\n\u25c6 On the other hand, System-on-Chips (SoCs) with integrated Field Programmable Gate Array (FPGA) are also widely available.|\n",
    "2510.13464": "|2025-10-15|Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition|Emily Miller\u7b49|[2510.13464](http://arxiv.org/pdf/2510.13464)|\u65e0|\u25c6 Visual Place Recognition (VPR) enables robots and autonomous vehicles to identify previously visited locations by matching current observations against a database of known places.\n\u25c6 However, VPR systems face significant challenges when deployed across varying visual environments, lighting conditions, seasonal changes, and viewpoints changes.\n\u25c6 Failure-critical VPR applications, such as loop closure detection in simultaneous localization and mapping (SLAM) pipelines, require robust estimation of place matching uncertainty.|\n",
    "2510.13287": "|2025-10-15|DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping|Nishant Chandna\u7b49|[2510.13287](http://arxiv.org/pdf/2510.13287)|\u65e0|\u25c6 LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for enabling precise navigation and environmental reconstruction across various applications.\n\u25c6 Although current point-to-plane ICP algorithms perform effec- tively in structured, feature-rich environments, they struggle in scenarios with sparse features, repetitive geometric structures, and high-frequency motion.\n\u25c6 This leads to degeneracy in 6- DOF pose estimation.|\n",
    "2510.14945": "|2025-10-16|3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation|JoungBin Lee\u7b49|[2510.14945](http://arxiv.org/pdf/2510.14945)|\u65e0|\u25c6 We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency.\n\u25c6 Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video.\n\u25c6 Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency.|\n",
    "2510.15803": "|2025-10-17|Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion|Zahra Arjmandi\u7b49|[2510.15803](http://arxiv.org/pdf/2510.15803)|\u65e0|\u25c6 This paper presents a novel fusion technique for LiDAR Simultaneous Localization and Mapping (SLAM), aimed at improving localization and 3D mapping using LiDAR sensor.\n\u25c6 Our approach centers on the Inferred Attention Fusion (INAF) module, which integrates AI with geometric odometry.\n\u25c6 Utilizing the KITTI dataset's LiDAR data, INAF dynamically adjusts attention weights based on environmental feedback, enhancing the system's adaptability and measurement accuracy.|\n",
    "2510.15220": "|2025-10-17|LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization|Kevin Christiansen Marsim\u7b49|[2510.15220](http://arxiv.org/pdf/2510.15220)|\u65e0|\u25c6 Autonomous navigation for legged robots in complex and dynamic environments relies on robust simultaneous localization and mapping (SLAM) systems to accurately map surroundings and localize the robot, ensuring safe and efficient operation.\n\u25c6 While prior sensor fusion-based SLAM approaches have integrated various sensor modalities to improve their robustness, these algorithms are still susceptible to estimation drift in challenging environments due to their reliance on unsuitable fusion strategies.\n\u25c6 Therefore, we propose a robust LiDAR-visual-inertial-kinematic odometry system that integrates information from multiple sensors, such as a camera, LiDAR, inertial measurement unit (IMU), and joint encoders, for visual and LiDAR-based odometry estimation.|\n",
    "2510.17422": "|2025-10-21|DeepDetect: Learning All-in-One Dense Keypoints|Shaharyar Ahmed Khan Tareen\u7b49|[2510.17422](http://arxiv.org/pdf/2510.17422)|\u65e0|\u25c6 Keypoint detection is the foundation of many computer vision tasks, including image registration, structure-from motion, 3D reconstruction, visual odometry, and SLAM.\n\u25c6 Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong performance yet suffer from key limitations: sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding, often failing to prioritize visually important regions.\n\u25c6 We present DeepDetect, an intelligent, all-in-one, dense keypoint detector that unifies the strengths of classical detectors using deep learning.|\n",
    "2510.16438": "|2025-10-18|LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching|Aidyn Ubingazhibov\u7b49|[2510.16438](http://arxiv.org/pdf/2510.16438)|\u65e0|\u25c6 Lines and points are complementary local features, whose combination has proven effective for applications such as SLAM and Structure-from-Motion.\n\u25c6 The backbone of these pipelines are the local feature matchers, establishing correspondences across images.\n\u25c6 Traditionally, point and line matching have been treated as independent tasks.|\n",
    "2510.16205": "|2025-10-17|VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments|Jo\u00e3o Carlos Virgolino Soares\u7b49|[2510.16205](http://arxiv.org/pdf/2510.16205)|\u65e0|\u25c6 Visual SLAM in dynamic environments remains challenging, as several existing methods rely on semantic filtering that only handles known object classes, or use fixed robust kernels that cannot adapt to unknown moving objects, leading to degraded accuracy when they appear in the scene.\n\u25c6 We present VAR-SLAM (Visual Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a lightweight semantic keypoint filter to deal with known moving objects, with Barron's adaptive robust loss to handle unknown ones.\n\u25c6 The shape parameter of the robust kernel is estimated online from residuals, allowing the system to automatically adjust between Gaussian and heavy-tailed behavior.|\n",
    "2510.18991": "|2025-10-21|Underwater Dense Mapping with the First Compact 3D Sonar|Chinmay Burgul\u7b49|[2510.18991](http://arxiv.org/pdf/2510.18991)|\u65e0|\u25c6 In the past decade, the adoption of compact 3D range sensors, such as LiDARs, has driven the developments of robust state-estimation pipelines, making them a standard sensor for aerial, ground, and space autonomy.\n\u25c6 Unfortunately, poor propagation of electromagnetic waves underwater, has limited the visibility-independent sensing options of underwater state-estimation to acoustic range sensors, which provide 2D information including, at-best, spatially ambiguous information.\n\u25c6 This paper, to the best of our knowledge, is the first study examining the performance, capacity, and opportunities arising from the recent introduction of the first compact 3D sonar.|\n",
    "2510.20549": "|2025-10-23|Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation|Marziyeh Bamdad\u7b49|[2510.20549](http://arxiv.org/pdf/2510.20549)|\u65e0|\u25c6 Despite advancements in SLAM technologies, robust operation under challenging conditions such as low-texture, motion-blur, or challenging lighting remains an open challenge.\n\u25c6 Such conditions are common in applications such as assistive navigation for the visually impaired.\n\u25c6 These challenges undermine localization accuracy and tracking stability, reducing navigation reliability and safety.|\n",
    "2510.22754": "|2025-10-26|TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments|Chunyu Li\u7b49|[2510.22754](http://arxiv.org/pdf/2510.22754)|\u65e0|\u25c6 Multi-agent cooperative SLAM often encounters challenges in similar indoor environments characterized by repetitive structures, such as corridors and rooms.\n\u25c6 These challenges can lead to significant inaccuracies in shared location identification when employing point cloud-based techniques.\n\u25c6 To mitigate these issues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that integrates text semantics and WiFi signal features to enhance location identification and loop closure detection.|\n",
    "2510.22740": "|2025-10-26|Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM|Sai Krishna Ghanta\u7b49|[2510.22740](http://arxiv.org/pdf/2510.22740)|\u65e0|\u25c6 We consider the distributed pose-graph optimization (PGO) problem, which is fundamental in accurate trajectory estimation in multi-robot simultaneous localization and mapping (SLAM).\n\u25c6 Conventional iterative approaches linearize a highly non-convex optimization objective, requiring repeated solving of normal equations, which often converge to local minima and thus produce suboptimal estimates.\n\u25c6 We propose a scalable, outlier-robust distributed planar PGO framework using Multi-Agent Reinforcement Learning (MARL).|\n",
    "2510.22669": "|2025-10-26|LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering|Wenkai Zhu\u7b49|[2510.22669](http://arxiv.org/pdf/2510.22669)|\u65e0|\u25c6 3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence.\n\u25c6 However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity.\n\u25c6 To address these challenges, we propose \\textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system.|\n",
    "2510.22600": "|2025-10-26|RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience|Huilin Yin\u7b49|[2510.22600](http://arxiv.org/pdf/2510.22600)|\u65e0|\u25c6 The reliability of Simultaneous Localization and Mapping (SLAM) is severely constrained in environments where visual inputs suffer from noise and low illumination.\n\u25c6 Although recent 3D Gaussian Splatting (3DGS) based SLAM frameworks achieve high-fidelity mapping under clean conditions, they remain vulnerable to compounded degradations that degrade mapping and tracking performance.\n\u25c6 A key observation underlying our work is that the original 3DGS rendering pipeline inherently behaves as an implicit low-pass filter, attenuating high-frequency noise but also risking over-smoothing.|\n",
    "2510.22588": "|2025-10-26|UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models|Wenming Tu\u7b49|[2510.22588](http://arxiv.org/pdf/2510.22588)|\u65e0|\u25c6 Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering.\n\u25c6 To address this limitation, we introduce UltraVoice, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control.\n\u25c6 Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles.|\n",
    "2510.22529": "|2025-10-26|Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing|Xiang Fei\u7b49|[2510.22529](http://arxiv.org/pdf/2510.22529)|\u65e0|\u25c6 Loop closure is critical in Simultaneous Localization and Mapping (SLAM) systems to reduce accumulative drift and ensure global mapping consistency.\n\u25c6 However, conventional methods struggle in perceptually aliased environments, such as narrow pipes, due to vector quantization, feature sparsity, and repetitive textures, while existing solutions often incur high computational costs.\n\u25c6 This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure detection method that achieves superior precision-recall, robustness, and computational efficiency.|\n",
    "2510.21215": "|2025-10-24|Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments|Shuoshuo Ding\u7b49|[2510.21215](http://arxiv.org/pdf/2510.21215)|\u65e0|\u25c6 Visual degradation caused by limited visibility, insufficient lighting, and feature scarcity in underwater environments presents significant challenges to visual-inertial simultaneous localization and mapping (SLAM) systems.\n\u25c6 To address these challenges, this paper proposes a graph-based visual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an inertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure sensor.\n\u25c6 The key innovation lies in the tight integration of four distinct sensor modalities to ensure reliable operation, even under degraded visual conditions.|\n",
    "2510.24571": "|2025-10-28|Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots|Hongxu Zhao\u7b49|[2510.24571](http://arxiv.org/pdf/2510.24571)|\u65e0|\u25c6 The calibration of extrinsic parameters and clock offsets between sensors for high-accuracy performance in underwater SLAM systems remains insufficiently explored.\n\u25c6 Existing methods for Doppler Velocity Log (DVL) calibration are either constrained to specific sensor configurations or rely on oversimplified assumptions, and none jointly estimate translational extrinsics and time offsets.\n\u25c6 We propose a Unified Iterative Calibration (UIC) framework for general DVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a Gaussian Process (GP) motion prior for high-fidelity motion interpolation.|\n",
    "2510.24533": "|2025-10-28|GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots|Yuan Shen\u7b49|[2510.24533](http://arxiv.org/pdf/2510.24533)|\u65e0|\u25c6 Accurate visual inertial simultaneous localization and mapping (VI SLAM) for underwater robots remains a significant challenge due to frequent visual degeneracy and insufficient inertial measurement unit (IMU) motion excitation.\n\u25c6 In this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system designed to address these issues.\n\u25c6 By leveraging the stereo camera's direct depth estimation ability, we eliminate the need to estimate scale during IMU initialization, enabling stable operation even under low acceleration dynamics.|\n",
    "2510.23988": "|2025-10-28|A Survey on Collaborative SLAM with 3D Gaussian Splatting|Phuc Nguyen Xuan\u7b49|[2510.23988](http://arxiv.org/pdf/2510.23988)|\u65e0|\u25c6 This survey comprehensively reviews the evolving field of multi-robot collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian Splatting (3DGS).\n\u25c6 As an explicit scene representation, 3DGS has enabled unprecedented real-time, high-fidelity rendering, ideal for robotics.\n\u25c6 However, its use in multi-robot systems introduces significant challenges in maintaining global consistency, managing communication, and fusing data from heterogeneous sources.|\n",
    "2510.25146": "|2025-10-29|EA3D: Online Open-World 3D Object Extraction from Streaming Videos|Xiaoyu Zhou\u7b49|[2510.25146](http://arxiv.org/pdf/2510.25146)|\u65e0|\u25c6 Current 3D scene understanding methods are limited by offline-collected multi-view data or pre-constructed 3D geometry.\n\u25c6 In this paper, we present ExtractAnything3D (EA3D), a unified online framework for open-world 3D object extraction that enables simultaneous geometric reconstruction and holistic scene understanding.\n\u25c6 Given a streaming video, EA3D dynamically interprets each frame using vision-language and 2D vision foundation encoders to extract object-level knowledge.|\n",
    "2510.26358": "|2025-10-30|AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM|Mirko Usuelli\u7b49|[2510.26358](http://arxiv.org/pdf/2510.26358)|\u65e0|\u25c6 Autonomous robots in orchards require real-time 3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion.\n\u25c6 We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering.\n\u25c6 Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradient-driven map lifecycle executed between keyframes preserves fine details and bounds memory.|\n",
    "2510.26131": "|2025-10-30|Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM|Ali Caglayan\u7b49|[2510.26131](http://arxiv.org/pdf/2510.26131)|\u65e0|\u25c6 Attention models have recently emerged as a powerful approach, demonstrating significant progress in various fields.\n\u25c6 Visualization techniques, such as class activation mapping, provide visual insights into the reasoning of convolutional neural networks (CNNs).\n\u25c6 Using network gradients, it is possible to identify regions where the network pays attention during image recognition tasks.|\n",
    "2510.27133": "|2025-10-31|WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond|Zhicong Sun\u7b49|[2510.27133](http://arxiv.org/pdf/2510.27133)|\u65e0|\u25c6 3D Gaussian splatting (3DGS) and its subsequent variants have led to remarkable progress in simultaneous localization and mapping (SLAM).\n\u25c6 While most recent 3DGS-based SLAM works focus on small-scale indoor scenes, developing 3DGS-based SLAM methods for large-scale forest scenes holds great potential for many real-world applications, especially for wildfire emergency response and forest management.\n\u25c6 However, this line of research is impeded by the absence of a comprehensive and high-quality dataset, and collecting such a dataset over real-world scenes is costly and technically infeasible.|\n",
    "2511.02395": "|2025-11-04|Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds|Leon Schwarzer\u7b49|[2511.02395](http://arxiv.org/pdf/2511.02395)|\u65e0|\u25c6 Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning.\n\u25c6 While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context.\n\u25c6 Radar sensors overcome this problem with their ability to provide a direct measurement of a point's Doppler velocity, which can be exploited for single-scan moving object segmentation.|\n",
    "2511.02036": "|2025-11-03|TurboMap: GPU-Accelerated Local Mapping for Visual SLAM|Parsa Hosseininejad\u7b49|[2511.02036](http://arxiv.org/pdf/2511.02036)|\u65e0|\u25c6 This paper presents TurboMap, a GPU-accelerated and CPU-optimized local mapping module for visual SLAM systems.\n\u25c6 We identify key performance bottlenecks in the local mapping process for visual SLAM and address them through targeted GPU and CPU optimizations.\n\u25c6 Specifically, we offload map point triangulation and fusion to the GPU, accelerate redundant keyframe culling on the CPU, and integrate a GPU-accelerated solver to speed up local bundle adjustment.|\n",
    "2511.01379": "|2025-11-03|CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels|Kun Hu\u7b49|[2511.01379](http://arxiv.org/pdf/2511.01379)|\u65e0|\u25c6 Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and GPS-denied underground coal mine environments presents significant challenges.\n\u25c6 Sensors must contend with abnormal operating conditions: GPS unavailability impedes scene reconstruction and absolute geographic referencing, uneven or slippery terrain degrades wheel odometer accuracy, and long, feature-poor tunnels reduce LiDAR effectiveness.\n\u25c6 To address these issues, we propose CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM framework based on the Iterated Error-State Kalman Filter (IESKF).|\n",
    "2511.01219": "|2025-11-03|Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference|Muhua Zhang\u7b49|[2511.01219](http://arxiv.org/pdf/2511.01219)|\u65e0|\u25c6 This paper addresses the Kidnapped Robot Problem (KRP), a core localization challenge of relocalizing a robot in a known map without prior pose estimate when localization loss or at SLAM initialization.\n\u25c6 For this purpose, a passive 2-D global relocalization framework is proposed.\n\u25c6 It estimates the global pose efficiently and reliably from a single LiDAR scan and an occupancy grid map while the robot remains stationary, thereby enhancing the long-term autonomy of mobile robots.|\n",
    "2511.01186": "|2025-11-03|LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping|Lijie Wang\u7b49|[2511.01186](http://arxiv.org/pdf/2511.01186)|\u65e0|\u25c6 Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding.\n\u25c6 Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration.\n\u25c6 Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale.|\n",
    "2511.00635": "|2025-11-01|Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles|Hyungtae Lim\u7b49|[2511.00635](http://arxiv.org/pdf/2511.00635)|\u65e0|\u25c6 As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted.\n\u25c6 Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions.\n\u25c6 In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration.|\n",
    "2511.04531": "|2025-11-06|Synchronous Observer Design for Landmark-Inertial SLAM with Almost-Global Convergence|Arkadeep Saha\u7b49|[2511.04531](http://arxiv.org/pdf/2511.04531)|\u65e0|\u25c6 Landmark Inertial Simultaneous Localisation and Mapping (LI-SLAM) is the problem of estimating the locations of landmarks in the environment and the robot's pose relative to those landmarks using landmark position measurements and measurements from Inertial Measurement Unit (IMU).\n\u25c6 This paper proposes a nonlinear observer for LI-SLAM posed in continuous time and analyses the observer in a base space that encodes all the observable states of LI-SLAM.\n\u25c6 The local exponential stability and almost-global asymptotic stability of the error dynamics in base space is established in the proof section and validated using simulations.|\n",
    "2511.04180": "|2025-11-06|PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration|Yizhen Yin\u7b49|[2511.04180](http://arxiv.org/pdf/2511.04180)|\u65e0|\u25c6 Existing Active SLAM methodologies face issues such as slow exploration speed and suboptimal paths.\n\u25c6 To address these limitations, we propose a hybrid framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement Learning framework and a Lightweight Stagnation Detection mechanism.\n\u25c6 The Path-Uncertainty Co-Optimization framework jointly optimizes travel distance and map uncertainty through a dual-objective reward function, balancing exploration and exploitation.|\n",
    "2511.03754": "|2025-11-04|Analytical modelling of a stop-less modular bus service with an application to charging strategies comparison|Haoran Zhao\u7b49|[2511.03754](http://arxiv.org/pdf/2511.03754)|\u65e0|\u25c6 Buses are a vital component of metropolitan public transport, yet conventional bus services often struggle with inefficiencies including extended dwelling time, which increases in-vehicle travel time for non-alighting passengers.\n\u25c6 A stop-less autonomous modular (SLAM) bus service has emerged as a solution, enabling dynamic capacity to reduce dwelling time.\n\u25c6 Meanwhile, the electrification of buses is advancing as a strategy to mitigate greenhouse gas emissions and reduces operators' costs, but introduces new operational constraints due to charging requirements.|\n",
    "2511.05404": "|2025-11-07|Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments|Laura Alejandra Encinar Gonzalez\u7b49|[2511.05404](http://arxiv.org/pdf/2511.05404)|\u65e0|\u25c6 Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration.\n\u25c6 In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity.\n\u25c6 This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments.|\n",
    "2511.06919": "|2025-11-10|Integration of Visual SLAM into Consumer-Grade Automotive Localization|Luis Diener\u7b49|[2511.06919](http://arxiv.org/pdf/2511.06919)|\u65e0|\u25c6 Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e.\n\u25c6 wheel odometry and IMUs, whose performance is limited by systematic errors and calibration.\n\u25c6 While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored.|\n",
    "2511.06749": "|2025-11-10|Semi-distributed Cross-modal Air-Ground Relative Localization|Weining Lu\u7b49|[2511.06749](http://arxiv.org/pdf/2511.06749)|\u65e0|\u25c6 Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks.\n\u25c6 However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy.\n\u25c6 To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework.|\n",
    "2511.05858": "|2025-11-08|ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface|Chuanyu Li\u7b49|[2511.05858](http://arxiv.org/pdf/2511.05858)|\u65e0|\u25c6 Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently.\n\u25c6 However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks.\n\u25c6 In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks.|\n",
    "2511.05816": "|2025-11-08|3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots|Taku Okawara\u7b49|[2511.05816](http://arxiv.org/pdf/2511.05816)|\u65e0|\u25c6 Limbed climbing robots are designed to explore challenging vertical walls, such as the skylights of the Moon and Mars.\n\u25c6 In such robots, the primary role of a hand-eye camera is to accurately estimate 3D positions of graspable points (i.e., convex terrain surfaces) thanks to its close-up views.\n\u25c6 While conventional climbing robots often employ RGB-D cameras as hand-eye cameras to facilitate straightforward 3D terrain mapping and graspable point detection, RGB-D cameras are large and consume considerable power.|\n",
    "2410.10669": "|2025-11-07|MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and Mapping|Taozhe Li\u7b49|[2410.10669](http://arxiv.org/pdf/2410.10669)|\u65e0|\u25c6 The Visual Simultaneous Localization and Mapping (V-SLAM) system has seen significant development in recent years, demonstrating high precision in environments with limited dynamic objects.\n\u25c6 However, their performance significantly deteriorates when deployed in settings with a higher presence of movable objects, such as environments with pedestrians, cars, and buses, which are common in outdoor scenes.\n\u25c6 To address this issue, we propose a Multilayer Perceptron (MLP)-based real-time stereo SLAM system that leverages complete geometry information to avoid information loss.|\n",
    "2411.08279": "|2025-08-11|MBA-SLAM: Motion Blur Aware Gaussian Splatting SLAM|Peng Wang\u7b49|[2411.08279](http://arxiv.org/pdf/2411.08279)|\u65e0|\u25c6 Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input.\n\u25c6 However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions.\n\u25c6 This often results in a significant reduction in both camera localization accuracy and map reconstruction quality.|\n",
    "2511.09372": "|2025-11-12|Generation-Agnostic Zero-Energy Devices for Sustainable Connectivity, Sensing, and Localization|Navid Amani\u7b49|[2511.09372](http://arxiv.org/pdf/2511.09372)|\u65e0|\u25c6 The massive scale of Internet of Things (IoT) connectivity expected in 6G networks raises unprecedented challenges in energy use, battery waste, and lifecycle sustainability.\n\u25c6 Current cellular IoT solutions remain bound to the lifetime of underlying network generations and rely on billions of disposable batteries, creating unsustainable economic and environmental costs.\n\u25c6 This article proposes generation-agnostic zero-energy devices (XG-ZEDs), a new class of backscatter based IoT devices that are battery-less, spectrum-agnostic, and future-proof across successive network generations.|\n",
    "2511.09302": "|2025-11-12|UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning|Yan Huang\u7b49|[2511.09302](http://arxiv.org/pdf/2511.09302)|\u65e0|\u25c6 Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods.\n\u25c6 The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely.\n\u25c6 Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view.|\n",
    "2511.09072": "|2025-11-12|SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields|Sangheon Yang\u7b49|[2511.09072](http://arxiv.org/pdf/2511.09072)|\u65e0|\u25c6 Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization.\n\u25c6 This approach is computationally expensive, limiting their real-time performance on resource-constrained devices.\n\u25c6 To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework.|\n",
    "2511.06765": "|2025-11-10|Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes|Meijun Guo\u7b49|[2511.06765](http://arxiv.org/pdf/2511.06765)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality.\n\u25c6 To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation.\n\u25c6 For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments.|\n",
    "2511.10699": "|2025-11-12|DualVision ArthroNav: Investigating Opportunities to Enhance Localization and Reconstruction in Image-based Arthroscopy Navigation via External Cameras|Hongchao Shu\u7b49|[2511.10699](http://arxiv.org/pdf/2511.10699)|\u65e0|\u25c6 Arthroscopic procedures can greatly benefit from navigation systems that enhance spatial awareness, depth perception, and field of view.\n\u25c6 However, existing optical tracking solutions impose strict workspace constraints and disrupt surgical workflow.\n\u25c6 Vision-based alternatives, though less invasive, often rely solely on the monocular arthroscope camera, making them prone to drift, scale ambiguity, and sensitivity to rapid motion or occlusion.|\n",
    "2511.13216": "|2025-11-17|GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry|Chiyun Noh\u7b49|[2511.13216](http://arxiv.org/pdf/2511.13216)|\u65e0|\u25c6 Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms.\n\u25c6 In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping.\n\u25c6 Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation.|\n",
    "2511.12653": "|2025-11-16|DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry|Cheng Liao|[2511.12653](http://arxiv.org/pdf/2511.12653)|\u65e0|\u25c6 Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms.\n\u25c6 This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry).\n\u25c6 Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model.|\n",
    "2511.11845": "|2025-11-14|Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture|K. A. I. N Jayarathne\u7b49|[2511.11845](http://arxiv.org/pdf/2511.11845)|\u65e0|\u25c6 Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments.\n\u25c6 This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions.\n\u25c6 The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning.|\n",
    "2511.14639": "|2025-11-18|SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology|Marco Acerbis\u7b49|[2511.14639](http://arxiv.org/pdf/2511.14639)|\u65e0|\u25c6 Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low.\n\u25c6 We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks.\n\u25c6 To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse.|\n",
    "2511.14335": "|2025-11-18|Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors|Jeryes Danial\u7b49|[2511.14335](http://arxiv.org/pdf/2511.14335)|\u65e0|\u25c6 Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera.\n\u25c6 Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive.\n\u25c6 Monocular SLAM also faces scale ambiguities, which affect its accuracy.|\n",
    "2511.14330": "|2025-11-18|MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning|Yizhen Yin\u7b49|[2511.14330](http://arxiv.org/pdf/2511.14330)|\u65e0|\u25c6 Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community.\n\u25c6 While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery.\n\u25c6 In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments.|\n",
    "2511.14149": "|2025-11-18|iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion|Hao Wang\u7b49|[2511.14149](http://arxiv.org/pdf/2511.14149)|\u65e0|\u25c6 Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model.\n\u25c6 However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose.\n\u25c6 This multi-round process incurs significant computational overhead, hindering real-time performance in robotics.|\n",
    "2511.15597": "|2025-11-19|Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition|Xufei Wang\u7b49|[2511.15597](http://arxiv.org/pdf/2511.15597)|\u65e0|\u25c6 LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving.\n\u25c6 However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting.\n\u25c6 To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism.|\n",
    "2511.14919": "|2025-11-18|A visual study of ICP variants for Lidar Odometry|Sebastian Dingler\u7b49|[2511.14919](http://arxiv.org/pdf/2511.14919)|\u65e0|\u25c6 Odometry with lidar sensors is a state-of-the-art method to estimate the ego pose of a moving vehicle.\n\u25c6 Many implementations of lidar odometry use variants of the Iterative Closest Point (ICP) algorithm.\n\u25c6 Real-world effects such as dynamic objects, non-overlapping areas, and sensor noise diminish the accuracy of ICP.|\n",
    "2511.16349": "|2025-11-20|CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering|Joni Vanherck\u7b49|[2511.16349](http://arxiv.org/pdf/2511.16349)|\u65e0|\u25c6 Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content.\n\u25c6 Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure.\n\u25c6 This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud.|\n",
    "2511.16282": "|2025-11-20|Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM|Gergely Dinya\u7b49|[2511.16282](http://arxiv.org/pdf/2511.16282)|\u65e0|\u25c6 We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT).\n\u25c6 The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation.\n\u25c6 To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands.|\n",
    "2511.16144": "|2025-11-20|LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM|Sibaek Lee\u7b49|[2511.16144](http://arxiv.org/pdf/2511.16144)|\u65e0|\u25c6 Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps.\n\u25c6 However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction.\n\u25c6 Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments.|\n",
    "2511.16091": "|2025-11-20|Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments|Renxiang Xiao\u7b49|[2511.16091](http://arxiv.org/pdf/2511.16091)|\u65e0|\u25c6 We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation.\n\u25c6 Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy.\n\u25c6 Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity.|\n",
    "2511.16048": "|2025-11-20|Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud|Qing Zhang\u7b49|[2511.16048](http://arxiv.org/pdf/2511.16048)|\u65e0|\u25c6 While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach.\n\u25c6 We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology.\n\u25c6 We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate.|\n",
    "2511.17384": "|2025-11-21|IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation|Yifan Li\u7b49|[2511.17384](http://arxiv.org/pdf/2511.17384)|\u65e0|\u25c6 While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning.\n\u25c6 Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity.\n\u25c6 To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning.|\n",
    "2511.17299": "|2025-11-21|MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning|Tom\u00e1\u0161 Musil\u7b49|[2511.17299](http://arxiv.org/pdf/2511.17299)|\u65e0|\u25c6 Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors.\n\u25c6 In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning.\n\u25c6 The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty.|\n",
    "2511.17207": "|2025-11-21|SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors|Kunyi Li\u7b49|[2511.17207](http://arxiv.org/pdf/2511.17207)|\u65e0|\u25c6 Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis.\n\u25c6 To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework.\n\u25c6 The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications.|\n",
    "2511.19031": "|2025-11-26|Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors|Yuchen Zhou\u7b49|[2511.19031](http://arxiv.org/pdf/2511.19031)|\u65e0|\u25c6 Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera.\n\u25c6 While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization.\n\u25c6 To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses.|\n",
    "2511.18857": "|2025-11-24|AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion|Changsheng Luo\u7b49|[2511.18857](http://arxiv.org/pdf/2511.18857)|\u65e0|\u25c6 Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail.\n\u25c6 Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection.\n\u25c6 This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm.|\n",
    "2511.18756": "|2025-11-24|SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map|Xueyu Du\u7b49|[2511.18756](http://arxiv.org/pdf/2511.18756)|\u65e0|\u25c6 Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation.\n\u25c6 To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints.\n\u25c6 Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates.|\n",
    "2511.18755": "|2025-11-24|Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing|Xiaotong Huang\u7b49|[2511.18755](http://arxiv.org/pdf/2511.18755)|\u65e0|\u25c6 3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence.\n\u25c6 However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.\n\u25c6 This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices.|\n",
    "2511.18694": "|2025-11-24|Stable Multi-Drone GNSS Tracking System for Marine Robots|Shuo Wen\u7b49|[2511.18694](http://arxiv.org/pdf/2511.18694)|\u65e0|\u25c6 Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface.\n\u25c6 Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence.\n\u25c6 In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots.|\n",
    "2511.18470": "|2025-11-23|Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span|Heeseung Yun\u7b49|[2511.18470](http://arxiv.org/pdf/2511.18470)|\u65e0|\u25c6 People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors.\n\u25c6 While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies.\n\u25c6 We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment.|\n",
    "2511.17992": "|2025-11-22|Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation|Chungeng Tian\u7b49|[2511.17992](http://arxiv.org/pdf/2511.17992)|\u65e0|\u25c6 The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge.\n\u25c6 While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction.\n\u25c6 Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators.|\n",
    "2511.17792": "|2025-11-21|Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?|Dingrui Wang\u7b49|[2511.17792](http://arxiv.org/pdf/2511.17792)|\u65e0|\u25c6 While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified.\n\u25c6 We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments.\n\u25c6 Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories.|\n",
    "2511.20566": "|2025-11-25|The origin of B-type runaway stars based on kinematics|Yanjun Guo\u7b49|[2511.20566](http://arxiv.org/pdf/2511.20566)|\u65e0|\u25c6 Runaway stars depart their birthplaces with high peculiar velocities.\n\u25c6 Two mechanisms are commonly invoked to explain their origin, the binary supernova scenario (BSS) and the dynamical ejection scenario (DES).\n\u25c6 Investigating the kinematic properties of runaway stars is key to understanding their origins.We intend to investigate the origins of 39 B-type runaway stars from LAMOST using orbital traceback analysis.|\n",
    "2511.20496": "|2025-11-25|Metric, inertially aligned monocular state estimation via kinetodynamic priors|Jiaxin Liu\u7b49|[2511.20496](http://arxiv.org/pdf/2511.20496)|\u65e0|\u25c6 Accurate state estimation for flexible robotic systems poses significant challenges, particular for platforms with dynamically deforming structures that invalidate rigid-body assumptions.\n\u25c6 This paper tackles this problem and allows to extend existing rigid-body pose estimation methods to non-rigid systems.\n\u25c6 Our approach hinges on two core assumptions: first, the elastic properties are captured by an injective deformation-force model, efficiently learned via a Multi-Layer Perceptron; second, we solve the platform's inherently smooth motion using continuous-time B-spline kinematic models.|\n",
    "2511.20343": "|2025-11-25|AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend|Hengyi Wang\u7b49|[2511.20343](http://arxiv.org/pdf/2511.20343)|\u65e0|\u25c6 We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks.\n\u25c6 The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness.\n\u25c6 Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization.|\n",
    "2511.20005": "|2025-11-25|Stellar Parameters of BOSS M dwarfs in SDSS-V DR19|Dan Qiu\u7b49|[2511.20005](http://arxiv.org/pdf/2511.20005)|\u65e0|\u25c6 We utilized the Stellar LAbel Machine (SLAM), a data-driven model based on Support Vector Regression, to derive stellar parameters ([Fe/H], $T_{\\rm eff}$, and $\\log{g}$) for SDSS-V M dwarfs using low-resolution optical spectra (R$\\sim$2000) obtained with the BOSS spectrographs.\n\u25c6 These parameters are calibrated using LAMOST F, G or K dwarf companions ([Fe/H]), and APOGEE Net ($T_{\\rm eff}$ and $\\log{g}$), respectively.\n\u25c6 Comparisons of SLAM predicted [Fe/H] values between two components of M+M dwarfs wide binaries show no bias but with a scatter of 0.11 dex.|\n",
    "2511.21083": "|2025-11-26|Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry|Feiyang Pan\u7b49|[2511.21083](http://arxiv.org/pdf/2511.21083)|\u65e0|\u25c6 Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality.\n\u25c6 Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms.\n\u25c6 Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked.|\n",
    "2511.20865": "|2025-11-25|Estimating Fog Parameters from a Sequence of Stereo Images|Yining Ding\u7b49|[2511.20865](http://arxiv.org/pdf/2511.20865)|\u65e0|\u25c6 We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically.\n\u25c6 In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem.\n\u25c6 By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous.|\n",
    "2511.23221": "|2025-11-28|Robust 3DGS-based SLAM via Adaptive Kernel Smoothing|Shouhe Zhang\u7b49|[2511.23221](http://arxiv.org/pdf/2511.23221)|\u65e0|\u25c6 In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy.\n\u25c6 We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking.\n\u25c6 To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM.|\n",
    "2511.23156": "|2025-12-01|Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures|Sanne M. van Essen\u7b49|[2511.23156](http://arxiv.org/pdf/2511.23156)|\u65e0|\u25c6 To ensure the safety of marine and coastal structures, extreme (design) values should be known at the design stage.\n\u25c6 But for such complex systems, estimating the magnitude of events which are both non-linear and rare is extremely challenging, and involves considerable computational cost to capture the high-fidelity physics.\n\u25c6 To address this challenge, we offer a new multi-fidelity screening method, Probabilistic Adaptive Screening (PAS), which accurately predicts extreme values of strongly non-linear wave-induced loads while minimising the required high-fidelity simulation duration.|\n",
    "2511.23030": "|2025-11-28|DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management|Casimir Feldmann\u7b49|[2511.23030](http://arxiv.org/pdf/2511.23030)|\u65e0|\u25c6 Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities.\n\u25c6 However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments.\n\u25c6 We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk.|\n",
    "2511.22968": "|2025-11-28|Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM|Shouhe Zhang\u7b49|[2511.22968](http://arxiv.org/pdf/2511.22968)|\u65e0|\u25c6 Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems.\n\u25c6 To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs.\n\u25c6 First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene's intrinsic properties, such as albedo, from transient lighting.|\n",
    "2511.22860": "|2025-11-28|MARVO: Marine-Adaptive Radiance-aware Visual Odometry|Sacchin Sundar\u7b49|[2511.22860](http://arxiv.org/pdf/2511.22860)|\u65e0|\u25c6 Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise.\n\u25c6 We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization.\n\u25c6 At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity.|\n",
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01753": "|2025-12-01|AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields|Zhihao Zhan\u7b49|[2512.01753](http://arxiv.org/pdf/2512.01753)|\u65e0|\u25c6 Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection.\n\u25c6 However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce.\n\u25c6 To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments.|\n",
    "2512.01296": "|2025-12-01|EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly|Xiaokun Pan\u7b49|[2512.01296](http://arxiv.org/pdf/2512.01296)|\u65e0|\u25c6 Real-time 3D reconstruction is a fundamental task in computer graphics.\n\u25c6 Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).\n\u25c6 Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.|\n",
    "2512.01018": "|2025-11-30|Integration of UWB Radar on Mobile Robots for Continuous Obstacle and Environment Mapping|Adelina Giurea\u7b49|[2512.01018](http://arxiv.org/pdf/2512.01018)|\u65e0|\u25c6 This paper presents an infrastructure-free approach for obstacle detection and environmental mapping using ultra-wideband (UWB) radar mounted on a mobile robotic platform.\n\u25c6 Traditional sensing modalities such as visual cameras and Light Detection and Ranging (LiDAR) fail in environments with poor visibility due to darkness, smoke, or reflective surfaces.\n\u25c6 In these visioned-impaired conditions, UWB radar offers a promising alternative.|\n",
    "2512.00771": "|2025-11-30|EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes|Xiaoshan Wu\u7b49|[2512.00771](http://arxiv.org/pdf/2512.00771)|\u65e0|\u25c6 Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction.\n\u25c6 Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction.\n\u25c6 However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras.|\n",
    "2512.00327": "|2025-11-29|Odometry Without Correspondence from Inertially Constrained Ruled Surfaces|Chenqi Zhu\u7b49|[2512.00327](http://arxiv.org/pdf/2512.00327)|\u65e0|\u25c6 Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow.\n\u25c6 This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality.\n\u25c6 Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence.|\n",
    "2512.02293": "|2025-12-02|VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM|Zihan Zhu\u7b49|[2512.02293](http://arxiv.org/pdf/2512.02293)|\u65e0|\u25c6 We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.\n\u25c6 Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.\n\u25c6 Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.|\n",
    "2512.03422": "|2025-12-03|What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models|Tianchen Deng\u7b49|[2512.03422](http://arxiv.org/pdf/2512.03422)|\u65e0|\u25c6 In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.\n\u25c6 While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.\n\u25c6 Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.|\n",
    "2512.04772": "|2025-12-04|TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards|Mauro Martini\u7b49|[2512.04772](http://arxiv.org/pdf/2512.04772)|\u65e0|\u25c6 In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation.\n\u25c6 However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials.\n\u25c6 The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions.|\n",
    "2512.07775": "|2025-12-08|OptMap: Geometric Map Distillation via Submodular Maximization|David Thorne\u7b49|[2512.07775](http://arxiv.org/pdf/2512.07775)|\u65e0|\u25c6 Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms.\n\u25c6 As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance.\n\u25c6 Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization.|\n",
    "2512.07221": "|2025-12-08|Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality|Zichao Shu\u7b49|[2512.07221](http://arxiv.org/pdf/2512.07221)|\u65e0|\u25c6 Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications.\n\u25c6 As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent.\n\u25c6 Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements.|\n",
    "2512.06868": "|2025-12-07|Dynamic Visual SLAM using a General 3D Prior|Xingguang Zhong\u7b49|[2512.06868](http://arxiv.org/pdf/2512.06868)|\u65e0|\u25c6 Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality.\n\u25c6 However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy.\n\u25c6 In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes.|\n",
    "2512.05299": "|2025-12-04|ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety|Ahmad Yehia\u7b49|[2512.05299](http://arxiv.org/pdf/2512.05299)|\u65e0|\u25c6 Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support.\n\u25c6 This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets.\n\u25c6 By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view.|\n",
    "2512.08653": "|2025-12-09|A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation|Doumegna Mawuto Koudjo Felix\u7b49|[2512.08653](http://arxiv.org/pdf/2512.08653)|\u65e0|\u25c6 Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior.\n\u25c6 This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing.\n\u25c6 Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion.|\n",
    "2512.08625": "|2025-12-09|OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics|Jisang Yoo\u7b49|[2512.08625](http://arxiv.org/pdf/2512.08625)|\u65e0|\u25c6 Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems.\n\u25c6 With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction.\n\u25c6 Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments.|\n",
    "2512.07969": "|2025-12-08|Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization|Alan Papalia\u7b49|[2512.07969](http://arxiv.org/pdf/2512.07969)|\u65e0|\u25c6 Robotic perception often requires solving large nonlinear least-squares (NLS) problems.\n\u25c6 While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution.\n\u25c6 Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties.|\n",
    "2512.09608": "|2025-12-10|Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization|Zhiheng Li\u7b49|[2512.09608](http://arxiv.org/pdf/2512.09608)|\u65e0|\u25c6 Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather.\n\u25c6 Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures.\n\u25c6 Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization.|\n",
    "2512.09411": "|2025-12-10|D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM|Siting Zhu\u7b49|[2512.09411](http://arxiv.org/pdf/2512.09411)|\u65e0|\u25c6 Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments.\n\u25c6 However, dense SLAM in dynamic environments remains challenging.\n\u25c6 Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects.|\n",
    "2512.10481": "|2025-12-11|Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks|Gaozhao Wang\u7b49|[2512.10481](http://arxiv.org/pdf/2512.10481)|\u65e0|\u25c6 Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment.\n\u25c6 In some scenarios, vision is occluded.\n\u25c6 The robot can then no longer obtain real-time scene state information through visual feedback.|\n",
    "2512.10360": "|2025-12-11|CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation|Liuyi Wang\u7b49|[2512.10360](http://arxiv.org/pdf/2512.10360)|\u65e0|\u25c6 Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps.\n\u25c6 While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks.\n\u25c6 To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR).|\n",
    "2512.10128": "|2025-12-10|Inertial Magnetic SLAM Systems Using Low-Cost Sensors|Chuan Huang\u7b49|[2512.10128](http://arxiv.org/pdf/2512.10128)|\u65e0|\u25c6 Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning.\n\u25c6 Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly.\n\u25c6 Moreover, they have bounded error within mapped regions.|\n"
  },
  "SFM": {
    "2505.00866": "|**2025-05-01**|**Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?**|Viktor Kocur et.al.|[2505.00866](http://arxiv.org/abs/2505.00866)|**[link](https://github.com/kocurvik/rdnet)**|\n",
    "2505.05473": "|**2025-05-08**|**DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion**|Qitao Zhao et.al.|[2505.05473](http://arxiv.org/abs/2505.05473)|null|\n",
    "2505.04612": "|**2025-05-20**|**FastMap: Revisiting Dense and Scalable Structure from Motion**|Jiahao Li et.al.|[2505.04612](http://arxiv.org/abs/2505.04612)|**[link](https://github.com/pals-ttic/fastmap)**|\n",
    "2505.03093": "|**2025-05-15**|**Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera**|Siming He et.al.|[2505.03093](http://arxiv.org/abs/2505.03093)|null|\n",
    "2505.01799": "|**2025-05-03**|**AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting**|Junhao Shi et.al.|[2505.01799](http://arxiv.org/abs/2505.01799)|null|\n",
    "2505.01729": "|**2025-05-03**|**PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth**|Bu Jin et.al.|[2505.01729](http://arxiv.org/abs/2505.01729)|null|\n",
    "2505.08215": "|**2025-05-13**|**Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People**|Haoshuai Zhou et.al.|[2505.08215](http://arxiv.org/abs/2505.08215)|null|\n",
    "2505.08013": "|**2025-05-12**|**RDD: Robust Feature Detector and Descriptor using Deformable Transformer**|Gonglin Chen et.al.|[2505.08013](http://arxiv.org/abs/2505.08013)|null|\n",
    "2505.07373": "|**2025-05-12**|**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**|Lintao Xiang et.al.|[2505.07373](http://arxiv.org/abs/2505.07373)|null|\n",
    "2505.06868": "|**2025-05-11**|**Symmetry in Fundamental Parameters of Galaxies on the Star-forming Main Sequence**|Zhicheng He et.al.|[2505.06868](http://arxiv.org/abs/2505.06868)|null|\n",
    "2505.06743": "|**2025-05-10**|**TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility**|Marius Baden et.al.|[2505.06743](http://arxiv.org/abs/2505.06743)|null|\n",
    "2505.12226": "|**2025-05-18**|**Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis**|Dong Yang et.al.|[2505.12226](http://arxiv.org/abs/2505.12226)|null|\n",
    "2505.10751": "|**2025-05-15**|**Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis**|Francisco Raverta Capua et.al.|[2505.10751](http://arxiv.org/abs/2505.10751)|**[link](https://github.com/lrse/sodm)**|\n",
    "2505.16882": "|**2025-05-22**|**Tracking the Flight: Exploring a Computational Framework for Analyzing Escape Responses in Plains Zebra (Equus quagga)**|Isla Duporge et.al.|[2505.16882](http://arxiv.org/abs/2505.16882)|**[link](https://github.com/neuroinformatics-unit/zebras-stitching)**|\n",
    "2505.15814": "|**2025-05-21**|**A Taxonomy of Structure from Motion Methods**|Federica Arrigoni et.al.|[2505.15814](http://arxiv.org/abs/2505.15814)|null|\n",
    "2505.23756": "|2025-05-29|Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping|Justin Lazarow\u7b49|[2505.23756](http://arxiv.org/pdf/2505.23756)|\u65e0|\u25c6 \u63d0\u51faRooms from Motion (RfM)\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u5148\u9a8c\u76f8\u673a\u4f4d\u59ff\u7684\u5ba4\u51853D\u7269\u4f53\u68c0\u6d4b\uff0c\u901a\u8fc7\u7269\u4f53\u4e2d\u5fc3\u5316\u6846\u67b6\u540c\u65f6\u5b8c\u6210\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u7528\u56fe\u50cf\u884d\u751f\u76843D\u5b9a\u5411\u5305\u56f4\u76d2\u66ff\u4ee3\u4f20\u7edf\u57fa\u4e8e2D\u5173\u952e\u70b9\u7684\u5339\u914d\u5668\uff0c\u4ece\u800c\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\u5e76\u751f\u6210\u5168\u5c40\u8bed\u4e493D\u7269\u4f53\u5730\u56fe\u3002  \n\u25c6 \u5728\u5df2\u6709\u76f8\u673a\u4f4d\u59ff\u65f6\uff0c\u901a\u8fc7\u5168\u5c403D\u5305\u56f4\u76d2\u4f18\u5316\u663e\u8457\u63d0\u5347\u5730\u56fe\u8d28\u91cf\uff0c\u4f18\u4e8e\u4f9d\u8d56\u70b9\u4e91\u6216\u591a\u89c6\u56fe\u7684\u8fc7\u53c2\u6570\u5316\u65b9\u6cd5\u3002  \n\u25c6 \u5b9e\u73b0\u7a00\u758f\u5b9a\u4f4d\u4e0e\u53c2\u6570\u5316\u5efa\u56fe\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4ec5\u4e0e\u573a\u666f\u4e2d\u7269\u4f53\u6570\u91cf\u6210\u6b63\u6bd4\uff0c\u6548\u7387\u66f4\u9ad8\u3002  \n\u25c6 \u5728CA-1M\u548cScanNet++\u6570\u636e\u96c6\u4e0a\uff0cRfM\u7684\u5b9a\u4f4d\u6027\u80fd\u4e0e\u5730\u56fe\u8d28\u91cf\u5747\u8d85\u8d8a\u57fa\u4e8e\u70b9\u4e91\u548c\u5bc6\u96c6\u4f53\u7d20\u7684\u9886\u5148\u65b9\u6cd5\u3002  \n\u25c6 \u6269\u5c55Cubify Anything\u81f3\u5168\u573a\u666f\uff0c\u5efa\u7acb\u901a\u7528\u7684\u7269\u4f53\u4e2d\u5fc3\u5316\u8868\u5f81\uff0c\u4e3a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2505.22759": "|2025-05-30|FAMA: The First Large-Scale Open-Sc...|Sara Papi\u7b49|[2505.22759](http://arxiv.org/pdf/2505.22759)|[\u4ee3\u7801](https://github.com/hlt-mt/fbk-fairseq)|\u25c6FAMA\u662f\u9996\u4e2a\u9762\u5411\u82f1\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u5927\u89c4\u6a21\u5f00\u6e90\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff0c\u586b\u8865\u4e86\u8bed\u97f3\u9886\u57df\u5f00\u653e\u79d1\u5b66\u7684\u7a7a\u767d\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u4f7f\u752815\u4e07+\u5c0f\u65f6\u5f00\u6e90\u8bed\u97f3\u6570\u636e\u8bad\u7ec3\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b1.6\u4e07\u5c0f\u65f6\u6e05\u6d17\u548c\u4f2a\u6807\u6ce8\u6570\u636e\u7684\u65b0\u6570\u636e\u96c6\u3002 ...|\n",
    "2505.22098": "|**2025-05-28**|**UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images**|Junhuan Liu et.al.|[2505.22098](http://arxiv.org/abs/2505.22098)|null|\n",
    "2505.22089": "|**2025-05-28**|**Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule**|San Jiang et.al.|[2505.22089](http://arxiv.org/abs/2505.22089)|null|\n",
    "2505.21356": "|**2025-05-30**|**Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations**|Whenty Ariyanti et.al.|[2505.21356](http://arxiv.org/abs/2505.21356)|null|\n",
    "2505.20729": "|**2025-05-27**|**Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting**|Xiangyu Sun et.al.|[2505.20729](http://arxiv.org/abs/2505.20729)|null|\n",
    "2505.20477": "|**2025-05-26**|**Robust fine-tuning of speech recognition models via model merging: application to disordered speech**|Alexandre Ducorroy et.al.|[2505.20477](http://arxiv.org/abs/2505.20477)|null|\n",
    "2505.19854": "|**2025-05-29**|**Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud**|Natsuki Takama et.al.|[2505.19854](http://arxiv.org/abs/2505.19854)|null|\n",
    "2505.19264": "|**2025-05-25**|**Improving Novel view synthesis of 360$^\\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images**|Guangan Chen et.al.|[2505.19264](http://arxiv.org/abs/2505.19264)|**[link](https://github.com/angchen-dev/hemisparsegs)**|\n",
    "2505.18484": "|**2025-05-24**|**Token-Level Logits Matter: A Closer Look at Speech Foundation Models for Ambiguous Emotion Recognition**|Jule Valendo Halim et.al.|[2505.18484](http://arxiv.org/abs/2505.18484)|null|\n",
    "2506.04225": "|2025-06-04|Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation|Tianyu Huang\u7b49|[2506.04225](http://arxiv.org/pdf/2506.04225)|\u65e0|\u25c6 Voyager\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u7528\u6237\u81ea\u5b9a\u4e49\u76f8\u673a\u8def\u5f84\u4e0b\u76843D\u70b9\u4e91\u5e8f\u5217\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u4e16\u754c\u4e00\u81f4\u573a\u666f\u751f\u6210\uff0c\u65e0\u9700\u4f9d\u8d56\u4f20\u7edf3D\u91cd\u5efa\u6d41\u7a0b\u3002  \n\u25c6 \u8be5\u6846\u67b6\u9996\u6b21\u6574\u5408\u4e86RGB\u4e0e\u6df1\u5ea6\u89c6\u9891\u7684\u8054\u5408\u751f\u6210\uff0c\u901a\u8fc7\u73b0\u6709\u4e16\u754c\u89c2\u6d4b\u6761\u4ef6\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u5e26\u70b9\u4e91\u5254\u9664\u7684\u4e16\u754c\u7f13\u5b58\u673a\u5236\u548c\u81ea\u56de\u5f52\u63a8\u7406\u65b9\u6cd5\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8fed\u4ee3\u573a\u666f\u6269\u5c55\uff0c\u5b9e\u73b0\u8d85\u957f\u8ddd\u79bb\uff08>100\u7c73\uff09\u76843D\u573a\u666f\u63a2\u7d22\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u5f15\u64ce\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u6df1\u5ea6\u9884\u6d4b\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u663e\u8457\u964d\u4f4e\u6570\u636e\u83b7\u53d6\u6210\u672c\u3002  \n\u25c6 \u5728\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u865a\u62df\u73b0\u5b9e\u3001\u6e38\u620f\u5f00\u53d1\u7b49\u9700\u8981\u52a8\u60013D\u573a\u666f\u6784\u5efa\u7684\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u6574\u4f53\u67b6\u6784\u6452\u5f03\u4e86\u591a\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u6a21\u578b\u76f4\u63a5\u8f93\u51fa\u51e0\u4f55\u4e00\u81f4\u7684\u53ef\u63a2\u7d223D\u573a\u666f\uff0c\u4e3a\u751f\u6210\u5f0f3D\u5efa\u6a21\u5f00\u8f9f\u65b0\u65b9\u5411\u3002|\n",
    "2506.03667": "|2025-06-04|Accelerating SfM-based Pose Estimation with Dominating Set|Joji Joseph\u7b49|[2506.03667](http://arxiv.org/pdf/2506.03667)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u652f\u914d\u96c6\u7684\u9884\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u52a0\u901f\u57fa\u4e8eSfM\u7684\u4f4d\u59ff\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8eAR/VR\u548c\u673a\u5668\u4eba\u7b49\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u9996\u6b21\u5c06\u56fe\u8bba\u4e2d\u7684\u652f\u914d\u96c6\u6982\u5ff5\u5f15\u5165SfM\u6a21\u578b\u4f18\u5316\uff0c\u5728\u4e0d\u663e\u8457\u635f\u5931\u7cbe\u5ea6\u524d\u63d0\u4e0b\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u3002  \n\u25c6 \u5728OnePose\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u666e\u9002\u6027\uff0c\u517c\u5bb9\u591a\u79cdSfM\u4f4d\u59ff\u4f30\u8ba1\u6280\u672f\uff0c\u5c55\u73b0\u5e7f\u6cdb\u9002\u7528\u6027\u3002  \n\u25c6 \u5b9e\u73b01.5-14.48\u500d\u7684\u52a0\u901f\u6548\u679c\uff0c\u540c\u65f6\u5c06\u53c2\u8003\u56fe\u50cf\u6570\u91cf\u548c\u70b9\u4e91\u89c4\u6a21\u5206\u522b\u7f29\u51cf17-23\u500d\u548c2.27-4\u500d\u3002  \n\u25c6 \u901a\u8fc7\u5e73\u8861\u901f\u5ea6\u4e0e\u7cbe\u5ea6\uff0c\u4e3a\u5b9e\u65f63D\u4f4d\u59ff\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u73b0\u6709\u6280\u672f\u74f6\u9888\u3002|\n",
    "2506.03265": "|2025-06-03|Nearby dwarf galaxies with extreme star formation rates: a window into dwarf-galaxy evolution in the early Universe|S. Kaviraj\u7b49|[2506.03265](http://arxiv.org/pdf/2506.03265)|\u65e0|\u25c6 \u7814\u7a76\u53d1\u73b0\u9644\u8fd1\u4f4e\u5149\u5ea6\u77ee\u661f\u7cfb\uff08\u8d28\u91cf10^7-10^8\u592a\u9633\u8d28\u91cf\uff09\u5b58\u5728\u6781\u7aef\u6052\u661f\u5f62\u6210\u7387\uff080.1-3\u592a\u9633\u8d28\u91cf/\u5e74\uff09\uff0c\u53ef\u4f5c\u4e3a\u65e9\u671f\u5b87\u5b99\uff08z~5.5\uff09\u77ee\u661f\u7cfb\u7684\u7c7b\u6bd4\u6837\u672c\u3002  \n\u25c6 \u901a\u8fc7\u5bf9\u6bd4\u6b63\u5e38\u77ee\u661f\u7cfb\u6837\u672c\uff0c\u53d1\u73b0\u6781\u7aef\u6052\u661f\u5f62\u6210\u7387\u5e76\u975e\u7531\u661f\u7cfb\u7ed3\u6784\u7d27\u51d1\u6027\u6216\u7279\u6b8a\u73af\u5883\uff08\u5982\u9760\u8fd1\u8282\u70b9/\u7ea4\u7ef4\u7ed3\u6784\uff09\u9a71\u52a8\u3002  \n\u25c6 \u63ed\u793a\u5177\u6709\u6781\u7aef\u6052\u661f\u5f62\u6210\u7387\u7684\u77ee\u661f\u7cfb\u4e2d\u76f8\u4e92\u4f5c\u7528\u661f\u7cfb\u548c\u65e9\u578b\u5f62\u6001\u6bd4\u4f8b\u663e\u8457\u5347\u9ad8\uff08\u5206\u522b\u589e\u52a0\u7ea65.6\u500d\u548c9\u500d\uff09\uff0c\u8868\u660e\u661f\u7cfb\u76f8\u4e92\u4f5c\u7528\u662f\u5173\u952e\u89e6\u53d1\u673a\u5236\u3002  \n\u25c6 \u6307\u51fa\u5f53\u524d\u57fa\u4e8e\u4e2d\u4f4e\u7ea2\u79fb\u6570\u636e\u7684\u4e3b\u5e8f\u661f\u5f62\u6210\u7387\u6f14\u5316\u6a21\u578b\u4f1a\u4f4e\u4f30\u65e9\u671f\u5b87\u5b99\uff08z~5.5\uff09\u77ee\u661f\u7cfb\u7684\u6052\u661f\u5f62\u6210\u7387\u3002  \n\u25c6 \u63d0\u51fa\u65e9\u671f\u5b87\u5b99\u77ee\u661f\u7cfb\u901a\u8fc7\u66f4\u9ad8\u6c14\u4f53\u4e30\u5ea6\u4e0e\u9891\u7e41\u76f8\u4e92\u4f5c\u7528\u7684\u5171\u540c\u4f5c\u7528\uff0c\u9a71\u52a8\u5176\u6052\u661f\u8d28\u91cf\u5feb\u901f\u7d2f\u79ef\u7684\u65b0\u6f14\u5316\u56fe\u666f\u3002|\n",
    "2506.01940": "|2025-06-02|Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent|Yaroslava Lochman\u7b49|[2506.01940](http://arxiv.org/pdf/2506.01940)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u4e14\u9c81\u68d2\u7684\u5404\u5411\u5f02\u6027\u65cb\u8f6c\u5e73\u5747\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u5bb6\u65cf\uff0c\u7b80\u5316\u4e86\u539f\u6709\u548c\u5f26\u8ddd\u79bb\u4f18\u5316\u7684\u590d\u6742\u5f62\u5f0f\u3002  \n\u25c6 \u9996\u6b21\u5c06\u5404\u5411\u5f02\u6027\u6269\u5c55\u5e94\u7528\u4e8e\u5757\u5750\u6807\u4e0b\u964d\u6cd5\uff0c\u5f00\u53d1\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u5feb\u901f\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u5c06\u8be5\u6c42\u89e3\u5668\u96c6\u6210\u5230\u5927\u89c4\u6a21\u9c81\u68d2\u65cb\u8f6c\u5e73\u5747\u6d41\u7a0b\u4e2d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u95ee\u9898\u89c4\u6a21\u589e\u5927\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u7684\u7ed3\u6784\u8fd0\u52a8\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002  \n\u25c6 \u514b\u670d\u4e86\u4f20\u7edf\u5c40\u90e8\u65b9\u6cd5\u5bf9\u521d\u59cb\u5316\u7684\u654f\u611f\u6027\uff0c\u907f\u514d\u4e86\u6700\u5c0f\u751f\u6210\u6811\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u6f02\u79fb\u7d2f\u79ef\u548c\u5c40\u90e8\u6781\u5c0f\u503c\u9677\u9631\u95ee\u9898\u3002  \n\u25c6 \u5728\u5168\u5c40\u6700\u4f18\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5404\u5411\u5f02\u6027\u65cb\u8f6c\u5e73\u5747\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2505.24200": "|2025-06-03|Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC|Qingzheng Wang\u7b49|[2505.24200](http://arxiv.org/pdf/2505.24200)|\u65e0|\u25c6 \u63d0\u51fa\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff08\u51bb\u7ed3\u4e0a\u6e38\u8bad\u7ec3\u3001\u90e8\u5206\u5fae\u8c03\u3001\u4f4e\u79e9\u9002\u5e94\uff09\u4f18\u5316\u591a\u8bed\u8a00\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFM\uff09\u5728ML-SUPERB 2.0\u4e0a\u7684\u8868\u73b0\u3002  \n\u25c6 \u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u7f13\u89e3\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u8bed\u8a00\u8bc6\u522b\uff08LID\uff09\u611f\u77e5\u7684CTC\u635f\u5931\u51fd\u6570\u4f5c\u4e3a\u6b63\u5219\u5316\u624b\u6bb5\uff0c\u8054\u5408\u4f18\u5316LID\u548cASR\u4efb\u52a1\u3002  \n\u25c6 \u5728ML-SUPERB 2.0\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff1aLID\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u9ad814%\uff0cASR\u5b57\u9519\u8bef\u7387\uff08CER\uff09\u76f8\u5bf9\u964d\u4f4e30%\u3002  \n\u25c6 \u7efc\u5408\u65b9\u6cd5\u5728Interspeech 2025 ML-SUPERB 2.0\u6311\u6218\u8d5b\u4e2d\u65a9\u83b7\u7b2c\u4e8c\u540d\uff0c\u9a8c\u8bc1\u4e86\u7b56\u7565\u7684\u6709\u6548\u6027\u3002|\n",
    "2506.05935": "|2025-06-06|SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction|Yuchao Zheng\u7b49|[2506.05935](http://arxiv.org/pdf/2506.05935)|\u65e0|\u25c6\u63d0\u51faSurGSplat\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u51e0\u4f55\u7ea6\u675f\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u6280\u672f\uff0c\u89e3\u51b3\u5185\u7aa5\u955c\u573a\u666f\u7a00\u758f\u7279\u5f81\u548c\u5149\u7167\u4e0d\u5747\u5bfc\u81f4\u7684\u4f20\u7edfSfM\u65b9\u6cd5\u91cd\u5efa\u5931\u8d25\u95ee\u9898\u3002  \n\u25c6\u9996\u521b\u5c06\u51e0\u4f55\u7ea6\u675f\u878d\u51653DGS\u4f18\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8840\u7ba1\u7b49\u5173\u952e\u89e3\u5256\u7ed3\u6784\u7684\u9ad8\u7cbe\u5ea6\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u624b\u672f\u573a\u666f\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u3002  \n\u25c6\u5f00\u53d1\u6e10\u8fdb\u5f0f\u4f18\u5316\u6846\u67b6\uff0c\u9010\u6b65\u7ec6\u5316\u91cd\u5efa\u7ec6\u8282\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u624b\u672f\u73af\u5883\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u3002  \n\u25c6\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u65b0\u578b\u89c6\u89d2\u5408\u6210(NVS)\u548c\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u672f\u4e2d\u5bfc\u822a\u63d0\u4f9b\u9ad8\u4fdd\u771f\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6\u901a\u8fc7\u4e13\u5c5e\u51e0\u4f55\u7ea6\u675f\u673a\u5236\u6709\u6548\u514b\u670d\u5185\u7aa5\u955c\u56fe\u50cf\u7279\u5f81\u7a00\u758f\u7684\u56fa\u6709\u6311\u6218\uff0c\u4e3a\u5fae\u521b\u624b\u672f\u63d0\u4f9b\u66f4\u53ef\u9760\u76843D\u573a\u666f\u7406\u89e3\u652f\u6301\u3002  \n\u25c6\u5f00\u6e90\u9879\u76ee\u7f51\u7ad9\u63d0\u4f9b\u5b8c\u6574\u6280\u672f\u7ec6\u8282\u548c\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u63a8\u52a8\u624b\u672f\u5bfc\u822a\u9886\u57df\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002|\n",
    "2506.05558": "|2025-06-05|On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images|Andreas Meuleman\u7b49|[2506.05558](http://arxiv.org/pdf/2506.05558)|\u65e0|\u25c6 \u63d0\u51fa\u5b9e\u65f6\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u56fe\u50cf\u91c7\u96c6\u5b8c\u6210\u540e\u7acb\u5373\u751f\u6210\u76f8\u673a\u4f4d\u59ff\u548c\u8bad\u7ec3\u597d\u76843D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\uff0c\u663e\u8457\u7f29\u77ed\u4f20\u7edf\u65b9\u6cd5\u6240\u9700\u7684\u5206\u949f\u5230\u5c0f\u65f6\u7ea7\u8ba1\u7b97\u65f6\u95f4\u3002  \n\u25c6 \u9488\u5bf9\u5927\u573a\u666f\u548c\u5bbd\u57fa\u7ebf\u56fe\u50cf\u5e8f\u5217\uff0c\u8bbe\u8ba1\u4e86\u5feb\u901f\u521d\u59cb\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6848\uff0c\u7ed3\u5408\u5b66\u4e60\u7279\u5f81\u548cGPU\u53cb\u597d\u7684\u5c0f\u578b\u675f\u8c03\u6574\uff0c\u63d0\u5347\u5904\u7406\u6548\u7387\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u9ad8\u65af\u56fe\u5143\u4f4d\u7f6e\u4e0e\u5f62\u72b6\u7684\u76f4\u63a5\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u91cf\u5f0f\u751f\u6210\u56fe\u5143\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4f4d\u59ff\u4e0e\u9ad8\u65af\u56fe\u5143\u7684\u5feb\u901f\u8054\u5408\u4f18\u5316\u3002  \n\u25c6 \u63d0\u51fa\u53ef\u6269\u5c55\u7684\u8f90\u5c04\u573a\u6784\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u805a\u7c7b\u5c063DGS\u56fe\u5143\u5b58\u50a8\u5728\u951a\u70b9\u4e2d\u5e76\u4eceGPU\u5378\u8f7d\uff0c\u6709\u6548\u7ba1\u7406\u5927\u89c4\u6a21\u573a\u666f\u7684\u5185\u5b58\u9700\u6c42\u3002  \n\u25c6 \u5f15\u5165\u52a8\u6001\u56fe\u5143\u5408\u5e76\u673a\u5236\uff0c\u6839\u636e\u89c6\u70b9\u9700\u6c42\u81ea\u9002\u5e94\u8c03\u65743DGS\u89c4\u6a21\uff0c\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u80fd\u5b9e\u65f6\u5904\u7406\u591a\u79cd\u91c7\u96c6\u573a\u666f\u548c\u4e0d\u540c\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff0c\u5728\u901f\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u6216\u4e24\u8005\u517c\u5907\u65b9\u9762\u4f18\u4e8e\u4ec5\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u7684\u73b0\u6709\u65b9\u6cd5\u3002|\n",
    "2506.04803": "|2025-06-05|SupeRANSAC: One RANSAC to Rule Them All|Daniel Barath|[2506.04803](http://arxiv.org/pdf/2506.04803)|[\u4ee3\u7801](https://github.com/danini/superansac)|\u25c6 SupeRANSAC\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684RANSAC\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRANSAC\u5728\u4e0d\u540c\u89c6\u89c9\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u7cfb\u7edf\u5206\u6790RANSAC\u5728\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u5355\u5e94\u6027\u77e9\u9635\u3001\u57fa\u7840\u77e9\u9635\u3001\u4f4d\u59ff\u4f30\u8ba1\u7b49\uff09\u4e2d\u7684\u6709\u6548\u6280\u672f\uff0c\u4f18\u5316\u4e86\u6574\u4f53\u6d41\u7a0b\u3002  \n\u25c6 \u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0cSupeRANSAC\u5728\u57fa\u7840\u77e9\u9635\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u5347\u4e866\u4e2aAUC\u70b9\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002  \n\u25c6 \u8be5\u6846\u67b6\u514b\u670d\u4e86\u73b0\u6709\u5e93\uff08\u5982OpenCV\u548cPoseLib\uff09\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\u7684\u7f3a\u9677\uff0c\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u7684\u7a33\u5b9a\u9ad8\u6027\u80fd\u3002  \n\u25c6 \u8bba\u6587\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u4efb\u52a1\u7279\u5b9a\u4f18\u5316\uff0c\u4e3a\u9c81\u68d2\u4f30\u8ba1\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u4fbf\u4e8e\u793e\u533a\u9a8c\u8bc1\u548c\u5e94\u7528\uff0c\u5df2\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u95ee\u9898\u4e0a\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002|\n",
    "2506.09448": "|2025-06-11|OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary|Yui Sudo\u7b49|[2506.09448](http://arxiv.org/pdf/2506.09448)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e0a\u4e0b\u6587\u504f\u7f6e\uff08CB\uff09\u65b9\u6cd5\u4e0e\u9884\u8bad\u7ec3\u7684\u5f00\u653eWhisper\u98ce\u683c\u8bed\u97f3\u6a21\u578b\uff08OWSM v3.1\uff09\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u53c2\u6570\u3002  \n\u25c6 \u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFMs\uff09\u7684\u5d4c\u5165\u77e5\u8bc6\uff0c\u5373\u4f7f\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u4e5f\u80fd\u6709\u6548\u63d0\u5347\u7f55\u89c1\u8bcd\u548c\u672a\u767b\u5f55\u8bcd\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u5728\u4fdd\u6301SFMs\u539f\u6709\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u504f\u7f6e\u8bcd\u9519\u8bef\u7387\uff08B-WER\uff09\uff0c\u5728LibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\u63d0\u534711.6\u4e2a\u767e\u5206\u70b9\u3002  \n\u25c6 \u6574\u4f53\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u6539\u55840.9\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5b9e\u65f6\u56e0\u5b50\uff08RTF\uff09\u51cf\u5c117.5%\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u6548\u7387\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684CB\u65b9\u6cd5\uff0c\u51f8\u663e\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\u7684\u91cd\u8981\u6027\u3002|\n",
    "2506.18792": "|2025-06-23|ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs|Michal Nazarczuk\u7b49|[2506.18792](http://arxiv.org/pdf/2506.18792)|\u65e0|\u25c6 \u63d0\u51faViDAR\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u5f15\u5165\u5355\u76ee\u89c6\u9891\u76844D\u91cd\u5efa\u4efb\u52a1\uff0c\u901a\u8fc7\u751f\u6210\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\u89e3\u51b3\u5355\u76ee\u8f93\u5165\u7684\u7ed3\u6784-\u8fd0\u52a8\u6b67\u4e49\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u573a\u666f\u7279\u5b9a\u7279\u5f81\u8fdb\u884c\u6761\u4ef6\u6269\u6563\uff0c\u5728\u4fdd\u6301\u5916\u89c2\u7ec6\u8282\u7684\u540c\u65f6\u6709\u6548\u7f13\u89e3\u5355\u76ee\u6a21\u7cca\u6027\u5bfc\u81f4\u7684\u4f2a\u5f71\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u6269\u6563\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u4e13\u95e8\u5904\u7406\u6269\u6563\u751f\u6210\u89c6\u56fe\u7684\u65f6\u7a7a\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u5408\u6210\u89c6\u56fe\u4e0e\u771f\u5b9e\u51e0\u4f55\u7684\u5bf9\u9f50\u7cbe\u5ea6\u3002  \n\u25c6 \u63d0\u51fa\u76f8\u673a\u4f4d\u59ff\u4f18\u5316\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u5408\u6210\u89c6\u89d2\u4e0e\u5e95\u5c42\u573a\u666f\u51e0\u4f55\u7684\u5339\u914d\u5173\u7cfb\uff0c\u589e\u5f3a\u52a8\u6001\u533a\u57df\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002  \n\u25c6 \u5728\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u7684DyCheck\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8fd0\u52a8\u4e30\u5bcc\u533a\u57df\u91cd\u5efa\u8d28\u91cf\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002  \n\u25c6 \u53d1\u5e03\u65b0\u8bc4\u6d4b\u57fa\u51c6\uff0c\u9996\u6b21\u9488\u5bf9\u573a\u666f\u4e2d\u9ad8\u52a8\u6001\u90e8\u5206\u7684\u91cd\u5efa\u6027\u80fd\u8fdb\u884c\u7cfb\u7edf\u5316\u6bd4\u8f83\uff0c\u63a8\u52a8\u9886\u57df\u8bc4\u4f30\u6807\u51c6\u53d1\u5c55\u3002|\n",
    "2506.18376": "|2025-06-23|Room temperature spin injection into commercial VCSELs at non-resonant wavelengths|Timur Almabetov\u7b49|[2506.18376](http://arxiv.org/pdf/2506.18376)|\u65e0|\u25c6 \u9996\u6b21\u5728\u5ba4\u6e29\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u5546\u7528\u5782\u76f4\u8154\u9762\u53d1\u5c04\u6fc0\u5149\u5668\uff08VCSEL\uff09\u7684\u975e\u5171\u632f\u6ce2\u957f\u81ea\u65cb\u6ce8\u5165\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5171\u632f\u6ce2\u957f\u9650\u5236\u3002  \n\u25c6 \u901a\u8fc7794 nm\u548c810 nm\u5149\u6cf5\u6d66\u5b9e\u9a8c\uff0c\u89c2\u5bdf\u523020%\u548c5%\u7684\u6700\u5927\u5706\u504f\u632f\u5ea6\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u6ce2\u957f\u5bf9\u81ea\u65cb\u6ce8\u5165\u6548\u7387\u7684\u5f71\u54cd\u673a\u5236\u3002  \n\u25c6 \u7ed3\u5408\u91cf\u5b50\u9631\u5149\u5b66\u53d6\u5411\u7814\u7a76\uff0c\u8bc1\u5b9e\u957f\u6ce2\u957f\u6fc0\u53d1\u4f1a\u5bfc\u81f4\u81ea\u65cb\u6ce8\u5165\u6548\u7387\u964d\u4f4e\uff0c\u4e3a\u5668\u4ef6\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002  \n\u25c6 \u6269\u5c55\u81ea\u65cb\u7ffb\u8f6c\u6a21\u578b\uff08SFM\uff09\uff0c\u9996\u6b21\u7eb3\u5165\u5b9e\u9645\u6fc0\u53d1\u6761\u4ef6\uff0c\u4f7f\u7406\u8bba\u6a21\u578b\u80fd\u51c6\u786e\u590d\u73b0\u5b9e\u9a8c\u89c2\u6d4b\u8d8b\u52bf\u3002  \n\u25c6 \u8be5\u6210\u679c\u4e3a\u81ea\u65cb\u6fc0\u5149\u5668\u7684\u4f4e\u9608\u503c\u3001\u9ad8\u901f\u8c03\u5236\u548c\u5168\u5149\u6570\u636e\u5904\u7406\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u73b0\u8def\u5f84\u3002|\n",
    "2506.19491": "|2025-06-24|Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications|Gen\u00eds Castillo G\u00f3mez-Raya\u7b49|[2506.19491](http://arxiv.org/pdf/2506.19491)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u795e\u7ecf\u4e09\u7ef4\u91cd\u5efa\uff08N3DR\uff09\u6280\u672f\u4e0e\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u7ec6\u4e09\u7ef4\u6570\u5b57\u91cd\u5efa\u9759\u6001\u5c0f\u7269\u4f53\u3002  \n\u25c6 \u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u5957\u57fa\u4e8eN3DR\u7684\u6d41\u7a0b\uff0c\u6574\u5408\u4e86Instant-ngp\u3001Nerfacto\u548cSplatfacto\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6 \u901a\u8fc7\u591a\u65e0\u4eba\u673a\u534f\u540c\u91c7\u96c6\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u5c0f\u578b\u65e0\u4eba\u673a\u5728\u52a8\u6001\u98de\u884c\u548c\u529f\u8017\u9650\u5236\u4e0b\u7684\u81ea\u4e3b\u6027\u4e0e\u4efb\u52a1\u80fd\u529b\u95ee\u9898\u3002  \n\u25c6 \u91c7\u7528\u591a\u79cd\u56fe\u50cf\u548c\u70b9\u4e91\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e0e\u4f20\u7edf\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u7b97\u6cd5\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86N3DR\u7684\u4f18\u8d8a\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6848\u80fd\u652f\u6301\u9ad8\u7cbe\u5ea6\u4e09\u7ef4\u5efa\u6a21\u548c\u5f02\u5e38\u68c0\u6d4b\uff0c\u62d3\u5c55\u4e86\u5c0f\u578b\u65e0\u4eba\u673a\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002  \n\u25c6 \u6574\u4f53\u7814\u7a76\u5c55\u793a\u4e86N3DR\u6280\u672f\u5728\u63d0\u5347\u5fae\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\u80fd\u529b\u65b9\u9762\u7684\u5e7f\u9614\u524d\u666f\u3002|\n",
    "2506.21460": "|2025-07-08|Wild refitting for black box prediction|Martin J. Wainwright|[2506.21460](http://arxiv.org/pdf/2506.21460)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"wild refitting\"\u7684\u9ad8\u6548\u8ba1\u7b97\u6d41\u7a0b\uff0c\u4ec5\u9700\u5355\u6b21\u6570\u636e\u96c6\u548c\u9884\u6d4b\u65b9\u6cd5\u7684\u9ed1\u7bb1\u8bbf\u95ee\uff0c\u901a\u8fc7\u6b8b\u5dee\u8ba1\u7b97\u3001\u5bf9\u79f0\u5316\u548c\u7f29\u653e\u4e09\u4e2a\u6b65\u9aa4\uff0c\u4e3a\u60e9\u7f5a\u975e\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u5b9e\u4f8b\u7ea7\u5747\u65b9\u9884\u6d4b\u8bef\u5dee\u7684\u9ad8\u6982\u7387\u4e0a\u754c\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528Rademacher\u6b8b\u5dee\u5bf9\u79f0\u5316\u6280\u672f\uff08\u7c7b\u4f3cwild bootstrap\u53d8\u4f53\uff09\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u7f29\u653e\u56e0\u5b50\u03c1\u8c03\u6574\u6b8b\u5dee\uff0c\u6784\u5efa\u4ee5\u5f53\u524d\u4f30\u8ba1\u4e3a\u4e2d\u5fc3\u7684\u4fee\u6b63\u9884\u6d4b\u95ee\u9898\u3002  \n\u25c6 \u5728\u5141\u8bb8\u566a\u58f0\u5f02\u8d28\u6027\u7684\u8f83\u6e29\u548c\u6761\u4ef6\u4e0b\uff0c\u7406\u8bba\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u6027\u80fd\uff1a\u5f53wild\u566a\u58f0\u5c3a\u5ea6\u03c1\u9009\u62e9\u9002\u5f53\u65f6\uff0cwild refit\u80fd\u786e\u4fdd\u9884\u6d4b\u8bef\u5dee\u4e0a\u754c\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u5173\u952e\u8bbe\u8ba1\u6307\u5bfc\uff0c\u5305\u62ec\u6b8b\u5dee\u6784\u5efa\u65b9\u6cd5\u3001wild\u5b50\u95ee\u9898\u4e2d\u566a\u58f0\u7f29\u653e\u91cf\u7684\u9009\u62e9\u4f9d\u636e\uff0c\u4ee5\u53ca\u9ed1\u7bb1\u7a0b\u5e8f\u5c40\u90e8\u7a33\u5b9a\u6027\u7684\u5206\u6790\u6846\u67b6\u3002  \n\u25c6 \u5c55\u793a\u4e86\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u5982\u57fa\u4e8e\u7ed3\u6784\u5316\u77e9\u9635\u60e9\u7f5a\u7684\u975e\u521a\u6027\u8fd0\u52a8\u6062\u590d\u3001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5148\u9a8c\u7684\u5373\u63d2\u5373\u7528\u56fe\u50cf\u4fee\u590d\uff0c\u4ee5\u53ca\u6838\u65b9\u6cd5\u7684\u968f\u673a\u8349\u56fe\u6280\u672f\u3002|\n",
    "2506.22069": "|2025-06-27|Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras|Petr Hruby\u7b49|[2506.22069](http://arxiv.org/pdf/2506.22069)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u626b\u63cf\u7ebf\u6295\u5f71\u4ea4\u70b9\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u6eda\u52a8\u5feb\u95e8\u76f8\u673a\u95f4\u7684\u76f8\u5bf9\u4f4d\u59ff\uff0c\u65e0\u9700\u663e\u5f0f\u5efa\u6a21\u76f8\u673a\u8fd0\u52a8\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5b9e\u73b0\u4e86\u5355\u89c6\u56fe\u5185\u626b\u63cf\u7ebf\u7684\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u6269\u5c55\u4e86\u6eda\u52a8\u5feb\u95e8\u76f8\u673a\u7684\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u4f5c\u4e3a\u6eda\u52a8\u5feb\u95e8\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u7684\u57fa\u7840\u6a21\u5757\uff0c\u652f\u6301\u72ec\u7acb\u8ba1\u7b97\u6bcf\u6761\u626b\u63cf\u7ebf\u7684\u4f4d\u59ff\uff0c\u4e14\u65e0\u9700\u8fd0\u52a8\u6a21\u578b\u5047\u8bbe\u3002  \n\u25c6 \u5728\u5df2\u77e5\u5185\u53c2\u548c\u65e0\u955c\u5934\u7578\u53d8\u7684\u6761\u4ef6\u4e0b\uff0c\u5206\u7c7b\u4e86\u901a\u7528\u548c\u7279\u5b9a\u573a\u666f\uff08\u5982\u5e73\u884c\u7ebf\u548c\u5df2\u77e5\u91cd\u529b\u65b9\u5411\uff09\u7684\u6700\u5c0f\u6c42\u89e3\u5668\u3002  \n\u25c6 \u9488\u5bf9\u5e73\u884c\u7ebf\u573a\u666f\uff0c\u5f00\u53d1\u4e86\u5e26/\u4e0d\u5e26\u91cd\u529b\u5148\u9a8c\u7684\u6700\u5c0f\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5c06\u5176\u4e0e1D\u76f8\u673a\u76842D\u7ed3\u6784\u4f30\u8ba1\u95ee\u9898\u5173\u8054\u5b9e\u73b0\u521b\u65b0\u6c42\u89e3\u3002  \n\u25c6 \u5728Fastec\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7528\u4e8e\u6eda\u52a8\u5feb\u95e8SfM\u521d\u59cb\u5316\u7684\u53ef\u884c\u6027\uff0c\u5c55\u73b0\u4e86\u8fdb\u4e00\u6b65\u5f00\u53d1\u7684\u6f5c\u529b\u3002|\n",
    "2506.21629": "|2025-06-24|ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes|Chenhao Zhang\u7b49|[2506.21629](http://arxiv.org/pdf/2506.21629)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700SfM\u9884\u5904\u7406\u7684\u65b9\u6cd5ICP-3DGS\uff0c\u5c06\u8fed\u4ee3\u6700\u8fd1\u70b9\uff08ICP\uff09\u4e0e\u57fa\u4e8e\u4f18\u5316\u7684\u4f4d\u59ff\u7ec6\u5316\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u5927\u8303\u56f4\u65e0\u8fb9\u754c\u573a\u666f\u4e2d\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u96be\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06ICP\u5f15\u51653D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u5e45\u5ea6\u76f8\u673a\u8fd0\u52a8\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u4f30\u8ba1\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u795e\u7ecf\u6e32\u67d3\u5bf9SfM\u5148\u9a8c\u7684\u4f9d\u8d56\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4f53\u7d20\u7684\u573a\u666f\u81f4\u5bc6\u5316\u7b56\u7565\uff0c\u6709\u6548\u6307\u5bfc\u5927\u89c4\u6a21\u573a\u666f\u7684\u91cd\u5efa\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u573a\u666f\u8986\u76d6\u7387\u548c\u51e0\u4f55\u7ec6\u8282\u7684\u5b8c\u6574\u6027\u3002  \n\u25c6 \u5728\u5ba4\u5185\u5916\u591a\u79cd\u5c3a\u5ea6\u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0cICP-3DGS\u5728\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u5f00\u6e90\u4e86\u5b8c\u6574\u4ee3\u7801\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u65e0\u9884\u8ba1\u7b97\u4f4d\u59ff\u7684\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u7684\u53d1\u5c55\u3002|\n",
    "2506.23808": "|2025-06-30|Towards Initialization-free Calibrated Bundle Adjustment|Carl Olsson\u7b49|[2506.23808](http://arxiv.org/pdf/2506.23808)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u521d\u59cb\u5316\u7684\u6807\u5b9a\u675f\u8c03\u6574\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u521d\u59cb\u91cd\u5efa\u9636\u6bb5\u76f4\u63a5\u5229\u7528\u76f8\u673a\u6807\u5b9a\u4fe1\u606f\uff0c\u751f\u6210\u63a5\u8fd1\u5ea6\u91cf\u7cbe\u5ea6\u7684\u91cd\u5efa\u7ed3\u679c\uff08\u4ec5\u5dee\u4e00\u4e2a\u76f8\u4f3c\u53d8\u6362\uff09\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u5177\u6709\u6807\u5b9a\u4fe1\u606f\u7684\u6210\u5bf9\u76f8\u5bf9\u65cb\u8f6c\u4f30\u8ba1\uff0c\u8fd9\u4e9b\u65cb\u8f6c\u4f30\u8ba1\u4ec5\u5bf9\u76f8\u4f3c\u53d8\u6362\u4fdd\u6301\u4e0d\u53d8\uff0c\u4ece\u800c\u63a8\u52a8\u89e3\u4fdd\u6301\u771f\u5b9e\u573a\u666f\u7684\u5ea6\u91cf\u7279\u5f81\u3002  \n\u25c6 \u5c06\u65cb\u8f6c\u5e73\u5747\u6280\u672f\u6574\u5408\u5230\u4f2a\u7269\u4f53\u7a7a\u95f4\u8bef\u5dee\uff08pOSE\uff09\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u6807\u5b9a\u4fe1\u606f\u4e0e\u521d\u59cb\u5316\u65e0\u5173\u7684SfM\uff08\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff09\u6d41\u7a0b\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u4f18\u5316\u76ee\u6807\u51fd\u6570\uff0c\u5373\u4f7f\u4ece\u968f\u673a\u521d\u59cb\u89e3\u51fa\u53d1\u4e5f\u80fd\u9ad8\u6982\u7387\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\uff0c\u83b7\u5f97\u7cbe\u786e\u7684\u63a5\u8fd1\u5ea6\u91cf\u91cd\u5efa\u3002  \n\u25c6 \u76f8\u6bd4\u73b0\u6709\u57fa\u4e8epOSE\u7684\u65b9\u6cd5\uff08\u4ec5\u80fd\u83b7\u5f97\u5c04\u5f71\u53d8\u6362\u89e3\u4e14\u9700\u8981\u66f4\u591a\u6570\u636e\uff09\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6548\u7387\u3002|\n",
    "2506.23611": "|2025-06-30|AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention|Ziao Liu\u7b49|[2506.23611](http://arxiv.org/pdf/2506.23611)|\u65e0|\u25c6 \u63d0\u51faAttentionGS\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u9ad8\u8d28\u91cf\u521d\u59cb\u70b9\u4e91\u76843D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\uff0c\u7a81\u7834\u4f20\u7edf3DGS\u5bf9SfM\u70b9\u4e91\u7684\u5f3a\u4f9d\u8d56\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165\u51e0\u4f55\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u8bad\u7ec3\u521d\u671f\u5feb\u901f\u6062\u590d\u573a\u666f\u5168\u5c40\u7ed3\u6784\uff0c\u89e3\u51b3\u968f\u673a\u521d\u59cb\u5316\u5bfc\u81f4\u7684\u6536\u655b\u96be\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u7eb9\u7406\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5728\u8bad\u7ec3\u540e\u671f\u7cbe\u7ec6\u5316\u5c40\u90e8\u7ec6\u8282\uff0c\u663e\u8457\u63d0\u5347\u7eb9\u7406\u7f3a\u5931\u573a\u666f\u7684\u6e32\u67d3\u8d28\u91cf\u3002  \n\u25c6 \u5f00\u53d1\u4e0d\u900f\u660e\u5ea6\u52a0\u6743\u68af\u5ea6\u7b56\u7565\uff0c\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u81f4\u5bc6\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u8868\u9762\u91cd\u5efa\u3002  \n\u25c6 \u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4f4e\u7eb9\u7406/\u53d7\u9650\u89c6\u89d2\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u590d\u6742\u573a\u666f\u76843D\u91cd\u5efa\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u6269\u5c55\u4e863DGS\u6280\u672f\u7684\u9002\u7528\u8fb9\u754c\u3002|\n",
    "2507.03306": "|2025-07-04|MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion|Peilin Tao\u7b49|[2507.03306](http://arxiv.org/pdf/2507.03306)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u591a\u76f8\u673a\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5168\u5c40\u8fd0\u52a8\u5e73\u5747\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u65cb\u8f6c\u5e73\u5747\u548c\u6df7\u5408\u5e73\u79fb\u5e73\u5747\u6a21\u5757\u63d0\u5347\u4f20\u7edf\u5168\u5c40SfM\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002  \n\u25c6 \u91c7\u7528\u5206\u5c42\u7b56\u7565\u7684\u65cb\u8f6c\u5e73\u5747\u65b9\u6cd5\uff1a\u5148\u4f30\u8ba1\u521a\u6027\u76f8\u673a\u5355\u5143\u5185\u7684\u76f8\u5bf9\u65cb\u8f6c\uff0c\u518d\u8ba1\u7b97\u5168\u5c40\u521a\u6027\u5355\u5143\u65cb\u8f6c\uff0c\u4f18\u5316\u591a\u76f8\u673a\u7cfb\u7edf\u7684\u65cb\u8f6c\u4f30\u8ba1\u7cbe\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u878d\u5408\u76f8\u673a\u95f4\u7ea6\u675f\u548c\u76f8\u673a-\u70b9\u7ea6\u675f\u7684\u5e73\u79fb\u5e73\u5747\u6a21\u5757\uff0c\u901a\u8fc7\u51f8\u8ddd\u79bb\u76ee\u6807\u51fd\u6570\u521d\u59cb\u5316\u76f8\u673a\u4f4d\u59ff\u548c3D\u70b9\uff0c\u5e76\u91c7\u7528\u65e0\u504f\u975e\u53cc\u7ebf\u6027\u89d2\u5ea6\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u7ec6\u5316\u3002  \n\u25c6 \u5728\u4fdd\u6301\u4e0e\u589e\u91cf\u5f0fSfM\u76f8\u5f53\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u5168\u5c40SfM\u65b9\u6cd5\u3002  \n\u25c6 \u6846\u67b6\u5145\u5206\u5229\u7528\u591a\u76f8\u673a\u7cfb\u7edf\u7684\u56fa\u6709\u76f8\u5bf9\u4f4d\u59ff\u7ea6\u675f\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u73af\u5883\u611f\u77e5\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u5b9e\u65f6SfM\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u4fbf\u4e8e\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u5e94\u7528\u9a8c\u8bc1\uff0c\u63a8\u52a8\u591a\u76f8\u673aSfM\u6280\u672f\u7684\u5b9e\u9645\u90e8\u7f72\u3002|\n",
    "2507.08448": "|2025-07-11|Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT|Wei Zhang\u7b49|[2507.08448](http://arxiv.org/pdf/2507.08448)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u524d\u9988\u5f0f3D\u91cd\u5efa\u65b0\u8303\u5f0f\uff0c\u4ee5DUSt3R\u4e3a\u4ee3\u8868\uff0c\u901a\u8fc7\u5355\u4e00\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u4ece\u65e0\u7ea6\u675f\u56fe\u50cf\u4e2d\u8054\u5408\u63a8\u65ad\u76f8\u673a\u4f4d\u59ff\u548c\u7a20\u5bc6\u51e0\u4f55\u7ed3\u6784\uff0c\u98a0\u8986\u4e86\u4f20\u7edf\u8fed\u4ee3\u4f18\u5316\u6d41\u7a0b\u3002  \n\u25c6 \u91c7\u7528\u57fa\u4e8eTransformer\u7684\u5bf9\u5e94\u5173\u7cfb\u5efa\u6a21\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u8de8\u56fe\u50cf\u7684\u9ad8\u6548\u7279\u5f81\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7eb9\u7406\u7f3a\u5931\u7b49\u6311\u6218\u6027\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u8054\u5408\u4f4d\u59ff\u4e0e\u51e0\u4f55\u56de\u5f52\u673a\u5236\uff0c\u5c06\u4f20\u7edf\u591a\u9636\u6bb5\u6d41\u7a0b\uff08\u5982SfM+MVS\uff09\u6574\u5408\u4e3a\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u5927\u5e45\u7b80\u5316\u4e86\u5de5\u4f5c\u6d41\u7a0b\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002  \n\u25c6 \u7cfb\u7edf\u5206\u6790\u4e86\u4ece\u53cc\u89c6\u56fe\u5230\u591a\u89c6\u56fe\u7684\u6269\u5c55\u7b56\u7565\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u6280\u672f\u8def\u5f84\u3002  \n\u25c6 \u901a\u8fc7\u4e0e\u4f20\u7edf\u65b9\u6cd5\uff08\u5982SfM\uff09\u548c\u65e9\u671f\u5b66\u4e60\u578b\u65b9\u6cd5\uff08\u5982MVSNet\uff09\u7684\u5bf9\u6bd4\uff0c\u9610\u660e\u4e86\u8be5\u8303\u5f0f\u5728\u6548\u7387\u3001\u6cdb\u5316\u6027\u548c\u6613\u7528\u6027\u65b9\u9762\u7684\u7a81\u7834\u6027\u4f18\u52bf\u3002  \n\u25c6 \u63a2\u8ba8\u4e86\u52a8\u6001\u573a\u666f\u5904\u7406\u3001\u6a21\u578b\u7cbe\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\u7b49\u672a\u6765\u6311\u6218\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002|\n",
    "2507.10827": "|2025-07-20|Supporting SENCOTEN Language Documentation Efforts with Automatic Speech Recognition|Mengzhe Geng\u7b49|[2507.10827](http://arxiv.org/pdf/2507.10827)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u901a\u8fc7\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6280\u672f\u652f\u6301\u6fd2\u5371\u8bed\u8a00SEN\u0106OTEN\u7684\u6587\u6863\u5316\u5de5\u4f5c\uff0c\u5177\u4f53\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cdASR\u9a71\u52a8\u7684\u6587\u6863\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\u589e\u5f3a\u6709\u9650\u7684\u8bed\u8a00\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002  \n\u25c6 \u5229\u7528\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u501f\u52a9\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFMs\uff09\u63d0\u5347ASR\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u3002  \n\u25c6 \u5f15\u5165n-gram\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6d45\u5c42\u878d\u5408\u6216n-best\u6062\u590d\u6280\u672f\uff0c\u6700\u5927\u5316\u5229\u7528\u73b0\u6709\u8bcd\u6c47\u6570\u636e\u3002  \n\u25c6 \u5728SEN\u0106OTEN\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8619.34%\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u548c5.09%\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\uff0c\u7ecf\u8fc7\u8fc7\u6ee4\u540e\u5206\u522b\u63d0\u5347\u81f314.32%\u548c3.45%\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u7279\u522b\u9488\u5bf9SEN\u0106OTEN\u7684\u591a\u5408\u6210\u7ed3\u6784\u548c\u91cd\u97f3\u9a71\u52a8\u7684\u97f3\u4f4d\u53d8\u6362\u7b49\u8bed\u8a00\u7279\u70b9\uff0c\u4f18\u5316\u4e86ASR\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3002  \n\u25c6 \u4e3a\u6fd2\u5371\u8bed\u8a00\u7684\u6570\u5b57\u5316\u4fdd\u62a4\u548c\u6559\u5b66\u8d44\u6e90\u521b\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002|\n",
    "2507.12095": "|2025-07-16|BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images|Davide Di Nucci\u7b49|[2507.12095](http://arxiv.org/pdf/2507.12095)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u901a\u8fc7\u7a00\u758f\u89c6\u89d2\u8f93\u5165\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76843D\u8f66\u8f86\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u89c6\u89d2\u7684\u5c40\u9650\u6027\u3002  \n\n\u25c6\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u56fe\u548c\u9c81\u68d2\u4f4d\u59ff\u4f30\u8ba1\u67b6\u6784\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u8f93\u5165\u5408\u6210\u65b0\u89c6\u89d2\u5e76\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002  \n\u25c6\u6539\u8fdb\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5f15\u5165\u9009\u62e9\u6027\u5149\u5ea6\u635f\u5931\u51fd\u6570\uff0c\u4ec5\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u50cf\u7d20\u8fdb\u884c\u8ba1\u7b97\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6\u91c7\u7528DUSt3R\u67b6\u6784\u66ff\u4ee3\u4f20\u7edf\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002  \n\u25c6\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u516c\u5171\u4ea4\u901a\u5de5\u5177\u8f66\u8f86\u7684\u65b0\u6570\u636e\u96c6\uff0c\u652f\u6301\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u3002  \n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8f93\u5165\u6761\u4ef6\u53d7\u9650\u65f6\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002|\n",
    "2507.11893": "|2025-07-23|Spatial Frequency Modulation for Semantic Segmentation|Linwei Chen\u7b49|[2507.11893](http://arxiv.org/pdf/2507.11893)|\u65e0|\u25c6 \u63d0\u51fa\u7a7a\u95f4\u9891\u7387\u8c03\u5236\uff08SFM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0b\u91c7\u6837\u524d\u5c06\u9ad8\u9891\u7279\u5f81\u8c03\u5236\u5230\u4f4e\u9891\uff0c\u4e0a\u91c7\u6837\u65f6\u518d\u89e3\u8c03\u56de\u6765\uff0c\u6709\u6548\u89e3\u51b3\u8bed\u4e49\u5206\u5272\u4e2d\u9ad8\u9891\u4fe1\u606f\u56e0\u4e0b\u91c7\u6837\u5bfc\u81f4\u7684\u6df7\u53e0\u5931\u771f\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u81ea\u9002\u5e94\u91cd\u91c7\u6837\uff08ARS\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u5bc6\u96c6\u91c7\u6837\u9ad8\u9891\u533a\u57df\u6765\u7f29\u653e\u4fe1\u53f7\uff0c\u5229\u7528\u9891\u7387\u7f29\u653e\u7279\u6027\u964d\u4f4e\u9ad8\u9891\u6210\u5206\u7684\u9891\u7387\uff0c\u5b9e\u73b0\u9ad8\u6548\u8c03\u5236\u3002  \n\u25c6 \u63d0\u51fa\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e0a\u91c7\u6837\uff08MSAU\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u975e\u5747\u5300\u4e0a\u91c7\u6837\u89e3\u8c03\u7279\u5f81\u5e76\u6062\u590d\u9ad8\u9891\u4fe1\u606f\uff0c\u540c\u65f6\u5229\u7528\u591a\u5c3a\u5ea6\u5bc6\u96c6\u4e0e\u7a00\u758f\u91c7\u6837\u533a\u57df\u7684\u4ea4\u4e92\u589e\u5f3a\u5206\u5272\u7cbe\u5ea6\u3002  \n\u25c6 \u6a21\u5757\u8bbe\u8ba1\u8f7b\u91cf\u4e14\u901a\u7528\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230CNN\u548cTransformer\u7b49\u591a\u79cd\u67b6\u6784\u4e2d\uff0c\u6269\u5c55\u6027\u5f3a\u3002  \n\u25c6 \u901a\u8fc7\u7279\u5f81\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u6df7\u53e0\u5e76\u4fdd\u7559\u7ec6\u8282\uff0c\u8fdb\u4e00\u6b65\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u3001\u5b9e\u4f8b\u5206\u5272\u548c\u5168\u666f\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002|\n",
    "2507.12595": "|2025-07-16|Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models|Orchid Chetia Phukan\u7b49|[2507.12595](http://arxiv.org/pdf/2507.12595)|\u65e0|\u25c6 \u63d0\u51fa\u591a\u8bed\u8a00\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFMs\uff09\u5728\u60c5\u611f\u4f2a\u9020\u68c0\u6d4b\uff08EFD\uff09\u4e2d\u7684\u6709\u6548\u6027\u5047\u8bbe\uff0c\u8ba4\u4e3a\u5176\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u80fd\u66f4\u597d\u6355\u6349\u97f3\u9ad8\u3001\u97f3\u8c03\u548c\u5f3a\u5ea6\u7684\u7ec6\u5fae\u53d8\u5316\u3002  \n\u25c6 \u901a\u8fc7\u5168\u9762\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u8bed\u8a00SFMs\u5728\u540c\u8bed\u8a00\uff08\u57df\u5185\uff09\u548c\u8de8\u8bed\u8a00\uff08\u57df\u5916\uff09\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u63d0\u51faTHAMA\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408Tucker\u5206\u89e3\u548cHadamard\u4e58\u79ef\uff0c\u5b9e\u73b0\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u6548\u4e92\u8865\u878d\u5408\u3002  \n\u25c6 THAMA\u4e0e\u591a\u8bed\u8a00SFMs\u534f\u540c\u5408\u4f5c\uff0c\u5728\u57df\u5185\u548c\u57df\u5916\u8bc4\u6d4b\u4e2d\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u8d85\u8d8a\u5355\u4e00\u6a21\u578b\u3001\u57fa\u7ebf\u878d\u5408\u65b9\u6cd5\u548c\u5148\u524dSOTA\u65b9\u6cd5\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u591a\u8bed\u8a00SFMs\u5728EFD\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u8de8\u8bed\u8a00\u60c5\u611f\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2507.13486": "|2025-07-17|Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation|Debao Huang\u7b49|[2507.13486](http://arxiv.org/pdf/2507.13486)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u8bef\u5dee\u4f20\u64ad\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u822a\u7a7a\u548c\u65e0\u4eba\u673a\u6444\u5f71\u6d4b\u91cf\u4e2d\u4eceSfM\u5230MVS\u4e24\u9636\u6bb5\u7684\u5168\u6d41\u7a0b\u4e0d\u786e\u5b9a\u6027\uff0c\u586b\u8865\u4e86MVS\u9636\u6bb5\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u7684\u7814\u7a76\u7a7a\u767d\u3002\n\n\u25c6 \u9488\u5bf9MVS\u9636\u6bb5\u975e\u53ef\u5fae\u3001\u591a\u6a21\u6001\u7684\u7279\u6027\uff0c\u521b\u65b0\u6027\u5730\u63d0\u51fa\u57fa\u4e8e\u53ef\u9760\u591a\u89c6\u89d2\u70b9\uff08n\u22656\uff09\u7684\u81ea\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5339\u914d\u4ee3\u4ef7\u7b49\u5173\u952e\u7279\u5f81\u56de\u5f52\u89c6\u5dee\u4e0d\u786e\u5b9a\u6027\u3002\n\n\u25c6 \u8be5\u65b9\u6cd5\u76f4\u63a5\u4eceMVS\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u81ea\u5305\u542b\u7684\u53ef\u97603D\u70b9\uff0c\u5177\u6709\u81ea\u76d1\u7763\u7279\u6027\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002\n\n\u25c6 \u63d0\u51fa\u7684\u8bef\u5dee\u534f\u65b9\u5dee\u77e9\u9635\u5efa\u6a21\u65b9\u6cd5\u4e25\u683c\u9075\u5faa\u6444\u5f71\u6d4b\u91cf\u8bef\u5dee\u4f20\u64ad\u8def\u5f84\uff0c\u80fd\u9002\u5e94\u4e0d\u540c\u573a\u666f\u7684\u9c81\u68d2\u6027\u9a8c\u8bc1\u9700\u6c42\u3002\n\n\u25c6 \u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u9ad8\u8fb9\u754c\u8986\u76d6\u7387\u7684\u540c\u65f6\u907f\u514d\u4e86\u4e0d\u786e\u5b9a\u6027\u9ad8\u4f30\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\n\n\u25c6 \u9996\u6b21\u5b9e\u73b0\u4e86\u6444\u5f71\u6d4b\u91cf\u70b9\u4e91\u9010\u70b9\u7cbe\u5ea6\u51ed\u8bc1\u7684\u6807\u51c6\u5316\u8f93\u51fa\uff0c\u4e3a\u573a\u666f\u4f9d\u8d56\u7684\u6444\u5f71\u6d4b\u91cf\u7cbe\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u8ba4\u8bc1\u7684\u91cf\u5316\u5de5\u5177\u3002|\n",
    "2507.15683": "|2025-07-21|Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing|Boni Hu\u7b49|[2507.15683](http://arxiv.org/pdf/2507.15683)|\u65e0|\u25c6 \u63d0\u51faHi^2-GSLoc\u6846\u67b6\uff0c\u9996\u6b21\u5c063D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5f15\u5165\u9065\u611f\u89c6\u89c9\u91cd\u5b9a\u4f4d\u4efb\u52a1\uff0c\u5229\u7528\u5176\u7d27\u51d1\u7684\u51e0\u4f55\u4e0e\u5916\u89c2\u8868\u5f81\u80fd\u529b\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u77db\u76fe\u3002  \n\u25c6 \u8bbe\u8ba1\u53cc\u5c42\u6b21\u7a00\u758f\u5230\u7a20\u5bc6\u3001\u7c97\u5230\u7cbe\u7684\u5b9a\u4f4d\u8303\u5f0f\uff1a\u7a00\u758f\u9636\u6bb5\u901a\u8fc7\u6e32\u67d3\u611f\u77e5\u91c7\u6837\u548c\u5730\u6807\u5f15\u5bfc\u68c0\u6d4b\u5668\u83b7\u53d6\u9c81\u68d2\u521d\u59cb\u4f4d\u59ff\uff0c\u7a20\u5bc6\u9636\u6bb5\u901a\u8fc7\u591a\u7ea7\u6805\u683c\u5316\u5339\u914d\u8fed\u4ee3\u4f18\u5316\u4f4d\u59ff\u3002  \n\u25c6 \u5f00\u53d1\u5206\u533a\u9ad8\u65af\u8bad\u7ec3\u3001GPU\u5e76\u884c\u5339\u914d\u548c\u52a8\u6001\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u7a81\u7834\u5927\u5c3a\u5ea6\u9065\u611f\u573a\u666f\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\u9ad8\u6d77\u62d4\u53d8\u5316\u548c\u8de8\u57df\u6570\u636e\u3002  \n\u25c6 \u521b\u65b0\u6027\u63d0\u51fa\u9ad8\u65af\u57fa\u5143\u4e00\u81f4\u6027\u6e32\u67d3\u611f\u77e5\u91c7\u6837\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u9760\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u7279\u5f81\u5339\u914d\u7684\u7a33\u5b9a\u6027\u548c\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002  \n\u25c6 \u5728\u4eff\u771f\u6570\u636e\u3001\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u98de\u884c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u517c\u5177\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff08\u7ade\u4e89\u6027\u6307\u6807\uff09\u3001\u53ec\u56de\u7387\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.15308": "|2025-07-21|Few-Shot Object Detection via Spatial-Channel State Space Model|Zhimeng Xin\u7b49|[2507.15308](http://arxiv.org/pdf/2507.15308)|\u65e0|\u25c6 \u63d0\u51fa\u7a7a\u95f4-\u901a\u9053\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff08SCSM\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u548c\u901a\u9053\u5173\u7cfb\uff0c\u89e3\u51b3\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7279\u5f81\u63d0\u53d6\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u7a7a\u95f4\u7279\u5f81\u5efa\u6a21\uff08SFM\uff09\u6a21\u5757\uff0c\u5e73\u8861\u7a7a\u95f4\u5173\u7cfb\u548c\u901a\u9053\u5173\u7cfb\u7684\u5b66\u4e60\uff0c\u63d0\u5347\u7279\u5f81\u8868\u793a\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06Mamba\u6a21\u578b\u5f15\u5165\u901a\u9053\u5e8f\u5217\u5efa\u6a21\uff0c\u63d0\u51fa\u901a\u9053\u72b6\u6001\u5efa\u6a21\uff08CSM\uff09\u6a21\u5757\uff0c\u5229\u7528\u901a\u9053\u95f4\u76f8\u5173\u6027\u52a8\u6001\u8c03\u6574\u901a\u9053\u6743\u91cd\u3002  \n\u25c6 \u901a\u8fc7SCSM\u6a21\u5757\uff0c\u6a21\u578b\u80fd\u591f\u81ea\u52a8\u5f3a\u5316\u6709\u6548\u901a\u9053\u7279\u5f81\u5e76\u4fee\u6b63\u65e0\u6548\u901a\u9053\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u5347\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002  \n\u25c6 \u5728VOC\u548cCOCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|\n",
    "2507.14798": "|2025-07-20|An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks|Xinyi Wu\u7b49|[2507.14798](http://arxiv.org/pdf/2507.14798)|\u65e0|\u25c6 \u9996\u6b21\u5bf9DUSt3R/MASt3R/VGGT\u4e09\u79cd\u57fa\u4e8eTransformer\u76843D\u91cd\u5efa\u6a21\u578b\u5728\u822a\u6444\u5f71\u50cf\u5757\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002  \n\u25c6 \u8bc1\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u6781\u7a00\u758f\u5f71\u50cf\uff08\u5c11\u4e8e10\u5f20\u3001\u5206\u8fa8\u7387\u4f4e\u81f3518\u50cf\u7d20\uff09\u4e0b\u4ecd\u80fd\u751f\u6210\u5b8c\u6574\u7a20\u5bc6\u70b9\u4e91\uff0c\u76f8\u6bd4\u4f20\u7edfCOLMAP\u65b9\u6cd5\u5b8c\u6574\u6027\u63d0\u5347\u9ad8\u8fbe50%\u3002  \n\u25c6 \u53d1\u73b0VGGT\u6a21\u578b\u5177\u6709\u663e\u8457\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4f18\u52bf\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u80fd\u529b\u3002  \n\u25c6 \u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u8868\u73b0\u4e3a\u4f4d\u59ff\u4f30\u8ba1\u53ef\u9760\u6027\u968f\u5f71\u50cf\u6570\u91cf\u589e\u52a0\u800c\u4e0b\u964d\u3002  \n\u25c6 \u63d0\u51faTransformer\u6a21\u578b\u867d\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4f20\u7edfSfM/MVS\u6d41\u7a0b\uff0c\u4f46\u5728\u4f4e\u5206\u8fa8\u7387\u3001\u7a00\u758f\u5f71\u50cf\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u53ef\u4f5c\u4e3a\u6709\u6548\u8865\u5145\u65b9\u6848\u3002  \n\u25c6 \u4e3a\u822a\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u76843D\u91cd\u5efa\u65b0\u601d\u8def\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f71\u50cf\u91cd\u53e0\u7387\u6781\u4f4e\u6216\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u7684\u5feb\u901f\u91cd\u5efa\u9700\u6c42\u3002|\n",
    "2507.16406": "|2025-07-22|Sparse-View 3D Reconstruction: Recent Advances and Open Challenges|Tanveer Younis\u7b49|[2507.16406](http://arxiv.org/pdf/2507.16406)|\u65e0|\u25c6 \u8be5\u8bba\u6587\u9996\u6b21\u5c06\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u9886\u57df\u7684\u51e0\u4f55\u65b9\u6cd5\u3001\u795e\u7ecf\u9690\u5f0f\u6a21\u578b\uff08\u5982NeRF\uff09\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u7eb3\u5165\u7edf\u4e00\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002  \n\u25c6 \u91cd\u70b9\u5bf9\u6bd4\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u51e0\u4f55\u6b63\u5219\u5316\u3001\u663e\u5f0f\u5f62\u72b6\u5efa\u6a21\u548c\u751f\u6210\u63a8\u7406\u65b9\u9762\u7684\u521b\u65b0\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u89e3\u51b3\u7a00\u758f\u89c6\u89d2\u4e0b\u6d6e\u6e38\u4f2a\u5f71\u548c\u4f4d\u59ff\u6a21\u7cca\u95ee\u9898\u4e0a\u7684\u72ec\u7279\u4f18\u52bf\u3002  \n\u25c6 \u63d0\u51fa\u5f53\u524d\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9762\u4e34\u7684\u6838\u5fc3\u6743\u8861\uff1a\u91cd\u5efa\u7cbe\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u76f8\u4e92\u5236\u7ea6\u5173\u7cfb\u3002  \n\u25c6 \u533a\u522b\u4e8e\u4ee5\u5f80\u7efc\u8ff0\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u548c3D\u9ad8\u65af\u6cfc\u6e85\u7b49\u65b0\u5174\u6280\u672f\u5728\u7a00\u758f\u91cd\u5efa\u4e2d\u7684\u8fc1\u79fb\u5e94\u7528\u6f5c\u529b\u3002  \n\u25c6 \u660e\u786e\u6307\u51fa\u8be5\u9886\u57df\u5c1a\u672a\u89e3\u51b3\u7684\u4e24\u5927\u6311\u6218\uff1a\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u65e0\u4f4d\u59ff\u7ea6\u675f\u7684\u91cd\u5efa\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u53d1\u5c55\"3D\u539f\u751f\u751f\u6210\u5148\u9a8c\"\u7684\u672a\u6765\u65b9\u5411\u3002  \n\u25c6 \u901a\u8fc7\u6574\u5408\u5b9e\u65f6\u91cd\u5efa\u9700\u6c42\u4e0e\u65e0\u7ea6\u675f\u6761\u4ef6\u8bbe\u5b9a\uff0c\u4e3a\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u7684\u5de5\u4e1a\u843d\u5730\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u7ebf\u56fe\u3002|\n",
    "2507.20117": "|2025-07-27|RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters|Xiaolin Liu\u7b49|[2507.20117](http://arxiv.org/pdf/2507.20117)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f63D\u4eba\u7fa4\u758f\u6563\u6a21\u62df\u6846\u67b6RESCUE\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u611f\u77e5-\u51b3\u7b56-\u8fd0\u52a8\uff08SDM\uff09\u6d41\u7a0b\u6765\u63d0\u5347\u758f\u6563\u4eff\u771f\u7684\u771f\u5b9e\u6027\u548c\u52a8\u6001\u9002\u5e94\u6027\u3002  \n\n\u25c6 \u63d0\u51fa\u57fa\u4e8eSDM\u6d41\u7a0b\u7684\u4eff\u771f\u6846\u67b6\uff0c\u9996\u6b21\u5c063D\u81ea\u9002\u5e94\u793e\u4f1a\u529b\u6a21\u578b\uff08SFM\uff09\u51b3\u7b56\u673a\u5236\u4e0e\u4e2a\u6027\u5316\u6b65\u6001\u63a7\u5236\u8fd0\u52a8\u6a21\u5757\u7ed3\u5408\uff0c\u5b9e\u73b0\u66f4\u7b26\u5408\u4eba\u7c7b\u884c\u4e3a\u903b\u8f91\u7684\u758f\u6563\u6a21\u62df\u3002  \n\u25c6 \u5f15\u5165\u52a8\u6001\u7fa4\u4f53\u611f\u77e5\u673a\u5236\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u5e76\u884c\u8fd0\u52a8\uff0c\u80fd\u9002\u5e94\u4e0d\u540c\u5730\u5f62\u548c\u573a\u666f\u9700\u6c42\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6a21\u578b\u5bf9\u590d\u6742\u73af\u5883\u9002\u5e94\u6027\u7684\u9650\u5236\u3002  \n\u25c6 \u5f00\u53d1\u4e2a\u6027\u5316\u6b65\u6001\u63a7\u5236\u6a21\u5757\uff0c\u901a\u8fc7\u8003\u8651\u4e2a\u4f53\u4f53\u578b\u5dee\u5f02\u548c\u5730\u5f62\u5f71\u54cd\uff0c\u9996\u6b21\u5b9e\u73b0\u758f\u6563\u8fc7\u7a0b\u4e2d\u4e2a\u4f53\u8fd0\u52a8\u7279\u5f81\u7684\u5dee\u5f02\u5316\u6a21\u62df\u3002  \n\u25c6 \u521b\u65b0\u63d0\u51fa\u90e8\u4ef6\u7ea7\u53d7\u529b\u53ef\u89c6\u5316\u6280\u672f\uff0c\u4e3a\u758f\u6563\u5206\u6790\u63d0\u4f9b\u76f4\u89c2\u7684\u529b\u5b66\u4ea4\u4e92\u6570\u636e\u652f\u6301\uff0c\u8f85\u52a9\u5b89\u5168\u7b56\u7565\u4f18\u5316\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u652f\u6301\u52a8\u6001\u8def\u5f84\u89c4\u5212\u548c\u5b9e\u65f6\u884c\u4e3a\u8c03\u6574\uff0c\u5728\u5d0e\u5c96\u5730\u5f62\u4e2d\u4ecd\u80fd\u751f\u6210\u89c6\u89c9\u53ef\u4fe1\u3001\u7b26\u5408\u73b0\u5b9e\u7684\u758f\u6563\u52a8\u753b\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u5e76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u771f\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u4e3a\u516c\u5171\u5b89\u5168\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u4eff\u771f\u5206\u6790\u5de5\u5177\u3002|\n",
    "2508.01936": "|2025-08-03|CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes|Yaxuan Li\u7b49|[2508.01936](http://arxiv.org/pdf/2508.01936)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9ad8\u5ea6\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7cfb\u7edfCVD-SfM\uff0c\u4e13\u95e8\u9488\u5bf9\u7a00\u758f\u56fe\u50cf\u8f93\u5165\u4e0b\u4e0d\u540c\u9ad8\u5ea6\u573a\u666f\u7684\u9c81\u68d2\u7cbe\u51c6\u5b9a\u4f4d\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u8de8\u89c6\u89d2Transformer\u3001\u6df1\u5ea6\u7279\u5f81\u4e0e\u8fd0\u52a8\u6062\u590d\u7ed3\u6784(SfM)\u6280\u672f\u878d\u5408\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\uff0c\u6709\u6548\u5e94\u5bf9\u590d\u6742\u73af\u5883\u6761\u4ef6\u548c\u89c6\u89d2\u53d8\u5316\u3002  \n\u25c6 \u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e13\u95e8\u6536\u96c6\u5e76\u53d1\u5e03\u4e86\u4e24\u4e2a\u65b0\u7684\u591a\u9ad8\u5ea6\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u6570\u636e\u96c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u5e73\u53f0\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u7cfb\u7edf\u5728\u591a\u9ad8\u5ea6\u7a00\u758f\u4f4d\u59ff\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002  \n\u25c6 \u6240\u63d0\u6846\u67b6\u7279\u522b\u9002\u5408\u65e0\u4eba\u673a\u5bfc\u822a\u3001\u641c\u6551\u884c\u52a8\u3001\u81ea\u52a8\u5316\u68c0\u6d4b\u7b49\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u573a\u666f\uff0c\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2508.01063": "|2025-08-01|Counting topological interface modes using simplicial characteristic classes|N. Bohlsen\u7b49|[2508.01063](http://arxiv.org/pdf/2508.01063)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u6d41-\u5355\u6781\u5b50\u5bf9\u5e94\u5173\u7cfb\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5384\u7c73\u7cfb\u7edf\u4e2d\u62d3\u6251\u754c\u9762\u6a21\uff08TIMs\uff09\u7684\u6570\u91cf\u3002  \n\u25c6 \u901a\u8fc7\u8ba1\u7b97\u56f4\u7ed5\u5916\u5c14\u70b9\u7684\u76f8\u7a7a\u95f4\u7403\u4e0a\u5c40\u90e8\u6781\u5316\u5411\u91cf\u590d\u7ebf\u4e1b\u7684\u9648\u6570\uff0c\u786e\u5b9aTIMs\u6570\u91cf\uff0c\u521b\u65b0\u6027\u5730\u5c06\u62d3\u6251\u4e0d\u53d8\u91cf\u4e0e\u7269\u7406\u73b0\u8c61\u76f4\u63a5\u5173\u8054\u3002  \n\u25c6 \u91c7\u7528\u79bb\u6563\u5411\u91cf\u4e1b\u7684\u5355\u7eaf\u7b2c\u4e00\u9648\u7c7b\u6784\u9020\u65b9\u6cd5\u8ba1\u7b97\u9648\u6570\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u89c4\u8303\u4e0d\u53d8\u6027\u3001\u65e0\u9700\u5bfc\u6570\u8fd0\u7b97\u3001\u7ed3\u6784\u4fdd\u6301\u6027\u5f3a\u4e14\u6297\u566a\u58f0\u5e72\u6270\u7684\u7279\u70b9\u3002  \n\u25c6 \u7b97\u6cd5\u5728\u8d64\u9053\u6d41\u4f53\u6ce2\u548c\u62d3\u6251\u6717\u7f2a\u5c14\u56de\u65cb\u6ce2\u7684\u6848\u4f8b\u4e2d\u6210\u529f\u590d\u73b0\u4e86\u9884\u671f\u7684TIMs\u6570\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002  \n\u25c6 \u63a2\u7d22\u4e86\u8be5\u7b97\u6cd5\u5728\u5b9e\u9a8c\u6d4b\u91cf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u5408\u6210\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u4f53\u6ce2\u6781\u5316\u6570\u636e\u9884\u6d4bTIMs\u6570\u91cf\uff0c\u4e3a\u5b9e\u9a8c\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002|\n",
    "2508.01019": "|2025-08-01|3D Reconstruction via Incremental Structure From Motion|Muhammad Zeeshan\u7b49|[2508.01019](http://arxiv.org/pdf/2508.01019)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u91cf\u5f0f\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u7684\u8be6\u7ec6\u5b9e\u73b0\u65b9\u6cd5\uff0c\u76f8\u6bd4\u5168\u5c40SfM\u6280\u672f\u66f4\u5177\u7075\u6d3b\u6027\u3002  \n\u25c6 \u901a\u8fc7\u9010\u6b65\u52a0\u5165\u65b0\u89c6\u56fe\u7684\u65b9\u5f0f\uff0c\u80fd\u591f\u5728\u7a00\u758f\u6216\u90e8\u5206\u91cd\u53e0\u7684\u6570\u636e\u96c6\u4e2d\u6062\u590d\u573a\u666f\u7ed3\u6784\u548c\u76f8\u673a\u8fd0\u52a8\u3002  \n\u25c6 \u91cd\u70b9\u7814\u7a76\u4e86\u51e0\u4f55\u4f30\u8ba1\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u5149\u675f\u6cd5\u5e73\u5dee\uff08bundle adjustment\uff09\u5b9e\u73b0\u8fed\u4ee3\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002  \n\u25c6 \u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u91cd\u6295\u5f71\u8bef\u5dee\u548c\u76f8\u673a\u8f68\u8ff9\u4e00\u81f4\u6027\u8bc4\u4f30\u4e86\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6 \u8bc1\u660e\u4e86\u589e\u91cf\u5f0fSfM\u5728\u89c6\u89c9\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u662f\u4e00\u79cd\u53ef\u9760\u7684\u7a00\u758f3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002|\n",
    "2508.04410": "|2025-08-06|Bursting at the seams: the star-forming main sequence and its scatter at z=3-9 using NIRCam photometry from JADES|C. Simmonds\u7b49|[2508.04410](http://arxiv.org/pdf/2508.04410)|\u65e0|\u25c6 \u9996\u6b21\u5229\u7528JADES\u5de1\u5929\u7684NIRCam\u6d4b\u5149\u6570\u636e\u7cfb\u7edf\u7814\u7a76\u4e86\u7ea2\u79fbz=3-9\u8303\u56f4\u5185\u6052\u661f\u5f62\u6210\u4e3b\u5e8f(SFMS)\u53ca\u5176\u5f25\u6563\uff0c\u6837\u672c\u6052\u661f\u8d28\u91cf\u5b8c\u5907\u6027\u4e0b\u9650\u8fbelog(M\u22c6/M\u2299)\u22488.1\u3002  \n\u25c6 \u53d1\u73b010Myr\u65f6\u95f4\u5c3a\u5ea6\u4e0b\u7684SFMS\u6f14\u5316\u7b26\u5408sSFR\u221d(1+z)^2.30\u7684\u5173\u7cfb\uff0c\u4e0e\u6697\u7269\u8d28\u6655\u8d28\u91cf\u5438\u79ef\u7387\u7684\u7406\u8bba\u9884\u6d4b\u9ad8\u5ea6\u543b\u5408\u3002  \n\u25c6 \u63ed\u793a\u4e86SFMS\u5f52\u4e00\u5316\u968f\u6052\u661f\u5f62\u6210\u7387(SFR)\u5e73\u5747\u65f6\u95f4\u5c3a\u5ea6\u53d8\u5316\u7684\u590d\u6742\u89c4\u5f8b\uff0c\u53cd\u6620\u4e86\u7206\u53d1\u6027\u6052\u661f\u5f62\u6210\u4e0e\u4e0a\u5347\u578b\u6052\u661f\u5f62\u6210\u5386\u53f2\u7684\u7efc\u5408\u6548\u5e94\u3002  \n\u25c6 \u9996\u6b21\u5b9a\u91cf\u5206\u6790SFMS\u5f25\u6563\u968fSFR\u5e73\u5747\u65f6\u95f4\u5c3a\u5ea6\u7684\u6f14\u53d8\uff1a\u4ece10Myr\u76840.4-0.5dex\u964d\u81f3100Myr\u76840.2dex\uff0c\u8868\u660e\u77ed\u671f\u6ce2\u52a8\u4e3b\u5bfc\u5f25\u6563\uff0c\u4f46\u957f\u671f\u53d8\u5316\u4e5f\u5b58\u5728\u3002  \n\u25c6 \u53d1\u73b0\u4f4e\u8d28\u91cf\u661f\u7cfb\u4e2d\u7206\u53d1\u6027\u6052\u661f\u5f62\u6210\u5386\u53f2\u66f4\u663e\u8457\uff0c\u5e76\u6307\u51fa\u9700\u8981\u5f15\u5165\u521d\u59cb\u8d28\u91cf\u51fd\u6570\u504f\u91cd\u3001\u6052\u661f\u5f62\u6210\u6548\u7387\u63d0\u5347\u6216\u7206\u53d1\u6027\u589e\u5f3a\u7b49\u673a\u5236\u6765\u89e3\u91caz>10\u89c2\u6d4b\u5230\u7684UV\u4eae\u661f\u7cfb\u8fc7\u91cf\u73b0\u8c61\u3002  \n\u25c6 \u5f3a\u8c03\u5728\u62df\u5408SFMS\u65f6\uff08\u7279\u522b\u662f\u5bf9\u7206\u53d1\u6027\u6052\u661f\u5f62\u6210\u5386\u53f2\u7684\u661f\u7cfb\uff09\uff0c\u51c6\u786e\u786e\u5b9a\u6052\u661f\u8d28\u91cf\u5b8c\u5907\u6027\u9650\u7684\u91cd\u8981\u6027\u3002|\n",
    "2508.04236": "|2025-08-06|PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction|Muhua Zhu\u7b49|[2508.04236](http://arxiv.org/pdf/2508.04236)|\u65e0|\u25c6 \u63d0\u51faPIS3R\u65b9\u6cd5\uff0c\u9996\u6b21\u901a\u8fc7\u6df1\u5ea63D\u91cd\u5efa\u89e3\u51b3\u5927\u89c6\u5dee\u56fe\u50cf\u62fc\u63a5\u96be\u9898\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u5728\u663e\u8457\u89c6\u5dee\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u3002  \n\u25c6 \u91c7\u7528\u89c6\u89c9\u51e0\u4f55\u9a71\u52a8\u7684Transformer\u7f51\u7edc\uff0c\u4ece\u5927\u89c6\u5dee\u56fe\u50cf\u4e2d\u8054\u5408\u4f30\u8ba1\u76f8\u673a\u5185\u5916\u53c2\u6570\u5e76\u5b8c\u6210\u7a20\u5bc63D\u573a\u666f\u91cd\u5efa\uff0c\u5b9e\u73b0\u51e0\u4f55\u7cbe\u786e\u7684\u521d\u59cb\u5bf9\u9f50\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u91cd\u5efa\u76843D\u70b9\u4e91\u91cd\u6295\u5f71\u5230\u53c2\u8003\u89c6\u56fe\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u7cbe\u51c6\u914d\u51c6\uff0c\u4fdd\u7559\u6240\u6709\u50cf\u7d20\u57283D\u6444\u5f71\u6d4b\u91cf\u4e2d\u7684\u51e0\u4f55\u5b8c\u6574\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u70b9\u4e91\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u9488\u5bf9\u521d\u59cb\u62fc\u63a5\u53ef\u80fd\u5b58\u5728\u7684\u7a7a\u6d1e\u6216\u566a\u58f0\u8fdb\u884c\u7cbe\u7ec6\u5316\u4fee\u590d\uff0c\u751f\u6210\u65e0\u7f1d\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u89c6\u5dee\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8f93\u51fa\u7ed3\u679c\u53ef\u76f4\u63a5\u652f\u6301SfM\u7b49\u4e0b\u6e383D\u89c6\u89c9\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2508.05205": "|2025-08-07|EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery|Bingyu Yang\u7b49|[2508.05205](http://arxiv.org/pdf/2508.05205)|\u65e0|\u25c6\u63d0\u51faEndoMatcher\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u9886\u57df\u9884\u8bad\u7ec3\u7684\u5185\u7aa5\u955c\u56fe\u50cf\u901a\u7528\u5339\u914d\u5668\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u8de8\u57df\u6570\u636e\u89e3\u51b3\u5185\u7aa5\u955c\u56fe\u50cf\u5339\u914d\u6cdb\u5316\u6027\u96be\u9898\u3002  \n\u25c6\u9996\u521b\u53cc\u5206\u652f\u89c6\u89c9Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u53cc\u91cd\u4ea4\u4e92\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u5f31\u7eb9\u7406\u3001\u5927\u89c6\u89d2\u53d8\u5316\u7b49\u590d\u6742\u5185\u7aa5\u955c\u573a\u666f\u7684\u5339\u914d\u9c81\u68d2\u6027\u3002  \n\u25c6\u6784\u5efa\u9996\u4e2a\u591a\u9886\u57df\u5185\u7aa5\u955c\u5339\u914d\u6570\u636e\u96c6Endo-Mix6\uff0c\u5305\u542b6\u4e2a\u9886\u57df\u7ea6120\u4e07\u771f\u5b9e/\u5408\u6210\u56fe\u50cf\u5bf9\uff0c\u901a\u8fc7\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u548c\u6a21\u62df\u53d8\u6362\u751f\u6210\u6807\u6ce8\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u4e0e\u9886\u57df\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\u3002  \n\u25c6\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u591a\u76ee\u6807\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u5e94\u5bf9\u8de8\u57df\u6570\u636e\u89c4\u6a21\u5dee\u5f02\u3001\u5206\u5e03\u504f\u79fb\u548c\u8bef\u5dee\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e0d\u540c\u9886\u57df\u7684\u5747\u8861\u5b66\u4e60\u4e0e\u8868\u5f81\u4f18\u5316\u3002  \n\u25c6\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u8de8\u5668\u5b98\u548c\u6210\u50cf\u6761\u4ef6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4ee5140%-201%\u7684\u5339\u914d\u5185\u70b9\u63d0\u5347\u7387\u548c9.4%\u7684\u65b9\u5411\u9884\u6d4b\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002|\n",
    "2508.05187": "|2025-08-07|Refining Gaussian Splatting: A Volumetric Densification Approach|Mohamed Abdul Gafoor\u7b49|[2508.05187](http://arxiv.org/pdf/2508.05187)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u60ef\u6027\u4f53\u79ef\u7684\u65b0\u578b\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u51fd\u6570\u7684\u60ef\u6027\u4f53\u79ef\u6307\u5bfc3D\u9ad8\u65af\u5206\u5e03\u7684\u7cbe\u7ec6\u5316\u8fc7\u7a0b\uff0c\u514b\u670d\u539f\u59cb3DGS\u5bc6\u5ea6\u7b56\u7565\u7684\u7f3a\u9677\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u4f20\u7edfSfM\u4e0e\u6df1\u5ea6\u56fe\u50cf\u5339\u914d(DIM)\u4e24\u79cd\u70b9\u4e91\u521d\u59cb\u5316\u65b9\u6cd5\u5bf93DGS\u91cd\u5efa\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4e3a\u521d\u59cb\u5316\u9009\u62e9\u63d0\u4f9b\u4f9d\u636e\u3002  \n\u25c6 \u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863DGS\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002  \n\u25c6 \u6539\u8fdb\u4e86\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236(ADC)\u6d41\u7a0b\uff0c\u901a\u8fc7\u66f4\u667a\u80fd\u7684\u5bc6\u96c6\u5316\u548c\u4fee\u526a\u673a\u5236\u4f18\u5316\u70b9\u57fa\u5143\u7ba1\u7406\uff0c\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u3002  \n\u25c6 \u63d0\u51fa\u7684\u4f53\u79ef\u611f\u77e5\u7b56\u7565\u4e3a3D\u9ad8\u65af\u5206\u5e03\u7684\u5f62\u72b6\u548c\u7a7a\u95f4\u5206\u5e03\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u589e\u5f3a\u4e86\u573a\u666f\u8868\u793a\u7684\u51e0\u4f55\u51c6\u786e\u6027\u3002|\n",
    "2508.07355": "|2025-08-10|GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction|Qilin Zhang\u7b49|[2508.07355](http://arxiv.org/pdf/2508.07355)|\u65e0|\u25c6 \u63d0\u51faGS4Buildings\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e493D\u5efa\u7b51\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\uff0c\u589e\u5f3a\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u5728\u5927\u89c4\u6a21\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\u7684\u91cd\u5efa\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u906e\u6321\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u95ee\u9898\u3002  \n\u25c6 \u76f4\u63a5\u57fa\u4e8e\u4f4e\u7ec6\u8282\u5c42\u6b21\uff08LoD2\uff09\u8bed\u4e49\u5efa\u7b51\u6a21\u578b\u521d\u59cb\u5316\u9ad8\u65af\u5206\u5e03\uff0c\u66ff\u4ee3\u4f20\u7edf\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6d41\u7a0b\uff0c\u7b80\u5316\u91cd\u5efa\u6d41\u7a0b\u5e76\u63d0\u5347\u9c81\u68d2\u6027\u3002  \n\u25c6 \u901a\u8fc7\u5efa\u7b51\u51e0\u4f55\u751f\u6210\u5148\u9a8c\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\uff0c\u5e76\u5c06\u5176\u878d\u5165\u4f18\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u8868\u9762\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u51c6\u786e\u6027\u3002  \n\u25c6 \u5f15\u5165\u53ef\u9009\u5efa\u7b51\u805a\u7126\u6a21\u5f0f\uff0c\u4ec5\u91cd\u5efa\u5efa\u7b51\u533a\u57df\uff0c\u51cf\u5c1171.8%\u7684\u9ad8\u65af\u57fa\u5143\u6570\u91cf\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7d27\u51d1\u7684\u8868\u793a\u3002  \n\u25c6 \u5728\u57ce\u5e02\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u91cd\u5efa\u5b8c\u6574\u5ea6\u63d0\u534720.5%\uff0c\u51e0\u4f55\u7cbe\u5ea6\u63d0\u9ad832.8%\uff0c\u4e3a\u667a\u6167\u57ce\u5e02\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u5e94\u7528\u63d0\u4f9b\u65b0\u601d\u8def\u3002|\n",
    "2508.06968": "|2025-08-09|Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View|Ulas Gunes\u7b49|[2508.06968](http://arxiv.org/pdf/2508.06968)|\u65e0|\u25c6\u9996\u6b21\u5728\u771f\u5b9e\u8d85180\u5ea6\u9c7c\u773c\u56fe\u50cf\u4e0a\u8bc4\u4f30\u4e24\u79cd\u9c7c\u773c\u517c\u5bb9\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff08Fisheye-GS\u548c3DGUT\uff09\uff0c\u586b\u8865\u4e86\u6781\u7aef\u7578\u53d8\u573a\u666f\u4e0b\u7684\u6280\u672f\u7a7a\u767d\u3002  \n\u25c6\u901a\u8fc7\u5ba4\u5185\u5916200\u5ea6\u9c7c\u773c\u76f8\u673a\u5b9e\u6d4b\uff0c\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u89c6\u573a\u89d2\uff08200/160/120\u5ea6\uff09\u4e0b\u4e24\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u5e73\u8861\uff1aFisheye-GS\u5728160\u5ea6\u8868\u73b0\u6700\u4f73\uff0c\u800c3DGUT\u5728\u5168200\u5ea6\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5b9a\u9ad8\u8d28\u91cf\u3002  \n\u25c6\u63d0\u51fa\u57fa\u4e8eUniK3D\u9884\u6d4b\u7684\u6df1\u5ea6\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4ec5\u97002-3\u5f20\u9c7c\u773c\u56fe\u5373\u53ef\u751f\u6210\u7a20\u5bc6\u70b9\u4e91\uff0c\u514b\u670d\u4f20\u7edfSfM\u5728\u5f3a\u7578\u53d8\u573a\u666f\u5931\u6548\u7684\u95ee\u9898\u3002  \n\u25c6\u9a8c\u8bc1UniK3D\u5728\u672a\u8bad\u7ec3\u771f\u5b9e\u9c7c\u773c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u96fe\u973e\u3001\u7729\u5149\u3001\u5929\u7a7a\u7b49\u590d\u6742\u573a\u666f\u4ecd\u80fd\u5b9e\u73b0\u4e0eSfM\u76f8\u5f53\u76843D\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6\u7814\u7a76\u6210\u679c\u8bc1\u5b9e\u4e86\u9c7c\u773c3DGS\u65b9\u6cd5\u5728\u7a00\u758f\u9ad8\u7578\u53d8\u56fe\u50cf\u4e2d\u8fdb\u884c\u5e7f\u89d23D\u91cd\u5efa\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002|\n",
    "2508.12292": "|2025-08-17|HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization|Hyebin Ahn\u7b49|[2508.12292](http://arxiv.org/pdf/2508.12292)|\u65e0|\u25c6 \u63d0\u51faHuBERT-VIC\u6a21\u578b\uff0c\u901a\u8fc7\u65b9\u5dee-\u4e0d\u53d8\u6027-\u534f\u65b9\u5dee\u6b63\u5219\u5316\uff08VICReg\uff09\u63d0\u5347\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u9996\u6b21\u5c06VICReg\u76ee\u6807\u5e94\u7528\u4e8e\u8bed\u97f3\u9886\u57df\uff0c\u8c03\u6574\u566a\u58f0\u8bed\u97f3\u8868\u5f81\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u591a\u6837\u5316\u58f0\u5b66\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b\u3002  \n\u25c6 \u901a\u8fc7\u8054\u5408\u4f18\u5316\u65b9\u5dee\u3001\u4e0d\u53d8\u6027\u548c\u534f\u65b9\u5dee\u7ea6\u675f\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u3002  \n\u25c6 \u5728HuBERT\u6a21\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0cLibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\u76f8\u5bf9\u6027\u80fd\u63d0\u534723.3%\uff08clean\uff09\u548c13.2%\uff08other\uff09\u3002  \n\u25c6 \u4e3a\u89e3\u51b3\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u56e0\u4f9d\u8d56\u5e72\u51c0\u6570\u636e\u8bad\u7ec3\u5bfc\u81f4\u7684\u566a\u58f0\u573a\u666f\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2508.12255": "|2025-08-17|What do Speech Foundation Models Learn? Analysis and Applications|Ankita Pasad|[2508.12255](http://arxiv.org/pdf/2508.12255)|\u65e0|\u25c6 \u63d0\u51fa\u8f7b\u91cf\u7ea7\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u7edf\u8ba1\u5de5\u5177\u548c\u65e0\u8bad\u7ec3\u4efb\u52a1\uff0c\u7cfb\u7edf\u7814\u7a76\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08SFMs\uff09\u5404\u5c42\u7f16\u7801\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u5b66\u77e5\u8bc6\u3002  \n\u25c6 \u9996\u6b21\u5bf9\u591a\u79cdSFMs\u548c\u7edf\u8ba1\u5de5\u5177\u8fdb\u884c\u6a2a\u5411\u5bf9\u6bd4\u7814\u7a76\uff0c\u63ed\u793a\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u8868\u793a\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5173\u8054\u6027\u3002  \n\u25c6 \u9488\u5bf9\u53e3\u8bed\u7406\u89e3\uff08SLU\uff09\u9886\u57df\u6570\u636e\u532e\u4e4f\u95ee\u9898\uff0c\u521b\u65b0\u6027\u5730\u8d21\u732e\u4e86\u53e3\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u548c\u5b9a\u4f4d\uff08NEL\uff09\u4efb\u52a1\uff0c\u6269\u5145SLU\u8bc4\u6d4b\u57fa\u51c6\u3002  \n\u25c6 \u9a8c\u8bc1\u7aef\u5230\u7aefSFM\u6a21\u578b\u5728SLU\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u7ea7\u8054\u5f0f\uff08\u8bed\u97f3\u8bc6\u522b+\u6587\u672c\u6a21\u578b\uff09\u65b9\u6cd5\u3002  \n\u25c6 \u5168\u9762\u8bc4\u4f30\u4e0d\u540cSFMs\u53ca\u9002\u914d\u7b56\u7565\u5bf9SLU\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002  \n\u25c6 \u901a\u8fc7\u5de5\u5177\u94fe\u548c\u6570\u636e\u96c6\u7684\u53cc\u91cd\u521b\u65b0\uff0c\u63a8\u52a8\u793e\u533a\u5bf9SFMs\u7684\u673a\u7406\u7406\u89e3\u4e0e\u5b9e\u7528\u5316\u5f00\u53d1\u3002|\n",
    "2508.13831": "|2025-08-19|Smooth Flow Matching|Jianbin Tan\u7b49|[2508.13831](http://arxiv.org/pdf/2508.13831)|\u65e0|\u672c\u6587\u63d0\u51faSmooth Flow Matching\uff08SFM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u51fd\u6570\u578b\u6570\u636e\u7684\u751f\u6210\u5efa\u6a21\u95ee\u9898\u3002\u5176\u6838\u5fc3\u8d21\u732e\u4e0e\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u4e13\u4e3a\u51fd\u6570\u578b\u6570\u636e\u8bbe\u8ba1\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u5bf9\u9ad8\u65af\u6027\u6216\u4f4e\u79e9\u5047\u8bbe\u7684\u9650\u5236\uff0c\u6784\u5efa\u534a\u53c2\u6570Copula\u6d41\u751f\u6210\u65e0\u9650\u7ef4\u975e\u9ad8\u65af\u51fd\u6570\u6570\u636e\u3002\n\u25c6 \u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u80fd\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u89c2\u6d4b\uff0c\u76f4\u63a5\u4fdd\u8bc1\u751f\u6210\u51fd\u6570\u7684\u5e73\u6ed1\u6027\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u51fd\u6570\u6570\u636e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u5c40\u9650\u3002\n\u25c6 \u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SFM\u5728\u5408\u6210\u6570\u636e\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u5728MIMIC-IV\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e0a\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u4e34\u5e8a\u8f68\u8ff9\u66ff\u4ee3\u6570\u636e\u3002\n\u25c6 \u4e3a\u9690\u79c1\u654f\u611f\u573a\u666f\u4e0b\u7684\u7edf\u8ba1\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7b49\u51fd\u6570\u578b\u6570\u636e\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6548\u7528\u4ef7\u503c\u3002|\n",
    "2508.14682": "|2025-08-20|GeMS: Efficient Gaussian Splatting for Extreme Motion Blur|Gopi Raju Matta\u7b49|[2508.14682](http://arxiv.org/pdf/2508.14682)|\u65e0|GeMS\u662f\u9996\u4e2a\u76f4\u63a5\u4ece\u6781\u7aef\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u8fdb\u884c3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u91cd\u5efa\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u4efb\u4f55\u6e05\u6670\u56fe\u50cf\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u63d0\u51faVGGSfM\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u6a21\u7cca\u8f93\u5165\u4e2d\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\u5e76\u751f\u6210\u70b9\u4e91\u3002\n\u25c6 \u5f15\u51653DGS-MCMC\u65b9\u6cd5\uff0c\u5c06\u9ad8\u65af\u5206\u5e03\u89c6\u4e3a\u6982\u7387\u5206\u5e03\u6837\u672c\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u53d6\u4ee3\u4e86\u4f9d\u8d56\u542f\u53d1\u5f0f densification \u548c pruning \u7684\u4f20\u7edf\u7b56\u7565\u3002\n\u25c6 \u8054\u5408\u4f18\u5316\u76f8\u673a\u8f68\u8ff9\u4e0e\u9ad8\u65af\u53c2\u6570\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u3002\n\u25c6 \u8fdb\u4e00\u6b65\u63d0\u51faGeMS-E\u53d8\u4f53\uff0c\u96c6\u6210\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\uff0c\u901a\u8fc7\u4e8b\u4ef6\u53cc\u79ef\u5206\u53bb\u6a21\u7cca\uff08EDI\uff09\u751f\u6210\u66f4\u6e05\u6670\u56fe\u50cf\u4ee5\u4f18\u5316\u521d\u59cb\u4f30\u8ba1\u3002\n\u8be5\u6846\u67b6\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4e25\u91cd\u6a21\u7cca\u4e0b3DGS\u91cd\u5efa\u7684\u6839\u672c\u6027\u6311\u6218\u3002|\n",
    "2508.15457": "|2025-08-21|Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework|Zongqi He\u7b49|[2508.15457](http://arxiv.org/pdf/2508.15457)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u6781\u7aef\u7a00\u758f\u89c6\u89d2\uff08\u5982\u4ec52\u4e2a\u8bad\u7ec3\u89c6\u56fe\uff09\u4e0b\u56e0\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u521d\u59cb\u5316\u5931\u8d25\u5bfc\u81f4\u7684\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700SfM\u7684\u65b0\u578b\u6846\u67b6\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u63d0\u51fa\u5bc6\u96c6\u7acb\u4f53\u6a21\u5757\u66ff\u4ee3SfM\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u5168\u5c40\u7a20\u5bc6\u70b9\u4e91\u91cd\u5efa\u5b9e\u73b0\u521d\u59cb\u5316\u3002\n\u25c6 \u8bbe\u8ba1\u8fde\u8d2f\u89c6\u56fe\u63d2\u503c\u6a21\u5757\uff0c\u901a\u8fc7\u63d2\u503c\u76f8\u673a\u59ff\u6001\u5e76\u751f\u6210\u89c6\u89d2\u4e00\u81f4\u7684\u5185\u5bb9\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff0c\u7f13\u89e3\u7a00\u758f\u8f93\u5165\u7684\u4fe1\u606f\u7a00\u7f3a\u95ee\u9898\u3002\n\u25c6 \u5f15\u5165\u591a\u5c3a\u5ea6\u62c9\u666e\u62c9\u65af\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u7a7a\u95f4\u611f\u77e5\u591a\u5c3a\u5ea6\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u51e0\u4f55\u7ed3\u6784\u548c\u6e32\u67d3\u5185\u5bb9\u7684\u8d28\u91cf\u3002\n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6781\u7aef\u7a00\u758f\u6761\u4ef6\u4e0bPSNR\u6307\u6807\u63d0\u53472.75dB\uff0c\u5408\u6210\u56fe\u50cf\u7578\u53d8\u5c0f\u4e14\u9ad8\u9891\u7ec6\u8282\u4e30\u5bcc\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002|\n",
    "2508.16465": "|2025-08-25|HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images|Anilkumar Swamy\u7b49|[2508.16465](http://arxiv.org/pdf/2508.16465)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHOSt3R\u7684\u3001\u65e0\u9700\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u624b-\u7269\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u6446\u8131\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u5173\u952e\u70b9\u68c0\u6d4b\u6280\u672f\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff08keypoint-free\uff09\u7684\u9c81\u68d2\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u5355\u76ee\u8fd0\u52a8\u89c6\u9891\u4e2d\u4f30\u8ba1\u624b\u548c\u7269\u4f53\u7684\u4e09\u7ef4\u53d8\u6362\u3002\n\u25c6 \u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8eSfM\u548c\u624b\u90e8\u5173\u952e\u70b9\u4f18\u5316\u7684\u65b9\u6cd5\u5728\u7269\u4f53\u51e0\u4f55\u591a\u6837\u3001\u7eb9\u7406\u5f31\u4ee5\u53ca\u624b\u7269\u4e25\u91cd\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u5931\u6548\u95ee\u9898\u3002\n\u25c6 \u5c06\u6240\u63d0\u7684\u53d8\u6362\u4f30\u8ba1\u65b9\u6cd5\u4e0e\u4e00\u4e2a\u591a\u89c6\u56fe\u91cd\u5efa\u6d41\u7a0b\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u624b-\u7269\u4e09\u7ef4\u5f62\u72b6\u6062\u590d\u3002\n\u25c6 \u8be5\u65b9\u6cd5\u4e0d\u53d7\u7ea6\u675f\uff0c\u4e0d\u4f9d\u8d56\u9884\u5148\u626b\u63cf\u7684\u7269\u4f53\u6a21\u677f\u6216\u76f8\u673a\u5185\u53c2\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u901a\u7528\u6027\u548c\u975e\u4fb5\u5165\u5f0f\u5e94\u7528\u6f5c\u529b\u3002\n\u25c6 \u5728SHOWMe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728HO3D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5bf9\u672a\u89c1\u7269\u4f53\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2508.16026": "|2025-08-22|NeuralMeshing: Complete Object Mesh Extraction from Casual Captures|Floris Erich\u7b49|[2508.16026](http://arxiv.org/pdf/2508.16026)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NeuralMeshing\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u65e5\u5e38\u62cd\u6444\u7684\u591a\u6bb5\u89c6\u9891\u4e2d\u81ea\u52a8\u91cd\u5efa\u7269\u4f53\u7684\u5b8c\u6574\u7f51\u683c\u6a21\u578b\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5982\u4e0b\uff1a\n\u25c6 \u4ec5\u9700\u901a\u8fc7\u666e\u901a\u89c6\u9891\u800c\u975e\u4e13\u4e1a\u626b\u63cf\u8bbe\u5907\u5373\u53ef\u5b9e\u73b0\u5b8c\u6574\u7269\u4f53\u5efa\u6a21\uff0c\u5927\u5e45\u964d\u4f4e\u786c\u4ef6\u95e8\u69db\u3002\n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u591a\u89c6\u9891\u878d\u5408\u7684\u5b8c\u6574\u91cd\u5efa\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u540e\u5904\u7406\u7684\u5b54\u6d1e\u586b\u5145\u6280\u672f\u3002\n\u25c6 \u91c7\u7528\u6700\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u6807\u6ce8\u65b9\u5f0f\uff08\u4ec5\u9700\u5728\u6bcf\u6bb5\u89c6\u9891\u4e2d\u6307\u5b9a\u4e00\u4e2a\u5df2\u77e5\u70b9\uff09\uff0c\u652f\u6301\u901a\u8fc7\u6807\u5b9a\u677f\u6216AR\u6807\u8bb0\u5b9e\u73b0\u81ea\u52a8\u5316\u3002\n\u25c6 \u7ed3\u5408\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6280\u672f\u5b9e\u73b0\u81ea\u52a8\u5e27\u5b9a\u4f4d\u4e0e\u4e09\u7ef4\u91cd\u5efa\u3002\n\u25c6 \u5f00\u6e90\u7cfb\u7edf\u4ee3\u7801\uff0c\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u53d1\u5c55\u3002|\n",
    "2508.17972": "|2025-08-25|SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization|Junyuan Deng\u7b49|[2508.17972](http://arxiv.org/pdf/2508.17972)|\u65e0|SAIL-Recon\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u7684\u524d\u9988Transformer\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u573a\u666f\u56de\u5f52\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5927\u91cf\u8f93\u5165\u56fe\u50cf\u7684\u95ee\u9898\u3002\n\n\u25c6 \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u878d\u5165\u573a\u666f\u56de\u5f52\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u8303\u5f0f\u7684\u4f18\u52bf\u6765\u63d0\u5347\u5904\u7406\u5927\u89c4\u6a21\u573a\u666f\u7684\u80fd\u529b\u3002\n\u25c6 \u65b9\u6cd5\u9996\u5148\u4ece\u951a\u56fe\u50cf\u5b50\u96c6\u8ba1\u7b97\u51fa\u4e00\u4e2a\u795e\u7ecf\u573a\u666f\u8868\u793a\uff0c\u7136\u540e\u57fa\u4e8e\u6b64\u8868\u793a\u5bf9\u56de\u5f52\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u91cd\u5efa\u6240\u6709\u8f93\u5165\u56fe\u50cf\u3002\n\u25c6 \u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u9ad8\u6548\u6269\u5c55\u5230\u5927\u89c4\u6a21\u573a\u666f\uff0c\u8fd8\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u65b0\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\n\u25c6 \u5728\u591a\u4e2a\u6743\u5a01\u6570\u636e\u96c6\uff08\u5982TUM-RGBD\u3001CO3Dv2\u548cTanks & Temples\uff09\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002|\n",
    "2509.01873": "|2025-09-02|Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction|Xueyang Kang|[2509.01873](http://arxiv.org/pdf/2509.01873)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u5f00\u53d1\u4e86\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4e09\u7ef4\u89c6\u89c9\u4e2d\u7684\u5173\u952e\u4efb\u52a1\u3002  \n\u25c6 \u9488\u5bf9\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u3001\u70b9\u4e91\u914d\u51c6\u3001\u6df1\u5ea6\u9884\u6d4b\u53ca\u4e09\u7ef4\u91cd\u5efa\u7b49\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u5b9a\u5236\u5316\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002  \n\u25c6 \u901a\u8fc7\u5f15\u5165\u6df1\u5ea6\u4fe1\u606f\u3001\u8868\u9762\u6cd5\u7ebf\u548c\u7b49\u53d8\u6027\u7ea6\u675f\u7b49\u51e0\u4f55\u5148\u9a8c\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u793a\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u3002  \n\u25c6 \u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\uff08\u5982SfM\u548cSLAM\uff09\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7279\u5f81\u6a21\u7cca\u548c\u51e0\u4f55\u7ec6\u8282\u7f3a\u5931\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u5728\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u548cVR/AR\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63a8\u52a8\u4e86\u4e09\u7ef4\u6620\u5c04\u4e0e\u573a\u666f\u91cd\u5efa\u6280\u672f\u7684\u53d1\u5c55\u3002|\n",
    "2509.06685": "|2025-09-10|VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes|Shengkai Zhang\u7b49|[2509.06685](http://arxiv.org/pdf/2509.06685)|\u65e0|VIM-GS\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u573a\u666f\u65b0\u9896\u89c6\u56fe\u5408\u6210\u7684\u89c6\u89c9-\u60ef\u6027\u5355\u76ee\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u3002\u5176\u6838\u5fc3\u8d21\u732e\u662f\u89e3\u51b3\u4e86\u5355\u76ee\u56fe\u50cf\u56e0\u7f3a\u4e4f\u51c6\u786e\u6df1\u5ea6\u4fe1\u606f\u800c\u5bfc\u81f4\u6e32\u67d3\u8d28\u91cf\u5dee\u7684\u95ee\u9898\u3002\u5177\u4f53\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u751f\u6210\u6846\u67b6\uff0c\u5de7\u5999\u5730\u878d\u5408\u4e86\u6765\u81ea\u89c6\u89c9-\u60ef\u6027SLAM\u7684\u7a00\u758f\u4f46\u7cbe\u786e\u7684\u6df1\u5ea6\uff0c\u4e0e\u6765\u81ea\u5927\u57fa\u7840\u6a21\u578b\u7684\u7a20\u5bc6\u4f46\u7c97\u7cd9\u7684\u6df1\u5ea6\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u5206\u5272\u7684\u6df1\u5ea6\u4f20\u64ad\u7b97\u6cd5\uff0c\u901a\u8fc7\u6e32\u67d3\u7ed3\u6784\u5316\u7269\u4f53\u7684\u50cf\u7d20\u6df1\u5ea6\uff0c\u6709\u6548\u5f25\u5408\u4e86\u7a00\u758f\u8f93\u5165\u4e0e\u7a20\u5bc6\u8f93\u51fa\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002\n\u25c6 \u5f00\u53d1\u4e86\u4e00\u4e2a\u52a8\u6001\u6df1\u5ea6\u4f18\u5316\u6a21\u5757\uff0c\u4e13\u95e8\u5904\u7406\u52a8\u6001\u7269\u4f53\u4e0a\u4e0d\u5b8c\u6574\u7684SLAM\u6df1\u5ea6\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u7c97\u7cd9\u7684\u5927\u6a21\u578b\u6df1\u5ea6\u4f30\u8ba1\u3002\n\u6700\u7ec8\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u4e14\u51c6\u786e\u7684\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u4ece\u800c\u652f\u6301\u4e86\u9ad8\u65af\u6cfc\u6e85\u5728\u5927\u573a\u666f\u4e2d\u7684\u9ad8\u6e05\u6e32\u67d3\uff0c\u5728\u516c\u5f00\u548c\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u5747\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6e32\u67d3\u8d28\u91cf\u3002|\n",
    "2509.09720": "|2025-09-09|Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision|Akansel Cosgun\u7b49|[2509.09720](http://arxiv.org/pdf/2509.09720)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63a8\u51fa\u4e86\u6fb3\u5927\u5229\u4e9a\u8d85\u5e02\u7269\u4f53\u6570\u636e\u96c6\uff08ASOS\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u8bbe\u8ba1\u7684\u5b9e\u7269\u4e0e3D\u6a21\u578b\u57fa\u51c6\u6570\u636e\u96c6\u3002\n\n\u25c6 \u63d0\u4f9b\u4e86\u5305\u542b50\u79cd\u5e38\u89c1\u8d85\u5e02\u5546\u54c1\u7684\u9ad8\u8d28\u91cf3D\u7eb9\u7406\u7f51\u683c\u6570\u636e\u96c6\uff0c\u6240\u6709\u7269\u54c1\u5747\u53ef\u4ece\u6fb3\u5927\u5229\u4e9a\u5927\u578b\u8fde\u9501\u8d85\u5e02\u8f7b\u677e\u8d2d\u5f97\uff0c\u6210\u672c\u4f4e\u5ec9\u4e14\u6613\u4e8e\u83b7\u53d6\u3002\n\u25c6 \u91c7\u7528\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6280\u672f\u548c\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u6280\u672f\u751f\u6210\u6c34\u5bc6\u7684\u4e09\u7ef4\u7f51\u683c\uff0c\u6a21\u578b\u8d28\u91cf\u9ad8\u3002\n\u25c6 \u6570\u636e\u96c6\u8986\u76d610\u4e2a\u4e0d\u540c\u7c7b\u522b\uff0c\u7269\u54c1\u5728\u5f62\u72b6\u3001\u5c3a\u5bf8\u548c\u91cd\u91cf\u4e0a\u5177\u6709\u591a\u6837\u6027\uff0c\u589e\u5f3a\u4e86\u5176\u5b9e\u7528\u6027\u3002\n\u25c6 \u4e13\u6ce8\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u9002\u7528\u6027\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u5408\u6210\u6a21\u578b\u6216\u4e13\u7528\u7269\u4f53\u53ef\u53ca\u6027\u6709\u9650\u7684\u7f3a\u9677\u3002\n\u25c6 \u8be5\u6570\u636e\u96c6\u975e\u5e38\u9002\u5408\u7528\u4e8e\u7269\u4f53\u68c0\u6d4b\u3001\u4f4d\u59ff\u4f30\u8ba1\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002|\n",
    "2509.11853": "|2025-09-15|Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting|Yi-Hsin Li\u7b49|[2509.11853](http://arxiv.org/pdf/2509.11853)|\u65e0|\u672c\u6587\u9488\u5bf9\u7a00\u758f\u89c6\u56fe\u5408\u6210\u4e2d\u51e0\u4f55\u4e0e\u5916\u89c2\u6062\u590d\u56f0\u96be\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u9a71\u52a8\u521d\u59cb\u5316\u76843D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\uff08SDI-GS\uff09\u3002  \n\u25c6 \u63d0\u51fa\u5229\u7528\u533a\u57df\u5206\u5272\u6280\u672f\u8bc6\u522b\u7ed3\u6784\u663e\u8457\u533a\u57df\uff0c\u66ff\u4ee3\u4f20\u7edf\u4f9d\u8d56\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6216\u591a\u89c6\u56fe\u7acb\u4f53\uff08MVS\uff09\u7684\u65b9\u6cd5\u3002  \n\u25c6 \u901a\u8fc7\u9009\u62e9\u6027\u4e0b\u91c7\u6837\u7a20\u5bc6\u70b9\u4e91\uff0c\u5927\u5e45\u51cf\u5c113D\u9ad8\u65af\u6570\u91cf\uff0c\u964d\u4f4e\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c\u3002  \n\u25c6 \u5728\u4fdd\u6301\u573a\u666f\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u9ad8\u8fbe50%\u7684\u9ad8\u65af\u6570\u91cf\u524a\u51cf\uff0c\u5e76\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6e32\u67d3\u8d28\u91cf\uff08PSNR\u3001SSIM\uff09\uff0c\u4ec5LPIPS\u7565\u6709\u4e0b\u964d\uff0c\u6709\u6548\u63a8\u52a8\u4e863DGS\u5728\u53d7\u9650\u89c6\u89d2\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u5316\u3002|\n",
    "2509.11829": "|2025-09-15|WAFER: A new method to retrieve sun-induced fluorescence based on spectral wavelet decompositions|Veronika Oehl\u7b49|[2509.11829](http://arxiv.org/pdf/2509.11829)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWAFER\u7684\u65b0\u578b\u65e5\u5149\u8bf1\u5bfc\u53f6\u7eff\u7d20\u8367\u5149\uff08SIF\uff09\u53cd\u6f14\u65b9\u6cd5\u3002  \n\u25c6 \u5229\u7528\u5c0f\u6ce2\u5206\u89e3\u6280\u672f\u5904\u7406\u53cd\u5c04\u8f90\u5c04\u548c\u53c2\u8003\u8f90\u5c04\u5149\u8c31\uff0c\u901a\u8fc7\u6bd4\u8f83\u5c0f\u6ce2\u7cfb\u6570\u6765\u72ec\u7acb\u83b7\u53d6\u76f8\u5bf9\u53cd\u5c04\u7387\uff0c\u907f\u514d\u4e86\u8367\u5149\u4e0e\u53cd\u5c04\u7387\u7684\u8026\u5408\u3002  \n\u25c6 \u53ef\u76f4\u63a5\u4ece\u5269\u4f59\u504f\u79fb\u91cf\u4e2d\u63a8\u5bfc\u51fa\u8367\u5149\u4fe1\u53f7\uff0c\u65e0\u9700\u5bf9\u8367\u5149\u5149\u8c31\u5f62\u72b6\u8fdb\u884c\u5148\u9a8c\u5047\u8bbe\uff0c\u51cf\u5c11\u4e86\u6a21\u578b\u504f\u5dee\u3002  \n\u25c6 \u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u610f\u6ce2\u957f\u7a97\u53e3\u548c\u5168\u5149\u8c31\u8303\u56f4\uff0c\u80fd\u5145\u5206\u5229\u7528\u6240\u6709\u53ef\u7528\u5149\u8c31\u6570\u636e\uff0c\u5e76\u53ef\u5206\u79bb\u4e0d\u540c\u9891\u7387\u7684\u5438\u6536\u7ebf\u7279\u5f81\u3002  \n\u25c6 \u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4e14\u5bf9\u53cd\u5c04\u7387\u5f62\u72b6\u7684\u5047\u8bbe\u8981\u6c42\u6781\u4f4e\uff0c\u589e\u5f3a\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002  \n\u25c6 \u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u5b9e\u5730\u6d4b\u91cf\u9a8c\u8bc1\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\uff08iFLD\u548cSFM\uff09\u7ed3\u679c\u4e00\u81f4\uff0c\u4f46\u989d\u5916\u63d0\u4f9b\u4e86\u63a2\u7d22\u771f\u5b9e\u8367\u5149\u5149\u8c31\u5f62\u72b6\u548c\u81ea\u7531\u9009\u62e9\u53cd\u6f14\u7a97\u53e3\u7684\u4f18\u52bf\u3002|\n",
    "2509.11097": "|2025-09-14|3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment|Nhut Le\u7b49|[2509.11097](http://arxiv.org/pdf/2509.11097)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63a8\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u707e\u540e\u8bc4\u4f30\u7684\u4e09\u7ef4\u65e0\u4eba\u673a\u57fa\u51c6\u6570\u636e\u96c63DAeroRelief\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\u3002  \n\u25c6\u9996\u521b\u9762\u5411\u707e\u540e\u573a\u666f\u7684\u4e09\u7ef4\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u771f\u5b9e\u707e\u5bb3\u73af\u5883\u4e2d\u7684\u7cbe\u7ec6\u7ed3\u6784\u635f\u4f24\u8bc6\u522b  \n\u25c6\u91c7\u7528\u4f4e\u6210\u672c\u65e0\u4eba\u673a\u91c7\u96c6\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5728\u5371\u9669\u533a\u57df\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u5b89\u5168\u7684\u5927\u89c4\u6a21\u4e09\u7ef4\u6570\u636e\u83b7\u53d6  \n\u25c6\u901a\u8fc7\u8fd0\u52a8\u91cd\u5efa\u548c\u591a\u89c6\u89d2\u7acb\u4f53\u6280\u672f\u751f\u6210\u5bc6\u96c6\u4e09\u7ef4\u70b9\u4e91\uff0c\u5e76\u7ed3\u5408\u4eba\u5de5\u4e8c\u7ef4\u6807\u6ce8\u6295\u5f71\u81f3\u4e09\u7ef4\u7a7a\u95f4\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bed\u4e49\u6ce8\u91ca  \n\u25c6\u6570\u636e\u96c6\u5305\u542b\u771f\u5b9e\u98d3\u98ce\u53d7\u707e\u533a\u57df\u7684\u5927\u89c4\u6a21\u5ba4\u5916\u573a\u666f\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u57ce\u5e02\u6216\u5ba4\u5185\u73af\u5883\u7684\u4e0d\u8db3  \n\u8bba\u6587\u8fd8\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u591a\u79cd\u5148\u8fdb\u4e09\u7ef4\u5206\u5272\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63a8\u52a8\u707e\u540e\u54cd\u5e94\u4e09\u7ef4\u89c6\u89c9\u7cfb\u7edf\u53d1\u5c55\u65b9\u9762\u7684\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2509.12759": "|2025-09-18|A-TDOM: Active TDOM via On-the-Fly 3DGS|Yiwei Xu\u7b49|[2509.12759](http://arxiv.org/pdf/2509.12759)|\u65e0|\u25c6 True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in various fields such as urban management, city planning, land surveying, etc.\n\u25c6 However, traditional TDOM generation methods generally rely on a complex offline photogrammetric pipeline, resulting in delays that hinder real-time applications.\n\u25c6 Moreover, the quality of TDOM may degrade due to various challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and scene occlusions.|\n",
    "2509.12458": "|2025-09-15|Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles|\u00c0lmos Veres-Vit\u00e0lyos\u7b49|[2509.12458](http://arxiv.org/pdf/2509.12458)|\u65e0|\u25c6 Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction.\n\u25c6 To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams.\n\u25c6 Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control.|\n",
    "2509.13414": "|2025-09-18|MapAnything: Universal Feed-Forward Metric 3D Reconstruction|Nikhil Keetha\u7b49|[2509.13414](http://arxiv.org/pdf/2509.13414)|\u65e0|\u25c6 We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras.\n\u25c6 MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame.\n\u25c6 Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more.|\n",
    "2509.15548": "|2025-09-26|MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild|Deming Li\u7b49|[2509.15548](http://arxiv.org/pdf/2509.15548)|\u65e0|\u25c6 In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis.\n\u25c6 Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting.\n\u25c6 In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS.|\n",
    "2509.17270": "|2025-09-21|Reference-aware SFM layers for intrusive intelligibility prediction|Hanlin Yu\u7b49|[2509.17270](http://arxiv.org/pdf/2509.17270)|\u65e0|\u25c6 Intrusive speech-intelligibility predictors that exploit explicit reference signals are now widespread, yet they have not consistently surpassed non-intrusive systems.\n\u25c6 We argue that a primary cause is the limited exploitation of speech foundation models (SFMs).\n\u25c6 This work revisits intrusive prediction by combining reference conditioning with multi-layer SFM representations.|\n",
    "2509.16329": "|2025-09-19|Investigating Polyglot Speech Foundation Models for Learning Collective Emotion from Crowds|Orchid Chetia Phukan\u7b49|[2509.16329](http://arxiv.org/pdf/2509.16329)|\u65e0|\u25c6 This paper investigates the polyglot (multilingual) speech foundation models (SFMs) for Crowd Emotion Recognition (CER).\n\u25c6 We hypothesize that polyglot SFMs, pre-trained on diverse languages, accents, and speech patterns, are particularly adept at navigating the noisy and complex acoustic environments characteristic of crowd settings, thereby offering a significant advantage for CER.\n\u25c6 To substantiate this, we perform a comprehensive analysis, comparing polyglot, monolingual, and speaker recognition SFMs through extensive experiments on a benchmark CER dataset across varying audio durations (1 sec, 500 ms, and 250 ms).|\n",
    "2509.18898": "|2025-09-23|DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring|Pengteng Li\u7b49|[2509.18898](http://arxiv.org/pdf/2509.18898)|\u65e0|\u25c6 In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\n\u25c6 We address the motion-deblurring problem in two ways.\n\u25c6 First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images.|\n",
    "2509.19898": "|2025-09-24|Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering|Jiangxue Yu\u7b49|[2509.19898](http://arxiv.org/pdf/2509.19898)|\u65e0|\u25c6 The integration of aerial and ground images has been a promising solution in 3D modeling of complex scenes, which is seriously restricted by finding reliable correspondences.\n\u25c6 The primary contribution of this study is a feature matching algorithm for aerial and ground images, whose core idea is to generate intermediate views to alleviate perspective distortions caused by the extensive viewpoint changes.\n\u25c6 First, by using aerial images only, sparse models are reconstructed through an incremental SfM (Structure from Motion) engine due to their large scene coverage.|\n",
    "2509.24126": "|2025-09-28|BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes|Athanasios Bacharis\u7b49|[2509.24126](http://arxiv.org/pdf/2509.24126)|\u65e0|\u25c6 Active vision (AV) has been in the spotlight of robotics research due to its emergence in numerous applications including agricultural tasks such as precision crop monitoring and autonomous harvesting to list a few.\n\u25c6 A major AV problem that gained popularity is the 3D reconstruction of targeted environments using 2D images from diverse viewpoints.\n\u25c6 While collecting and processing a large number of arbitrarily captured 2D images can be arduous in many practical scenarios, a more efficient solution involves optimizing the placement of available cameras in 3D space to capture fewer, yet more informative, images that provide sufficient visual information for effective reconstruction of the environment of interest.|\n",
    "2509.23991": "|2025-09-28|RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization|Dongki Jung\u7b49|[2509.23991](http://arxiv.org/pdf/2509.23991)|\u65e0|\u25c6 The increasing use of 360 images across various domains has emphasized the need for robust depth estimation techniques tailored for omnidirectional images.\n\u25c6 However, obtaining large-scale labeled datasets for 360 depth estimation remains a significant challenge.\n\u25c6 In this paper, we propose RPG360, a training-free robust 360 monocular depth estimation method that leverages perspective foundation models and graph optimization.|\n",
    "2509.23947": "|2025-09-28|CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting|Drago\u015f-Andrei Chileban\u7b49|[2509.23947](http://arxiv.org/pdf/2509.23947)|\u65e0|\u25c6 Automatic car damage detection has been a topic of significant interest for the auto insurance industry as it promises faster, accurate, and cost-effective damage assessments.\n\u25c6 However, few works have gone beyond 2D image analysis to leverage 3D reconstruction methods, which have the potential to provide a more comprehensive and geometrically accurate representation of the damage.\n\u25c6 Moreover, recent methods employing 3D representations for novel view synthesis, particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to generate accurate and coherent 3D reconstructions from a limited number of views.|\n",
    "2509.25191": "|2025-10-08|VGGT-X: When VGGT Meets Dense Novel View Synthesis|Yang Liu\u7b49|[2509.25191](http://arxiv.org/pdf/2509.25191)|\u65e0|\u25c6 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS).\n\u25c6 Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures.\n\u25c6 Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS.|\n",
    "2510.01665": "|2025-10-02|Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale|Yongbo Chen\u7b49|[2510.01665](http://arxiv.org/pdf/2510.01665)|\u65e0|\u25c6 Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention.\n\u25c6 We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset.\n\u25c6 Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework.|\n",
    "2510.05388": "|2025-10-06|The Prevalence of Bursty Star Formation in Low-Mass Galaxies at z=1-7 from H\u03b1-to-UV Diagnostics|Marissa N. Perry\u7b49|[2510.05388](http://arxiv.org/pdf/2510.05388)|\u65e0|\u25c6 We present an analysis of bursty star-formation histories (SFHs) of 346 star-forming galaxies at $1\\lesssim z<7$, selected from JWST/NIRSpec G395M and PRISM spectroscopy provided by the CEERS and RUBIES surveys.\n\u25c6 We analyze the correlation of star-formation rate vs.\n\u25c6 stellar mass (the star-forming main sequence, SFMS) for our sample and find no significant difference between the intrinsic scatter in the H$\\alpha$-based SFMS and the UV-continuum-based SFMS.|\n",
    "2510.06681": "|2025-10-08|The Star-forming Main Sequence and Bursty Star-formation Histories at $z>1.4$ in JADES and AURORA|Leonardo Clarke\u7b49|[2510.06681](http://arxiv.org/pdf/2510.06681)|\u65e0|\u25c6 We analyze JWST spectroscopic and HST+JWST photometric observations of 659 star-forming galaxies at $1.4 < z < 9$ from DR3 of the JADES survey and the AURORA Cycle 1 program.\n\u25c6 We measure the star-forming main sequence (SFMS) for galaxies above $10^{8.5}\\rm\\ M_\\odot$ where the sample is largely representative, estimating star-formation rates (SFRs) using the H$\\alpha$ line flux and rest-frame far UV (1600\\AA) continuum measurements, each independently corrected for dust attenuation.\n\u25c6 We find that the intrinsic, measurement-error-subtracted scatter in the SFMS ($\\sigma_{\\rm int}$) increases with decreasing stellar mass for the H$\\alpha$-based SFMS, and we find no mass dependence of $\\sigma_{\\rm int}$ in the UV-based SFMS.|\n",
    "2510.09489": "|2025-10-10|Two-Stage Gaussian Splatting Optimization for Outdoor Scene Reconstruction|Deborah Pintani\u7b49|[2510.09489](http://arxiv.org/pdf/2510.09489)|\u65e0|\u25c6 Outdoor scene reconstruction remains challenging due to the stark contrast between well-textured, nearby regions and distant backgrounds dominated by low detail, uneven illumination, and sky effects.\n\u25c6 We introduce a two-stage Gaussian Splatting framework that explicitly separates and optimizes these regions, yielding higher-fidelity novel view synthesis.\n\u25c6 In stage one, background primitives are initialized within a spherical shell and optimized using a loss that combines a background-only photometric term with two geometric regularizers: one constraining Gaussians to remain inside the shell, and another aligning them with local tangential planes.|\n",
    "2510.12387": "|2025-10-14|Scene Coordinate Reconstruction Priors|Wenjing Bian\u7b49|[2510.12387](http://arxiv.org/pdf/2510.12387)|\u65e0|\u25c6 Scene coordinate regression (SCR) models have proven to be powerful implicit scene representations for 3D vision, enabling visual relocalization and structure-from-motion.\n\u25c6 SCR models are trained specifically for one scene.\n\u25c6 If training images imply insufficient multi-view constraints SCR models degenerate.|\n",
    "2510.13540": "|2025-10-15|Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos|Maximilian Weiherer\u7b49|[2510.13540](http://arxiv.org/pdf/2510.13540)|\u65e0|\u25c6 We present a neural parametric 3D breast shape model and, based on this model, introduce a low-cost and accessible 3D surface reconstruction pipeline capable of recovering accurate breast geometry from a monocular RGB video.\n\u25c6 In contrast to widely used, commercially available yet prohibitively expensive 3D breast scanning solutions and existing low-cost alternatives, our method requires neither specialized hardware nor proprietary software and can be used with any device that is able to record RGB videos.\n\u25c6 The key building blocks of our pipeline are a state-of-the-art, off-the-shelf Structure-from-motion pipeline, paired with a parametric breast model for robust and metrically correct surface reconstruction.|\n",
    "2510.13310": "|2025-10-15|InstantSfM: Fully Sparse and Parallel Structure-from-Motion|Jiankun Zhong\u7b49|[2510.13310](http://arxiv.org/pdf/2510.13310)|\u65e0|\u25c6 Structure-from-Motion (SfM), a method that recovers camera poses and scene geometry from uncalibrated images, is a central component in robotic reconstruction and simulation.\n\u25c6 Despite the state-of-the-art performance of traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive CPU-specialized implementations of bundle adjustment (BA) or global positioning (GP) introduce significant computational overhead when handling large-scale scenarios, leading to a trade-off between accuracy and speed in SfM.\n\u25c6 Moreover, the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes with the curse of limited flexibility, as they lack support for various external optimization options.|\n",
    "2510.13084": "|2025-10-15|Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation|Yi Zuo\u7b49|[2510.13084](http://arxiv.org/pdf/2510.13084)|\u65e0|\u25c6 Text-to-image (T2I) diffusion models have recently demonstrated significant progress in video editing.\n\u25c6 However, existing video editing methods are severely limited by their high computational overhead and memory consumption.\n\u25c6 Furthermore, these approaches often sacrifice visual fidelity, leading to undesirable temporal inconsistencies and artifacts such as blurring and pronounced mosaic-like patterns.|\n",
    "2510.15467": "|2025-10-17|MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes|Lingfeng Xuan\u7b49|[2510.15467](http://arxiv.org/pdf/2510.15467)|\u65e0|\u25c6 Structure from Motion (SfM) estimates camera poses and reconstructs point clouds, forming a foundation for various tasks.\n\u25c6 However, applying SfM to driving scenes captured by multi-camera systems presents significant difficulties, including unreliable pose estimation, excessive outliers in road surface reconstruction, and low reconstruction efficiency.\n\u25c6 To address these limitations, we propose a Multi-camera Reconstruction and Aggregation Structure-from-Motion (MRASfM) framework specifically designed for driving scenes.|\n",
    "2510.15271": "|2025-10-17|CuSfM: CUDA-Accelerated Structure-from-Motion|Jingrui Yu\u7b49|[2510.15271](http://arxiv.org/pdf/2510.15271)|\u65e0|\u25c6 Efficient and accurate camera pose estimation forms the foundational requirement for dense reconstruction in autonomous navigation, robotic perception, and virtual simulation systems.\n\u25c6 This paper addresses the challenge via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that leverages GPU parallelization to efficiently employ computationally intensive yet highly accurate feature extractors, generating comprehensive and non-redundant data associations for precise camera pose estimation and globally consistent mapping.\n\u25c6 The system supports pose optimization, mapping, prior-map localization, and extrinsic refinement.|\n",
    "2510.17479": "|2025-10-20|Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS|Feng Zhou\u7b49|[2510.17479](http://arxiv.org/pdf/2510.17479)|\u65e0|\u25c6 Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training views, leading to artifacts like blurring in novel view rendering.\n\u25c6 Prior work addresses it either by enhancing the initialization (\\emph{i.e.}, the point cloud from Structure-from-Motion (SfM)) or by adding training-time constraints (regularization) to the 3DGS optimization.\n\u25c6 Yet our controlled ablations reveal that initialization is the decisive factor: it determines the attainable performance band in sparse-view 3DGS, while training-time constraints yield only modest within-band improvements at extra cost.|\n",
    "2510.17434": "|2025-10-21|Leveraging AV1 motion vectors for Fast and Dense Feature Matching|Julien Zouein\u7b49|[2510.17434](http://arxiv.org/pdf/2510.17434)|\u65e0|\u25c6 We repurpose AV1 motion vectors to produce dense sub-pixel correspondences and short tracks filtered by cosine consistency.\n\u25c6 On short videos, this compressed-domain front end runs comparably to sequential SIFT while using far less CPU, and yields denser matches with competitive pairwise geometry.\n\u25c6 As a small SfM demo on a 117-frame clip, MV matches register all images and reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows with match density.|\n",
    "2510.17422": "|2025-10-21|DeepDetect: Learning All-in-One Dense Keypoints|Shaharyar Ahmed Khan Tareen\u7b49|[2510.17422](http://arxiv.org/pdf/2510.17422)|\u65e0|\u25c6 Keypoint detection is the foundation of many computer vision tasks, including image registration, structure-from motion, 3D reconstruction, visual odometry, and SLAM.\n\u25c6 Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong performance yet suffer from key limitations: sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding, often failing to prioritize visually important regions.\n\u25c6 We present DeepDetect, an intelligent, all-in-one, dense keypoint detector that unifies the strengths of classical detectors using deep learning.|\n",
    "2510.16438": "|2025-10-18|LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching|Aidyn Ubingazhibov\u7b49|[2510.16438](http://arxiv.org/pdf/2510.16438)|\u65e0|\u25c6 Lines and points are complementary local features, whose combination has proven effective for applications such as SLAM and Structure-from-Motion.\n\u25c6 The backbone of these pipelines are the local feature matchers, establishing correspondences across images.\n\u25c6 Traditionally, point and line matching have been treated as independent tasks.|\n",
    "2510.19044": "|2025-10-21|The slope and scatter of the star forming main sequence at z~5 : reconciling observations with simulations|Claudia Di Cesare\u7b49|[2510.19044](http://arxiv.org/pdf/2510.19044)|\u65e0|\u25c6 Galaxies exhibit a tight correlation between their star-formation rate and stellar mass over a wide redshift range known as the star-forming main sequence (SFMS).\n\u25c6 With JWST, we can now investigate the SFMS at high redshifts down to masses of $\\sim10^6$ M$_{\\odot}$, using sensitive star-formation rate tracers such as H$\\alpha$ emission -- which allow us to probe the variability in star formation histories.\n\u25c6 We present inferences of the SFMS based on 316 H$\\alpha$-selected galaxies at $z\\sim4$-$5$ with $\\log(\\rm M_\\star/M_\\odot) = 6.4$ -$10.6$.|\n",
    "2510.20529": "|2025-10-23|RubbleSim: A Photorealistic Structural Collapse Simulator for Confined Space Mapping|Constantine Frost\u7b49|[2510.20529](http://arxiv.org/pdf/2510.20529)|\u65e0|\u25c6 Despite well-reported instances of robots being used in disaster response, there is scant published data on the internal composition of the void spaces within structural collapse incidents.\n\u25c6 Data collected during these incidents is mired in legal constraints, as ownership is often tied to the responding agencies, with little hope of public release for research.\n\u25c6 While engineered rubble piles are used for training, these sites are also reluctant to release information about their proprietary training grounds.|\n",
    "2510.22961": "|2025-10-27|Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition|Jing-Xuan Zhang\u7b49|[2510.22961](http://arxiv.org/pdf/2510.22961)|\u65e0|\u25c6 Unified speech recognition aims to perform auditory, visual, and audiovisual speech recognition within a single model framework.\n\u25c6 While speech foundation models (SFMs) have demonstrated remarkable performance in auditory tasks, their adaptation to multimodal scenarios remains underexplored.\n\u25c6 This paper presents UASR-LLM, a novel framework that adapts frozen SFMs to unified VSR, ASR, and AVSR tasks by leveraging large language models (LLMs) as text decoders.|\n",
    "2510.25577": "|2025-10-29|Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models|Harm Lameris\u7b49|[2510.25577](http://arxiv.org/pdf/2510.25577)|\u65e0|\u25c6 Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations.\n\u25c6 This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal.\n\u25c6 One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice.|\n",
    "2510.25178": "|2025-10-27|SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution|Dharma Teja Donepudi|[2510.25178](http://arxiv.org/pdf/2510.25178)|\u65e0|\u25c6 Intra-sentence multilingual speech synthesis (code-switching TTS) remains a major challenge due to abrupt language shifts, varied scripts, and mismatched prosody between languages.\n\u25c6 Conventional TTS systems are typically monolingual and fail to produce natural, intelligible speech in mixed-language contexts.\n\u25c6 We introduce Script-First Multilingual Synthesis with Adaptive Locale Resolution (SFMS-ALR), an engine-agnostic framework for fluent, real-time code-switched speech generation.|\n",
    "2511.02329": "|2025-11-04|Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization|Shaohan Li\u7b49|[2511.02329](http://arxiv.org/pdf/2511.02329)|\u65e0|\u25c6 We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations).\n\u25c6 Our core innovation is a location solver that adapts message-passing least squares (MPLS) -- originally developed for group synchronization -- to camera location estimation.\n\u25c6 We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss.|\n",
    "2511.01261": "|2025-11-03|Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play|Jiatong Shi\u7b49|[2511.01261](http://arxiv.org/pdf/2511.01261)|\u65e0|\u25c6 Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction.\n\u25c6 Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges.\n\u25c6 Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles.|\n",
    "2511.00362": "|2025-11-01|Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery|Momen Khandoker Ope\u7b49|[2511.00362](http://arxiv.org/pdf/2511.00362)|\u65e0|\u25c6 Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise.\n\u25c6 Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts.\n\u25c6 As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form.|\n",
    "2511.04745": "|2025-11-06|Sub-Gyr variability around the SFMS and its contribution to the scatter|A. Camps-Fari\u00f1a\u7b49|[2511.04745](http://arxiv.org/pdf/2511.04745)|\u65e0|\u25c6 We aim to measure the evolution of individual galaxies around the Star Formation Main Sequence (SFMS) during the last Gyr as a function of their stellar mass to quantify how much of its scatter is due to short-term variability.We derived star formation histories using full spectral fitting for a sample of 8,960 galaxies from the MaNGA survey to track the position of the galaxies in the SFMS during the last Gyr.The variability correlates with both the stellar mass of the galaxies and their current position in both the SFMS and the mass-metallicity relation (MZR), with the position in the latter strongly affecting variability in SFR.\n\u25c6 While most of the fluctuations are compatible with stochasticity, there is a very weak but statistically significant preference for $\\sim135-150$ Myr time-scales.\n\u25c6 These results support a strong self-regulation of SFR within galaxies, establishing characteristic intensities and time-scales for bursts of star formation and quenching episodes.|\n",
    "2511.06857": "|2025-11-10|Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation|Fanding Li\u7b49|[2511.06857](http://arxiv.org/pdf/2511.06857)|\u65e0|\u25c6 A simultaneous enhancement of accuracy and diversity of predictions remains a challenge in ambiguous medical image segmentation (AMIS) due to the inherent trade-offs.\n\u25c6 While truncated diffusion probabilistic models (TDPMs) hold strong potential with a paradigm optimization, existing TDPMs suffer from entangled accuracy and diversity of predictions with insufficient fidelity and plausibility.\n\u25c6 To address the aforementioned challenges, we propose Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel inference paradigm and dedicated model components.|\n",
    "2511.06775": "|2025-11-10|SDSS-ALMA Legacy Value Archival Gas Exploration (SALVAGE) - I: global star formation is governed by central (not global) molecular gas|Scott Wilkinson\u7b49|[2511.06775](http://arxiv.org/pdf/2511.06775)|\u65e0|\u25c6 Star-forming galaxies form tight relations between their stellar mass, star-formation rate, and molecular gas reservoir on global and resolved scales.\n\u25c6 On the path to quiescence, the exchange between gas and stars must inevitably be broken.\n\u25c6 Understanding the mechanisms governing star formation and quenching therefore requires observations of both the stellar and molecular gas components.|\n",
    "2511.10580": "|2025-11-13|From Fold to Function: Dynamic Modeling and Simulation-Driven Design of Origami Mechanisms|Tianhui Han\u7b49|[2511.10580](http://arxiv.org/pdf/2511.10580)|\u65e0|\u25c6 Origami-inspired mechanisms can transform flat sheets into functional three-dimensional dynamic structures that are lightweight, compact, and capable of complex motion.\n\u25c6 These properties make origami increasingly valuable in robotic and deployable systems.\n\u25c6 However, accurately simulating their folding behavior and interactions with the environment remains challenging.|\n",
    "2511.10174": "|2025-11-13|M3Scope a 3D multimode multiplane microscope for imaging nanoscale dynamics in soft matter|Steven Huysecom\u7b49|[2511.10174](http://arxiv.org/pdf/2511.10174)|\u65e0|\u25c6 Fast, volumetric imaging that integrates multiple imaging modalities is essential for probing dynamic, heterogeneous soft and biological matter.\n\u25c6 Here, we present the M3Scope, a simple yet versatile multiplane microscope that extends widefield detection with a modular multimode cube to enable dual color fluorescence, polarization-resolved, and correlative brightfield fluorescence imaging while (i) preserving simultaneous 3D acquisition at high frame rates (100 fps) and (ii) requiring minimal realignment.\n\u25c6 We demonstrate its potential by investigating polymer dynamics across multiple spatial and temporal scales.|\n",
    "2511.10079": "|2025-11-13|Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks|Yizheng Wang\u7b49|[2511.10079](http://arxiv.org/pdf/2511.10079)|\u65e0|\u25c6 Friction modeling plays a crucial role in achieving high-precision motion control in robotic operating systems.\n\u25c6 Traditional static friction models (such as the Stribeck model) are widely used due to their simple forms; however, they typically require predefined functional assumptions, which poses significant challenges when dealing with unknown functional structures.\n\u25c6 To address this issue, this paper proposes a physics-inspired machine learning approach based on the Kolmogorov Arnold Network (KAN) for static friction modeling of robotic joints.|\n",
    "2511.10076": "|2025-11-13|Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints|Xiangyue Zhang\u7b49|[2511.10076](http://arxiv.org/pdf/2511.10076)|\u65e0|\u25c6 Reliable co-speech motion generation requires precise motion representation and consistent structural priors across all joints.\n\u25c6 Existing generative methods typically operate on local joint rotations, which are defined hierarchically based on the skeleton structure.\n\u25c6 This leads to cumulative errors during generation, manifesting as unstable and implausible motions at end-effectors.|\n",
    "2511.09885": "|2025-11-13|PuffyBot: An Untethered Shape Morphing Robot for Multi-environment Locomotion|Shashwat Singh\u7b49|[2511.09885](http://arxiv.org/pdf/2511.09885)|\u65e0|\u25c6 Amphibians adapt their morphologies and motions to accommodate movement in both terrestrial and aquatic environments.\n\u25c6 Inspired by these biological features, we present PuffyBot, an untethered shape morphing robot capable of changing its body morphology to navigate multiple environments.\n\u25c6 Our robot design leverages a scissor-lift mechanism driven by a linear actuator as its primary structure to achieve shape morphing.|\n",
    "2511.09827": "|2025-11-13|AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting|Aymen Mir\u7b49|[2511.09827](http://arxiv.org/pdf/2511.09827)|\u65e0|\u25c6 We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction.\n\u25c6 Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes.\n\u25c6 By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes.|\n",
    "2511.09502": "|2025-11-12|DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation|Jerrin Bright\u7b49|[2511.09502](http://arxiv.org/pdf/2511.09502)|\u65e0|\u25c6 Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships.\n\u25c6 However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios.\n\u25c6 Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation.|\n",
    "2511.09484": "|2025-11-12|SPIDER: Scalable Physics-Informed Dexterous Retargeting|Chaoyi Pan\u7b49|[2511.09484](http://arxiv.org/pdf/2511.09484)|\u65e0|\u25c6 Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive.\n\u25c6 In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem.\n\u25c6 However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots.|\n",
    "2511.09079": "|2025-11-12|3D PIC simulation and theoretical modeling of RF Laser pulse in magnetized plasma for the generation of multidimensional relativistic Wakefields|A. A. Molavi Choobini\u7b49|[2511.09079](http://arxiv.org/pdf/2511.09079)|\u65e0|\u25c6 The present study, investigates the modulation of plasma wakefields in dense magnetized plasma driven by relativistic electron beams under transverse RF excitation.\n\u25c6 A self consistent theoretical framework, comprising the RF vector potential, Maxwells equations, and relativistic electron motion, is extended through full 3D electromagnetic particle in cell simulations.\n\u25c6 The results reveal systematic amplification and reshaping of wakefields under the combined action of external magnetic fields and RF drivers.|\n",
    "2511.08995": "|2025-11-12|Group-Theoretic Structure Governing Identifiability in Inverse Problems|Isshin Arai\u7b49|[2511.08995](http://arxiv.org/pdf/2511.08995)|\u65e0|\u25c6 In physical systems possessing symmetry, reconstructing the underlying causal structure from observational data constitutes an inverse problem of fundamental importance.\n\u25c6 In this work, we formulate the inverse problem of causal inference within the framework of group-representation theory, clarifying the structure of the representation spaces to which the {\\it causality} and estimation maps belong.\n\u25c6 This formulation leads to both theoretical and practical limits of reconstructability (identifiability).|\n",
    "2511.11368": "|2025-11-14|Free3D: 3D Human Motion Emerges from Single-View 2D Supervision|Sheng Liu\u7b49|[2511.11368](http://arxiv.org/pdf/2511.11368)|\u65e0|\u25c6 Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions.\n\u25c6 This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations.\n\u25c6 Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility.|\n",
    "2511.11344": "|2025-11-14|YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation|Pavel Rojtberg\u7b49|[2511.11344](http://arxiv.org/pdf/2511.11344)|\u65e0|\u25c6 We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation.\n\u25c6 While synthetic data has become fundamental in frame-based computer vision, event-based vision lacks comparable comprehensive resources.\n\u25c6 Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology.|\n",
    "2511.11115": "|2025-11-14|The Spatial Evolution of Star Clusters in NGC 628 with JWST|Anne S. M. Buckner\u7b49|[2511.11115](http://arxiv.org/pdf/2511.11115)|\u65e0|\u25c6 We examine the spatial distribution of star clusters in NGC 628 using the statistical tool INDICATE to quantify clustering tendencies.\n\u25c6 Our sample, based on HST and JWST observations, is the most complete to date, spanning ages from 1 Myr to >100 Myr.\n\u25c6 We find cluster spatial behaviour varies with galactic position, age, and mass.|\n",
    "2511.10968": "|2025-11-14|Discovery of an X-ray bridge between the comma-shaped gas and the main cluster in MCXC J0157.4-0550|Chong Yang\u7b49|[2511.10968](http://arxiv.org/pdf/2511.10968)|\u65e0|\u25c6 We report the discovery of a faint X-ray bridge connecting between the comma-shaped gas and the main cluster in MCXC J0157.4-0550, using {\\it XMM-Newton} image.\n\u25c6 The filamentary structure is found in a model-independent manner in both topological features and Gaussian Gradient Magnitude filtering.\n\u25c6 The X-ray surface brightness profile perpendicular to the filament is detected at a $5.5\u03c3$ level.|\n",
    "2511.10948": "|2025-11-14|DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition|Ren Zhang\u7b49|[2511.10948](http://arxiv.org/pdf/2511.10948)|\u65e0|\u25c6 Micro expression recognition (MER) is crucial for inferring genuine emotion.\n\u25c6 Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions.\n\u25c6 However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion.|\n",
    "2511.10929": "|2025-11-14|A High-Precision Dynamical Model of Callisto: Incorporating Rotation Effects within Multi-Layer Internal Structure Models|Kai Huang\u7b49|[2511.10929](http://arxiv.org/pdf/2511.10929)|\u65e0|\u25c6 China is planing to launch the Tianwen-4 mission around the year 2030, with its aim being the exploration of Jupiter and its moon, Callisto.\n\u25c6 Within the realm of deep space exploration, the accuracy of ephemerides is of great importance.\n\u25c6 Current ephemerides employ a simplified rotation model for Callisto, which this study addresses by proposing a novel dynamical model.|\n",
    "2511.10874": "|2025-11-14|Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation|Yorai Shaoul\u7b49|[2511.10874](http://arxiv.org/pdf/2511.10874)|\u65e0|\u25c6 Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale.\n\u25c6 Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks.\n\u25c6 To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning.|\n",
    "2511.10830": "|2025-11-13|A validated lumped-element model for bioinspired acoustic flow sensing toward the performance limit|Wei Sun\u7b49|[2511.10830](http://arxiv.org/pdf/2511.10830)|\u65e0|\u25c6 Flow sensing is fundamental to both biological survival and technological innovation.\n\u25c6 Inspired by biological mechanoreceptors, artificial flow sensors detect subtle fluid motion using slender, viscous-driven structures.\n\u25c6 Among these, acoustic flow sensors that mimic nature's velocity-sensitive ears have the potential to transform vector sound detection.|\n",
    "2511.10806": "|2025-11-13|From Attention to Frequency: Integration of Vision Transformer and FFT-ReLU for Enhanced Image Deblurring|Syed Mumtahin Mahmud\u7b49|[2511.10806](http://arxiv.org/pdf/2511.10806)|\u65e0|\u25c6 Image deblurring is vital in computer vision, aiming to recover sharp images from blurry ones caused by motion or camera shake.\n\u25c6 While deep learning approaches such as CNNs and Vision Transformers (ViTs) have advanced this field, they often struggle with complex or high-resolution blur and computational demands.\n\u25c6 We propose a new dual-domain architecture that unifies Vision Transformers with a frequency-domain FFT-ReLU module, explicitly bridging spatial attention modeling and frequency sparsity.|\n",
    "2511.10790": "|2025-11-13|Towards Attribution of Generators and Emotional Manipulation in Cross-Lingual Synthetic Speech using Geometric Learning|Girish\u7b49|[2511.10790](http://arxiv.org/pdf/2511.10790)|\u65e0|\u25c6 In this work, we address the problem of finegrained traceback of emotional and manipulation characteristics from synthetically manipulated speech.\n\u25c6 We hypothesize that combining semantic-prosodic cues captured by Speech Foundation Models (SFMs) with fine-grained spectral dynamics from auditory representations can enable more precise tracing of both emotion and manipulation source.\n\u25c6 To validate this hypothesis, we introduce MiCuNet, a novel multitask framework for fine-grained tracing of emotional and manipulation attributes in synthetically generated speech.|\n",
    "2511.13479": "|2025-11-17|Ultrafast electron diffractive imaging of the dissociation of pre-excited molecules|Yanwei Xiong\u7b49|[2511.13479](http://arxiv.org/pdf/2511.13479)|\u65e0|\u25c6 Gas phase ultrafast electron diffraction (GUED) has become a powerful technique to directly observe the structural dynamics of photoexcited molecules.\n\u25c6 GUED reveals information about the nuclear motions that is complementary to the information on the electronic states provided by spectroscopic measurements.\n\u25c6 GUED experiments so far have utilized a single laser pulse to excite the molecules and an electron pulse to probe the dynamics.|\n",
    "2511.13364": "|2025-11-17|An Automated Framework for Analyzing Structural Evolution in On-the-fly Non-adiabatic Molecular Dynamics Using Autoencoder and Multiple Molecular Descriptors|Hangxu Liu\u7b49|[2511.13364](http://arxiv.org/pdf/2511.13364)|\u65e0|\u25c6 A major challenge in nonadiabatic molecular dynamics is to automatically and objectively identify the key reaction coordinates that drive molecules toward distinct excited-state decay channels.\n\u25c6 Traditional manual analyses are inefficient and rely heavily on expert intuition, creating a bottleneck for interpreting complex photochemical processes.\n\u25c6 To overcome this, we introduce a fully automated machine-learning framework that directly extracts these coordinates from on-the-fly trajectory surface hopping data.|\n",
    "2511.13266": "|2025-11-17|Antisymmetric Mueller Generator as the Universal Origin of Geometric Phase in Classical Polarization and Quantum Two-Level Systems|Jos\u00e9 J. Gil|[2511.13266](http://arxiv.org/pdf/2511.13266)|\u65e0|\u25c6 We show that the antisymmetric part of the Mueller matrix of any ideal retarder uniquely determines the geometric phase observed in classical polarization optics.\n\u25c6 This antisymmetric block encodes the angular-velocity pseudovector that governs the tangential component of the Stokesvector motion on the Poincar\u00e9 sphere and thus fully determines the associated geometric phase, while the symmetric block is geometrically neutral in the sense that it does not contribute to the phase.\n\u25c6 We further demonstrate that the same antisymmetric generator arises in the adjoint action of any SU(2) unitary operator and fully determines the geometric phase of a qubit, independently of adiabaticity or cyclicity.|\n",
    "2511.13252": "|2025-11-17|The Spontaneous Genesis of Solar Prominence Structures Driven by Supergranulation in Three-Dimensional Simulations|Huanxin Chen\u7b49|[2511.13252](http://arxiv.org/pdf/2511.13252)|\u65e0|\u25c6 Solar prominences usually have a horizontally elongated body with many feet extending to the solar surface, resembling a multi-arch bridge with many bridge piers.\n\u25c6 The basic mechanism by which solar prominences acquire these common structures during their evolution, however, remains an unresolved question.\n\u25c6 For the first time, our three-dimensional magneto-frictional simulation, driven by supergranular motions, self-consistently replicates the commonly observed multi-arch bridge morphology and its characteristic structures of solar quiescent prominences in a magnetic flux rope.|\n",
    "2511.13161": "|2025-11-17|Infrared photometry and CaT spectroscopy of the most metal-poor in-situ globular cluster VVV-CL001|W. Haro Moya\u7b49|[2511.13161](http://arxiv.org/pdf/2511.13161)|\u65e0|\u25c6 Globular clusters in the Galactic bulge are difficult to study due to high extinction and severe crowding.\n\u25c6 VVV-CL001 is an old, metal-poor, and fast cluster in the inner bulge, whose extreme properties make it a key probe of the early chemical and dynamical evolution of the Milky Way.\n\u25c6 We derive its fundamental parameters by combining spectroscopy, astrometry, and near-infrared photometry.|\n",
    "2511.12731": "|2025-11-16|Kagome metals|Domenico Di Sante\u7b49|[2511.12731](http://arxiv.org/pdf/2511.12731)|\u65e0|\u25c6 Three important driving forces for creating qualitatively new phases in quantum materials are the topology of the materials' electronic band structures, frustration in the electrons' motion or magnetic interactions, and strong correlations between their charge, spin, and orbital degrees of freedom.\n\u25c6 In very few material systems do all of these aspects come together to contribute on an equal footing to stabilize new electronic states with unprecedented properties; however the search for such systems can be guided by models of configurational motifs or key sublattices that can host such physics.\n\u25c6 One of the most fascinating structural motifs for realizing this rich interplay of frustration, electronic topology, and electron correlation effects is the kagome lattice.|\n",
    "2511.12418": "|2025-11-16|Examining Turbulence in Galactic Molecular Clouds - II: Continuity of Turbulence Cascading in a Portion of the Local Arm|Yuehui Ma\u7b49|[2511.12418](http://arxiv.org/pdf/2511.12418)|\u65e0|\u25c6 We use $^{12}$CO (J=1-0) MWISP data to study turbulence in a segment of the Local Arm.\n\u25c6 Velocity slices at different kinematic distances show similar spatial power spectra (SPSs) and structure functions (SFs), demonstrating that the entire region forms a single turbulent field with a cascade extending from $\\sim 400$ pc to sub-parsec scales.\n\u25c6 The SPS slopes of both the intensity and velocity fields exhibit a systematic scale dependence that approaches the values expected from turbulence models.|\n",
    "2511.12415": "|2025-11-16|Towards Rotation-only Imaging Geometry: Rotation Estimation|Xinrui Li\u7b49|[2511.12415](http://arxiv.org/pdf/2511.12415)|\u65e0|\u25c6 Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images.\n\u25c6 The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment.\n\u25c6 Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation.|\n",
    "2511.14371": "|2025-11-18|Blur-Robust Detection via Feature Restoration: An End-to-End Framework for Prior-Guided Infrared UAV Target Detection|Xiaolin Wang\u7b49|[2511.14371](http://arxiv.org/pdf/2511.14371)|\u65e0|\u25c6 Infrared unmanned aerial vehicle (UAV) target images often suffer from motion blur degradation caused by rapid sensor movement, significantly reducing contrast between target and background.\n\u25c6 Generally, detection performance heavily depends on the discriminative feature representation between target and background.\n\u25c6 Existing methods typically treat deblurring as a preprocessing step focused on visual quality, while neglecting the enhancement of task-relevant features crucial for detection.|\n",
    "2511.14351": "|2025-11-18|Hubble Space Telescope proper motions of Large Magellanic Cloud star clusters -- II. Kinematic structure of young and intermediate-age clusters|F. Niederhofer\u7b49|[2511.14351](http://arxiv.org/pdf/2511.14351)|\u65e0|\u25c6 In this paper, we explore the kinematic properties of a sample of 19 young (<1 Gyr) and intermediate-age (1-2.5 Gyr) massive star clusters within the Large Magellanic Cloud (LMC).\n\u25c6 We analyse the proper motions of the clusters, which have been measured based on multi-epoch Hubble Space Telescope (HST) observations.\n\u25c6 Additionally, we infer from the HST data homogeneous and robust estimates for the distances, ages and metallicities of the clusters.|\n",
    "2511.14300": "|2025-11-18|Vortex stability in pseudo-Hermitian theories|R. A. Battye\u7b49|[2511.14300](http://arxiv.org/pdf/2511.14300)|\u65e0|\u25c6 Pseudo-Hermitian (including $\\mathcal{PT}$-symmetric) field theories support phenomenology that cannot be replicated in standard Hermitian theories.\n\u25c6 We describe a concrete example in which the vortex solutions that are realised in a prototypical pseudo-Hermitian field theory exhibit a novel metastability, despite the model parameters residing within the naively stable regime of exact antilinear symmetry of the vacuum theory.\n\u25c6 This instability is identified analytically and confirmed through numerical simulations, and it arises from the small breaking of the underlying antilinear symmetry of the pseudo-Hermitian theory due to the presence of the topological defect.|\n",
    "2511.14297": "|2025-11-18|Model-Based Clustering of Football Event Sequences: A Marked Spatio-Temporal Point Process Mixture Approach|Koffi Amezouwui\u7b49|[2511.14297](http://arxiv.org/pdf/2511.14297)|\u65e0|\u25c6 We propose a novel mixture model for football event data that clusters entire possessions to reveal their temporal, sequential, and spatial structure.\n\u25c6 Each mixture component models possessions as marked spatio-temporal point processes: event types follow a finite Markov chain with an absorbing state for ball loss, event times follow a conditional Gamma process to account for dispersion, and spatial locations evolve via truncated Brownian motion.\n\u25c6 To aid interpretation, we derive summary indicators from model parameters capturing possession speed, number of events, and spatial dynamics.|\n",
    "2511.14243": "|2025-11-18|Newborn jet in the symbiotic system R Aquarii|T. Liimets\u7b49|[2511.14243](http://arxiv.org/pdf/2511.14243)|\u65e0|\u25c6 R Aquarii (R Aqr) is a well-known symbiotic binary that has attracted renewed interest during its recent periastron passage, an event that occurs only once every about 40 years.\n\u25c6 This passage marks the first to be observed with modern, state-of-the-art instruments.\n\u25c6 We investigate the inner, sub-arcsecond active region of R Aqr during this recent periastron passage, with the goal of gaining insight into the jet-launching mechanisms at work in this system.|\n",
    "2511.14205": "|2025-11-18|FreeMusco: Motion-Free Learning of Latent Control for Morphology-Adaptive Locomotion in Musculoskeletal Characters|Minkwan Kim\u7b49|[2511.14205](http://arxiv.org/pdf/2511.14205)|\u65e0|\u25c6 We propose FreeMusco, a motion-free framework that jointly learns latent representations and control policies for musculoskeletal characters.\n\u25c6 By leveraging the musculoskeletal model as a strong prior, our method enables energy-aware and morphology-adaptive locomotion to emerge without motion data.\n\u25c6 The framework generalizes across human, non-human, and synthetic morphologies, where distinct energy-efficient strategies naturally appear--for example, quadrupedal gaits in Chimanoid versus bipedal gaits in Humanoid.|\n",
    "2511.14148": "|2025-11-18|AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models|Yuhua Jiang\u7b49|[2511.14148](http://arxiv.org/pdf/2511.14148)|\u65e0|\u25c6 Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots.\n\u25c6 However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM).\n\u25c6 Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure.|\n",
    "2511.13988": "|2025-11-17|B2F: End-to-End Body-to-Face Motion Generation with Style Reference|Bokyung Jang\u7b49|[2511.13988](http://arxiv.org/pdf/2511.13988)|\u65e0|\u25c6 Human motion naturally integrates body movements and facial expressions, forming a unified perception.\n\u25c6 If a virtual character's facial expression does not align well with its body movements, it may weaken the perception of the character as a cohesive whole.\n\u25c6 Motivated by this, we propose B2F, a model that generates facial motions aligned with body movements.|\n",
    "2511.13980": "|2025-11-17|Enabling Real-Time Volumetric Imaging in Interventional Radiology Suits via a Deep Learning Framework Robust to C-arm Tilt|Fawazilla Utomo\u7b49|[2511.13980](http://arxiv.org/pdf/2511.13980)|\u65e0|\u25c6 Contemporary interventional imaging lacks the real-time 3D guidance needed for the precise localization of mobile thoracic targets.\n\u25c6 While Cone-Beam CT (CBCT) provides 3D data, it is often too slow for dynamic motion tracking.\n\u25c6 Deep learning frameworks that reconstruct 3D volumes from sparse 2D projections offer a promising solution, but their performance under the geometrically complex, non-zero tilt acquisitions common in interventional radiology is unknown.|\n",
    "2511.15645": "|2025-11-19|MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling|Shanshan Zhang|[2511.15645](http://arxiv.org/pdf/2511.15645)|\u65e0|\u25c6 Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications.\n\u25c6 Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted.\n\u25c6 However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO.|\n",
    "2511.15365": "|2025-11-19|Covariant Measures of Non-Markovianity in Curved Spacetime|Tushar Waghmare|[2511.15365](http://arxiv.org/pdf/2511.15365)|\u65e0|\u25c6 Standard measures of quantum non-Markovianity are usually defined in terms of dynamical maps on a preferred time foliation and therefore do not extend straightforwardly to curved spacetimes, where no global time coordinate exists and causal structure is primary.\n\u25c6 We develop a covariant framework for open quantum dynamics along arbitrary timelike worldlines by building multi-time quantum processes (process tensors) from overlapping causal diamonds.\n\u25c6 For an Unruh--DeWitt detector weakly coupled to a scalar field in a Hadamard state, we define a foliation-independent measure of non-Markovianity as the operational distance between the physical process tensor and the convex set of Markovian (CP-divisible) processes.|\n",
    "2511.15159": "|2025-11-19|Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation|Firdavs Nasriddinov\u7b49|[2511.15159](http://arxiv.org/pdf/2511.15159)|\u65e0|\u25c6 High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition.\n\u25c6 Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations.\n\u25c6 We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation.|\n",
    "2511.15153": "|2025-11-19|SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection|Chun-Jung Lin\u7b49|[2511.15153](http://arxiv.org/pdf/2511.15153)|\u65e0|\u25c6 Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation.\n\u25c6 However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations.\n\u25c6 While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection.|\n",
    "2511.14848": "|2025-11-18|Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video|Yarin Bekor\u7b49|[2511.14848](http://arxiv.org/pdf/2511.14848)|\u65e0|\u25c6 We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video.\n\u25c6 Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence.\n\u25c6 Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction.|\n",
    "2511.16662": "|2025-11-20|TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing|Eddie Pokming Sheung\u7b49|[2511.16662](http://arxiv.org/pdf/2511.16662)|\u65e0|\u25c6 With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge.\n\u25c6 Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics.\n\u25c6 To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars.|\n",
    "2511.16484": "|2025-11-20|Flow and Depth Assisted Video Prediction with Latent Transformer|Eliyas Suleyman\u7b49|[2511.16484](http://arxiv.org/pdf/2511.16484)|\u65e0|\u25c6 Video prediction is a fundamental task for various downstream applications, including robotics and world modeling.\n\u25c6 Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction.\n\u25c6 We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion.|\n",
    "2511.16159": "|2025-11-20|Two Epochs of VLBI Observations of 8 KISSR Seyfert & LINER Galaxies: Suggestions of Fast and Filamentary Outflows|Preeti Kharb\u7b49|[2511.16159](http://arxiv.org/pdf/2511.16159)|\u65e0|\u25c6 We present here the results from a second epoch of phase-referenced VLBA observations of 8 Seyfert and LINER galaxies from the KISSR sample.\n\u25c6 These sources were chosen based on the presence of double peaks or asymmetries in their emission lines as observed in SDSS spectra.\n\u25c6 Parsec-scale radio emission is detected in 7 of the 8 sources in the second epoch.|\n",
    "2511.16978": "|2025-11-21|TRAO Survey of the Nearby Filamentary Molecular Clouds, the Universal Nursery of Stars (TRAO-FUNS). IV. Filaments and Dense Cores in the W40 and Serpens South Regions of Aquila|Satyajeet Moharana\u7b49|[2511.16978](http://arxiv.org/pdf/2511.16978)|\u65e0|\u25c6 We present the results of molecular line observations toward the W40 and Serpens South regions of the Aquila molecular cloud complex, conducted as part of the TRAO-FUNS project to investigate the role of filamentary structures in the formation of dense cores and stars in molecular clouds.\n\u25c6 We performed a Gaussian decomposition of the C$^{18}$O spectra to disentangle multiple velocity components along the line-of-sight and a `Friends-of-Friends' algorithm on these decomposed components to identify 24 velocity-coherent filaments in the observed region.\n\u25c6 The `FellWalker' algorithm is applied on the N$_{2}$H$^{+}$ integrated intensity map to identify the dense cores embedded within the filaments.|\n",
    "2511.16966": "|2025-11-21|One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements|Yiheng Bian\u7b49|[2511.16966](http://arxiv.org/pdf/2511.16966)|\u65e0|\u25c6 Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered.\n\u25c6 This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk.\n\u25c6 We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction.|\n",
    "2511.19275": "|2025-11-24|Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization|Ellie L. Zhang\u7b49|[2511.19275](http://arxiv.org/pdf/2511.19275)|\u65e0|\u25c6 Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design.\n\u25c6 Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments.\n\u25c6 Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs.|\n",
    "2511.19185": "|2025-11-24|A Deep-Learning-Based Framework for Focal Mechanism Determination and Its Application to the 2022 Luding Earthquake Sequence|Ziye Yu\u7b49|[2511.19185](http://arxiv.org/pdf/2511.19185)|\u65e0|\u25c6 P-wave first-motion polarity plays an important role in resolving focal mechanisms of small to moderate earthquakes (M <= 4.5).\n\u25c6 High-quality focal mechanism solutions for abundant small events can greatly improve our understanding of regional tectonics, fault geometries, and stress-field characteristics.\n\u25c6 In this study, we develop an automated focal mechanism determination framework that integrates deep neural networks with P-wave first-motion polarity observations, and apply it to the 2022 Luding earthquake sequence.|\n",
    "2511.19172": "|2025-11-24|MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes|Kehua Chen\u7b49|[2511.19172](http://arxiv.org/pdf/2511.19172)|\u65e0|\u25c6 Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction.\n\u25c6 However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge.\n\u25c6 To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments.|\n",
    "2511.18975": "|2025-11-24|The variability of blazars throughout the electromagnetic spectrum|Claudia M. Raiteri|[2511.18975](http://arxiv.org/pdf/2511.18975)|\u65e0|\u25c6 With their jet pointing towards us, blazars are ideal tools to study the physics and structure of extragalactic jets.\n\u25c6 Their powerful jets are cosmic particle accelerators and are alleged to be one of the production sites of the high-energy neutrinos detected by the IceCube Observatory.\n\u25c6 Doppler beaming of the jet nonthermal radiation increases blazar brightness, blue-shifts their emission, and shortens their variability time scales, which are observed to range from years down to minutes.|\n",
    "2511.18886": "|2025-11-24|MagicWorld: Interactive Geometry-driven Video World Exploration|Guangyuan Li\u7b49|[2511.18886](http://arxiv.org/pdf/2511.18886)|\u65e0|\u25c6 Recent interactive video world model methods generate scene evolution conditioned on user instructions.\n\u25c6 Although they achieve impressive results, two key limitations remain.\n\u25c6 First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes.|\n",
    "2511.18786": "|2025-11-24|STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution|Junyang Chen\u7b49|[2511.18786](http://arxiv.org/pdf/2511.18786)|\u65e0|\u25c6 We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions.\n\u25c6 The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation.\n\u25c6 To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions.|\n",
    "2511.18745": "|2025-11-24|On the role of fractional Brownian motion in models of chemotaxis and stochastic gradient ascent|Gustavo Cornejo-Olea\u7b49|[2511.18745](http://arxiv.org/pdf/2511.18745)|\u65e0|\u25c6 Cell migration often exhibits long-range temporal correlations and anomalous diffusion, even in the absence of external guidance cues such as chemical gradients or topographical constraints.\n\u25c6 These observations raise a fundamental question: do such correlations simply reflect internal cellular processes, or do they enhance a cell's ability to navigate complex environments?\n\u25c6 In this work, we explore how temporally correlated noise (modeled using fractional Brownian motion) influences chemotactic search dynamics.|\n",
    "2511.18559": "|2025-11-23|C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction|Kuan Wei Huang\u7b49|[2511.18559](http://arxiv.org/pdf/2511.18559)|\u65e0|\u25c6 Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos.\n\u25c6 However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs.\n\u25c6 ground) or modalities (e.g., photos vs.|\n",
    "2511.18549": "|2025-11-23|Non-Symplectic Deformations of Geometric Quantisation|Kerr Maxwell|[2511.18549](http://arxiv.org/pdf/2511.18549)|\u65e0|\u25c6 We introduce the notion of geometric pseudo-quantisation based on geometric quantisation with a weakened curvature condition.\n\u25c6 We show how such a structure arises naturally from simple deformations of the symplectic structure and pullbacks of prequantum data by non-symplectic diffeomorphisms.\n\u25c6 Our main result is deriving the equations of motion for some simple pseudo-quantisations.|\n",
    "2511.18537": "|2025-11-23|Zero-Shot Video Deraining with Video Diffusion Models|Tuomas Varanka\u7b49|[2511.18537](http://arxiv.org/pdf/2511.18537)|\u65e0|\u25c6 Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion.\n\u25c6 Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases.\n\u25c6 In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities.|\n",
    "2511.20647": "|2025-11-25|Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization|Tahira Kazimi\u7b49|[2511.20647](http://arxiv.org/pdf/2511.20647)|\u65e0|\u25c6 While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt.\n\u25c6 We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt.\n\u25c6 To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations.|\n",
    "2511.20469": "|2025-11-25|Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features|Ben Hamscher\u7b49|[2511.20469](http://arxiv.org/pdf/2511.20469)|\u65e0|\u25c6 Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories.\n\u25c6 Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns.\n\u25c6 This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos.|\n",
    "2511.20343": "|2025-11-25|AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend|Hengyi Wang\u7b49|[2511.20343](http://arxiv.org/pdf/2511.20343)|\u65e0|\u25c6 We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks.\n\u25c6 The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness.\n\u25c6 Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization.|\n",
    "2511.20287": "|2025-11-25|Stochastic Dynamics of Skyrmions on a Racetrack: Impact of Equilibrium and Nonequilibrium Noise|Anton V. Hlushchenko\u7b49|[2511.20287](http://arxiv.org/pdf/2511.20287)|\u65e0|\u25c6 Current-driven motion of domain walls and skyrmions is central to the operation of non-volatile magnetic memory devices.\n\u25c6 Racetrack memory requires current densities high enough to generate velocities above 50 m/s, but such conditions also enhance spin-current noise.\n\u25c6 We develop a theoretical framework based on the stochastic Thiele equation to analyze the effects of equilibrium (thermal) and nonequilibrium (spin-current) fluctuations on skyrmion dynamics.|\n",
    "2511.20228": "|2025-11-25|Numerical Simulation of the Cleaning Process of Microchannel by an External Flow|Boris S. Maryshev\u7b49|[2511.20228](http://arxiv.org/pdf/2511.20228)|\u65e0|\u25c6 This paper describes the problem of drift of solid non-interacting particles in a microchannel, which can stick to its walls under the action of the van der Waals forces and break away from the wall due to thermal noise and viscous stresses arising from the flow.\n\u25c6 The pressure drop is given between the channel inlet and outlet.\n\u25c6 At the initial moment of time, the channel walls are contaminated with adhered particles, i.e.|\n",
    "2511.19854": "|2025-11-25|STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction|Jiankuo Zhao\u7b49|[2511.19854](http://arxiv.org/pdf/2511.19854)|\u65e0|\u25c6 Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task.\n\u25c6 Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness.\n\u25c6 Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids).|\n",
    "2511.19687": "|2025-11-24|Infrared absorption spectroscopy of a single polyatomic molecular ion|Zhenlin Wu\u7b49|[2511.19687](http://arxiv.org/pdf/2511.19687)|\u65e0|\u25c6 Absorption spectroscopy is a fundamental tool for probing molecular structure.\n\u25c6 However, performing absorption spectroscopy on individual molecules is challenging due to the low signal-to-noise ratio.\n\u25c6 Here, we report on a nondestructive absorption spectroscopy on a mid-infrared vibrational transition in a single molecular ion that is co-trapped with an atomic ion.|\n",
    "2511.19685": "|2025-11-24|KIC 5623923: A Faint Eclipsing Binary Consisting of $\u03b4$ Scuti pulsations|Tao-Zhi Yang\u7b49|[2511.19685](http://arxiv.org/pdf/2511.19685)|\u65e0|\u25c6 In this paper, we present a detailed analysis of the light variation of KIC 5623923 using high-precision time-series data from the $Kepler$ mission.\n\u25c6 The analysis reveals this target is an eclipsing binary system with $\u03b4$ Scuti type pulsations from the primary component, rather than from the secondary as previously reported.\n\u25c6 The frequency analysis of three short-cadence data reveals 41 significant frequencies, including the orbital frequency ($f_{orb}$ = 0.827198 d$^{-1}$) due to orbital motion from binary system and the pulsational frequencies.|\n",
    "2511.21690": "|2025-11-26|TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos|Seungjae Lee\u7b49|[2511.21690](http://arxiv.org/pdf/2511.21690)|\u65e0|\u25c6 Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging.\n\u25c6 While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use.\n\u25c6 We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos.|\n",
    "2511.21565": "|2025-11-26|UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes|Kang Du\u7b49|[2511.21565](http://arxiv.org/pdf/2511.21565)|\u65e0|\u25c6 Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction.\n\u25c6 Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting.\n\u25c6 This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable.|\n",
    "2511.21428": "|2025-11-26|From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings|Jiajie Zhang\u7b49|[2511.21428](http://arxiv.org/pdf/2511.21428)|\u65e0|\u25c6 We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training.\n\u25c6 Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel \"Latent Action Energy\" metric to discover and segment semantically coherent action primitives.\n\u25c6 The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training.|\n",
    "2511.21132": "|2025-11-26|DeepRFTv2: Kernel-level Learning for Image Deblurring|Xintian Mao\u7b49|[2511.21132](http://arxiv.org/pdf/2511.21132)|\u65e0|\u25c6 It is well-known that if a network aims to learn how to deblur, it should understand the blur process.\n\u25c6 Blurring is naturally caused by the convolution of the sharp image with the blur kernel.\n\u25c6 Thus, allowing the network to learn the blur process in the kernel-level can significantly improve the image deblurring performance.|\n",
    "2511.20788": "|2025-11-25|Hund-projected Kanamori model: an effective description of Hund's metals near the Mott insulating regime|Johan Carlstr\u00f6m|[2511.20788](http://arxiv.org/pdf/2511.20788)|\u65e0|\u25c6 Hund's coupling plays a decisive role in shaping electron correlations of multi-orbital systems, giving rise to a class of materials--Hund's metals--that combine local-moment physics with metallic transport.\n\u25c6 Here we derive an effective low-energy description of such a system near the Mott insulating regime, starting from the multi-orbital Hubbard-Kanamori Hamiltonian and projecting onto the high-spin manifold favored by Hund's first rule.\n\u25c6 The resulting Hund-projected Kanamori model captures the interplay between carrier motion and magnetic correlations in the presence of strong Hund's coupling.|\n",
    "2511.20755": "|2025-11-25|From Observations to Simulations: A Neural-Network Approach to Intracluster Medium Kinematics|E. Gatuzz\u7b49|[2511.20755](http://arxiv.org/pdf/2511.20755)|\u65e0|\u25c6 We present a systematic comparison between {\\it XMM-Newton} velocity maps of the Virgo, Centaurus, Ophiuchus and A3266 clusters and synthetic velocity maps generated from the Illustris TNG-300 simulations.\n\u25c6 Our goal is to constrain the physical conditions and dynamical states of the intracluster medium (ICM) through a data-driven approach.\n\u25c6 We employ a Siamese Convolutional Neural Network (CNN) designed to identify the most analogous simulated cluster to each observed system based on the morphology of their line-of-sight velocity maps.|\n",
    "2511.23428": "|2025-11-28|DisMo: Disentangled Motion Representations for Open-World Motion Transfer|Thomas Ressler-Antal\u7b49|[2511.23428](http://arxiv.org/pdf/2511.23428)|\u65e0|\u25c6 Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames.\n\u25c6 However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators.\n\u25c6 To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective.|\n",
    "2511.23407": "|2025-11-28|From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products|Jan Baumg\u00e4rtner\u7b49|[2511.23407](http://arxiv.org/pdf/2511.23407)|\u65e0|\u25c6 To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal.\n\u25c6 Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs.\n\u25c6 We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state.|\n",
    "2511.23267": "|2025-11-28|Simulating AGN feedback in galaxy clusters with pre-existing turbulence|Jia-Lun Li\u7b49|[2511.23267](http://arxiv.org/pdf/2511.23267)|\u65e0|\u25c6 Feedback from active galactic nuclei (AGN) is believed to play a significant role in suppressing cooling flows in cool-core (CC) clusters.\n\u25c6 Turbulence in the intracluster medium (ICM), which may be induced by AGN activity or pre-existing motions, has been proposed as a potential heating mechanism based on analysis of Chandra X-ray surface brightness fluctuations.\n\u25c6 However, subsequent simulation results have found the subdominant role of turbulence in heating the ICM.|\n",
    "2511.23045": "|2025-11-28|Nonequilibrium dynamics of magnetic hopfions driven by spin-orbit torque|Shoya Kasai\u7b49|[2511.23045](http://arxiv.org/pdf/2511.23045)|\u65e0|\u25c6 Hopfions--three-dimensional topological solitons with knotted spin texture--have recently garnered attention in topological magnetism due to their unique topology characterized by the Hopf number $H$, a topological invariant derived from knot theory.\n\u25c6 In contrast to two-dimensional skyrmions, which are typically limited to small topological invariants, i.e., skyrmion numbers, hopfions can, in principle, be stabilized with arbitrary Hopf numbers.\n\u25c6 However, the nonequilibrium dynamics, especially interconversion between different Hopf numbers, remain poorly understood.|\n",
    "2511.22857": "|2025-11-28|GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera|Jiaye Wu\u7b49|[2511.22857](http://arxiv.org/pdf/2511.22857)|\u65e0|\u25c6 Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects.\n\u25c6 While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion.\n\u25c6 However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle.|\n",
    "2511.22815": "|2025-11-28|Captain Safari: A World Engine|Yu-Cheng Chou\u7b49|[2511.22815](http://arxiv.org/pdf/2511.22815)|\u65e0|\u25c6 World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion.\n\u25c6 However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion.\n\u25c6 To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory.|\n",
    "2511.22012": "|2025-11-27|An Efficient and Accurate Surrogate Modeling of Flapping Dynamics in Inverted Elastic Foils using Hypergraph Neural Networks|Aarshana R. Parekh\u7b49|[2511.22012](http://arxiv.org/pdf/2511.22012)|\u65e0|\u25c6 Cantilevered elastic foils can undergo self-induced, large-amplitude flapping when subject to fluid flow, a widely observed phenomenon of fluid-structure interaction, from fluttering leaves or the movement of fish fins.\n\u25c6 When harnessed in steady currents, these oscillations enable the extraction of kinetic energy from the flow.\n\u25c6 However, accurately predicting these dynamics requires high-fidelity simulations that are prohibitively expensive to perform across the broad configuration space needed for design optimization.|\n",
    "2511.21887": "|2025-11-26|UniArt: Unified 3D Representation for Generating 3D Articulated Objects with Open-Set Articulation|Bu Jin\u7b49|[2511.21887](http://arxiv.org/pdf/2511.21887)|\u65e0|\u25c6 Articulated 3D objects play a vital role in realistic simulation and embodied robotics, yet manually constructing such assets remains costly and difficult to scale.\n\u25c6 In this paper, we present UniArt, a diffusion-based framework that directly synthesizes fully articulated 3D objects from a single image in an end-to-end manner.\n\u25c6 Unlike prior multi-stage techniques, UniArt establishes a unified latent representation that jointly encodes geometry, texture, part segmentation, and kinematic parameters.|\n",
    "2512.02007": "|2025-12-01|The Astrometric Resoeccentric Degeneracy: Eccentric Single Planets Mimic 2:1 Resonant Planet Pairs in Astrometry|Daniel A. Yahalomi\u7b49|[2512.02007](http://arxiv.org/pdf/2512.02007)|\u65e0|\u25c6 Detections of long-period giant exoplanets will expand dramatically with Gaia Data Release 4 (DR4), but interpreting these signals will require care.\n\u25c6 We derive the astrometric resoeccentric degeneracy: an astrometric analogue of the well-known radial velocity degeneracy in which a single eccentric planet can mimic two circular planets near a 2:1 period ratio.\n\u25c6 To first order in eccentricity, the sky-projected motion of a single eccentric orbit decomposes into a fundamental mode and first harmonic with an amplitude proportional to that eccentricity.|\n",
    "2512.01886": "|2025-12-01|Active chromospheric fibril singularity: Coordinated observations from Solar Orbiter, SST, and IRIS|Reetika Joshi\u7b49|[2512.01886](http://arxiv.org/pdf/2512.01886)|\u65e0|\u25c6 The fine structures of the solar chromosphere, driven by photospheric motions, play a crucial role in the dynamics of solar magnetic fields.\n\u25c6 Many have been already identified such as fibrils, filament feet, and arch filament systems.\n\u25c6 Still, high resolution observations show a wealth of structures that remain elusive.|\n",
    "2512.01629": "|2025-12-02|SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge|Yumeng He\u7b49|[2512.01629](http://arxiv.org/pdf/2512.01629)|\u65e0|\u25c6 Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures.\n\u25c6 We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image.\n\u25c6 Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images.|\n",
    "2512.01561": "|2025-12-01|X-CME: From In Situ Flux-Rope Reconstruction to CME Propagation Forecasting|Marti Masso Moreno\u7b49|[2512.01561](http://arxiv.org/pdf/2512.01561)|\u65e0|\u25c6 Accurate forecasts of Coronal Mass Ejection (CME) arrival times and impact geometry remain a major challenge for space-weather operations.\n\u25c6 Coronagraph-based techniques typically achieve mean absolute errors of order ten hours, while in situ measurements at L1 provide excellent magnetic-field information but only tens of minutes of warning.\n\u25c6 In this work we introduce X-CME, a framework that links in situ flux-rope reconstructions at intermediate heliocentric distances with a physics-based CME propagation model.|\n",
    "2512.01342": "|2025-12-01|InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision|Chenting Wang\u7b49|[2512.01342](http://arxiv.org/pdf/2512.01342)|\u65e0|\u25c6 Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues.\n\u25c6 In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks.\n\u25c6 We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning.|\n",
    "2512.01108": "|2025-11-30|Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception|Gabriel Olin\u7b49|[2512.01108](http://arxiv.org/pdf/2512.01108)|\u65e0|\u25c6 Intercepting fast moving objects, by its very nature, is challenging because of its tight time constraints.\n\u25c6 This problem becomes further complicated in the presence of sensor noise because noisy sensors provide, at best, incomplete information, which results in a distribution over target states to be intercepted.\n\u25c6 Since time is of the essence, to hit the target, the planner must begin directing the interceptor, in this case a robot arm, while still receiving information.|\n",
    "2512.01095": "|2025-11-30|CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions|Simon Kohaut\u7b49|[2512.01095](http://arxiv.org/pdf/2512.01095)|\u65e0|\u25c6 We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions.\n\u25c6 CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes.\n\u25c6 CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition.|\n",
    "2512.01075": "|2025-11-30|Euclid Structural-Thermal-Optical Performance|Euclid Collaboration\u7b49|[2512.01075](http://arxiv.org/pdf/2512.01075)|\u65e0|\u25c6 The Euclid system performance is defined in terms of image quality metrics tuned to the weak gravitational lensing (WL) cosmological probe.\n\u25c6 WL induces stringent requirements on the shape and stability of the VIS instrument system point spread function (PSF).\n\u25c6 The PSF is affected by error contributions from the telescope, the focal plane and image motion, and is controlled by a global error budget with error allocations to each contributor.|\n",
    "2512.00710": "|2025-11-30|Evolution of Flare Ribbon Bead-like Structures in a Solar Flare|Ryan J. French\u7b49|[2512.00710](http://arxiv.org/pdf/2512.00710)|\u65e0|\u25c6 We present fast cadence and high resolution observations of flare ribbons from the Solar Orbiter Extreme Ultraviolet Imager (EUI).\n\u25c6 Utilizing the short-exposure observations from the EUI High Resolution Imager in EUV (HRIEUV), we find small-scale blob/bead-like kernel structures propagating within a hook at the end of a flare ribbon, during the impulsive phase of a C9.9-class solar flare.\n\u25c6 These bead structures are dynamic, with well-resolved spatial separations as low as ~420-840 kilometers (3-6 pixels) - below the observable limit of full-disk solar imagers.|\n",
    "2512.00637": "|2025-11-29|Strings at the Tip of the Cone and Black Hole Entropy From the Worldsheet: Part I|Amr Ahmadain\u7b49|[2512.00637](http://arxiv.org/pdf/2512.00637)|\u65e0|\u25c6 We study the nonlinear sigma model (NLSM) worldsheet action describing the motion of closed bosonic strings in the target space of a two-dimensional (2D) flat cone in polar coordinates.\n\u25c6 We calculate the cylinder partition function.\n\u25c6 We first place the cylindrical worldsheet on a rectangular lattice before taking the continuum limit.|\n",
    "2512.03000": "|2025-12-03|DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling|Kairun Wen\u7b49|[2512.03000](http://arxiv.org/pdf/2512.03000)|\u65e0|\u25c6 Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities.\n\u25c6 However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet.\n\u25c6 To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video.|\n",
    "2512.02922": "|2025-12-02|Asymptotics for additive functionals of particle systems via Stein's method|Arturo Jaramillo\u7b49|[2512.02922](http://arxiv.org/pdf/2512.02922)|\u65e0|\u25c6 We consider additive functionals of systems of random measures whose initial configuration is given by a Poisson point process, and whose individual components evolve according to arbitrary Markovian or non-Markovian measure valued dynamics, with no structural assumptions beyond basic moment bounds.\n\u25c6 In this setting and under adequate conditions, we establish a general third moment theorem for the normalized functionals.\n\u25c6 Building on this result, we obtain the first quantitative bounds in the Wasserstein distance for a variety of moving-measure models initialized by Poisson-driven clouds of points, turning qualitative central limit theorems into explicit rates of convergence.|\n",
    "2512.02903": "|2025-12-02|Symmetry transformation group arising from the Laplace-Runge-Lenz vector|Stephen C. Anco\u7b49|[2512.02903](http://arxiv.org/pdf/2512.02903)|\u65e0|\u25c6 The Kepler problem in classical mechanics exhibits a rich structure of conserved quantities, highlighted by the Laplace--Runge--Lenz (LRL) vector.\n\u25c6 Through Noether's theorem in reverse, the LRL vector gives rise to a corresponding infinitesimal dynamical symmetry on the kinematical variables, which is well known in the literature.\n\u25c6 However, the physically relevant part of the LRL vector is its direction angle in the plane of motion (since its magnitude is just a function of energy and angular momentum).|\n",
    "2512.02453": "|2025-12-02|ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation|Kerui Chen\u7b49|[2512.02453](http://arxiv.org/pdf/2512.02453)|\u65e0|\u25c6 Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion.\n\u25c6 However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge.\n\u25c6 In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation.|\n",
    "2512.02375": "|2025-12-02|On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning|Liyuan Lou\u7b49|[2512.02375](http://arxiv.org/pdf/2512.02375)|\u65e0|\u25c6 Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance.\n\u25c6 However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area.\n\u25c6 This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time.|\n",
    "2512.02175": "|2025-12-01|Sampling on Metric Graphs|Rajat Vadiraj Dwaraknath\u7b49|[2512.02175](http://arxiv.org/pdf/2512.02175)|\u65e0|\u25c6 Metric graphs are structures obtained by associating edges in a standard graph with segments of the real line and gluing these segments at the vertices of the graph.\n\u25c6 The resulting structure has a natural metric that allows for the study of differential operators and stochastic processes on the graph.\n\u25c6 Brownian motions in these domains have been extensively studied theoretically using their generators.|\n",
    "2512.04012": "|2025-12-03|Emergent Outlier View Rejection in Visual Geometry Grounded Transformers|Jisang Han\u7b49|[2512.04012](http://arxiv.org/pdf/2512.04012)|\u65e0|\u25c6 Reliable 3D reconstruction from in-the-wild image collections is often hindered by \"noisy\" images-irrelevant inputs with little or no view overlap with others.\n\u25c6 While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions.\n\u25c6 In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images.|\n",
    "2512.03981": "|2025-12-03|DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment|Sheng-Hao Liao\u7b49|[2512.03981](http://arxiv.org/pdf/2512.03981)|\u65e0|\u25c6 Drag-based image editing using generative models provides intuitive control over image structures.\n\u25c6 However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision.\n\u25c6 Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts.|\n",
    "2512.03969": "|2025-12-03|Vortex Dynamics from Burst-and-Coast Motion of Anguilliform and Carangiform Swimmers|Zahra Maleksabet\u7b49|[2512.03969](http://arxiv.org/pdf/2512.03969)|\u65e0|\u25c6 Fish perform various propulsive maneuvers while swimming by generating traveling waves along their bodies and producing thrust through tail strokes.\n\u25c6 Anguilliform swimmers spread motion along the body, while carangiform swimmers' motion is more prominent near their tails.\n\u25c6 Many species also switch between continuous undulation and intermittent swimming, such as burst-and-coast maneuver, which can save energy but can also change the wake structure and hydrodynamic forces.|\n",
    "2512.03717": "|2025-12-03|Geometrical structure of the Wigner flow information quantifiers and hyperbolic stability in the phase-space framework|Alex E. Bernardini|[2512.03717](http://arxiv.org/pdf/2512.03717)|\u65e0|\u25c6 Quantifiers of stationarity, classicality, purity and vorticity are derived from phase-space differential geometrical structures within the Weyl-Wigner framework, after which they are related to the hyperbolic stability of classical and quantum-modified Hamiltonian (non-linear) equations of motion.\n\u25c6 By examining the equilibrium regime produced by such an autonomous system of ordinary differential equations, a correspondence between Wigner flow properties and hyperbolic stability boundaries in the phase-space is identified.\n\u25c6 Explicit analytical expressions for equilibrium-stability parameters are obtained for quantum Gaussian ensembles, wherein information quantifiers driven by Wigner currents are identified.|\n",
    "2512.03691": "|2025-12-03|Terahertz light driven coherent excitation of a zone-folded Raman-active phonon mode in the Spin-Ladder System $\u03b1'$-NaV$_2$O$_5$|Flavio Giorgianni\u7b49|[2512.03691](http://arxiv.org/pdf/2512.03691)|\u65e0|\u25c6 We investigate the out-of-equilibrium lattice dynamics in the spin-ladder system $\u03b1'$-NaV$_2$O$_5$ using intense terahertz (THz) pump and near-infrared (NIR) probe spectroscopy.\n\u25c6 When quasi-single-cycle THz pulses interact with $\u03b1'$-NaV$_2$O$_5$ in its low-temperature, dimerized charge-ordered phase, they induce coherent oscillations in the time domain at the zone-folded Raman-active phonon frequency of 1.85 THz.\n\u25c6 By combining pump-probe measurements with lattice dynamics modeling based on equation-of-motion approach, we propose that these oscillations arise from a nonlinear coupling between Raman-active and infrared (IR)-active phonon modes, with the latter being resonantly excited by the THz pulses.|\n",
    "2512.03619": "|2025-12-03|LAMP: Language-Assisted Motion Planning for Controllable Video Generation|Muhammed Burak Kizil\u7b49|[2512.03619](http://arxiv.org/pdf/2512.03619)|\u65e0|\u25c6 Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion.\n\u25c6 Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited.\n\u25c6 We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras.|\n",
    "2512.03301": "|2025-12-02|Comparing Unsupervised and Supervised Semantic Speech Tokens: A Case Study of Child ASR|Mohan Shi\u7b49|[2512.03301](http://arxiv.org/pdf/2512.03301)|\u65e0|\u25c6 Discrete speech tokens have gained attention for their storage efficiency and integration with Large Language Models (LLMs).\n\u25c6 They are commonly categorized into acoustic and semantic tokens, with the latter being more advantageous for Automatic Speech Recognition (ASR).\n\u25c6 Traditionally, unsupervised K-means clustering has been used to extract semantic speech tokens from Speech Foundation Models (SFMs).|\n",
    "2512.03168": "|2025-12-02|The Origins of the Bulk flow|Richard Watkins\u7b49|[2512.03168](http://arxiv.org/pdf/2512.03168)|\u65e0|\u25c6 We analyze the origin of the large scale bulk flow using the CosmicFlows 4 (CF4) peculiar velocity catalog.\n\u25c6 We decompose the observed motions into internal components, generated by mass fluctuations within 200Mpc/h, and external ones arising from structures beyond this volume.\n\u25c6 A weighted average technique is developed to test the model's self consistency while minimizing the impact of non Gaussian distance errors.|\n",
    "2512.05114": "|2025-12-04|Deep infant brain segmentation from multi-contrast MRI|Malte Hoffmann\u7b49|[2512.05114](http://arxiv.org/pdf/2512.05114)|\u65e0|\u25c6 Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures.\n\u25c6 However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints.\n\u25c6 Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts.|\n",
    "2512.05049": "|2025-12-04|QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory|Yu-Chao Hsu\u7b49|[2512.05049](http://arxiv.org/pdf/2512.05049)|\u65e0|\u25c6 Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate.\n\u25c6 However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity.\n\u25c6 In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs.|\n",
    "2512.05040": "|2025-12-04|Geometric Data Science|Olga D Anosova\u7b49|[2512.05040](http://arxiv.org/pdf/2512.05040)|\u65e0|\u25c6 This book introduces the new research area of Geometric Data Science, where data can represent any real objects through geometric measurements.\n\u25c6 The first part of the book focuses on finite point sets.\n\u25c6 The most important result is a complete and continuous classification of all finite clouds of unordered points under rigid motion in any Euclidean space.|\n",
    "2512.04972": "|2025-12-04|Internal superfluid response and torque evolution in the giant glitch of PSR J1718-3718|Peng Liu\u7b49|[2512.04972](http://arxiv.org/pdf/2512.04972)|\u65e0|\u25c6 We investigate the post-glitch rotational evolution of pulsars by analyzing the 2007 giant glitch of PSR J1718$-$3718 using a vortex creep model that incorporates both inward and outward nonlinear vortex motion, along with a time-varying external torque.\n\u25c6 A comprehensive fitting framework is developed, constrained by prior knowledge of moment of inertia participation from previous glitch studies.\n\u25c6 We apply a Markov Chain Monte Carlo approach to quantify uncertainties and parameter correlations.|\n",
    "2512.04646": "|2025-12-04|Canonical Rough Path over Tempered Fractional Brownian Motion: Existence, Construction, and Applications|Atef Lechiheb|[2512.04646](http://arxiv.org/pdf/2512.04646)|\u65e0|\u25c6 We construct a canonical geometric rough path over $d$-dimensional tempered fractional Brownian motion (tfBm) for any Hurst parameter $H > 1/4$ and tempering parameter $\u03bb> 0$.\n\u25c6 The main challenge stems from the non-homogeneous nature of the tfBm covariance, which exhibits a power-law structure at small scales and exponential decay at large scales.\n\u25c6 Our primary contribution is a detailed analysis of this covariance, proving it has finite 2D $\u03c1$-variation for $\u03c1= 1/(2H)$.|\n",
    "2512.04534": "|2025-12-04|Refa\u00e7ade: Editing Object with Given Reference Texture|Youze Huang\u7b49|[2512.04534](http://arxiv.org/pdf/2512.04534)|\u65e0|\u25c6 Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored.\n\u25c6 In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos.\n\u25c6 To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture.|\n",
    "2512.04399": "|2025-12-04|Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation|Haoqi Han\u7b49|[2512.04399](http://arxiv.org/pdf/2512.04399)|\u65e0|\u25c6 In robotic hand research, minimizing the number of actuators while maintaining human-hand-consistent dimensions and degrees of freedom constitutes a fundamental challenge.\n\u25c6 Drawing bio-inspiration from human hand kinematic configurations and muscle distribution strategies, this work proposes a novel 15-DoF dexterous robotic hand, with detailed analysis of its mechanical architecture, electrical system, and control system.\n\u25c6 The bionic hand employs a new tendon-driven mechanism, significantly reducing the number of motors required by traditional tendon-driven systems while enhancing motion performance and simplifying the mechanical structure.|\n",
    "2512.04303": "|2025-12-03|Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications|Gasser Elazab\u7b49|[2512.04303](http://arxiv.org/pdf/2512.04303)|\u65e0|\u25c6 Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control.\n\u25c6 However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability.\n\u25c6 To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure.|\n",
    "2512.05398": "|2025-12-05|The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos|Zhuoyuan Wu\u7b49|[2512.05398](http://arxiv.org/pdf/2512.05398)|\u65e0|\u25c6 Estimating accurate camera poses, 3D scene geometry, and object motion from in-the-wild videos is a long-standing challenge for classical structure from motion pipelines due to the presence of dynamic objects.\n\u25c6 Recent learning-based methods attempt to overcome this challenge by training motion estimators to filter dynamic objects and focus on the static background.\n\u25c6 However, their performance is largely limited by the availability of large-scale motion segmentation datasets, resulting in inaccurate segmentation and, therefore, inferior structural 3D understanding.|\n",
    "2512.05362": "|2025-12-05|PoolNet: Deep Learning for 2D to 3D Video Process Validation|Sanchit Kaul\u7b49|[2512.05362](http://arxiv.org/pdf/2512.05362)|\u65e0|\u25c6 Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task.\n\u25c6 In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data.\n\u25c6 To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data.|\n",
    "2512.08930": "|2025-12-09|Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment|Youming Deng\u7b49|[2512.08930](http://arxiv.org/pdf/2512.08930)|\u65e0|\u25c6 Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand.\n\u25c6 Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images.\n\u25c6 While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks.|\n",
    "2512.07969": "|2025-12-08|Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization|Alan Papalia\u7b49|[2512.07969](http://arxiv.org/pdf/2512.07969)|\u65e0|\u25c6 Robotic perception often requires solving large nonlinear least-squares (NLS) problems.\n\u25c6 While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution.\n\u25c6 Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties.|\n",
    "2512.10517": "|2025-12-11|3D Blood Pulsation Maps|Maurice Rohr\u7b49|[2512.10517](http://arxiv.org/pdf/2512.10517)|\u65e0|\u25c6 We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps.\n\u25c6 These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging.\n\u25c6 Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis.|\n"
  },
  "Visual Localization": {
    "2504.20379": "|**2025-05-01**|**GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting**|Jongwon Lee et.al.|[2504.20379](http://arxiv.org/abs/2504.20379)|null|\n",
    "2505.01113": "|**2025-05-02**|**NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization**|Xun Li et.al.|[2505.01113](http://arxiv.org/abs/2505.01113)|null|\n",
    "2505.03565": "|**2025-05-06**|**Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and Low-Visibility Conditions**|Lukas Schichler et.al.|[2505.03565](http://arxiv.org/abs/2505.03565)|null|\n",
    "2505.03422": "|**2025-05-06**|**LiftFeat: 3D Geometry-Aware Local Feature Matching**|Yepeng Liu et.al.|[2505.03422](http://arxiv.org/abs/2505.03422)|**[link](https://github.com/lyp-deeplearning/liftfeat)**|\n",
    "2505.03242": "|**2025-05-06**|**Seeing the Abstract: Translating the Abstract Language for Vision Language Models**|Davide Talon et.al.|[2505.03242](http://arxiv.org/abs/2505.03242)|**[link](https://github.com/davidetalon/fashionact)**|\n",
    "2505.03836": "|**2025-05-04**|**OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery**|Chongsheng Zhang et.al.|[2505.03836](http://arxiv.org/abs/2505.03836)|**[link](https://github.com/cszhanglmu/obd-finder)**|\n",
    "2505.01956": "|**2025-05-13**|**SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment**|Ganesh Sapkota et.al.|[2505.01956](http://arxiv.org/abs/2505.01956)|null|\n",
    "2505.12254": "|**2025-05-18**|**MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark**|Yiwei Ou et.al.|[2505.12254](http://arxiv.org/abs/2505.12254)|null|\n",
    "2505.11620": "|**2025-05-16**|**Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization**|Aaron Wilhelm et.al.|[2505.11620](http://arxiv.org/abs/2505.11620)|null|\n",
    "2505.11121": "|**2025-05-16**|**Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing**|Mathis J\u00fcrgen Adler et.al.|[2505.11121](http://arxiv.org/abs/2505.11121)|null|\n",
    "2505.16447": "|**2025-05-22**|**TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition**|Oliver Grainge et.al.|[2505.16447](http://arxiv.org/abs/2505.16447)|null|\n",
    "2505.15877": "|**2025-05-21**|**Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval**|Siting Li et.al.|[2505.15877](http://arxiv.org/abs/2505.15877)|null|\n",
    "2505.15867": "|**2025-05-21**|**SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval**|Nikolaos Chaidos et.al.|[2505.15867](http://arxiv.org/abs/2505.15867)|**[link](https://github.com/nickhaidos/scenir-icml2025)**|\n",
    "2505.13828": "|**2025-05-20**|**Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models**|Kiarash Naghavi Khanghah et.al.|[2505.13828](http://arxiv.org/abs/2505.13828)|null|\n",
    "2505.23763": "|**2025-05-29**|**Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch**|Aneeshan Sain et.al.|[2505.23763](http://arxiv.org/abs/2505.23763)|null|\n",
    "2505.22859": "|**2025-05-28**|**4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians**|Hidenobu Matsuki et.al.|[2505.22859](http://arxiv.org/abs/2505.22859)|null|\n",
    "2505.22098": "|**2025-05-28**|**UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images**|Junhuan Liu et.al.|[2505.22098](http://arxiv.org/abs/2505.22098)|null|\n",
    "2505.22089": "|**2025-05-28**|**Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule**|San Jiang et.al.|[2505.22089](http://arxiv.org/abs/2505.22089)|null|\n",
    "2505.21754": "|2025-05-29|Visual Loop Closure Detection Through Deep Graph Consensus|Martin B\u00fcchner\u7b49|[2505.21754](http://arxiv.org/pdf/2505.21754)|\u65e0|\u25c6 Visual loop closure detection traditionally relies on place recognition methods to retrieve candidate loops that are validated using computationally expensive RANSAC-based geometric verification.\n\u25c6 As false positive loop closures significantly degrade downstream pose graph estimates, verifying a large number of candidates in online simultaneous localization and mapping scenarios is constrained by limited time and compute resources.\n\u25c6 While most deep loop closure detection approaches only operate on pairs of keyframes, we relax this constraint by considering neighborhoods of multiple keyframes when detecting loops.|\n",
    "2505.21647": "|**2025-05-27**|**QuARI: Query Adaptive Retrieval Improvement**|Eric Xing et.al.|[2505.21647](http://arxiv.org/abs/2505.21647)|null|\n",
    "2505.20764": "|**2025-05-27**|**ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval**|Eric Xing et.al.|[2505.20764](http://arxiv.org/abs/2505.20764)|null|\n",
    "2505.20291": "|**2025-05-26**|**Visualized Text-to-Image Retrieval**|Di Wu et.al.|[2505.20291](http://arxiv.org/abs/2505.20291)|**[link](https://github.com/xiaowu0162/visualize-then-retrieve)**|\n",
    "2505.19952": "|**2025-05-26**|**Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval**|Rong-Cheng Tu et.al.|[2505.19952](http://arxiv.org/abs/2505.19952)|null|\n",
    "2505.19944": "|**2025-05-26**|**Can Visual Encoder Learn to See Arrows?**|Naoyuki Terashita et.al.|[2505.19944](http://arxiv.org/abs/2505.19944)|null|\n",
    "2506.02291": "|2025-06-02|Entity Image and Mixed-Modal Image Retrieval Datas...|Cristian-Ioan Blaga\u7b49|[2506.02291](http://arxiv.org/pdf/2506.02291)|\u65e0|\u25c6\u63d0\u51fa\u9996\u4e2a\u7ed3\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u6df7\u5408\u6a21\u6001\u56fe\u50cf\u68c0\u7d22\u57fa\u51c6MMIR\uff0c\u5305\u542b\u5355\u5b9e\u4f53\u56fe\u50cf\u548c\u591a\u5b9e\u4f53\u56fe\u50cf\u4e24\u79cd\u590d\u6742\u67e5\u8be2\u7c7b\u578b\u3002  \n\u25c6\u53d1\u5e03Entity Image\u548cMMIR\u4e24\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4f17\u5305\u6807\u6ce8\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\uff0c...|\n",
    "2506.00976": "|2025-06-01|Quantization-based Bounds on the Wasserstein Metri...|Jonathan Bobrutsky\u7b49|[2506.00976](http://arxiv.org/pdf/2506.00976)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5316\u7f51\u683c\u7684\u9ad8\u6548Wasserstein\u8ddd\u79bb\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u7f51\u683c\u4e0a\u7684Kantorovich\u95ee\u9898\u7cbe\u786e\u6c42\u89e3\u7ed3\u5408\u5347\u5c3a\u5ea6\u6821\u6b63\u6b65\u9aa4\uff0c\u5728\u4fdd\u63012%\u8bef\u5dee\u5185\u5b9e\u73b010-100\u500d\u52a0\u901f\u3002\u25c6\u521b\u65b0\u6027\u5730\u5728\u539f\u59cb\u7a7a\u95f4...|\n",
    "2505.24441": "|2025-05-30|SORCE: Small Object Retrieval in Com...|Chunxu Liu\u7b49|[2505.24441](http://arxiv.org/pdf/2505.24441)|[\u4ee3\u7801](https://github.com/mcg-nju/sorce)|\u25c6\u63d0\u51fa\u65b0\u4efb\u52a1SORCE\uff08\u590d\u6742\u73af\u5883\u4e2d\u7684\u5c0f\u7269\u4f53\u68c0\u7d22\uff09\uff0c\u4e13\u6ce8\u4e8e\u901a\u8fc7\u6587\u672c\u67e5\u8be2\u68c0\u7d22\u590d\u6742\u56fe\u50cf\u4e2d\u7684\u4e0d\u663e\u773c\u5c0f\u7269\u4f53\u3002  \n\u25c6\u6784\u5efa\u65b0\u57fa\u51c6SORCE-1K\uff0c\u5305\u542b\u590d\u6742\u73af\u5883\u56fe\u50cf\u548c\u63cf\u8ff0\u5c0f\u7269\u4f53\u7684\u6587\u672c\u67e5\u8be2\uff0c\u63ed\u793a\u73b0\u6709T2IR\u65b9\u6cd5...|\n",
    "2506.07045": "|2025-06-08|Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs|Yikun Ji\u7b49|[2506.07045](http://arxiv.org/pdf/2506.07045)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u53ef\u89e3\u91caAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u5b9a\u4f4d\u548c\u6587\u672c\u63a8\u7406\u80fd\u529b\uff0c\u4e0d\u4ec5\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff0c\u8fd8\u80fd\u63d0\u4f9b\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002  \n\u25c6 \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8fb9\u754c\u6846\u6807\u6ce8\u548c\u63cf\u8ff0\u6027\u6587\u672c\u7684\u6570\u636e\u96c6\uff0c\u7a81\u51faAI\u751f\u6210\u56fe\u50cf\u7684\u5408\u6210\u4f2a\u5f71\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u7684\u63a8\u7406\u57fa\u7840\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u9010\u6b65\u5e73\u8861\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u548c\u6587\u672c\u89e3\u91ca\u8fde\u8d2f\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MLLMs\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u5fae\u8c03MLLMs\uff0c\u4f7f\u5176\u80fd\u591f\u540c\u65f6\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u7f3a\u9677\u5e76\u751f\u6210\u5408\u7406\u7684\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u548c\u5b9a\u4f4d\u89c6\u89c9\u7455\u75b5\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2506.06602": "|2025-06-07|Zero Shot Composed Image Retrieval|Santhosh Kakarla\u7b49|[2506.06602](http://arxiv.org/pdf/2506.06602)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBLIP-2\u7684\u8f7b\u91cf\u7ea7Q-Former\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u5230\u5355\u4e00\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08Zero-shot CIR\uff09\u7684\u6027\u80fd\u3002  \n\u25c6 \u5728FashionIQ\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06Recall@10\u6307\u6807\u4ece\u539f\u5148\u768420-25%\u5927\u5e45\u63d0\u5347\u81f345.6%\uff08\u886c\u886b\uff09\u300140.1%\uff08\u88d9\u5b50\uff09\u548c50.4%\uff08T\u6064\uff09\uff0c\u5e73\u5747Recall@50\u8fbe\u523067.6%\u3002  \n\u25c6 \u63a2\u7d22\u4e86Retrieval-DPO\u65b9\u6cd5\uff0c\u5c1d\u8bd5\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u635f\u5931\u5fae\u8c03CLIP\u6587\u672c\u7f16\u7801\u5668\uff0c\u4f46\u53d1\u73b0\u5176\u6548\u679c\u8fdc\u4f4e\u4e8e\u57fa\u7ebf\uff08\u4ec50.02% Recall@10\uff09\u3002  \n\u25c6 \u5206\u6790\u4e86Retrieval-DPO\u5931\u8d25\u7684\u56db\u5927\u539f\u56e0\uff1a\u7f3a\u4e4f\u56fe\u50cf-\u6587\u672c\u8054\u5408\u878d\u5408\u3001\u76ee\u6807\u51fd\u6570\u4e0eTop-K\u6307\u6807\u4e0d\u5339\u914d\u3001\u8d1f\u6837\u672c\u8d28\u91cf\u4f4e\uff0c\u4ee5\u53ca\u89c6\u89c9\u548cTransformer\u5c42\u51bb\u7ed3\u3002  \n\u25c6 \u7814\u7a76\u8868\u660e\uff0c\u6709\u6548\u7684\u57fa\u4e8e\u504f\u597d\u7684CIR\u9700\u8981\u771f\u6b63\u7684\u591a\u6a21\u6001\u878d\u5408\u3001\u4e0e\u6392\u540d\u76f8\u5173\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4ee5\u53ca\u7cbe\u5fc3\u7b5b\u9009\u7684\u8d1f\u6837\u672c\u3002|\n",
    "2506.06220": "|2025-06-06|GenIR: Generative Visual Feedback for Mental Image Retrieval|Diji Yang\u7b49|[2506.06220](http://arxiv.org/pdf/2506.06220)|\u65e0|\u25c6 \u63d0\u51faMental Image Retrieval (MIR)\u4efb\u52a1\uff0c\u7814\u7a76\u7528\u6237\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u4ece\u6a21\u7cca\u5fc3\u7406\u56fe\u50cf\u4e2d\u68c0\u7d22\u76ee\u6807\u56fe\u50cf\u7684\u771f\u5b9e\u573a\u666f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u7814\u7a76\u7684\u7a7a\u767d\u3002  \n\u25c6 \u8bbe\u8ba1GenIR\u65b9\u6cd5\uff0c\u9996\u6b21\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u53ef\u89c6\u5316\u53cd\u9988\uff0c\u5c06AI\u7cfb\u7edf\u5bf9\u7528\u6237\u610f\u56fe\u7684\u7406\u89e3\u8f6c\u5316\u4e3a\u76f4\u89c2\u7684\u5408\u6210\u56fe\u50cf\uff0c\u514b\u670d\u4f20\u7edf\u62bd\u8c61\u8bed\u8a00\u53cd\u9988\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002  \n\u25c6 \u6784\u5efa\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u8f6eMIR\u6570\u636e\u96c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u652f\u6301\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eGenIR\u5728\u591a\u8f6e\u4ea4\u4e92\u68c0\u7d22\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210\u5f0f\u89c6\u89c9\u53cd\u9988\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u5f00\u521b\u6027\u5730\u5c06\u751f\u6210\u6a21\u578b\u4e0e\u4ea4\u4e92\u5f0f\u68c0\u7d22\u7ed3\u5408\uff0c\u4e3a\u5fc3\u7406\u56fe\u50cf\u68c0\u7d22\u9886\u57df\u5960\u5b9a\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u4eba\u673a\u534f\u540c\u641c\u7d22\u7cfb\u7edf\u7684\u53d1\u5c55\u3002|\n",
    "2506.06205": "|2025-06-06|Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning|Sheng Chen\u7b49|[2506.06205](http://arxiv.org/pdf/2506.06205)|\u65e0|\u25c6\u63d0\u51faAstra\u53cc\u6a21\u578b\u67b6\u6784\uff08Astra-Global\u548cAstra-Local\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u591a\u6a21\u6001\u5b66\u4e60\u5b9e\u73b0\u901a\u7528\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u7a81\u7834\u4f20\u7edf\u6a21\u5757\u5316\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002  \n\u25c6Astra-Global\u9996\u6b21\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u6df7\u5408\u62d3\u6251-\u8bed\u4e49\u56fe\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u548c\u5168\u5c40\u5b9a\u4f4d\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002  \n\u25c6Astra-Local\u91c7\u7528\u81ea\u76d1\u7763\u8bad\u7ec3\u76844D\u65f6\u7a7a\u7f16\u7801\u5668\u751f\u6210\u9c81\u68d2\u7279\u5f81\uff0c\u652f\u6301\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u548c\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u7b49\u591a\u4efb\u52a1\u5b66\u4e60\u3002  \n\u25c6\u521b\u65b0\u6027\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u548c\u63a9\u7801ESDF\u635f\u5931\u7684\u89c4\u5212\u5934\uff0c\u6709\u6548\u964d\u4f4e\u78b0\u649e\u98ce\u9669\uff0c\u751f\u6210\u66f4\u5b89\u5168\u7684\u5c40\u90e8\u8f68\u8ff9\u3002  \n\u25c6\u91cc\u7a0b\u8ba1\u5934\u901a\u8fc7Transformer\u7f16\u7801\u5668\u878d\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76f8\u5bf9\u4f4d\u59ff\u9884\u6d4b\u3002  \n\u25c6\u5728\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u7684\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u7aef\u5230\u7aef\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0c\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2506.04764": "|2025-06-05|HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular Visual Place Recognition|Suhan Woo\u7b49|[2506.04764](http://arxiv.org/pdf/2506.04764)|\u65e0|\u25c6 \u63d0\u51faHypeVPR\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u5f15\u5165\u900f\u89c6\u5230\u73af\u89c6\uff08P2E\uff09\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u4efb\u52a1\uff0c\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u66f4\u9002\u5408\u8868\u793a\u5c42\u6b21\u7ed3\u6784\u7684\u7279\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u5206\u5c42\u7279\u5f81\u805a\u5408\u673a\u5236\uff0c\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7ec4\u7ec7\u5c40\u90e8\u5230\u5168\u5c40\u7684\u7279\u5f81\u8868\u793a\uff0c\u6709\u6548\u6355\u6349\u5168\u666f\u56fe\u50cf\u7684\u56fa\u6709\u5c42\u6b21\u5173\u7cfb\u3002  \n\u25c6 \u5f00\u53d1\u9ad8\u6548\u7684\u7c97\u5230\u7cbe\u641c\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5339\u914d\u901f\u5ea6\uff08\u6700\u9ad8\u8fbe5\u500d\uff09\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u89e3\u51b3\u8de8\u56fe\u50cf\u7c7b\u578b\u7684\u9c81\u68d2\u5339\u914d\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u7684\u8ddd\u79bb\u4fdd\u6301\u7279\u6027\uff0c\u4f18\u5316\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u589e\u5f3a\u4e0d\u540c\u89c6\u89d2\u4e0b\u63cf\u8ff0\u7b26\u7684\u533a\u5206\u80fd\u529b\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u68c0\u7d22\u65f6\u95f4\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\uff0c\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7814\u7a76\u3002|\n",
    "2506.04619": "|2025-06-05|Deep Learning Reforms Image Matching: A Survey and Outlook|Shihua Zhang\u7b49|[2506.04619](http://arxiv.org/pdf/2506.04619)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5982\u4f55\u9010\u6b65\u9769\u65b0\u4f20\u7edf\u56fe\u50cf\u5339\u914d\u6d41\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\u3002  \n\u25c6\u521b\u65b0\u70b9\u4e00\uff1a\u9996\u6b21\u4ece\"\u9010\u6b65\u66ff\u4ee3\u4f20\u7edf\u6a21\u5757\"\u548c\"\u7aef\u5230\u7aef\u5408\u5e76\u591a\u6b65\u9aa4\"\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5bf9\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\uff08\u5305\u62ec\u53ef\u5b66\u4e60\u68c0\u6d4b-...|\n",
    "2506.08526": "|2025-06-10|Robust Visual Localization via Semantic-Guided Multi-Scale Transformer|Zhongtao Tian\u7b49|[2506.08526](http://arxiv.org/pdf/2506.08526)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\u4e0e\u8bed\u4e49\u573a\u666f\u7406\u89e3\u7684\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316Transformer\u548c\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u51e0\u4f55\u7ec6\u8282\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u7a7a\u95f4\u7cbe\u5ea6\u7684\u540c\u65f6\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u795e\u7ecf\u573a\u666f\u8868\u5f81\u63d0\u4f9b\u7684\u8bed\u4e49\u76d1\u7763\u4fe1\u53f7\uff0c\u6307\u5bfc\u7f51\u7edc\u5b66\u4e60\u89c6\u89d2\u4e0d\u53d8\u7279\u5f81\uff0c\u6709\u6548\u7f16\u7801\u6301\u4e45\u7ed3\u6784\u4fe1\u606f\u5e76\u6291\u5236\u52a8\u6001\u73af\u5883\u5e72\u6270\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6Transformer\u67b6\u6784\uff0c\u5229\u7528\u8de8\u5c42\u7ea7\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u4e0d\u540c\u5c3a\u5ea6\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728TartanAir\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u7269\u4f53\u3001\u5149\u7167\u53d8\u5316\u548c\u906e\u6321\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u4f4d\u59ff\u56de\u5f52\u65b9\u6cd5\u3002  \n\u25c6 \u9996\u6b21\u9a8c\u8bc1\u4e86\u8bed\u4e49\u5f15\u5bfc\u4e0e\u591a\u5c3a\u5ea6\u5904\u7406\u7684\u534f\u540c\u7b56\u7565\u5bf9\u73b0\u5b9e\u52a8\u6001\u73af\u5883\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9c81\u68d2\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2506.09748": "|2025-06-11|Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints|Xiangkai Zhang\u7b49|[2506.09748](http://arxiv.org/pdf/2506.09748)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8de8\u6e90\u56fe\u50cf\u5339\u914d\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u548c\u7ed3\u6784\u7ea6\u675f\u7684\u7c97\u5339\u914d\u6a21\u5757\u4e0e\u8f7b\u91cf\u7ea7\u7ec6\u7c92\u5ea6\u5339\u914d\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7edd\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u3002  \n\u25c6 \u5728\u7c97\u5339\u914d\u6a21\u5757\u4e2d\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5728\u8bed\u4e49\u548c\u7ed3\u6784\u7ea6\u675f\u4e0b\u5efa\u7acb\u533a\u57df\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u6709\u6548\u514b\u670d\u4e86\u8de8\u6e90\u5dee\u5f02\u548c\u65f6\u53d8\u56e0\u7d20\u5e26\u6765\u7684\u6311\u6218\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u7ec6\u7c92\u5ea6\u5339\u914d\u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u53d6\u7cbe\u7ec6\u7279\u5f81\u5efa\u7acb\u50cf\u7d20\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002  \n\u25c6 \u6784\u5efa\u4e86\u4e0d\u4f9d\u8d56\u76f8\u5bf9\u5b9a\u4f4d\u6280\u672f\u7684\u65e0\u4eba\u673a\u7edd\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u6d41\u7a0b\uff0c\u901a\u8fc7\u56fe\u50cf\u68c0\u7d22\u6a21\u5757\u4e0e\u5206\u5c42\u5339\u914d\u6a21\u5757\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u57fa\u4e8e\u89c6\u89c9\u7684\u5168\u5c40\u5b9a\u4f4d\u3002  \n\u25c6 \u5728\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548c\u65b0\u63d0\u51fa\u7684CS-UAV\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002|\n",
    "2506.10182": "|2025-06-11|Improving Personalized Search with Regularized Low-Rank Parameter Updates|Fiona Ryan\u7b49|[2506.10182](http://arxiv.org/pdf/2506.10182)|[\u4ee3\u7801](https://github.com/adobe-research/polar-vl)|\u25c6 \u63d0\u51fa\u4e00\u79cd\u6b63\u5219\u5316\u4f4e\u79e9\u53c2\u6570\u66f4\u65b0\u65b9\u6cd5\uff0c\u4ec5\u9700\u5fae\u8c03\u8bed\u8a00\u7f16\u7801\u5668\u6700\u540e\u4e00\u5c42\u7684\u5c11\u91cf\u53c2\u6570\uff0c\u5373\u53ef\u6709\u6548\u9002\u5e94\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\uff0c\u907f\u514d\u4f20\u7edf\u6587\u672c\u53cd\u8f6c\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u53d1\u73b0\u53c2\u6570\u76f8\u52a0\u7b56\u7565\u80fd\u6709\u6548\u6574\u5408\u591a\u4e2a\u5df2\u5b66\u4e60\u4e2a\u6027\u5316\u6982\u5ff5\u7684\u53c2\u6570\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u591a\u6982\u5ff5\u7ec4\u5408\u7684\u8bc6\u522b\u80fd\u529b\u3002  \n\u25c6 \u5f15\u5165\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63cf\u8ff0\u7684\u56fe\u50cf\u68c0\u7d22\u8bc4\u4f30\u6307\u6807\uff0c\u91cf\u5316\u5fae\u8c03\u540e\u6a21\u578b\u5bf9\u901a\u7528\u77e5\u8bc6\u7684\u4fdd\u7559\u7a0b\u5ea6\u3002  \n\u25c6 \u5728DeepFashion2\u548cConCon-Chi\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e2a\u6027\u5316\u68c0\u7d22\u51c6\u786e\u7387\u8f83\u4e4b\u524d\u65b9\u6cd5\u63d0\u53474%-22%\u3002  \n\u25c6 \u901a\u8fc7\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5185\u90e8\u8868\u5f81\u7684\u9488\u5bf9\u6027\u9002\u914d\uff0c\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u573a\u666f\u4e0b\u4e2a\u6027\u5316\u6982\u5ff5\u4e0e\u901a\u7528\u77e5\u8bc6\u878d\u5408\u7684\u96be\u9898\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u4f4e\u79e9\u53c2\u6570\u66f4\u65b0\u5728\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u5bf9\"\u6211\u7684\u72d7Fido\"\u7b49\u4e2a\u6027\u5316\u6982\u5ff5\u7684\u8de8\u4e0a\u4e0b\u6587\u8bc6\u522b\u80fd\u529b\u3002|\n",
    "2506.10030": "|2025-06-10|Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment|Tianyu Chen\u7b49|[2506.10030](http://arxiv.org/pdf/2506.10030)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u591a\u6a21\u6001RAG\u7cfb\u7edf\u4e2d\u56fe\u50cf\u77e5\u8bc6\u7248\u6743\u4fdd\u62a4\u7684\u6c34\u5370\u6846\u67b6AQUA\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u6c34\u5370\u5d4c\u5165\u65b9\u6cd5\uff1a\u57fa\u4e8e\u9996\u5b57\u6bcd\u7f29\u5199\u7684\u89e6\u53d1\u5668\u548c\u7a7a\u95f4\u5173\u7cfb\u7ebf\u7d22\uff0c\u786e\u4fdd\u6c34\u5370\u4fe1\u53f7\u5728\u56fe\u50cf\u5230\u6587\u672c\u7684\u95f4\u63a5\u4f20\u64ad\u4e2d\u4fdd\u6301\u6709\u6548\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u6c34\u5370\u7684\u9ad8\u6548\u6027\u3001\u5f3a\u9c81\u68d2\u6027\u548c\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7a33\u5b9a\u5de5\u4f5c\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u73b0\u6709RAG\u6c34\u5370\u6280\u672f\u4ec5\u5173\u6ce8\u6587\u672c\u77e5\u8bc6\u800c\u5ffd\u7565\u56fe\u50cf\u4fdd\u62a4\u7684\u5c40\u9650\u6027\uff0c\u6269\u5c55\u4e86\u7248\u6743\u4fdd\u62a4\u8303\u56f4\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AQUA\u5728\u8de8\u6a21\u6001\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u652f\u6301\u5bf9\u8d21\u732e\u6570\u636e\u7684\u7cbe\u51c6\u7248\u6743\u8ffd\u8e2a\u3002  \n\u25c6 \u4e3aRAG-as-a-Service\u73af\u5883\u4e2d\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u5171\u4eab\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7248\u6743\u5b89\u5168\u4fdd\u969c\u65b9\u6848\u3002|\n",
    "2506.11167": "|2025-06-11|Towards a general-purpose foundation model for fMRI analysis|Cheng Wang\u7b49|[2506.11167](http://arxiv.org/pdf/2506.11167)|\u65e0|\u25c6 \u63d0\u51faNeuroSTORM\uff0c\u9996\u4e2a\u9762\u5411fMRI\u5206\u6790\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u76f4\u63a5\u4ece4D fMRI\u6570\u636e\u5b66\u4e60\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u590d\u6742\u9884\u5904\u7406\u548c\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u5bfc\u81f4\u7684\u590d\u73b0\u6027\u548c\u8fc1\u79fb\u6027\u4e0d\u8db3\u95ee\u9898\u3002  \n\u25c6 \u91c7\u7528Mamba\u67b6\u6784\u548c\u79fb\u4f4d\u626b\u63cf\u7b56\u7565\uff0c\u9ad8\u6548\u5904\u7406\u5b8c\u65744D fMRI\u4f53\u79ef\uff0c\u7a81\u7834\u4f20\u7edf\u65f6\u7a7a\u5efa\u6a21\u6548\u7387\u74f6\u9888\u3002  \n\u25c6 \u8bbe\u8ba1\u7a7a\u95f4-\u65f6\u95f4\u8054\u5408\u4f18\u5316\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5fae\u8c03\uff08prompt tuning\uff09\uff0c\u663e\u8457\u63d0\u5347\u8de8\u4efb\u52a1\u8fc1\u79fb\u80fd\u529b\u3002  \n\u25c6 \u57fa\u4e8e\u8d85\u5927\u89c4\u6a21\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0828.65\u767e\u4e07\u5e27fMRI\uff0c50,000+\u53d7\u8bd5\u8005\uff0c\u8de8\u591a\u4e2d\u5fc3\u53ca5-100\u5c81\u5e74\u9f84\u8303\u56f4\uff09\uff0c\u5efa\u7acb\u8fc4\u4eca\u6700\u5168\u9762\u7684\u8111\u529f\u80fd\u8868\u5f81\u5e93\u3002  \n\u25c6 \u5728\u4e94\u9879\u4efb\u52a1\uff08\u5e74\u9f84/\u6027\u522b\u9884\u6d4b\u3001\u8868\u578b\u9884\u6d4b\u3001\u75be\u75c5\u8bca\u65ad\u3001fMRI-\u56fe\u50cf\u68c0\u7d22\u3001\u4efb\u52a1\u6001\u5206\u7c7b\uff09\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u7f8e\u3001\u97e9\u3001\u6fb3\u4e34\u5e8a\u6570\u636e\u9a8c\u8bc1\u4e2d\u5c55\u73b0\u5353\u8d8a\u8bca\u65ad\u6027\u80fd\u3002  \n\u25c6 \u5f00\u6e90\u6807\u51c6\u5316\u6a21\u578b\u6846\u67b6\uff0c\u4e3afMRI\u4e34\u5e8a\u7814\u7a76\u63d0\u4f9b\u53ef\u590d\u73b0\u3001\u53ef\u8fc1\u79fb\u7684\u57fa\u7840\u5de5\u5177\uff0c\u63a8\u52a8\u8111\u75be\u75c5\u8bca\u65ad\u7684\u8de8\u4e2d\u5fc3\u5e94\u7528\u3002|\n",
    "2506.13509": "|2025-06-16|A Semantically-Aware Relevance Measure for Content-Based Medical Image Retrieval Evaluation|Xiaoyang Wei\u7b49|[2506.13509](http://arxiv.org/pdf/2506.13509)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u611f\u77e5\u76f8\u5173\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\uff08CBIR\uff09\u7684\u6027\u80fd\u8bc4\u4f30\u96be\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u533b\u5b66\u6587\u672c\uff08\u5982\u653e\u5c04\u5b66\u62a5\u544a\u6216\u6587\u732e\u63cf\u8ff0\uff09\u4e2d\u9690\u542b\u7684\u533b\u5b66\u6982\u5ff5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002  \n\u25c6 \u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u91cf\u5316\u533b\u5b66\u6982\u5ff5\u95f4\u7684\u8bed\u4e49\u8ddd\u79bb\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5c06\u533b\u5b66\u6982\u5ff5\u89c6\u4e3a\u72ec\u7acb\u6807\u7b7e\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u6355\u6349\u6982\u5ff5\u95f4\u7684\u7ec6\u5fae\u5173\u8054\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u57fa\u4e8e\u8fd1\u4f3c\u5339\u914d\u7684\u76f8\u5173\u6027\u8bc4\u5206\u673a\u5236\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e24\u7ec4\u533b\u5b66\u6982\u5ff5\u7684\u76f8\u4f3c\u6027\u95f4\u63a5\u8861\u91cf\u533b\u5b66\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\u3002  \n\u25c6 \u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u4e3a\u533b\u5b66CBIR\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u4e34\u5e8a\u8bed\u4e49\u7684\u65b0\u6807\u51c6\u3002|\n",
    "2506.13496": "|2025-06-19|Hierarchical Multi-Positive Contrastive Learning for Patent Image Retrieval|Kshitij Kavimandan\u7b49|[2506.13496](http://arxiv.org/pdf/2506.13496)|\u65e0|\u25c6\u63d0\u51fa\u5206\u5c42\u591a\u6b63\u4f8b\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u51fd\u6570\uff0c\u9996\u6b21\u5229\u7528Locarno\u56fd\u9645\u5206\u7c7b\u4f53\u7cfb\uff08LIC\uff09\u7684\u5c42\u7ea7\u5173\u7cfb\u6307\u5bfc\u4e13\u5229\u56fe\u50cf\u68c0\u7d22\u3002  \n\u25c6\u901a\u8fc7\u5c42\u7ea7\u5206\u7c7b\u6811\u52a8\u6001\u5206\u914d\u591a\u7ec4\u6b63\u6837\u672c\u5bf9\uff0c\u6839\u636e\u4e13\u5229\u56fe\u50cf\u5728LIC\u4e2d\u7684\u5c42\u7ea7\u8ddd\u79bb\u8d4b\u4e88\u4e0d\u540c\u76f8\u4f3c\u5ea6\u6743\u91cd\u3002  \n\u25c6\u7a81\u7834\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u4ec5\u4f7f\u7528\u5355\u4e00\u6837\u672c\u5bf9\u7684\u9650\u5236\uff0c\u80fd\u540c\u65f6\u5b66\u4e60\u8de8\u5c42\u7ea7\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5173\u8054\u3002  \n\u25c6\u5728DeepPatent2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u666e\u9002\u6027\uff0c\u53ef\u9002\u914d\u591a\u79cd\u89c6\u89c9\u548c\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u3002  \n\u25c6\u7279\u522b\u4f18\u5316\u4e86\u5c0f\u53c2\u6570\u91cf\u6a21\u578b\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5177\u6709\u663e\u8457\u90e8\u7f72\u4f18\u52bf\u3002  \n\u25c6\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u4e13\u5229\u56fe\u50cf\u7684\u6280\u672f\u7ec6\u8282\u548c\u590d\u6742\u8bed\u4e49\uff0c\u63d0\u5347\u8de8\u7c7b\u522b\u68c0\u7d22\u51c6\u786e\u7387\u3002|\n",
    "2506.13133": "|2025-06-16|EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints for Visual Place Recognition|Bingxi Liu\u7b49|[2506.13133](http://arxiv.org/pdf/2506.13133)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b80\u5355\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7279\u5f81\uff08MoF\uff09\u65b9\u6cd5\u5728\u5177\u8eab\u7ea6\u675f\u4e0b\u4f18\u5316\u5168\u5c40\u7279\u5f81\uff0c\u63d0\u5347\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u6027\u80fd\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u5177\u8eab\u7ea6\u675f\u5728VPR\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5e76\u6839\u636e\u73b0\u6709\u6570\u636e\u96c6\u5c06\u5176\u5206\u7c7b\u4e3aGPS\u6807\u7b7e\u3001\u65f6\u5e8f\u6233\u3001\u5c40\u90e8\u7279\u5f81\u5339\u914d\u548c\u81ea\u76f8\u4f3c\u77e9\u9635\u7b49\u7c7b\u578b\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684MoF\u6743\u91cd\u8ba1\u7b97\u7b56\u7565\uff0c\u91c7\u7528\u591a\u5ea6\u91cf\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u878d\u5408\u591a\u79cd\u7279\u5f81\u4fe1\u606f\u3002  \n\u25c6 \u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4ec5\u970025 KB\u989d\u5916\u53c2\u6570\u548c\u6bcf\u5e2710\u5fae\u79d2\u5904\u7406\u65f6\u95f4\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002  \n\u25c6 \u5728Pitts-30k\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u57fa\u4e8eDINOv2\u7684\u57fa\u7ebf\u6027\u80fd\u63d0\u53470.9%\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\uff0c\u9002\u5408\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u3002|\n",
    "2506.13073": "|2025-06-16|SuperPlace: The Renaissance of Classical Feature Aggregation for Visual Place Recognition in the Era of Foundation Models|Bingxi Liu\u7b49|[2506.13073](http://arxiv.org/pdf/2506.13073)|\u65e0|\u25c6 \u63d0\u51faSuperPlace\u6846\u67b6\uff0c\u91cd\u65b0\u5229\u7528\u7ecf\u5178\u7279\u5f81\u805a\u5408\u65b9\u6cd5\uff08\u5982GeM\u548cNetVLAD\uff09\uff0c\u5728\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\u4f18\u5316\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u6027\u80fd\u3002  \n\u25c6 \u5f00\u53d1\u76d1\u7763\u6807\u7b7e\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8de8\u591a\u4e2aVPR\u6570\u636e\u96c6\u7684\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u63d0\u51faG\u00b2M\u7279\u5f81\u805a\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ccGeM\u7ed3\u6784\u5b66\u4e60\u7279\u5f81\u56fe\u7684\u4e3b\u6210\u5206\u5e76\u6821\u51c6\u8f93\u51fa\uff0c\u4ec5\u9700\u5341\u5206\u4e4b\u4e00\u7279\u5f81\u7ef4\u5ea6\u5373\u53ef\u8fbe\u5230\u4f18\u5f02\u6548\u679c\u3002  \n\u25c6 \u8bbe\u8ba1NetVLAD-Linear\uff08NVL\uff09\u7684\u4e8c\u6b21\u5fae\u8c03\u7b56\u7565\uff08FT\u00b2\uff09\uff0c\u5148\u5728\u9ad8\u7ef4\u7a7a\u95f4\u5b66\u4e60\u7279\u5f81\u5411\u91cf\uff0c\u518d\u901a\u8fc7\u5355\u7ebf\u6027\u5c42\u538b\u7f29\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eSuperPlace\u7684\u4f18\u8d8a\u6027\uff0cG\u00b2M\u5728\u4f4e\u7ef4\u5ea6\u4e0b\u8868\u73b0\u7a81\u51fa\uff0cNVL-FT\u00b2\u5728MSLS\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e00\u3002|\n",
    "2506.12401": "|2025-06-14|Feature Complementation Architecture for Visual Place Recognition|Weiwei Wang\u7b49|[2506.12401](http://arxiv.org/pdf/2506.12401)|\u65e0|\u25c6 \u63d0\u51fa\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u4e92\u8865\u7f51\u7edc\uff08LGCN\uff09\uff0c\u901a\u8fc7\u5e76\u884cCNN-ViT\u6df7\u5408\u67b6\u6784\u89e3\u51b3\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u4e2d\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u52a8\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08DFM\uff09\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u548c\u901a\u9053\u4f9d\u8d56\u5173\u7cfb\u5b9e\u73b0\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\uff0c\u63d0\u5347\u7279\u5f81\u8868\u8fbe\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728\u51bb\u7ed3\u7684ViT\u4e3b\u5e72\u4e2d\u5f15\u5165\u8f7b\u91cf\u7ea7\u9891\u57df-\u7a7a\u95f4\u878d\u5408\u9002\u914d\u5668\uff0c\u4ee5\u53ef\u63a7\u53c2\u6570\u91cf\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u9002\u914d\uff0c\u589e\u5f3aViT\u5206\u652f\u5bf9VPR\u4efb\u52a1\u7684\u9002\u5e94\u80fd\u529b\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eLGCN\u5728\u591a\u4e2aVPR\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\u3002  \n\u25c6 \u6574\u4f53\u67b6\u6784\u517c\u987e\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2506.14707": "|2025-06-17|HARMONY: A Scalable Distributed Vector Database for High-Throughput Approximate Nearest Neighbor Search|Qian Xu\u7b49|[2506.14707](http://arxiv.org/pdf/2506.14707)|\u65e0|\u25c6 \u63d0\u51faHarmony\u5206\u5e03\u5f0f\u5411\u91cf\u6570\u636e\u5e93\uff0c\u89e3\u51b3\u5355\u673a\u5904\u7406\u9ad8\u7ef4\u5411\u91cf\u65f6\u7684\u5185\u5b58\u548c\u6548\u7387\u74f6\u9888\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u591a\u7c92\u5ea6\u5206\u533a\u7b56\u7565\uff0c\u7ed3\u5408\u57fa\u4e8e\u7ef4\u5ea6\u548c\u57fa\u4e8e\u5411\u91cf\u7684\u5206\u533a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8ba1\u7b97\u8d1f\u8f7d\u5747\u8861\u3002  \n\u25c6 \u901a\u8fc7\u4f18\u5316\u5206\u533a\u7b56\u7565\u6709\u6548\u964d\u4f4e\u8282\u70b9\u95f4\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u541e\u5410\u91cf\u3002  \n\u25c6 \u5f15\u5165\u57fa\u4e8e\u8ddd\u79bb\u8ba1\u7b97\u5355\u8c03\u6027\u7684\u65e9\u671f\u505c\u6b62\u526a\u679d\u673a\u5236\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002  \n\u25c6 \u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHarmony\u5728\u56db\u8282\u70b9\u914d\u7f6e\u4e0b\u5e73\u5747\u541e\u5410\u91cf\u8fbe\u5230\u73b0\u6709\u65b9\u6848\u76844.63\u500d\u3002  \n\u25c6 \u9488\u5bf9\u503e\u659c\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6027\u80fd\u6bd4\u4f20\u7edf\u5206\u5e03\u5f0f\u65b9\u6848\u63d0\u534758%\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u8d1f\u8f7d\u9002\u5e94\u80fd\u529b\u3002|\n",
    "2506.14178": "|2025-06-17|TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Indoor Localization and Mapping|Jeewon Kim\u7b49|[2506.14178](http://arxiv.org/pdf/2506.14178)|\u65e0|\u25c6 \u63d0\u51faTACS-Graphs\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5730\u9762\u673a\u5668\u4eba\u53ef\u901a\u884c\u6027\uff08traversability\uff09\u4e0e\u623f\u95f4\u5206\u5272\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4f20\u7edf3D\u573a\u666f\u56fe\u4e2d\u623f\u95f4\u5c42\u5206\u5272\u4e0d\u4e00\u81f4\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u53ef\u901a\u884c\u6027\u7ea6\u675f\u91cd\u65b0\u5b9a\u4e49\u623f\u95f4\u8fb9\u754c\uff0c\u514b\u670d\u4f53\u7d20\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u51e0\u4f55\u90bb\u8fd1\u6027\u5bfc\u81f4\u7684\u6b20\u5206\u5272\uff08\u5f00\u653e\u7a7a\u95f4\u8bef\u5224\uff09\u548c\u8fc7\u5206\u5272\uff08\u590d\u6742\u73af\u5883\u788e\u7247\u5316\uff09\u7f3a\u9677\u3002  \n\u25c6 \u6784\u5efa\u62d3\u6251\u4e0e\u8bed\u4e49\u66f4\u4e00\u81f4\u7684\u573a\u666f\u56fe\uff0c\u5728\u7ed3\u6784\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u623f\u95f4\u5c42\u8bed\u4e49\u5206\u5272\u3002  \n\u25c6 \u5f00\u53d1\u57fa\u4e8e\u4e00\u81f4\u6027\u573a\u666f\u56fe\u7684\u95ed\u73af\u68c0\u6d4b\u65b9\u6cd5\uff08CoSG-LCD\uff09\uff0c\u5229\u7528\u589e\u5f3a\u7684\u5206\u5272\u4e00\u81f4\u6027\u63d0\u5347\u95ed\u73af\u68c0\u6d4b\u6548\u7387\uff0c\u8fdb\u800c\u63d0\u9ad8\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u5728\u573a\u666f\u56fe\u4e00\u81f4\u6027\u548c\u4f4d\u59ff\u56fe\u4f18\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u73af\u5883\u8868\u5f81\u3002|\n",
    "2506.15180": "|2025-06-18|ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections|Ziling Huang\u7b49|[2506.15180](http://arxiv.org/pdf/2506.15180)|\u65e0|\u25c6 \u63d0\u51faReSeDis\u4efb\u52a1\uff0c\u9996\u6b21\u5c06\u5927\u89c4\u6a21\u56fe\u50cf\u68c0\u7d22\u4e0e\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u7ed3\u5408\uff0c\u8981\u6c42\u6a21\u578b\u6839\u636e\u6587\u672c\u63cf\u8ff0\u5728\u56fe\u50cf\u5e93\u4e2d\u68c0\u7d22\u76ee\u6807\u5e76\u7cbe\u786e\u5b9a\u4f4d\u5176\u4f4d\u7f6e\uff08\u8fb9\u754c\u6846\u6216\u5206\u5272\u63a9\u7801\uff09\u3002  \n\u25c6 \u6784\u5efa\u9996\u4e2a\u9488\u5bf9\u8be5\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u786e\u4fdd\u6bcf\u4e2a\u63cf\u8ff0\u552f\u4e00\u5bf9\u5e94\u5206\u6563\u5728\u5927\u89c4\u6a21\u591a\u6837\u5316\u56fe\u50cf\u5e93\u4e2d\u7684\u76ee\u6807\u5b9e\u4f8b\uff0c\u907f\u514d\u8bef\u5339\u914d\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u8054\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u540c\u65f6\u8861\u91cf\u68c0\u7d22\u53ec\u56de\u7387\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u89e3\u51b3\u73b0\u6709\u6280\u672f\u53ea\u80fd\u5355\u72ec\u8bc4\u4f30\u67d0\u4e00\u65b9\u9762\u7684\u5c40\u9650\u3002  \n\u25c6 \u63d0\u4f9b\u57fa\u4e8e\u51bb\u7ed3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63ed\u793a\u8be5\u4efb\u52a1\u672a\u6765\u7814\u7a76\u7684\u5de8\u5927\u63d0\u5347\u7a7a\u95f4\u3002  \n\u25c6 \u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u771f\u5b9e\u7aef\u5230\u7aef\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f25\u8865\u73b0\u6709\u6280\u672f\uff08\u89c6\u89c9\u5b9a\u4f4d\u5047\u8bbe\u76ee\u6807\u5fc5\u7136\u5b58\u5728\uff0c\u6587\u672c\u68c0\u7d22\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\uff09\u7684\u4e0d\u8db3\u3002|\n",
    "2506.16745": "|2025-06-20|Class Agnostic Instance-level Descriptor for Visual Instance Search|Qi-Ying Sun\u7b49|[2506.16745](http://arxiv.org/pdf/2506.16745)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763ViT\u7684\u7c7b\u65e0\u5173\u5b9e\u4f8b\u7ea7\u63cf\u8ff0\u7b26\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u5b9e\u4f8b\u641c\u7d22\u4e2d\u7f3a\u4e4f\u6709\u6548\u5b9e\u4f8b\u7ea7\u7279\u5f81\u8868\u793a\u7684\u95ee\u9898\u3002  \n\u25c6\u901a\u8fc7\u5c42\u6b21\u5316\u5206\u89e3\u7279\u5f81\u96c6\uff0c\u5c06\u5b9e\u4f8b\u533a\u57df\u53d1\u73b0\u5efa\u6a21\u4e3a\u68c0\u6d4b\u7d27\u51d1\u7279\u5f81\u5b50\u96c6\u7684\u8fc7\u7a0b\uff0c\u751f\u6210\u591a\u5c42\u6b21\u7684\u8bed\u4e49\u7279\u5f81\u5b50\u96c6\u3002  \n\u25c6\u6784\u5efa\u7684\u7279\u5f81\u5c42\u6b21\u7ed3\u6784\u4e2d\uff0c\u975e\u53f6\u8282\u70b9\u548c\u53f6\u8282\u70b9\u5bf9\u5e94\u56fe\u50cf\u4e2d\u4e0d\u540c\u8bed\u4e49\u5c3a\u5ea6\u7684\u5b9e\u4f8b\u533a\u57df\uff0c\u6709\u6548\u5904\u7406\u4e86\u7269\u4f53\u5d4c\u5165\u548c\u906e\u6321\u95ee\u9898\u3002  \n\u25c6\u751f\u6210\u7684\u8282\u70b9\u7279\u5f81\u6784\u6210\u56fe\u50cf\u7684\u5168\u9762\u5b9e\u4f8b\u8868\u793a\uff0c\u9002\u7528\u4e8e\u5df2\u77e5\u548c\u672a\u77e5\u7269\u4f53\u7c7b\u522b\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6\u5728\u4e09\u4e2a\u5b9e\u4f8b\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002|\n",
    "2506.16353": "|2025-06-19|MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval|Chao He\u7b49|[2506.16353](http://arxiv.org/pdf/2506.16353)|[\u4ee3\u7801](https://github.com/shuaichaochao/mambahash)|\u25c6 \u9996\u6b21\u5c06\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08Mamba\uff09\u5f15\u5165\u5927\u89c4\u6a21\u56fe\u50cf\u54c8\u5e0c\u68c0\u7d22\u4efb\u52a1\uff0c\u63a2\u7d22\u5176\u5728\u8be5\u9886\u57df\u7684\u9002\u7528\u6027\u548c\u4f18\u52bf\u3002  \n\u25c6 \u63d0\u51fa\u5206\u9636\u6bb5\u7684\u4e3b\u5e72\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u7ec4Mamba\u64cd\u4f5c\u5b9e\u73b0\u591a\u65b9\u5411\u626b\u63cf\uff0c\u6709\u6548\u5efa\u6a21\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002  \n\u25c6 \u8bbe\u8ba1\u901a\u9053\u4ea4\u4e92\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u589e\u5f3a\u8de8\u901a\u9053\u4fe1\u606f\u4ea4\u6d41\uff0c\u63d0\u5347\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u3002  \n\u25c6 \u5f00\u53d1\u81ea\u9002\u5e94\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u589e\u52a0\u7279\u5f81\u591a\u6837\u6027\u5e76\u5f3a\u5316\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u80fd\u529b\u3002  \n\u25c6 \u5728CIFAR-10\u3001NUS-WIDE\u548cIMAGENET\u7b49\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6df1\u5ea6\u54c8\u5e0c\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u6548\u7387\u548c\u68c0\u7d22\u6027\u80fd\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u4e3a\u7ebf\u6027\u590d\u6742\u5ea6\u6a21\u578b\u5728\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u65b0\u601d\u8def\u3002|\n",
    "2506.16273": "|2025-06-19|Fine-grained Image Retrieval via Dual-Vision Adaptation|Xin Jiang\u7b49|[2506.16273](http://arxiv.org/pdf/2506.16273)|\u65e0|\u25c6\u63d0\u51fa\u53cc\u89c6\u89c9\u9002\u5e94\uff08DVA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6837\u672c\u548c\u7279\u5f81\u534f\u540c\u9002\u914d\u89e3\u51b3\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\uff08FGIR\uff09\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u6613\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u540c\u65f6\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6\u8bbe\u8ba1\u5bf9\u8c61\u611f\u77e5\u9002\u914d\uff08Object-Perceptual Adaptation\uff09\uff0c\u901a\u8fc7\u4fee\u6539\u8f93\u5165\u6837\u672c\u5f15\u5bfc\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u805a\u7126\u5bf9\u7c7b\u522b\u9884\u6d4b\u5173\u952e\u7684\u7269\u4f53\u53ca\u5c40\u90e8\u7279\u5f81\u3002  \n\u25c6\u63d0\u51fa\u4e0a\u4e0b\u6587\u5185\u9002\u914d\uff08In-Context Adaptation\uff09\uff0c\u4ec5\u5f15\u5165\u5c11\u91cf\u53ef\u8c03\u53c2\u6570\u8fdb\u884c\u7279\u5f81\u9002\u914d\uff0c\u4f7f\u8c03\u6574\u540e\u7684\u7279\u5f81\u66f4\u8d34\u8fd1\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u907f\u514d\u4fee\u6539\u539f\u59cb\u9884\u8bad\u7ec3\u53c2\u6570\u3002  \n\u25c6\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u673a\u5236\u63d0\u51fa\u5224\u522b\u611f\u77e5\u8fc1\u79fb\uff08Discrimination Perception Transfer\uff09\uff0c\u5c06\u5bf9\u8c61\u611f\u77e5\u9002\u914d\u4e2d\u7684\u5224\u522b\u77e5\u8bc6\u9ad8\u6548\u8fc1\u79fb\u81f3\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u5e73\u8861\u68c0\u7d22\u6548\u7387\u4e0e\u6027\u80fd\u3002  \n\u25c6\u5b9e\u9a8c\u8868\u660eDVA\u57283\u4e2a\u5206\u5e03\u5185\u548c3\u4e2a\u5206\u5e03\u5916\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53ef\u5b66\u4e60\u53c2\u6570\u91cf\u663e\u8457\u5c11\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|\n",
    "2506.15988": "|2025-06-19|Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation|Connor Malone\u7b49|[2506.15988](http://arxiv.org/pdf/2506.15988)|[\u4ee3\u7801](https://github.com/QVPR/aarapsiproject)|\u25c6 \u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u56db\u79cd\u5e38\u89c1\u5bf9\u6297\u653b\u51fb\u548c\u56db\u79cdVPR\u4e13\u7528\u653b\u51fb\u5bf9\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u5b9a\u4f4d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73af\u7cfb\u7edf\u6846\u67b6\uff0c\u5c06VPR\u3001\u5bf9\u6297\u653b\u51fb\u68c0\u6d4b\u5668\uff08AAD\uff09\u548c\u4e3b\u52a8\u5bfc\u822a\u51b3\u7b56\u76f8\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u52bf\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u5b9e\u9a8c\u8303\u5f0f\uff0c\u8bc1\u660e\u5373\u4f7fAAD\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u6709\u9650\uff08\u5982\u771f\u9633\u6027\u738775%\u3001\u5047\u9633\u6027\u738725%\uff09\uff0c\u4e5f\u80fd\u663e\u8457\u964d\u4f4e\u5e73\u5747\u6cbf\u8f68\u5b9a\u4f4d\u8bef\u5dee\u7ea650%\u3002  \n\u25c6 \u9996\u6b21\u7814\u7a76\u4e86\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u6cd5\uff08FGSM\uff09\u5bf9\u6297\u653b\u51fb\u5728VPR\u4e2d\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u591a\u9879\u5173\u952e\u8bc4\u4f30\u6307\u6807\uff08\u5982\u6cbf\u8f68\u8bef\u5dee\u3001\u53d7\u653b\u51fb\u65f6\u95f4\u6bd4\u4f8b\u3001\u4e0d\u5b89\u5168\u72b6\u6001\u65f6\u95f4\u6bd4\u4f8b\u7b49\uff09\uff0c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cf\u5316\u4f9d\u636e\u3002  \n\u25c6 \u5f3a\u8c03\u4e86AAD\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002|\n",
    "2506.15851": "|2025-06-18|Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles|Qiyuan Wu\u7b49|[2506.15851](http://arxiv.org/pdf/2506.15851)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u50cf\u7279\u5f81\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u8f7b\u91cf\u7ea7\u4f20\u611f\u5668\u8bef\u5dee\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u4e8c\u7ef4\u8bef\u5dee\u5206\u5e03\u3002  \n\u25c6 \u901a\u8fc7\u6761\u4ef6\u5316\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u9690\u542b\u5730\u6355\u6349\u4e86\u672a\u6807\u6ce8\u7684\u5173\u952e\u73af\u5883\u56e0\u7d20\uff08\u5982\u57ce\u5e02/\u9ad8\u901f\u3001\u52a8\u6001/\u9759\u6001\u573a\u666f\u3001\u5b63\u8282\u53d8\u5316\uff09\u3002  \n\u25c6 \u91c7\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u66ff\u4ee3\u4f20\u7edf\u9ad8\u65af\u5206\u5e03\uff0c\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u6d4b\u91cf\u8bef\u5dee\u7279\u6027\u3002  \n\u25c6 \u5728Ithaca365\u591a\u5929\u6c14/\u5149\u7167\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u51c6\u786e\u6027\uff0c\u6db5\u76d6\u6674\u5929\u3001\u591c\u95f4\u548c\u96ea\u5929\u7b49\u590d\u6742\u573a\u666f\u3002  \n\u25c6 \u63d0\u51fa\u72ec\u7279\u7684\u4f20\u611f\u5668\u95e8\u63a7\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u5b9a\u4f4d\u6ee4\u6ce2\u5668\u8bc4\u4f30\u4f20\u611f\u5668\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u8054\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6027\u80fd\u3002  \n\u25c6 \u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u3002|\n",
    "2506.18902": "|2025-06-24|jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval|Michael G\u00fcnther\u7b49|[2506.18902](http://arxiv.org/pdf/2506.18902)|\u65e0|\u25c6 \u63d0\u51fajina-embeddings-v4\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a38\u4ebf\u53c2\u6570\u7684\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u6587\u672c\u548c\u56fe\u50cf\u7684\u8868\u793a\u3002  \n\u25c6 \u91c7\u7528\u65b0\u9896\u7684\u67b6\u6784\uff0c\u652f\u6301\u5355\u5411\u91cf\u548c\u591a\u5411\u91cf\u5d4c\u5165\uff0c\u5e76\u91c7\u7528\u540e\u671f\u4ea4\u4e92\u98ce\u683c\u3002  \n\u25c6 \u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u9002\u914d\u5668\uff0c\u4f18\u5316\u4e86\u591a\u79cd\u68c0\u7d22\u573a\u666f\u7684\u6027\u80fd\uff0c\u5305\u62ec\u57fa\u4e8e\u67e5\u8be2\u7684\u4fe1\u606f\u68c0\u7d22\u3001\u8de8\u6a21\u6001\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u7f16\u7a0b\u4ee3\u7801\u641c\u7d22\u3002  \n\u25c6 \u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u89c6\u89c9\u4e30\u5bcc\u5185\u5bb9\uff08\u5982\u8868\u683c\u3001\u56fe\u8868\u3001\u56fe\u8868\u548c\u6df7\u5408\u5a92\u4f53\u683c\u5f0f\uff09\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002  \n\u25c6 \u63d0\u51faJina-VDR\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u4e30\u5bcc\u56fe\u50cf\u68c0\u7d22\u80fd\u529b\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002|\n",
    "2506.18246": "|2025-06-26|Referring Expression Instance Retrieval and A Strong End-to-End Baseline|Xiangzhao Hao\u7b49|[2506.18246](http://arxiv.org/pdf/2506.18246)|\u65e0|\u25c6 \u63d0\u51fa\u65b0\u4efb\u52a1REIR\uff08Referring Expression Instance Retrieval\uff09\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\uff08TIR\uff09\u7cbe\u5ea6\u4e0d\u8db3\u548c\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff08REC\uff09\u6269\u5c55\u6027\u5dee\u7684\u7a7a\u767d\uff0c\u652f\u6301\u8de8\u5927\u89c4\u6a21\u56fe\u5e93\u7684\u5b9e\u4f8b\u7ea7\u68c0\u7d22\u4e0e\u5b9a\u4f4d\u3002  \n\u25c6 \u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6REIRCOCO\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ec6\u7c92\u5ea6\u6307\u4ee3\u8868\u8fbe\uff0c\u57fa\u4e8eMSCOCO\u548cRefCOCO\u5b9e\u4f8b\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u3002  \n\u25c6 \u63d0\u51fa\u7aef\u5230\u7aef\u57fa\u7ebf\u65b9\u6cd5CLARE\uff0c\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u8bbe\u8ba1\uff0c\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u4e0eREC\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165Mix of Relation Experts\uff08MORE\uff09\u6a21\u5757\uff0c\u663e\u5f0f\u5efa\u6a21\u5b9e\u4f8b\u95f4\u5173\u7cfb\uff0c\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u68c0\u7d22\u7cbe\u5ea6\u3002  \n\u25c6 \u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6CLIA\uff08Contrastive Language-Instance Alignment\uff09\u4f18\u5316\u8bed\u8a00-\u5b9e\u4f8b\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u5728REIR\u3001TIR\u548cREC\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\u3002  \n\u25c6 \u9a8c\u8bc1\u4e86CLARE\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u4e00\u6a21\u578b\u540c\u65f6\u652f\u6301\u5b9e\u4f8b\u68c0\u7d22\u3001\u7c97\u7c92\u5ea6\u68c0\u7d22\u548c\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u4e09\u7c7b\u4efb\u52a1\u3002|\n",
    "2506.20467": "|2025-06-25|Visualizing intercalation effects in 2D materials using AFM based techniques|Karmen Kapusti\u0107\u7b49|[2506.20467](http://arxiv.org/pdf/2506.20467)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08AFM\uff09\u7684\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u4e8c\u7ef4\u6750\u6599\uff08\u5982MoS2/\u77f3\u58a8\u70ef/Ir(111)\uff09\u4e2d\u786b\u63d2\u5c42\u5f15\u8d77\u7684\u5c40\u90e8\u7ed3\u6784\u548c\u7535\u5b50\u6027\u8d28\u53d8\u5316\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8d85\u9ad8\u771f\u7a7a\u6280\u672f\u7684\u8017\u65f6\u3001\u9ad8\u6210\u672c\u548c\u7a7a\u95f4\u9650\u5236\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7AFM\u5f62\u8c8c\u6210\u50cf\u76f4\u63a5\u89c2\u5bdf\u5230\u63d2\u5c42\u5bfc\u81f4\u7684\u7ed3\u6784\u53d8\u5316\uff0c\u5e76\u7ed3\u5408\u76f8\u4f4d\u6210\u50cf\u4e0e\u529b\u5b66\u6d4b\u91cf\uff0c\u9996\u6b21\u53d1\u73b0\u63d2\u5c42\u533a\u57df\u6768\u6c0f\u6a21\u91cf\u548c\u7c98\u9644\u529b\u7684\u964d\u4f4e\u3002  \n\u25c6 \u5229\u7528\u5f00\u5c14\u6587\u63a2\u9488\u529b\u663e\u5fae\u955c\uff08KPFM\uff09\u63ed\u793a\u4e86\u63d2\u5c42\u533a\u57df\u7684\u8868\u9762\u7535\u52bf\u548c\u529f\u51fd\u6570\u53d8\u5316\uff0c\u4e3a\u63d2\u5c42\u6548\u5e94\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u7535\u5b50\u5b66\u7279\u5f81\u8bc1\u636e\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u5149\u8bf1\u5bfc\u529b\u663e\u5fae\u955c\uff08PiFM\uff09\u68c0\u6d4b\u63d2\u5c42\u533a\u57df\u7684\u5149\u5b66\u54cd\u5e94\u589e\u5f3a\uff0c\u62d3\u5c55\u4e86AFM\u6280\u672f\u5728\u5149\u5b66\u6027\u8d28\u8868\u5f81\u4e2d\u7684\u5e94\u7528\u3002  \n\u25c6 \u7efc\u5408\u591a\u79cdAFM\u6280\u672f\u5b9e\u73b0\u4e86\u63d2\u5c42\u6548\u5e94\u7684\u591a\u7ef4\u5ea6\u6620\u5c04\uff08\u7ed3\u6784\u3001\u529b\u5b66\u3001\u7535\u5b50\u3001\u5149\u5b66\uff09\uff0c\u4e3a\u4e8c\u7ef4\u6750\u6599\u6027\u80fd\u8c03\u63a7\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u7406\u8bba\u4f9d\u636e\u3002  \n\u25c6 \u8bc1\u660e\u4e86AFM\u6280\u672f\u5728\u4e8c\u7ef4\u6750\u6599\u63d2\u5c42\u7814\u7a76\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u666e\u9002\u6027\uff0c\u4e3a\u672a\u6765\u6750\u6599\u8bbe\u8ba1\u548c\u5668\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u8868\u5f81\u65b9\u6848\u3002|\n",
    "2506.20312": "|2025-06-25|On the Burstiness of Faces in Set|Jiong Wang|[2506.20312](http://arxiv.org/pdf/2506.20312)|\u65e0|\u25c6 \u9996\u6b21\u63ed\u793a\u4e86\u96c6\u5408\u4eba\u8138\u8bc6\u522b(SFR)\u4e2d\u666e\u904d\u5b58\u5728\u7684\"\u7a81\u53d1\u6027\"\u73b0\u8c61\uff0c\u5373\u7279\u5b9a\u5c5e\u6027\u4eba\u8138\u5728\u96c6\u5408\u4e2d\u9ad8\u9891\u51fa\u73b0\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u548c\u8bc4\u4f30\u5e72\u6270\u3002  \n\u25c6 \u63d0\u51fa\u4e09\u79cd\u7a81\u53d1\u6027\u4eba\u8138\u68c0\u6d4b\u7b56\u7565\uff1a\u57fa\u4e8eQuickshift++\u7684\u805a\u7c7b\u65b9\u6cd5\u3001\u7279\u5f81\u81ea\u76f8\u4f3c\u6027\u5206\u6790\u548c\u5e7f\u4e49\u6700\u5927\u6c60\u5316(GMP)\u6280\u672f\uff0c\u6709\u6548\u8bc6\u522b\u96c6\u5408\u4e2d\u7684\u9ad8\u9891\u4eba\u8138\u3002  \n\u25c6 \u5728\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u8c03\u6574\u91c7\u6837\u6bd4\u4f8b\u6291\u5236\u7a81\u53d1\u6027\u5f71\u54cd\uff0c\u5728\u8bc4\u4f30\u9636\u6bb5\u589e\u5f3a\u4f4e\u9891\u4eba\u8138\u7684\u8d21\u732e\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u65e0\u7ea6\u675f\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002  \n\u25c6 \u521b\u65b0\u6027\u63d0\u51fa\u8d28\u91cf\u611f\u77e5GMP\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u611f\u77e5\u4eba\u8138\u8d28\u91cf\u5e76\u5bf9\u4f4e\u8d28\u91cf\u56fe\u50cf\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u539f\u59cbGMP\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7a81\u53d1\u6027\u73b0\u8c61\u7684\u5e7f\u6cdb\u5b58\u5728\uff0c\u8bc1\u660e\u6291\u5236\u7a81\u53d1\u6027\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6709SFR\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u4e3a\u96c6\u5408\u4eba\u8138\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2506.21101": "|2025-06-26|OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography|Caoshuo Li\u7b49|[2506.21101](http://arxiv.org/pdf/2506.21101)|\u65e0|\u25c6 \u63d0\u51faOracleFusion\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u8bed\u4e49\u6392\u7248\u6280\u672f\u5e94\u7528\u4e8e\u7532\u9aa8\u6587\u7834\u8bd1\uff0c\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u751f\u6210\u8bed\u4e49\u589e\u5f3a\u7684\u77e2\u91cf\u5b57\u4f53\u3002  \n\u25c6 \u7b2c\u4e00\u9636\u6bb5\u91c7\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7ed3\u5408\u7a7a\u95f4\u611f\u77e5\u63a8\u7406\uff08SAR\uff09\uff0c\u5b9e\u73b0\u5bf9\u7532\u9aa8\u6587\u5b57\u5f62\u7684\u7ed3\u6784\u5206\u6790\u4e0e\u5173\u952e\u90e8\u4ef6\u89c6\u89c9\u5b9a\u4f4d\u3002  \n\u25c6 \u7b2c\u4e8c\u9636\u6bb5\u521b\u65b0\u6027\u5f15\u5165\u7532\u9aa8\u6587\u7ed3\u6784\u5411\u91cf\u878d\u5408\uff08OSVF\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5b57\u5f62\u7ed3\u6784\u7ea6\u675f\u548c\u5b57\u5f62\u4fdd\u6301\u7ea6\u675f\uff0c\u786e\u4fdd\u751f\u6210\u7ed3\u679c\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u4e0e\u8bed\u4e49\u51c6\u786e\u6027\u3002  \n\u25c6 \u5728\u89c6\u89c9\u5448\u73b0\u4e0a\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\uff0c\u751f\u6210\u517c\u5177\u7f8e\u5b66\u8d28\u91cf\u4e0e\u53ef\u8bfb\u6027\u7684\u5b57\u5f62\u8868\u8fbe\uff0c\u4e3a\u4e13\u5bb6\u7834\u8bd1\u63d0\u4f9b\u76f4\u89c2\u8f85\u52a9\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eOracleFusion\u5728\u8bed\u4e49\u76f8\u5173\u6027\u3001\u89c6\u89c9\u5438\u5f15\u529b\u548c\u5b57\u5f62\u4fdd\u6301\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u7834\u8bd1\u6548\u7387\u3002  \n\u25c6 \u6846\u67b6\u53ef\u5bf9\u672a\u89e3\u8bfb\u7532\u9aa8\u6587\u5b57\u7b26\u63d0\u4f9b\u4e13\u5bb6\u7ea7\u89c1\u89e3\uff0c\u6210\u4e3a\u63a8\u52a8\u7532\u9aa8\u6587\u7814\u7a76\u7684\u5b9e\u7528\u5de5\u5177\u3002|\n",
    "2506.22336": "|2025-06-27|MatChA: Cross-Algorithm Matching with Feature Augmentation|Paula Carb\u00f3 Cubero\u7b49|[2506.22336](http://arxiv.org/pdf/2506.22336)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u89e3\u51b3\u8de8\u7279\u5f81\u68c0\u6d4b\u5668\u89c6\u89c9\u5b9a\u4f4d\u95ee\u9898\u7684\u65b9\u6cd5MatChA\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u5fc5\u987b\u4f7f\u7528\u76f8\u540c\u68c0\u6d4b\u5668\u7684\u9650\u5236\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u901a\u8fc7\u7279\u5f81\u63cf\u8ff0\u7b26\u589e\u5f3a\u6280\u672f\u63d0\u5347\u8de8\u68c0\u6d4b\u5668\u7279\u5f81\u5339\u914d\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u70b9\u91cd\u590d\u7387\u4f4e\u548c\u63cf\u8ff0\u7b26\u533a\u5206\u5ea6\u4e0d\u8db3\u7684\u96be\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u5c06\u7279\u5f81\u8f6c\u6362\u5230\u6f5c\u5728\u7a7a\u95f4\u7684\u65b9\u6848\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u4e0d\u540c\u7b97\u6cd5\u751f\u6210\u63cf\u8ff0\u7b26\u7684\u517c\u5bb9\u5339\u914d\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u7279\u5f81\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u5339\u914d\u548c\u89c6\u89c9\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6848\u4f9d\u8d56\u5171\u540c\u5173\u952e\u70b9\u7684\u5047\u8bbe\uff0c\u66f4\u8d34\u5408\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u540c\u8bbe\u5907\u4f7f\u7528\u4e0d\u540c\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\u7684\u590d\u6742\u573a\u666f\u3002|\n",
    "2506.22939": "|2025-06-28|Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data|Ghufran A. Omran\u7b49|[2506.22939](http://arxiv.org/pdf/2506.22939)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a  \n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cCuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN)\u201d\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u6570\u636e\u7684\u573a\u666f\u5206\u7c7b\u3002  \n\u25c6 \u901a\u8fc7\u7ed3\u5408\u53cc\u5411\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u4f18\u5316\u7b97\u6cd5\uff08Cuttlefish\u4f18\u5316\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u9065\u611f\u6570\u636e\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002  \n\u25c6 \u5728\u5b9e\u9a8c\u4e2d\uff0cCO-BRNN\u7684\u51c6\u786e\u7387\u8fbe\u523097%\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u79cd\u65b9\u6cd5\uff08\u5982MLP-CNN\u3001CNN-LSTM\u3001LSTM-CRF\u7b49\uff09\uff0c\u5c55\u73b0\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9\u5927\u89c4\u6a21\u3001\u9ad8\u566a\u58f0\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5f3a\u8c03\u4e86\u7269\u7406\u9a8c\u8bc1\u5728\u536b\u661f\u6570\u636e\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u786e\u4fdd\u6a21\u578b\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002  \n\u25c6 \u4e3a\u9065\u611f\u573a\u666f\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u601d\u8def\uff0c\u53ef\u5e94\u7528\u4e8e\u707e\u5bb3\u63a7\u5236\u3001\u751f\u6001\u76d1\u6d4b\u3001\u57ce\u5e02\u89c4\u5212\u7b49\u591a\u4e2a\u9886\u57df\u3002|\n",
    "2506.22864": "|2025-06-28|Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval|Li-Cheng Shen\u7b49|[2506.22864](http://arxiv.org/pdf/2506.22864)|\u65e0|\u25c6 \u63d0\u51faMask-aware TIR\uff08MaTIR\uff09\u65b0\u4efb\u52a1\uff0c\u9996\u6b21\u5c06\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff08TIR\uff09\u4e0e\u6307\u4ee3\u8868\u8fbe\u5206\u5272\uff08RES\uff09\u7edf\u4e00\uff0c\u8981\u6c42\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u641c\u7d22\u548c\u7cbe\u786e\u76ee\u6807\u5206\u5272\u3002  \n\u25c6 \u8bbe\u8ba1\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528SAM 2\u548cAlpha-CLIP\u79bb\u7ebf\u751f\u6210\u5bf9\u8c61\u63a9\u7801\u548c\u533a\u57df\u7ea7\u5d4c\u5165\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5206\u5272\u611f\u77e5\u68c0\u7d22\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u91cd\u65b0\u6392\u5e8f\u5e76\u751f\u6210\u76ee\u6807\u6846\uff0c\u4e0e\u63a9\u7801\u5339\u914d\u63d0\u5347\u7cbe\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u7ed3\u5408\u5206\u5272\u6a21\u578b\uff08SAM 2\uff09\u4e0e\u8de8\u6a21\u6001\u68c0\u7d22\u6280\u672f\uff08Alpha-CLIP\uff09\uff0c\u5728\u79bb\u7ebf\u9636\u6bb5\u9884\u8ba1\u7b97\u63a9\u7801\u548c\u5d4c\u5165\uff0c\u663e\u8457\u964d\u4f4e\u5728\u7ebf\u68c0\u7d22\u8ba1\u7b97\u6210\u672c\u3002  \n\u25c6 \u5f15\u5165MLLM\u8fdb\u884c\u7ed3\u679c\u91cd\u6392\u548c\u5b9a\u4f4d\u4f18\u5316\uff0c\u5229\u7528\u5176\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u7387\u4e0e\u5206\u5272\u8d28\u91cf\u3002  \n\u25c6 \u5728COCO\u548cD$^3$\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u68c0\u7d22\u7cbe\u5ea6\u548c\u5206\u5272\u6548\u679c\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u8de8\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2507.00659": "|2025-07-01|LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment|Juelin Zhu\u7b49|[2507.00659](http://arxiv.org/pdf/2507.00659)|\u65e0|\u25c6 \u63d0\u51faLoD-Loc v2\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u4f4e\u7ec6\u8282\u5c42\u6b21\uff08LoD1\uff09\u57ce\u5e02\u6a21\u578b\u7684\u65e0\u4eba\u673a\u7a7a\u4e2d\u89c6\u89c9\u5b9a\u4f4d\uff0c\u7a81\u7834\u4ee5\u5f80\u4f9d\u8d56\u9ad8\u7ec6\u8282\u6a21\u578b\uff08LoD2/LoD3\uff09\u7684\u9650\u5236\u3002  \n\u25c6 \u91c7\u7528\u7c97\u5230\u7cbe\u7684\u53cc\u9636\u6bb5\u7b56\u7565\uff1a\u901a\u8fc7\u663e\u5f0f\u8f6e\u5ed3\u5bf9\u9f50\u6784\u5efa\u59ff\u6001\u4ee3\u4ef7\u4f53\u79ef\u7b5b\u9009\u7c97\u59ff\u6001\uff0c\u518d\u7ed3\u5408\u7c92\u5b50\u6ee4\u6ce2\u4e0e\u591a\u5149\u675f\u8ddf\u8e2a\u8fdb\u884c\u7cbe\u7ec6\u4f18\u5316\u3002  \n\u25c6 \u521b\u65b0\u6027\u8bbe\u8ba1\u59ff\u6001\u4ee3\u4ef7\u4f53\u79ef\uff0c\u901a\u8fc7\u5747\u5300\u91c7\u6837\u59ff\u6001\u5047\u8bbe\u5e76\u91cf\u5316\u6295\u5f71\u8f6e\u5ed3\u4e0e\u9884\u6d4b\u8f6e\u5ed3\u7684\u5bf9\u9f50\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u6982\u7387\u5206\u5e03\u5efa\u6a21\u3002  \n\u25c6 \u63d0\u51fa\u591a\u5149\u675f\u8ddf\u8e2a\u7684\u7c92\u5b50\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u663e\u8457\u6269\u5927\u6536\u655b\u57df\u5bb9\u9519\u8303\u56f4\uff0c\u53ef\u9002\u5e94\u66f4\u5927\u521d\u59cb\u59ff\u6001\u8bef\u5dee\u3002  \n\u25c6 \u53d1\u5e03\u9996\u4e2a\u8986\u76d610.7\u5e73\u65b9\u516c\u91cc\u7684LoD1\u57ce\u5e02\u6a21\u578b\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9eRGB\u67e5\u8be2\u56fe\u50cf\u4e0e\u59ff\u6001\u771f\u503c\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u3002  \n\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9ad8/\u4f4eLoD\u6a21\u578b\u4e0b\u5747\u5b9e\u73b0\u6700\u4f18\u7cbe\u5ea6\uff0c\u751a\u81f3\u8d85\u8d8a\u57fa\u4e8e\u7eb9\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4e3a\u5168\u7403\u57ce\u5e02\u5b9a\u4f4d\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2507.04735": "|2025-07-07|An analysis of vision-language models for fabric retrieval|Francesco Giuliari\u7b49|[2507.04735](http://arxiv.org/pdf/2507.04735)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u751f\u6210\u4e24\u79cd\u6587\u672c\u63cf\u8ff0\uff08\u81ea\u7531\u81ea\u7136\u8bed\u8a00\u548c\u7ed3\u6784\u5316\u5c5e\u6027\u63cf\u8ff0\uff09\uff0c\u89e3\u51b3\u4e86\u7ec7\u7269\u9886\u57df\u516c\u5f00\u6570\u636e\u96c6\u7684\u7f3a\u5931\u95ee\u9898\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e09\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08CLIP\u3001LAION-CLIP\u548cMeta Perception Encoder\uff09\u5728\u96f6\u6837\u672c\u7ec7\u7269\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002  \n\u25c6 \u53d1\u73b0\u7ed3\u6784\u5316\u5c5e\u6027\u63cf\u8ff0\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u590d\u6742\u7684\u7ec7\u7269\u7c7b\u522b\u4e2d\uff0c\u63ed\u793a\u4e86\u6587\u672c\u63cf\u8ff0\u5f62\u5f0f\u5bf9\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u5173\u952e\u5f71\u54cd\u3002  \n\u25c6 \u9a8c\u8bc1\u4e86Meta Perception Encoder\u51ed\u501f\u66f4\u5f3a\u7684\u7279\u5f81\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u7ec7\u7269\u68c0\u7d22\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u4f9d\u636e\u3002  \n\u25c6 \u6307\u51fa\u96f6\u6837\u672c\u68c0\u7d22\u5728\u7ec6\u7c92\u5ea6\u7ec7\u7269\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002  \n\u25c6 \u7ed3\u5408\u6280\u672f\u6027\u6587\u672c\u63cf\u8ff0\u4e0e\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7b56\u7565\uff0c\u4e3a\u5236\u9020\u4e1a\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002|\n",
    "2507.04667": "|2025-07-08|What's Making That Sound Right Now? Video-centric Audio-Visual Localization|Hahyeon Choi\u7b49|[2507.04667](http://arxiv.org/pdf/2507.04667)|\u65e0|\u25c6 \u63d0\u51faAVATAR\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9996\u6b21\u5f15\u5165\u89c6\u9891\u4e2d\u5fc3\u5316\u89c6\u89d2\uff0c\u89e3\u51b3\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\uff08AVL\uff09\u7814\u7a76\u4ec5\u5173\u6ce8\u9759\u6001\u56fe\u50cf\u7684\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u56db\u79cd\u590d\u6742\u573a\u666f\uff08\u5355\u58f0\u6e90\u3001\u6df7\u5408\u58f0\u6e90\u3001\u591a\u5b9e\u4f53\u3001\u5c4f\u5e55\u5916\u58f0\u6e90\uff09\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u58f0\u6e90\u53ef\u89c1\u4e14\u5355\u4e00\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u5f00\u53d1TAVLO\u6a21\u578b\uff0c\u9996\u521b\u9ad8\u5206\u8fa8\u7387\u65f6\u5e8f\u5efa\u6a21\u673a\u5236\uff0c\u6709\u6548\u6355\u6349\u58f0\u97f3\u4e0e\u89c6\u89c9\u5bf9\u8c61\u7684\u52a8\u6001\u5173\u8054\u3002  \n\u25c6 \u5b9e\u8bc1\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u56e0\u4f9d\u8d56\u5168\u5c40\u97f3\u9891\u7279\u5f81\u548c\u9010\u5e27\u6620\u5c04\uff0c\u96be\u4ee5\u8ffd\u8e2a\u65f6\u5e8f\u53d8\u5316\uff0c\u800cTAVLO\u901a\u8fc7\u65f6\u5e8f\u5efa\u6a21\u5b9e\u73b0\u7cbe\u51c6\u5bf9\u9f50\u3002  \n\u25c6 \u5efa\u7acb\u89c6\u9891\u4e2d\u5fc3\u5316AVL\u65b0\u6807\u51c6\uff0c\u9996\u6b21\u7cfb\u7edf\u8bba\u8bc1\u65f6\u5e8f\u52a8\u6001\u5bf9\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\u7684\u5173\u952e\u5f71\u54cd\u3002|\n",
    "2507.04662": "|2025-07-07|Simultaneous Localization and Mapping Using Active mmWave Sensing in 5G NR|Tao Du\u7b49|[2507.04662](http://arxiv.org/pdf/2507.04662)|\u65e0|\u25c6 \u63d0\u51fa\u5229\u7528\u6beb\u7c73\u6ce25G NR\u7cfb\u7edf\u8fdb\u884c\u4e3b\u52a8\u611f\u77e5\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u6fc0\u5149\u96f7\u8fbe\u7684\u70b9\u4e91\u751f\u6210\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u88ab\u52a8SLAM\u6280\u672f\u4f9d\u8d56\u955c\u9762\u53cd\u5c04\u5047\u8bbe\u7684\u5c40\u9650\u3002  \n\u25c6 \u91c7\u7528\u4e8c\u8fdb\u5236\u641c\u7d22\u65b9\u6cd5\u4ece\u6bcf\u4e2a\u6ce2\u675f\u65b9\u5411\u7684\u529f\u7387\u5ef6\u8fdf\u5256\u9762\u4e2d\u63d0\u53d6\u70b9\u4e91\uff0c\u63d0\u9ad8\u4e86\u73af\u5883\u611f\u77e5\u7684\u7cbe\u5ea6\u548c\u7ec6\u8282\u3002  \n\u25c6 \u901a\u8fc7\u591a\u76ee\u6807\u70b9\u6821\u51c6\u786c\u4ef6\u5ef6\u8fdf\uff0c\u786e\u4fdd\u70b9\u4e91\u6570\u636e\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u540e\u7eed\u5b9a\u4f4d\u548c\u5efa\u56fe\u63d0\u4f9b\u53ef\u9760\u8f93\u5165\u3002  \n\u25c6 \u5229\u7528\u70b9\u4e91\u914d\u51c6\u7b97\u6cd5\u4ece\u8fde\u7eed\u8f68\u8ff9\u89c6\u89d2\u4f30\u8ba1\u7ec8\u7aef\u4f4d\u59ff\u53d8\u5316\uff0c\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002  \n\u25c6 \u7ed3\u5408\u95ed\u73af\u68c0\u6d4b\u4e0e\u4f4d\u59ff\u56fe\u4f18\u5316\u6280\u672f\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u611f\u77e5\u7ed3\u679c\uff0c\u5b8c\u6210\u7cbe\u786e\u7684\u7ec8\u7aef\u5b9a\u4f4d\u548c\u65e0\u7ebf\u7535\u5730\u56fe\u91cd\u5efa\u3002  \n\u25c6 \u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4e3a5G NR\u5728SLAM\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8df5\u4f9d\u636e\u3002|\n",
    "2507.04503": "|2025-07-06|U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration|Xiaofan Li\u7b49|[2507.04503](http://arxiv.org/pdf/2507.04503)|\u65e0|\u25c6 \u63d0\u51faU-ViLAR\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u540c\u65f6\u7eb3\u5165\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u89c6\u89c9\u7279\u5f81\u6620\u5c04\u5230\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7a7a\u95f4\uff0c\u589e\u5f3a\u4e0e\u9ad8\u7cbe\u5730\u56fe\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u89c6\u89d2\u5dee\u5f02\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u7279\u5f81\u5173\u8054\u6a21\u5757\uff08Perceptual Uncertainty-guided Association\uff09\uff0c\u6709\u6548\u964d\u4f4e\u611f\u77e5\u8bef\u5dee\u5bf9\u5339\u914d\u7684\u5f71\u54cd\u3002  \n\u25c6 \u5f00\u53d1\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u914d\u51c6\u6a21\u5757\uff08Localization Uncertainty-guided Registration\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u5b9a\u4f4d\u7f6e\u4fe1\u5ea6\u4f18\u5316\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u3002  \n\u25c6 \u5b9e\u73b0\u5173\u8054\uff08\u5927\u8303\u56f4\u7c97\u5b9a\u4f4d\uff09\u4e0e\u914d\u51c6\uff08\u7cbe\u7ec6\u5b9a\u4f4d\uff09\u7684\u534f\u540c\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u5927\u573a\u666f\u8986\u76d6\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u5398\u7c73\u7ea7\u5b9a\u4f4d\u51c6\u786e\u6027\u3002  \n\u25c6 \u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u5b9e\u6d4b\u9a8c\u8bc1\uff0c\u5728GNSS\u5931\u6548\u3001\u52a8\u6001\u969c\u788d\u7269\u7b49\u590d\u6742\u573a\u666f\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u7efc\u5408\u7cbe\u5ea6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002|\n",
    "2507.03831": "|2025-07-04|Query-Based Adaptive Aggregation for Multi-Dataset Joint Training Toward Universal Visual Place Recognition|Jiuhong Xiao\u7b49|[2507.03831](http://arxiv.org/pdf/2507.03831)|\u65e0|\u25c6\u63d0\u51fa\u57fa\u4e8e\u67e5\u8be2\u7684\u81ea\u9002\u5e94\u805a\u5408\uff08QAA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u67e5\u8be2\u5411\u91cf\u4f5c\u4e3a\u53c2\u8003\u7801\u672c\uff0c\u6709\u6548\u63d0\u5347\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u4fe1\u606f\u5bb9\u91cf\uff0c\u907f\u514d\u4f20\u7edf\u7279\u5f81\u805a\u5408\u5c42\u7684\u4fe1\u606f\u9971\u548c\u95ee\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u5f15\u5165\u8de8\u67e5\u8be2\u76f8\u4f3c\u5ea6\uff08CS\uff09\u8ba1\u7b97\u673a\u5236\uff0c\u5229\u7528\u67e5\u8be2\u7ea7\u56fe\u50cf\u7279\u5f81\u4e0e\u53c2\u8003\u7801\u672c\u7684\u76f8\u4f3c\u6027\u751f\u6210\u9c81\u68d2\u63cf\u8ff0\u7b26\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6\u9996\u6b21\u5b9e\u73b0\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u7684\u901a\u7528\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\uff08VPR\uff09\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u5355\u6570\u636e\u96c6\u5cf0\u503c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5e73\u8861\u6cdb\u5316\u8868\u73b0\u3002  \n\u25c6\u901a\u8fc7\u53ef\u89c6\u5316\u5206\u6790\u63ed\u793a\u5b66\u4e60\u5230\u7684\u67e5\u8be2\u5411\u91cf\u5177\u6709\u8de8\u6570\u636e\u96c6\u7684\u591a\u6837\u5316\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4e3a\u591a\u6e90\u6570\u636e\u878d\u5408\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u4f9d\u636e\u3002  \n\u25c6\u5728\u8ba1\u7b97\u6548\u7387\u548c\u53c2\u6570\u91cf\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u672a\u663e\u8457\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\u3002  \n\u25c6\u5f00\u6e90\u4ee3\u7801\u5e76\u8f85\u4ee5\u8be6\u5c3d\u7684\u6d88\u878d\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86QAA\u673a\u5236\u7684\u53ef\u6269\u5c55\u6027\u548c\u6838\u5fc3\u7ec4\u4ef6\u6709\u6548\u6027\u3002|\n",
    "2507.05970": "|2025-07-08|Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval|Haiwen Li\u7b49|[2507.05970](http://arxiv.org/pdf/2507.05970)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u4e09\u5143\u7ec4\u751f\u6210\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCIR\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u5bfc\u81f4\u7684\u6269\u5c55\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u53d7\u9650\u95ee\u9898\u3002  \n\u25c6 \u6784\u5efa\u4e86\u9996\u4e2a\u5168\u5408\u6210\u6570\u636e\u96c6CIRHS\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u63d0\u793a\u8bcd\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63a7\u5236\u751f\u6210\u5177\u6709\u76f8\u540c\u5143\u7d20\u7684\u56fe\u50cf\u5bf9\uff0c\u7ecf\u7b5b\u9009\u91cd\u7ec4\u5f62\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u63d0\u51fa\u6df7\u5408\u4e0a\u4e0b\u6587\u5bf9\u9f50\u6846\u67b6\uff08CoAlign\uff09\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u5bf9\u9f50\u4e0e\u5c40\u90e8\u63a8\u7406\u7684\u534f\u540c\u4f18\u5316\uff0c\u80fd\u591f\u5b66\u4e60\u66f4\u9c81\u68d2\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u8868\u5f81\u3002  \n\u25c6 \u9996\u6b21\u9a8c\u8bc1\u4e86\u5728\u5168\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3CIR\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0cCoAlign\u5728\u4e09\u4e2a\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002  \n\u25c6 \u5728\u76d1\u7763\u8bad\u7ec3\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u5148\u8fdbCIR\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u68c0\u7d22\u6846\u67b6\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u548cCIRHS\u6570\u636e\u96c6\u5c06\u4fc3\u8fdbCIR\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002|\n",
    "2507.05631": "|2025-07-08|OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval|Zhiwei Chen\u7b49|[2507.05631](http://arxiv.org/pdf/2507.05631)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u5206\u5272\u7684\u7126\u70b9\u6620\u5c04\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u4e3b\u5bfc\u533a\u57df\u5206\u5272\u548c\u53cc\u91cd\u7126\u70b9\u6620\u5c04\u6a21\u5757\uff0c\u6709\u6548\u533a\u5206\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u533a\u57df\u4e0e\u566a\u58f0\uff0c\u63d0\u5347\u67e5\u8be2\u7279\u5f81\u8d28\u91cf\u3002  \n\u25c6 \u8bbe\u8ba1\u6587\u672c\u5f15\u5bfc\u7684\u7126\u70b9\u4fee\u6b63\u6a21\u5757\uff0c\u5229\u7528\u4fee\u6539\u6587\u672c\u7684\u8bed\u4e49\u4fe1\u606f\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u8003\u56fe\u50cf\u7684\u89c6\u89c9\u7126\u70b9\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6587\u672c\u4f18\u5148\u7ea7\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\u3002  \n\u25c6 \u9996\u6b21\u5728\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u4e2d\u5f15\u5165\u89c6\u89c9\u4e3b\u5bfc\u533a\u57df\u5206\u5272\u6280\u672f\uff0c\u51cf\u5c11\u566a\u58f0\u5e72\u6270\u5bf9\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u7684\u8d1f\u9762\u5f71\u54cd\u3002  \n\u25c6 \u901a\u8fc7\u53cc\u7126\u70b9\u6620\u5c04\u673a\u5236\u540c\u6b65\u4f18\u5316\u89c6\u89c9\u4e0e\u6587\u672c\u7279\u5f81\u63d0\u53d6\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u7528\u6237\u590d\u6742\u4fee\u6539\u610f\u56fe\u7684\u7406\u89e3\u80fd\u529b\u3002  \n\u25c6 \u6784\u5efa\u5b8c\u6574\u7f51\u7edcOFFSET\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\uff0c\u4e3aCIR\u9886\u57df\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\uff0c\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u53d1\u5c55\u3002|\n",
    "2507.05513": "|2025-07-07|Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model|Mengyao Xu\u7b49|[2507.05513](http://arxiv.org/pdf/2507.05513)|\u65e0|\u25c6 \u63d0\u51fallama-nemoretriever-colembed\u6a21\u578b\uff0c\u5b9e\u73b0\u6587\u672c-\u56fe\u50cf\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u9876\u5c16\u6027\u80fd\uff0c\u5728ViDoRe V1/V2\u57fa\u51c6\u4e0aNDCG@5\u5206\u522b\u8fbe\u523091.0\u548c63.5\uff0c\u5237\u65b0\u699c\u5355\u8bb0\u5f55\u3002  \n\u25c6 \u57fa\u4e8eNVIDIA Eagle2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u67b6\u6784\u6539\u9020\uff0c\u5c06\u56e0\u679c\u6ce8\u610f\u529b\u66ff\u6362\u4e3a\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\u80fd\u529b\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165ColBERT\u98ce\u683c\u7684\u5ef6\u8fdf\u4ea4\u4e92\u673a\u5236\uff0c\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u5339\u914d\u7cbe\u5ea6\u3002  \n\u25c6 \u8bbe\u8ba1\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u9884\u8bad\u7ec3\u518d\u5fae\u8c03\uff0c\u6709\u6548\u589e\u5f3a\u6a21\u578b\u68c0\u7d22\u80fd\u529b\u3002  \n\u25c6 \u5168\u9762\u5206\u6790\u6a21\u578b\u5728\u5b58\u50a8\u6548\u7387\u4e0e\u68c0\u7d22\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4f18\u5316\u4f9d\u636e\u3002  \n\u25c6 \u53d1\u5e031B\u548c3B\u4e24\u79cd\u53c2\u6570\u91cf\u53d8\u4f53\uff0c\u5176\u4e2d3B\u7248\u672c\u6210\u4e3a\u5f53\u524d\u6027\u80fd\u6700\u4f18\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u3002|\n",
    "2507.07079": "|2025-07-09|Evaluating Attribute Confusion in Fashion Text-to-Image Generation|Ziyue Liu\u7b49|[2507.07079](http://arxiv.org/pdf/2507.07079)|\u65e0|\u25c6 \u9488\u5bf9\u65f6\u5c1a\u9886\u57df\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u4efb\u52a1\u4e2d\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u5c5e\u6027\u5173\u8054\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u5b9a\u4f4d\u548c\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u7684\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\u3002  \n\u25c6 \u901a\u8fc7\u5355\u5b9e\u4f53\u5b9a\u4f4d\u7b56\u7565\uff0c\u5728\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4e0a\u540c\u6b65\u5206\u6790\u5c5e\u6027\u6df7\u6dc6\u73b0\u8c61\uff08\u5982\u5c5e\u6027\u6b63\u786e\u751f\u6210\u4f46\u5f52\u5c5e\u9519\u8bef\u5b9e\u4f53\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u590d\u6742\u7ec4\u5408\u8bed\u4e49\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u5c40\u90e8\u5316\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u521b\u65b0\u6027\u5730\u63d0\u51fa\u81ea\u52a8\u6307\u6807L-VQAScore\uff0c\u7ed3\u5408\u89c6\u89c9\u5b9a\u4f4d\u4e0eVQA\u6280\u672f\uff0c\u540c\u65f6\u68c0\u6d4b\u5c5e\u6027\u6b63\u786e\u53cd\u6620\uff08reflection\uff09\u548c\u9519\u8bef\u6cc4\u6f0f\uff08leakage\uff09\u60c5\u51b5\u3002  \n\u25c6 \u6784\u5efa\u4e86\u5305\u542b\u6311\u6218\u6027\u7ec4\u5408\u5bf9\u9f50\u573a\u666f\u7684\u65b0\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86L-VQAScore\u5728\u7ec6\u7c92\u5ea6\u5b9e\u4f53-\u5c5e\u6027\u5173\u8054\u8bc4\u4f30\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002  \n\u25c6 \u8be5\u5de5\u4f5c\u4e3a\u65f6\u5c1a\u9886\u57dfT2I\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5ba2\u89c2\u8bc4\u4f30\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4e3b\u89c2\u8bc4\u4ef7\u7684\u4f9d\u8d56\uff0c\u63a8\u52a8\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u8bed\u4e49\u573a\u666f\u4e0b\u7684\u7cbe\u51c6\u4f18\u5316\u3002|\n",
    "2507.06654": "|2025-07-09|MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval|Naoya Sogi\u7b49|[2507.06654](http://arxiv.org/pdf/2507.06654)|\u65e0|\u25c6 \u63d0\u51fa\u65b0\u4efb\u52a1CDR-CA\uff08\u590d\u5408\u5c5e\u6027\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u4f18\u5316\uff09\uff0c\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4e2d\u4e0d\u540c\u5e94\u7528\u573a\u666f\u5bf9\u591a\u6837\u6027\u9700\u6c42\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u591a\u5c5e\u6027\u591a\u6837\u6027\u6309\u9700\u8c03\u6574\u3002  \n\u25c6 \u63d0\u51fa\u591a\u6e90\u884c\u5217\u5f0f\u70b9\u8fc7\u7a0b\uff08MS-DPPs\uff09\uff0c\u5c06\u4f20\u7edfDPP\u6269\u5c55\u4e3a\u591a\u6e90\u5f62\u5f0f\uff0c\u901a\u8fc7\u6d41\u5f62\u8868\u793a\u6784\u5efa\u7edf\u4e00\u76f8\u4f3c\u6027\u77e9\u9635\uff0c\u652f\u6301\u591a\u5c5e\u6027\u8054\u5408\u4f18\u5316\u3002  \n\u25c6 \u5f15\u5165\u5207\u7ebf\u5f52\u4e00\u5316\uff08Tangent Normalization\uff09\u6280\u672f\uff0c\u6709\u6548\u878d\u5408\u4e0d\u540c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u52a8\u6001\u9002\u5e94\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u7684\u9700\u6c42\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u4e86MS-DPPs\u5728\u591a\u6837\u6027\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u5408\u5c5e\u6027\u63a7\u5236\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002  \n\u25c6 \u516c\u5f00\u4ee3\u7801\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u7684\u5b9e\u7528\u5316\u591a\u6837\u6027\u4f18\u5316\u63d0\u4f9b\u65b0\u57fa\u7ebf\u3002|\n",
    "2507.07467": "|2025-07-10|SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation|Juyeop Han\u7b49|[2507.07467](http://arxiv.org/pdf/2507.07467)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u573a\u666f\u5750\u6807\u56de\u5f52\uff08SCR\uff09\u548c\u8bc1\u636e\u5b66\u4e60\u7684\u65b0\u578b\u611f\u77e5\u611f\u77e5\u8f68\u8ff9\u751f\u6210\u6846\u67b6SCREP\uff0c\u7528\u4e8eGPS\u62d2\u6b62\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u98de\u884c\u3002  \n\u25c6 \u901a\u8fc7\u8bc1\u636e\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316SCR\u4f4d\u59ff\u4f30\u8ba1\u5668\uff0c\u80fd\u591f\u9884\u6d4b\u50cf\u7d20\u4e0d\u786e\u5b9a\u6027\u5e76\u5f15\u5bfc\u76f8\u673a\u671d\u5411\u53ef\u9760\u6027\u9ad8\u7684\u573a\u666f\u5750\u6807\u533a\u57df\u3002  \n\u25c6 \u91c7\u7528\u6eda\u52a8\u65f6\u57df\u8f68\u8ff9\u4f18\u5316\u5668\uff0c\u5b9e\u65f6\u8c03\u6574\u98de\u884c\u8f68\u8ff9\u4ee5\u6700\u5927\u5316\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u7ed3\u5408\u56fa\u5b9a\u6ede\u540e\u5e73\u6ed1\u5668\u878d\u5408\u4f4e\u9891SCR\u6570\u636e\u4e0e\u9ad8\u9891IMU\u6570\u636e\u3002  \n\u25c6 \u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u56fa\u5b9a\u504f\u822a\u548c\u524d\u89c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u5c06\u5e73\u79fb\uff08\u65cb\u8f6c\uff09\u5e73\u5747\u8bef\u5dee\u964d\u4f4e\u4e8654%/15%\uff0840%/31%\uff09\u3002  \n\u25c6 \u901a\u8fc7\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u65f6\u6027\u548c\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u611f\u77e5-\u63a7\u5236\u95ed\u73af\u7684\u9ad8\u6548\u8fd0\u884c\u3002|\n",
    "2507.07384": "|2025-07-10|VP-SelDoA: Visual-prompted Selective DoA Estimation of Target Sound via Semantic-Spatial Matching|Yu Chen\u7b49|[2507.07384](http://arxiv.org/pdf/2507.07384)|\u65e0|\u25c6 \u63d0\u51fa\u8de8\u5b9e\u4f8b\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\uff08CI-AVL\uff09\u65b0\u4efb\u52a1\uff0c\u5229\u7528\u540c\u7c7b\u58f0\u97f3\u4e8b\u4ef6\u7684\u4e0d\u540c\u5b9e\u4f8b\u56fe\u50cf\u5b9a\u4f4d\u76ee\u6807\u58f0\u6e90\uff0c\u51cf\u5c11\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1VP-SelDoA\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7ea7\u6a21\u6001\u878d\u5408\u548cFrequency-Temporal ConMamba\u67b6\u6784\u751f\u6210\u76ee\u6807\u9009\u62e9\u6027\u63a9\u7801\uff0c\u5b9e\u73b0\u591a\u58f0\u6e90\u573a\u666f\u4e0b\u7684\u76ee\u6807\u58f0\u6e90\u9694\u79bb\u3002  \n\u25c6 \u63d0\u51fa\u8bed\u4e49-\u7a7a\u95f4\u5339\u914d\u673a\u5236\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u5bf9\u9f50\u5f02\u6784\u7684\u8bed\u4e49\u4e0e\u7a7a\u95f4\u7279\u5f81\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u89c6\u89c9\u8bed\u4e49\u4e0e\u58f0\u5b66\u7a7a\u95f4\u7279\u5f81\u9519\u4f4d\u95ee\u9898\u3002  \n\u25c6 \u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6VGG-SSL\uff0c\u5305\u542b296\u7c7b\u58f0\u97f3\u4e8b\u4ef6\u768413,981\u6761\u7a7a\u95f4\u97f3\u9891\u7247\u6bb5\uff0c\u4e3aCI-AVL\u7814\u7a76\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002  \n\u25c6 \u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u548c\u51c6\u786e\u7387\uff08ACC\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5206\u522b\u8fbe\u523012.04\u548c78.23%\u3002|\n",
    "2507.07135": "|2025-07-08|FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval|Fran\u00e7ois Gard\u00e8res\u7b49|[2507.07135](http://arxiv.org/pdf/2507.07135)|\u65e0|\u25c6 \u63d0\u51fa\u4e86FACap\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u81ea\u52a8\u6784\u5efa\u7684\u65f6\u5c1a\u9886\u57df\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u4e13\u4e1a\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u4fee\u6539\u6587\u672c\uff0c\u964d\u4f4e\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002  \n\u25c6 \u63d0\u51fa\u4e86FashionBLIP-2\u6a21\u578b\uff0c\u901a\u8fc7\u5728FACap\u4e0a\u5fae\u8c03\u901a\u7528BLIP-2\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u591a\u5934\u67e5\u8be2-\u5019\u9009\u5339\u914d\u673a\u5236\uff0c\u63d0\u5347\u4e86\u65f6\u5c1a\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u5904\u7406\u80fd\u529b\u3002  \n\u25c6 \u5728Fashion IQ\u57fa\u51c6\u548c\u589e\u5f3a\u7248enhFashionIQ\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u6548\u679c\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5c1a\u9886\u57df\u7ec4\u5408\u68c0\u7d22\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u6587\u672c\u4fee\u6539\u573a\u666f\u3002  \n\u25c6 \u4e3a\u7535\u5546\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65f6\u5c1a\u56fe\u50cf\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u6784\u5efa\u9886\u57df\u4e13\u7528\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u9002\u914d\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002|\n",
    "2507.08546": "|2025-07-11|RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features|Inye Na\u7b49|[2507.08546](http://arxiv.org/pdf/2507.08546)|\u65e0|\u25c6 \u63d0\u51faRadiomicsRetrieval\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u624b\u5de5\u8bbe\u8ba1\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u5d4c\u5165\u7ed3\u5408\uff0c\u5b9e\u73b0\u80bf\u7624\u7ea7\u522b\u76843D\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\uff0c\u7a81\u7834\u73b0\u67092D\u65b9\u6cd5\u7684\u5c40\u9650\u3002  \n\u25c6 \u91c7\u7528\u53ef\u63d0\u793a\u5206\u5272\u6a21\u578b\uff08\u5982SAM\uff09\u751f\u6210\u80bf\u7624\u7279\u5f02\u6027\u56fe\u50cf\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u5bf9\u9f50\uff0c\u589e\u5f3a\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u3002  \n\u25c6 \u5f15\u5165\u89e3\u5256\u4f4d\u7f6e\u5d4c\u5165\uff08APE\uff09\uff0c\u4e3a\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u5168\u5c40\u89e3\u5256\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u57fa\u4e8e\u4f4d\u7f6e\u7684\u7075\u6d3b\u67e5\u8be2\u3002  \n\u25c6 \u6846\u67b6\u4ec5\u9700\u6700\u5c0f\u7528\u6237\u4ea4\u4e92\uff08\u5982\u5355\u70b9\u6807\u6ce8\uff09\uff0c\u663e\u8457\u964d\u4f4e\u5206\u5272\u5f00\u9500\uff0c\u9002\u5e94\u591a\u6837\u4e34\u5e8a\u573a\u666f\u3002  \n\u25c6 \u652f\u6301\u6df7\u5408\u67e5\u8be2\u6a21\u5f0f\uff08\u56fe\u50cf\u5d4c\u5165\u6216\u9009\u5b9a\u653e\u5c04\u7ec4\u5b66\u5c5e\u6027\uff09\uff0c\u63d0\u5347\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u53ca\u533b\u5b66\u7814\u7a76\u7684\u5b9e\u7528\u6027\u3002  \n\u25c6 \u5728\u80ba\u90e8CT\u548c\u8111\u90e8MRI\u516c\u5f00\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\uff0c\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u7279\u5f02\u6027\uff0cAPE\u5bf9\u57fa\u4e8e\u4f4d\u7f6e\u7684\u641c\u7d22\u81f3\u5173\u91cd\u8981\u3002|\n",
    "2507.08420": "|2025-07-11|LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps|Haitian Wang\u7b49|[2507.08420](http://arxiv.org/pdf/2507.08420)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408LiDAR\u3001GNSS\u548cIMU\u6570\u636e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u8fdb\u884c\u901f\u5ea6\u5bf9\u9f50\uff0c\u89e3\u51b3\u57ce\u5e02\u89c4\u6a213D\u5efa\u56fe\u65f6\u7684\u7d2f\u79ef\u6f02\u79fb\u95ee\u9898\u3002  \n\u25c6 \u91c7\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u4f18\u5316GNSS\u548cIMU\u4fe1\u53f7\uff0c\u7ed3\u5408\u57fa\u4e8e\u6b63\u6001\u5206\u5e03\u53d8\u6362\uff08NDT\uff09\u7684\u5c40\u90e8\u5efa\u56fe\u4e0e\u4f4d\u59ff\u56fe\u4f18\u5316\uff0c\u63d0\u5347\u5c40\u90e8\u7cbe\u5ea6\u3002  \n\u25c6 \u5f15\u5165GNSS\u7ea6\u675f\u951a\u70b9\u548c\u91cd\u53e0\u6bb5\u7cbe\u7ec6\u914d\u51c6\u6280\u672f\uff0c\u663e\u8457\u6539\u5584\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5c06\u5e73\u5747\u5168\u5c40\u5bf9\u9f50\u8bef\u5dee\u4ece3.32\u7c73\u964d\u4f4e\u81f31.24\u7c73\uff08\u63d0\u534761.4%\uff09\u3002  \n\u25c6 \u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b21\u6761\u57ce\u5e02\u73af\u7ebf\u768412.8\u4e07\u5e27128\u7ebfLiDAR\u6570\u636e\u3001\u540c\u6b65RTK-GNSS\u8f68\u8ff9\u53caMEMS-IMU\u6d4b\u91cf\u503c\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u9053\u8def\u4e2d\u5fc3\u7ebf\u548c\u4ea4\u53c9\u53e3\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u91cf\u5316\u5168\u5c40\u4e0e\u5c40\u90e8\u7cbe\u5ea6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5efa\u7acb\u65b0\u57fa\u51c6\u3002  \n\u25c6 \u6240\u6784\u5efa\u7684\u9ad8\u7cbe\u5ea6\u5730\u56fe\u652f\u6301\u667a\u6167\u57ce\u5e02\u89c4\u5212\u3001\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u7b49\u5e94\u7528\uff0c\u540c\u65f6\u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002|\n",
    "2507.08404": "|2025-07-11|Deep Hashing with Semantic Hash Centers for Image Retrieval|Li Chen\u7b49|[2507.08404](http://arxiv.org/pdf/2507.08404)|\u65e0|\u25c6 \u63d0\u51fa\u8bed\u4e49\u54c8\u5e0c\u4e2d\u5fc3\u6982\u5ff5\uff0c\u901a\u8fc7\u6570\u636e\u4f9d\u8d56\u7684\u76f8\u4f3c\u6027\u8ba1\u7b97\u6355\u6349\u7c7b\u522b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u53d6\u4ee3\u4f20\u7edf\u6570\u636e\u65e0\u5173\u7684\u54c8\u5e0c\u4e2d\u5fc3\u751f\u6210\u65b9\u6cd5\u3002  \n\u25c6 \u8bbe\u8ba1\u4e09\u9636\u6bb5\u6846\u67b6SHC\uff1a\u5148\u8bad\u7ec3\u5206\u7c7b\u7f51\u7edc\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u518d\u4f18\u5316\u751f\u6210\u4fdd\u7559\u8bed\u4e49\u7ed3\u6784\u7684\u54c8\u5e0c\u4e2d\u5fc3\uff0c\u6700\u540e\u8bad\u7ec3\u6df1\u5ea6\u54c8\u5e0c\u7f51\u7edc\u751f\u6210\u4e8c\u8fdb\u5236\u7801\u3002  \n\u25c6 \u5f00\u53d1\u65b0\u578b\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u76f8\u5173\u6027\u7684\u540c\u65f6\u5f3a\u5236\u6700\u5c0f\u4e2d\u5fc3\u95f4\u8ddd\uff0c\u907f\u514d\u54c8\u5e0c\u7801\u8fc7\u5ea6\u76f8\u4f3c\u7684\u95ee\u9898\u3002  \n\u25c6 \u9996\u6b21\u5c06\u7c7b\u522b\u8bed\u4e49\u5173\u7cfb\u5efa\u6a21\u4e3a\u6c49\u660e\u7a7a\u95f4\u7684\u8ddd\u79bb\u7ea6\u675f\uff0c\u4f7f\u76f8\u4f3c\u7c7b\u522b\u7684\u54c8\u5e0c\u4e2d\u5fc3\u8ddd\u79bb\u66f4\u8fd1\uff0c\u4e0d\u76f8\u4f3c\u7c7b\u522b\u66f4\u8fdc\u3002  \n\u25c6 \u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0cMAP@100/1000/ALL\u6307\u6807\u5e73\u5747\u63d0\u53477.26%/7.62%/11.71%\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002  \n\u25c6 \u63d0\u51fa\u7684\u6570\u636e\u4f9d\u8d56\u76f8\u4f3c\u6027\u8ba1\u7b97\u65b9\u6cd5\u80fd\u81ea\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5206\u5e03\uff0c\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2507.08021": "|2025-07-08|Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis|Li Li\u7b49|[2507.08021](http://arxiv.org/pdf/2507.08021)|\u65e0|\u25c6 \u9996\u6b21\u5bf9\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u6f14\u793a\u914d\u7f6e\u8fdb\u884c\u7cfb\u7edf\u6027\u5916\u90e8\u7814\u7a76\uff0c\u63a2\u7d22\u4e86\u793a\u4f8b\u6570\u91cf\u3001\u56fe\u50cf\u68c0\u7d22\u548c\u63cf\u8ff0\u5206\u914d\u4e09\u4e2a\u7ef4\u5ea6\u7684\u7b56\u7565\u3002  \n\u25c6 \u901a\u8fc7\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5178\u578b\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u6ce8\u610f\u529b\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u91cf\u5316\u6307\u6807\u4ee5\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\u3002  \n\u25c6 \u7ed3\u5408\u5916\u90e8\u5b9e\u9a8c\u4e0e\u5185\u90e8\u673a\u5236\u5206\u6790\u7684\u53cc\u91cd\u89c6\u89d2\uff0c\u63d0\u4f9b\u4e86\u7406\u89e3\u591a\u6a21\u6001ICL\u7684\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u793a\u4f8b\u914d\u7f6e\u5982\u4f55\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002  \n\u25c6 \u63d0\u51fa\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u6a21\u578b\u52a0\u901f\u4e0e\u538b\u7f29\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6790\u7684\u6a21\u578b\u4f18\u5316\u53ef\u884c\u6027\u3002  \n\u25c6 \u5bf9\u6bd4\u4e86\u76f8\u540c\u67b6\u6784\u4e0e\u9884\u8bad\u7ec3\u7b56\u7565\u7684LMM\u6027\u80fd\u5dee\u5f02\uff0c\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u7279\u5f81\u89d2\u5ea6\u89e3\u91ca\u4e86\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u7684\u539f\u56e0\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u7ed3\u5408\u5916\u90e8\u8bc4\u4f30\u4e0e\u5185\u90e8\u6307\u6807\u7684\u65b0\u65b9\u6cd5\u8bba\uff0c\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u5927\u6a21\u578b\u7814\u7a76\u9886\u57df\u3002|\n",
    "2507.10473": "|2025-07-14|GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space|David G. Shatwell\u7b49|[2507.10473](http://arxiv.org/pdf/2507.10473)|\u65e0|\u25c6 GT-Loc\u9996\u6b21\u63d0\u51fa\u8054\u5408\u5b66\u4e60\u56fe\u50cf\u62cd\u6444\u65f6\u95f4\u548c\u5730\u7406\u4f4d\u7f6e\u7684\u7edf\u4e00\u5d4c\u5165\u7a7a\u95f4\uff0c\u901a\u8fc7\u591a\u7f16\u7801\u5668\uff08\u56fe\u50cf\u3001\u65f6\u95f4\u3001\u5730\u70b9\uff09\u5728\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u4e09\u8005\u8868\u5f81\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u65f6\u95f4\u9884\u6d4b\u4f9d\u8d56\u5730\u7406\u4fe1\u606f\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u73af\u5f62\u65f6\u5e8f\u5ea6\u91cf\u5b66\u4e60\u76ee\u6807\uff0c\u5c06\u65f6\u95f4\u5dee\u5f02\u5efa\u6a21\u4e3a\u73af\u9762\uff08toroidal\uff09\u4e0a\u7684\u8f6f\u76ee\u6807\uff0c\u66ff\u4ee3\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u7684\u786c\u6b63\u8d1f\u6837\u672c\uff0c\u66f4\u8d34\u5408\u65f6\u95f4\u5468\u671f\u6027\u7684\u672c\u8d28\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8054\u5408\u4f18\u5316\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u5373\u4f7f\u5bf9\u6bd4\u90a3\u4e9b\u5728\u63a8\u7406\u9636\u6bb5\u4f7f\u7528\u771f\u5b9e\u5730\u7406\u4f4d\u7f6e\u4f5c\u4e3a\u8f93\u5165\u7684\u65b9\u6cd5\uff0c\u4ecd\u5c55\u73b0\u51fa\u66f4\u9ad8\u7cbe\u5ea6\u3002  \n\u25c6 \u5728\u6807\u51c6\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u7edf\u4e00\u5d4c\u5165\u7a7a\u95f4\u652f\u6301\u7ec4\u5408\u68c0\u7d22\uff08\u5982\"\u590f\u5b63\u9ec4\u660f\u7684\u5df4\u9ece\"\uff09\u548c\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u68c0\u7d22\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u63d0\u51fa\u65b0\u57fa\u51c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u4e0e\u5730\u7406\u7ebf\u7d22\u7684\u6df1\u5c42\u5173\u8054\uff0c\u4e3a\u8de8\u6a21\u6001\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002|\n",
    "2507.10403": "|2025-07-14|Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources|Daniele Rege Cambrin\u7b49|[2507.10403](http://arxiv.org/pdf/2507.10403)|\u65e0|\u25c6 \u63d0\u51faCrisisLandMark\u6570\u636e\u96c6\uff0c\u5305\u542b64.7\u4e07\u5f20Sentinel-1 SAR\u548cSentinel-2\u591a\u5149\u8c31\u56fe\u50cf\uff0c\u5e76\u914d\u5bf9\u4e86\u7ed3\u6784\u5316\u6587\u672c\u6807\u6ce8\uff0c\u8986\u76d6\u571f\u5730\u8986\u76d6\u3001\u571f\u5730\u5229\u7528\u548c\u5371\u673a\u4e8b\u4ef6\uff0c\u6570\u636e\u6765\u6e90\u6743\u5a01\u3002  \n\u25c6 \u5f00\u53d1CLOSP\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u4f5c\u4e3a\u6865\u6881\uff0c\u5c06\u672a\u914d\u5bf9\u7684\u5149\u5b66\u548cSAR\u56fe\u50cf\u5bf9\u9f50\u5230\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u68c0\u7d22\u3002  \n\u25c6 CLOSP\u5728\u68c0\u7d22\u6027\u80fd\u4e0a\u53d6\u5f97\u7a81\u7834\uff0cnDGC\u6307\u6807\u6bd4\u73b0\u6709\u6a21\u578b\u63d0\u534754%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u679c\u3002  \n\u25c6 \u63d0\u51fa\u7edf\u4e00\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5149\u5b66\u56fe\u50cf\u7684\u4e30\u5bcc\u8bed\u4e49\u77e5\u8bc6\u95f4\u63a5\u8f85\u52a9SAR\u56fe\u50cf\u89e3\u8bd1\uff0c\u514b\u670dSAR\u56fe\u50cf\u89e3\u8bd1\u7684\u56fa\u6709\u56f0\u96be\u3002  \n\u25c6 \u6269\u5c55GeoCLOSP\u6a21\u578b\uff0c\u6574\u5408\u5730\u7406\u5750\u6807\u4fe1\u606f\uff0c\u5728\u901a\u7528\u8bed\u4e49\u4efb\u52a1\u548c\u5730\u7406\u4f4d\u7f6e\u76f8\u5173\u7684\u5371\u673a\u4e8b\u4ef6\u68c0\u7d22\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u6210\u4e3a\u7279\u5b9a\u9886\u57df\u7684\u4e13\u5bb6\u3002  \n\u25c6 \u5f3a\u8c03\u591a\u4f20\u611f\u5668\u6570\u636e\u548c\u5730\u7406\u4e0a\u4e0b\u6587\u6574\u5408\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u9065\u611f\u6863\u6848\u7684\u5168\u9762\u5229\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2507.10265": "|2025-07-14|Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures|Xinlong Ding\u7b49|[2507.10265](http://arxiv.org/pdf/2507.10265)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u2014\u2014\u4e07\u82b1\u7b52\u80cc\u666f\u653b\u51fb\uff08KBA\uff09\uff0c\u901a\u8fc7\u591a\u91cd\u590d\u5236\u5bf9\u79f0\u7eb9\u7406\u6784\u9020\u5706\u5f62\u80cc\u666f\u56fe\u6848\uff0c\u6709\u6548\u5e72\u6270\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u6a21\u578b\u3002  \n\u25c6\u9996\u6b21\u5229\u7528\u81ea\u7136\u7eb9\u7406\u7247\u6bb5\u6784\u5efa\u5177\u6709\u591a\u91cd\u590d\u5236\u5bf9\u79f0\u6027\u7684\u5706\u76d8\u7ed3\u6784\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u4fdd\u6301\u9ad8\u5ea6\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u7684\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e86\u6295\u5f71\u65b9\u5411\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u4f18\u5316\u4e07\u82b1\u7b52\u7eb9\u7406\u7247\u6bb5\u7684\u7a7a\u95f4\u5206\u5e03\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u653b\u51fb\u6548\u679c\u3002  \n\u25c6\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u653b\u51fb\u591a\u79cd\u4e3b\u6d41\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u7a00\u758f\u8f93\u5165\u573a\u666f\u4e0b\u80cc\u666f\u7eb9\u7406\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7684\u5173\u952e\u5f71\u54cd\u3002  \n\u25c6\u4e3a\u5bf9\u6297\u653b\u51fb\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c06\u51e0\u4f55\u5bf9\u79f0\u6027\u4e0e\u81ea\u7136\u7eb9\u7406\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u590d\u6742\u5bf9\u6297\u6837\u672c\u751f\u6210\u7684\u9ad8\u6548\u653b\u51fb\u3002|\n",
    "2507.10571": "|2025-07-09|Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning|Konstantinos I. Roumeliotis\u7b49|[2507.10571](http://arxiv.org/pdf/2507.10571)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6a21\u5757\u5316Agentic AI\u89c6\u89c9\u5206\u7c7b\u6846\u67b6\uff0c\u5c06\u901a\u7528\u591a\u6a21\u6001\u667a\u80fd\u4f53\u4e0e\u975e\u89c6\u89c9\u63a8\u7406\u534f\u8c03\u5668\u3001RAG\u6a21\u5757\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u5143\u63a8\u7406\u7684\u5206\u79bb\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u4fe1\u4efb\u611f\u77e5\u534f\u8c03\u673a\u5236\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6307\u6807\uff08ECE/OCR/CCC\uff09\u52a8\u6001\u8c03\u8282\u5bf9\u591a\u667a\u80fd\u4f53\u7684\u4fe1\u4efb\u5ea6\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u51c6\u786e\u7387\u63d0\u534777.94%\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u57fa\u4e8eCLIP\u56fe\u50cf\u68c0\u7d22\u548c\u91cd\u8bc4\u4f30\u5faa\u73af\u7684\u4fe1\u4efb\u6821\u51c6\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u76f8\u4f3c\u6848\u4f8b\u4fee\u6b63\u667a\u80fd\u4f53\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u589e\u5f3a\u9884\u6d4b\u53ef\u89e3\u91ca\u6027\u3002  \n\u25c6 \u5728\u82f9\u679c\u53f6\u75c5\u5bb3\u8bca\u65ad\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e09\u79cd\u914d\u7f6e\uff1a\u96f6\u6837\u672c\u7f6e\u4fe1\u534f\u8c03\u3001\u5fae\u8c03\u667a\u80fd\u4f53\u4f18\u5316\u3001\u4ee5\u53caRAG\u589e\u5f3a\u7684\u4fe1\u4efb\u6821\u51c6\u534f\u8c03\uff0c\u6700\u9ad8\u8fbe85.63%\u51c6\u786e\u7387\u3002  \n\u25c6 \u53d1\u73b0GPT-4o\u5177\u6709\u66f4\u4f18\u6821\u51c6\u6027\uff0c\u800cQwen-2.5-VL\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002  \n\u25c6 \u5f00\u6e90\u5168\u90e8\u6a21\u578b\u3001\u63d0\u793a\u8bcd\u3001\u8f6f\u4ef6\u4ee3\u7801\u53ca\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4e3a\u53ef\u4fe1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5efa\u7acb\u53ef\u590d\u73b0\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u751f\u7269\u8bca\u65ad\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002|\n",
    "2507.12416": "|2025-07-16|QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval|Jaehyun Kwak\u7b49|[2507.12416](http://arxiv.org/pdf/2507.12416)|\u65e0|\u25c6 \u63d0\u51faQuRe\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u8d1f\u6837\u672c\u91c7\u6837\u89e3\u51b3\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u4e2d\u5047\u9634\u6027\u95ee\u9898\uff0c\u4f18\u5316\u5956\u52b1\u6a21\u578b\u76ee\u6807\u4ee5\u63d0\u5347\u68c0\u7d22\u76f8\u5173\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u65b0\u9896\u7684\u786c\u8d1f\u6837\u672c\u91c7\u6837\u7b56\u7565\uff0c\u9009\u62e9\u76ee\u6807\u56fe\u50cf\u540e\u76f8\u5173\u6027\u5206\u6570\u4e24\u6b21\u9661\u964d\u4e4b\u95f4\u7684\u56fe\u50cf\uff0c\u6709\u6548\u8fc7\u6ee4\u5047\u9634\u6027\u6837\u672c\u3002  \n\u25c6 \u521b\u5efaHP-FashionIQ\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5728CIR\u4efb\u52a1\u4e2d\u660e\u786e\u6355\u83b7\u7528\u6237\u504f\u597d\uff0c\u8d85\u8d8a\u4f20\u7edf\u4ec5\u5173\u6ce8\u76ee\u6807\u56fe\u50cf\u68c0\u7d22\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eQuRe\u5728FashionIQ\u548cCIRR\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728HP-FashionIQ\u4e0a\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u504f\u597d\u6700\u5f3a\u7684\u5bf9\u9f50\u6027\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u4e3aCIR\u9886\u57df\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u65b9\u6cd5\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u5bfc\u5411\u7684\u8bc4\u4f30\u6846\u67b6\u3002|\n",
    "2507.11834": "|2025-07-16|CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning|Peiwen Xia\u7b49|[2507.11834](http://arxiv.org/pdf/2507.11834)|\u65e0|\u25c6 \u63d0\u51faCorrMoE\u6846\u67b6\uff0c\u9996\u6b21\u9488\u5bf9\u8de8\u573a\u666f\u548c\u8de8\u57df\u5bf9\u5e94\u70b9\u4fee\u526a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u57df\u4e0d\u4e00\u81f4\u548c\u573a\u666f\u7ed3\u6784\u591a\u6837\u65f6\u7684\u6027\u80fd\u74f6\u9888\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u53bb\u98ce\u683c\u5316\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u901a\u8fc7\u9690\u5f0f\u548c\u663e\u5f0f\u56fe\u7279\u5f81\u7684\u98ce\u683c\u6df7\u5408\uff0c\u6709\u6548\u51cf\u5c11\u57df\u7279\u5f02\u6027\u8868\u5f81\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u63d0\u5347\u8de8\u57df\u9c81\u68d2\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u53cc\u878d\u5408\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff08Bi-Fusion MoE\uff09\uff0c\u7ed3\u5408\u7ebf\u6027\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u4e13\u5bb6\u8def\u7531\uff0c\u81ea\u9002\u5e94\u6574\u5408\u591a\u89c6\u89d2\u7279\u5f81\u4ee5\u5e94\u5bf9\u573a\u666f\u591a\u6837\u6027\u3002  \n\u25c6 \u5728\u7279\u5f81\u878d\u5408\u4e2d\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u4f18\u5316\uff0c\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u964d\u4f4e\u4f20\u7edfTransformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u4e13\u5bb6\u6a21\u578b\u7684\u52a8\u6001\u9002\u5e94\u6027\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8de8\u57df\u548c\u8de8\u573a\u666f\u4efb\u52a1\u4e2d\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002|\n",
    "2507.12823": "|2025-07-17|FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and Adaptive Reconciliation for Composed Image Retrieval|Jeong-Woo Park\u7b49|[2507.12823](http://arxiv.org/pdf/2507.12823)|\u65e0|\u25c6 \u63d0\u51faFAR-Net\u591a\u9636\u6bb5\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u65e9\u671f\u548c\u665a\u671f\u878d\u5408\u4f18\u52bf\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9-\u6587\u672c\u6a21\u6001\u878d\u5408\u4e2d\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\uff08ESAM\uff09\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5173\u8054\uff0c\u5f25\u8865\u665a\u671f\u878d\u5408\u5bf9\u5c40\u90e8\u5bf9\u9f50\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u5f15\u5165\u81ea\u9002\u5e94\u534f\u8c03\u6a21\u5757\uff08ARM\uff09\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u5d4c\u5165\u7684\u65e9\u671f\u878d\u5408\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e73\u8861\u6587\u672c\u663e\u5f0f\u63cf\u8ff0\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u6574\u5408ESAM\u4e0eARM\uff0c\u5f62\u6210\u4e92\u8865\u673a\u5236\uff0c\u540c\u65f6\u6355\u6349\u5168\u5c40\u8bed\u4e49\u548c\u5c40\u90e8\u7ec6\u8282\uff0c\u63d0\u5347\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u7cbe\u5ea6\u3002  \n\u25c6 \u5728CIRR\u548cFashionIQ\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0cRecall@1\u6700\u9ad8\u63d0\u53472.4%\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002|\n",
    "2507.12819": "|2025-07-17|MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval|Jeong-Woo Park\u7b49|[2507.12819](http://arxiv.org/pdf/2507.12819)|\u65e0|\u25c6 \u63d0\u51faMCoT-RE\u6846\u67b6\uff0c\u9996\u6b21\u5728\u96f6\u6837\u672c\u8bad\u7ec3\u81ea\u7531\u7684\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u5f15\u5165\u591a\u89d2\u5ea6\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\u6280\u672f\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u6a21\u6001\u4ea4\u4e92\u4e0d\u8db3\u7684\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u53cc\u8def\u5f84\u751f\u6210\u7b56\u7565\uff0c\u5206\u522b\u751f\u6210\u4fa7\u91cd\u6587\u672c\u4fee\u6539\u7684 caption \u548c\u878d\u5408\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684 caption\uff0c\u5e73\u8861\u663e\u5f0f\u4fee\u6539\u4e0e\u9690\u5f0f\u89c6\u89c9\u7ebf\u7d22\u7684\u5229\u7528\u3002  \n\u25c6 \u8bbe\u8ba1\u4e24\u9636\u6bb5\u68c0\u7d22\u6d41\u7a0b\uff1a\u9996\u9636\u6bb5\u7528\u4fee\u6539\u5bfc\u5411 caption \u7c97\u7b5b\u5019\u9009\u56fe\u50cf\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7ed3\u5408\u53cc caption \u548c\u53c2\u8003\u56fe\u50cf\u8fdb\u884c\u591a\u7c92\u5ea6\u91cd\u6392\u5e8f\uff0c\u63d0\u5347\u7cbe\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u601d\u7ef4\u94fe\u6269\u5c55\u81f3\u591a\u6a21\u6001\u573a\u666f\uff0c\u6307\u5bfc MLLM \u540c\u65f6\u5904\u7406\u6587\u672c\u6307\u4ee4\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u907f\u514d\u4fe1\u606f\u4e22\u5931\u3002  \n\u25c6 \u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff0cFashionIQ \u7684 Recall@10 \u63d0\u9ad8 6.24%\uff0cCIRR \u7684 Recall@1 \u63d0\u5347 8.58%\uff0c\u8fbe\u5230\u96f6\u6837\u672c\u65b9\u6cd5\u6700\u4f18\u6027\u80fd\u3002  \n\u25c6 \u6574\u4e2a\u6846\u67b6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4ec5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4fdd\u6301\u9ad8\u6548\u4f4e\u6210\u672c\u4f18\u52bf\u7684\u540c\u65f6\u7a81\u7834\u73b0\u6709\u6280\u672f\u74f6\u9888\u3002|\n",
    "2507.15109": "|2025-07-20|LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM|Mohammad-Maher Nakshbandi\u7b49|[2507.15109](http://arxiv.org/pdf/2507.15109)|\u65e0|\u25c6 \u63d0\u51faLoopNet\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u5927\u89c4\u6a21SLAM\u4e2d\u7684\u95ed\u73af\u68c0\u6d4b\u95ee\u9898\uff0c\u517c\u987e\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u6027\u9700\u6c42\u3002  \n\u25c6 \u91c7\u7528\u6539\u8fdb\u7684ResNet\u67b6\u6784\uff0c\u652f\u6301\u52a8\u6001\u89c6\u89c9\u6570\u636e\u96c6\u7684\u5728\u7ebf\u91cd\u8bad\u7ec3\uff0c\u5e76\u9488\u5bf9\u5d4c\u5165\u5f0f\u8bbe\u5907\u8fdb\u884c\u4f18\u5316\uff0c\u9002\u5e94\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u3002  \n\u25c6 \u521b\u65b0\u6027\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u7b56\u7565\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883\uff0c\u540c\u65f6\u8f93\u51fa\u67e5\u8be2\u7d22\u5f15\u548c\u9884\u6d4b\u8d28\u91cf\u8bc4\u4f30\uff0c\u589e\u5f3a\u7cfb\u7edf\u53ef\u9760\u6027\u3002  \n\u25c6 \u5229\u7528DISK\u63cf\u8ff0\u7b26\u66ff\u4ee3\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u6216\u5e38\u89c4\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5149\u7167\u3001\u89c6\u89d2\u7b49\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u95ed\u73af\u68c0\u6d4b\u6027\u80fd\u3002  \n\u25c6 \u5f00\u6e90\u4e86\u65b0\u578b\u95ed\u73af\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6LoopDB\uff0c\u586b\u8865\u73b0\u6709\u6570\u636e\u5728\u52a8\u6001\u573a\u666f\u548c\u5d4c\u5165\u5f0f\u786c\u4ef6\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u6574\u4f53\u65b9\u6848\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u5747\u5df2\u516c\u5f00\uff0c\u63a8\u52a8SLAM\u9886\u57df\u7814\u7a76\u53ef\u590d\u73b0\u6027\u3002|\n",
    "2507.15089": "|2025-07-20|Visual Place Recognition for Large-Scale UAV Applications|Ioannis Tsampikos Papapetros\u7b49|[2507.15089](http://arxiv.org/pdf/2507.15089)|\u65e0|\u25c6 \u63d0\u51fa\u4e86LASED\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6100\u4e07\u5f20\u56fe\u7247\uff0c\u8986\u76d6\u7231\u6c99\u5c3c\u4e9a17\u4e07\u4e2a\u72ec\u7279\u5730\u70b9\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5730\u7406\u548c\u65f6\u95f4\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u822a\u7a7a\u573a\u666f\u4e2d\u7684\u8bad\u7ec3\u6548\u679c\u3002  \n\u25c6 \u6570\u636e\u96c6\u91c7\u7528\u7ed3\u6784\u5316\u8bbe\u8ba1\uff0c\u786e\u4fdd\u5730\u70b9\u5206\u79bb\u6e05\u6670\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u4e0d\u8db3\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002  \n\u25c6 \u63d0\u51fa\u4f7f\u7528\u53ef\u8f6c\u5411\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08steerable CNNs\uff09\u5904\u7406\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u65cb\u8f6c\u6a21\u7cca\u95ee\u9898\uff0c\u5229\u7528\u5176\u65cb\u8f6c\u7b49\u53d8\u6027\u751f\u6210\u65b9\u5411\u4e0d\u53d8\u7684\u7279\u5f81\u8868\u793a\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8eLASED\u8bad\u7ec3\u7684\u6a21\u578b\u53ec\u56de\u7387\u663e\u8457\u9ad8\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u51f8\u663e\u4e86\u5730\u7406\u8986\u76d6\u548c\u65f6\u95f4\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002  \n\u25c6 \u53ef\u8f6c\u5411CNN\u5728\u65cb\u8f6c\u6a21\u7cca\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u53ec\u56de\u7387\u6bd4\u6700\u4f73\u975e\u53ef\u8f6c\u5411\u7f51\u7edc\u63d0\u9ad812%\uff0c\u6709\u6548\u63d0\u5347\u4e86\u822a\u7a7a\u89c6\u89c9\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u7ed3\u5408\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u6570\u636e\u96c6\u548c\u65cb\u8f6c\u7b49\u53d8\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u822a\u7a7a\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002|\n",
    "2507.14902": "|2025-07-20|U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs|Xiaojie Li\u7b49|[2507.14902](http://arxiv.org/pdf/2507.14902)|\u65e0|\u25c6 \u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8eMLLMs\u7684\u901a\u7528\u591a\u6a21\u6001\u68c0\u7d22\uff08UMR\uff09\u4e2d\u5f71\u54cd\u5d4c\u5165\u5b66\u4e60\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63ed\u793a\u4e86\u5e38\u88ab\u5ffd\u89c6\u7684\u8bad\u7ec3\u7ec6\u8282\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684MLLM\u5d4c\u5165\u5b66\u4e60\u6846\u67b6U-MARVEL\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8fc7\u6e21\u3001\u56f0\u96be\u8d1f\u6837\u672c\u6316\u6398\u548c\u91cd\u6392\u5e8f\u84b8\u998f\u7b49\u7b56\u7565\u4f18\u5316\u5d4c\u5165\u751f\u6210\u548c\u8bad\u7ec3\u8fc7\u7a0b\u3002  \n\u25c6 \u5728\u76d1\u7763\u5b66\u4e60\u573a\u666f\u4e0b\uff0cU-MARVEL\u5728M-BEIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002  \n\u25c6 \u6846\u67b6\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u3001\u6587\u672c-\u89c6\u9891\u68c0\u7d22\u7b49\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u7814\u7a76\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u7cfb\u7edf\u5316\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u53d1\u5c55\u3002|\n",
    "2507.14477": "|2025-07-19|OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition|Zhenyu Li\u7b49|[2507.14477](http://arxiv.org/pdf/2507.14477)|\u65e0|\u25c6 OptiCorNet\u63d0\u51fa\u9996\u4e2a\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff0c\u5c06\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u4e0e\u65f6\u5e8f\u5dee\u5206\u7edf\u4e00\u5230\u5355\u4e00\u6a21\u5757\u4e2d\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5355\u5e27\u5d4c\u5165\u65b9\u6cd5\u7684\u5c40\u9650\u3002  \n\u25c6 \u521b\u65b0\u8bbe\u8ba1\u53ef\u5fae\u5206\u65f6\u5e8f\u5dee\u5206\u7b97\u5b50\uff08DSD\uff09\uff0c\u901a\u8fc7\u56fa\u5b9a\u6743\u91cd\u5dee\u5206\u6838\u6355\u6349\u65b9\u5411\u6027\u5e8f\u5217\u5dee\u5f02\uff0c\u7ed3\u5408LSTM\u7cbe\u4fee\u6a21\u5757\uff0c\u6709\u6548\u5efa\u6a21\u77ed\u65f6\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u957f\u7a0b\u65f6\u5e8f\u5173\u8054\u3002  \n\u25c6 \u5f15\u5165\u6b8b\u5dee\u6295\u5f71\u673a\u5236\u589e\u5f3a\u63cf\u8ff0\u7b26\u5224\u522b\u529b\uff0c\u751f\u6210\u7684\u7d27\u51d1\u5e8f\u5217\u63cf\u8ff0\u7b26\u5bf9\u89c6\u89d2\u53d8\u5316\u548c\u5916\u89c2\u5dee\u5f02\u5177\u6709\u663e\u8457\u9c81\u68d2\u6027\u3002  \n\u25c6 \u91c7\u7528\u56db\u5143\u7ec4\u635f\u5931\u51fd\u6570\u540c\u6b65\u4f18\u5316\u6279\u6b21\u5185\u6b63\u6837\u672c\u5bf9\u9f50\u4e0e\u591a\u8d1f\u6837\u672c\u5206\u79bb\uff0c\u663e\u8457\u63d0\u5347\u8de8\u573a\u666f\u7c7b\u95f4\u53ef\u5206\u6027\u3002  \n\u25c6 \u9996\u6b21\u5b9e\u73b0\u65f6\u5e8f\u805a\u5408\u7684\u7aef\u5230\u7aef\u8054\u5408\u5b66\u4e60\uff0c\u76f8\u6bd4\u540e\u5904\u7406\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u5e8f\u5217\u7ea7\u5d4c\u5165\uff0c\u5728\u5b63\u8282\u548c\u89c6\u89d2\u53d8\u5316\u573a\u666f\u4e0b\u53d6\u5f97\u7a81\u7834\u6027\u6027\u80fd\u63d0\u5347\u3002  \n\u25c6 \u8f7b\u91cf\u7ea71D\u5377\u79ef\u7f16\u7801\u5668\u8bbe\u8ba1\u786e\u4fdd\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002|\n",
    "2507.14215": "|2025-07-16|Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired|Jiayu\u7b49|[2507.14215](http://arxiv.org/pdf/2507.14215)|\u65e0|\u25c6 \u5f00\u53d1\u4e86JerryNet\uff0c\u4e00\u79cd\u5b9a\u5236CNN\u67b6\u6784\uff0c\u53ef\u5b9e\u65f6\u7cbe\u786e\u5b9a\u4f4d9\u4e2a\u65b9\u5411\u7684\u58f0\u6e90\uff0c\u65b9\u5411\u8bc6\u522b\u51c6\u786e\u7387\u8fbe91.1%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002  \n\u25c6 \u57fa\u4e8eCLAP\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u7eaf\u97f3\u9891\u5206\u7c7b\uff0c\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u548cAudioSet\u4e0a\u5206\u522b\u8fbe\u523098.5%\u548c95%\u7684\u51c6\u786e\u7387\u3002  \n\u25c6 \u63d0\u51fa\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7cbe\u786e\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u58f0\u6e90\uff0c\u91c7\u7528Yolov9\u76ee\u6807\u68c0\u6d4b\u548c\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\u6a21\u5757\uff0ccIoU\u8fbe0.892\uff0cAUC\u4e3a0.658\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u786c\u4ef6\u7cfb\u7edf\uff0c\u5305\u62ec\u56db\u9ea6\u514b\u98ce\u77e9\u5f62\u9635\u5217\u548c\u773c\u955c\u5f0f\u6444\u50cf\u5934\uff0c\u901a\u8fc7\u8155\u5e26\u663e\u793a\u65b9\u5411\u7b49\u5173\u952e\u4fe1\u606f\uff0c\u63d0\u5347\u804b\u54d1\u4eba\u7fa4\u7684\u5b9e\u65f6\u4ea4\u4e92\u4f53\u9a8c\u3002  \n\u25c6 \u586b\u8865\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u9488\u5bf9\u5f31\u52bf\u7fa4\u4f53\u7684\u6280\u672f\u7a7a\u767d\uff0c\u4e3a\u65b0\u4e00\u4ee3\u65e0\u969c\u788d\u8bbe\u5907\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\u3002  \n\u25c6 \u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u5168\u9762\u9a8c\u8bc1\u7cfb\u7edf\u6027\u80fd\uff0c\u5404\u9879\u6307\u6807\u5747\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2507.17455": "|2025-07-23|VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization|Sania Waheed\u7b49|[2507.17455](http://arxiv.org/pdf/2507.17455)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u5730\u7406\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u68c0\u7d22\u5f0f\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u65b9\u6cd5\u7684\u4f18\u52bf\u3002  \n\u25c6 \u5229\u7528VLM\u751f\u6210\u5730\u7406\u5148\u9a8c\u4fe1\u606f\uff0c\u6709\u6548\u7f29\u5c0f\u68c0\u7d22\u641c\u7d22\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u611f\u77e5\u6df7\u6dc6\u4e0a\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u91cd\u6392\u5e8f\u673a\u5236\uff0c\u7ed3\u5408\u7279\u5f81\u76f8\u4f3c\u5ea6\u548c\u521d\u59cb\u5750\u6807\u90bb\u8fd1\u6027\uff0c\u7b5b\u9009\u5730\u7406\u5408\u7406\u6027\u6700\u9ad8\u7684\u5339\u914d\u7ed3\u679c\u3002  \n\u25c6 \u5728\u591a\u4e2a\u5730\u7406\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8857\u9053\u7ea7\uff08\u63d0\u53474.51%\uff09\u548c\u57ce\u5e02\u7ea7\uff08\u63d0\u534713.52%\uff09\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002  \n\u25c6 \u901a\u8fc7VLM\u4e0eVPR\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u9ad8\u7cbe\u5ea6\u7684\u884c\u661f\u7ea7\u5730\u7406\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u5355\u4e00\u65b9\u6cd5\u5b58\u5728\u7684\u5e7b\u89c9\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002|\n",
    "2507.17412": "|2025-07-23|Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging|Farnaz Khun Jush\u7b49|[2507.17412](http://arxiv.org/pdf/2507.17412)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u9884\u5206\u5272\u6570\u636e\u548c\u5668\u5b98\u7279\u5f02\u6027\u6570\u636e\u96c6\u7684CBIR\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u4e2d\u5927\u578b\u975e\u7ed3\u6784\u5316\u56fe\u50cf\u5f52\u6863\u7cfb\u7edf\uff08\u5982PACS\uff09\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165C-MIR\u65b9\u6cd5\uff0c\u5c06ColBERT\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5ef6\u8fdf\u4ea4\u4e92\u673a\u5236\u9002\u914d\u4e8e3D\u533b\u5b66\u5f71\u50cf\u91cd\u6392\u5e8f\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\u3002  \n\u25c6 \u5728\u56db\u79cd\u80bf\u7624\u90e8\u4f4d\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7ed3\u5408\u4e09\u79cd\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4e09\u79cd\u6570\u636e\u5e93\u914d\u7f6e\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u666e\u9002\u6027\u3002  \n\u25c6 \u7814\u7a76\u53d1\u73b0C-MIR\u80fd\u81ea\u52a8\u5b9a\u4f4d\u611f\u5174\u8da3\u533a\u57df\uff0c\u65e0\u9700\u6570\u636e\u9884\u5206\u5272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u6570\u636e\u589e\u5f3a\u7684\u8ba1\u7b97\u6210\u672c\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eC-MIR\u5728\u80bf\u7624\u6807\u8bb0\uff08\u5c24\u5176\u7ed3\u80a0\u548c\u80ba\u764c\uff09\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08p<0.05\uff09\uff0c\u5e76\u5728\u80bf\u7624\u5206\u671f\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002  \n\u25c6 \u8be5\u7814\u7a76\u4e3a\u5148\u8fdb\u68c0\u7d22\u6280\u672f\u5728\u533b\u7597\u5b9e\u8df5\u4e2d\u7684\u843d\u5730\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u4f18\u5316\u4e34\u5e8a\u8bca\u65ad\u6d41\u7a0b\u3002|\n",
    "2507.18444": "|2025-07-24|DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition|Haiyang Jiang\u7b49|[2507.18444](http://arxiv.org/pdf/2507.18444)|\u65e0|\u25c6 \u63d0\u51faDSFormer\u53cc\u5c3a\u5ea6\u4ea4\u53c9\u5b66\u4e60Transformer\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u5411\u4fe1\u606f\u4f20\u9012\u6574\u5408CNN\u6700\u540e\u4e24\u5c42\u7684\u53cc\u5c3a\u5ea6\u7279\u5f81\uff0c\u540c\u65f6\u6355\u6349\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u7a7a\u95f4\u7ec6\u8282\u3002  \n\u25c6 \u8bbe\u8ba1\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5355\u5c3a\u5ea6\u5185\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u5171\u4eab\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u5b66\u4e60\uff0c\u589e\u5f3a\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u63d0\u51fa\u591a\u89c6\u89d2\u5757\u805a\u7c7b\u7b56\u7565\uff0c\u91cd\u6784SF-XL\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5206\u533a\u65b9\u5f0f\uff0c\u4f18\u5316\u6570\u636e\u7ec4\u7ec7\u4ee5\u63d0\u5347\u5bf9\u89c6\u89d2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u7ed3\u5408\u4e0a\u8ff0\u6280\u672f\uff0c\u751f\u6210\u9002\u5e94\u73af\u5883\u53d8\u5316\u7684\u9c81\u68d2\u5168\u5c40\u5d4c\u5165\u8868\u5f81\uff0c\u76f8\u6bd4\u5148\u524d\u5206\u533a\u65b9\u6cd5\u51cf\u5c11\u7ea630%\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002  \n\u25c6 \u4ec5\u4f7f\u7528512\u7ef4\u5168\u5c40\u63cf\u8ff0\u7b26\u5373\u5b9e\u73b0\u5168\u5c40\u68c0\u7d22\uff0c\u5728\u591a\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aDELG\u3001Patch-NetVLAD\u7b49\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002  \n\u25c6 \u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.20934": "|2025-07-28|Exploring text-to-image generation for historical document image retrieval|Melissa Cote\u7b49|[2507.20934](http://arxiv.org/pdf/2507.20934)|\u65e0|\u25c6 \u63d0\u51faT2I-QBE\u65b0\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u6280\u672f\u5e94\u7528\u4e8e\u6587\u6863\u56fe\u50cf\u68c0\u7d22\uff08DIR\uff09\u9886\u57df\uff0c\u586b\u8865\u4e86\u57fa\u4e8e\u5c5e\u6027\u68c0\u7d22\uff08ABDIR\uff09\u4e0e\u57fa\u4e8e\u793a\u4f8b\u68c0\u7d22\uff08QBE\uff09\u4e4b\u95f4\u7684\u6280\u672f\u9e3f\u6c9f\u3002  \n\u25c6 \u5229\u7528\u751f\u6210\u5f0fAI\uff08Leonardo.Ai\uff09\u5c06\u6587\u672c\u63d0\u793a\uff08\u5305\u542b\u6587\u6863\u7c7b\u578b\u63cf\u8ff0\u548cABDIR\u98ce\u683c\u5c5e\u6027\u5217\u8868\uff09\u8f6c\u5316\u4e3a\u67e5\u8be2\u56fe\u50cf\uff0c\u65e0\u9700\u7528\u6237\u63d0\u4f9b\u771f\u5b9e\u67e5\u8be2\u6837\u672c\uff0c\u89e3\u51b3\u4e86QBE\u4f9d\u8d56\u73b0\u6709\u6837\u672c\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u9488\u5bf9\u5386\u53f2\u6587\u6863\u7684\u89c6\u89c9\u591a\u6837\u6027\u548c\u72ec\u7279\u6027\u8bbe\u8ba1\u68c0\u7d22\u65b9\u6848\uff0c\u901a\u8fc7CNN\u63d0\u53d6\u751f\u6210\u56fe\u50cf\u4e0e\u6570\u636e\u96c6\u4e2d\u6587\u6863\u7684\u7279\u5f81\u8fdb\u884c\u76f8\u4f3c\u5ea6\u5339\u914d\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210\u56fe\u50cf\u4f5c\u4e3a\u67e5\u8be2\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u5728HisIR19\u5386\u53f2\u6587\u6863\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u68c0\u7d22\u76f8\u5173\u6587\u6863\uff0c\u4e3a\u65e0\u6837\u672c\u573a\u666f\u4e0b\u7684\u6587\u6863\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u9996\u6b21\u63a2\u7d22\u4e86T2I\u751f\u6210\u6280\u672f\u4e0e\u4f20\u7edfQBE\u8303\u5f0f\u7684\u7ed3\u5408\uff0c\u4e3aDIR\u9886\u57df\u5f00\u8f9f\u4e86\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b0\u7814\u7a76\u65b9\u5411\u3002|\n",
    "2507.20892": "|2025-07-28|PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs|Sergey Bakulin\u7b49|[2507.20892](http://arxiv.org/pdf/2507.20892)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7ecf\u5178\u6a21\u578b\u89c4\u5212\u7b97\u6cd5\u7684\u6df7\u5408\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\uff0c\u7a81\u7834\u4e86\u7eaf\u7aef\u5230\u7aef\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u91c7\u7528\u5206\u5c42\u7cfb\u7edf\u67b6\u6784\uff0c\u6574\u5408\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3001\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u3001\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u548c\u4f4d\u59ff\u4f30\u8ba1\u7b49\u591a\u9879\u524d\u6cbf\u6280\u672f\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u4f7f\u7528\u62d3\u6251\u56fe\u4f5c\u4e3a\u73af\u5883\u8868\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u73af\u5883\u9002\u5e94\u80fd\u529b\u3002  \n\u25c6 \u76f8\u6bd4\u7aef\u5230\u7aef\u65b9\u6848\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86\u9ed1\u7bb1\u6a21\u578b\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002  \n\u25c6 \u5728\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u4f9d\u8d56\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u4f18\u52bf\u3002|\n",
    "2507.20564": "|2025-07-28|ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning|Duc-Tai Dinh\u7b49|[2507.20564](http://arxiv.org/pdf/2507.20564)|\u65e0|\u25c6 \u63d0\u51faZSE-Cap\u7cfb\u7edf\uff0c\u5728EVENTA\u7ade\u8d5b\u4e2d\u65e0\u9700\u5fae\u8c03\u5373\u83b7\u5f97\u7b2c\u56db\u540d\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u7684\u5f3a\u5927\u80fd\u529b\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u96c6\u6210CLIP\u3001SigLIP\u548cDINOv2\u4e09\u79cd\u6a21\u578b\u7684\u76f8\u4f3c\u5ea6\u5206\u6570\uff0c\u63d0\u5347\u56fe\u50cf\u68c0\u7d22\u6027\u80fd\u3002  \n\u25c6 \u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u5f15\u5bfcGemma 3\u6a21\u578b\uff0c\u5b9e\u73b0\u6587\u7ae0\u9ad8\u5c42\u4e8b\u4ef6\u4e0e\u56fe\u50cf\u89c6\u89c9\u5185\u5bb9\u7684\u5173\u8054\u751f\u6210\u63cf\u8ff0\u3002  \n\u25c6 \u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u96c6\u6210\u548c\u63d0\u793a\u6280\u672f\uff0c\u5728\u79c1\u6709\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f970.42002\u7684\u9ad8\u5206\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u63d0\u4f9b\u5f00\u6e90\u4ee3\u7801\uff0c\u4fc3\u8fdb\u96f6\u6837\u672c\u56fe\u50cf\u68c0\u7d22\u4e0e\u63cf\u8ff0\u751f\u6210\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002|\n",
    "2507.20538": "|2025-07-28|Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments|Gilhwan Kang\u7b49|[2507.20538](http://arxiv.org/pdf/2507.20538)|\u65e0|\u25c6 Uni-Mapper\u63d0\u51fa\u9996\u4e2a\u52a8\u6001\u611f\u77e5\u7684\u591a\u6a21\u6001LiDAR\u5730\u56fe\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u591a\u4f20\u611f\u5668\u5730\u56fe\u878d\u5408\u7684\u96be\u9898\u3002  \n\u25c6 \u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u81ea\u7531\u7a7a\u95f4\u54c8\u5e0c\u7684\u7c97\u5230\u7ec6\u52a8\u6001\u7269\u4f53\u5254\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u5e8f\u5360\u7528\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u5e76\u79fb\u9664\u52a8\u6001\u5bf9\u8c61\uff0c\u63d0\u5347\u573a\u666f\u4e00\u81f4\u6027\u3002  \n\u25c6 \u521b\u65b0\u8bbe\u8ba1\u52a8\u6001\u611f\u77e5\u7684\u95ed\u73af\u68c0\u6d4b\u6a21\u5757\uff0c\u7ed3\u5408\u4fdd\u7559\u7684\u9759\u6001\u5c40\u90e8\u7279\u5f81\u751f\u6210\u5168\u5c40\u63cf\u8ff0\u7b26\uff0c\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5730\u70b9\u8bc6\u522b\u9c81\u68d2\u6027\u3002  \n\u25c6 \u63d0\u51fa\u96c6\u4e2d\u5f0f\u951a\u8282\u70b9\u7b56\u7565\u4f18\u5316\u4f4d\u59ff\u56fe\uff0c\u6709\u6548\u89e3\u51b3\u5730\u56fe\u5408\u5e76\u65f6\u7684\u4f1a\u8bdd\u5185\u6f02\u79fb\u8bef\u5dee\u548c\u8de8\u5730\u56fe\u95ed\u73af\u95ee\u9898\u3002  \n\u25c6 \u6846\u67b6\u652f\u6301\u5f02\u6784LiDAR\uff08\u5982\u673a\u68b0\u5f0f\u4e0e\u56fa\u6001\u96f7\u8fbe\uff09\u7684\u8de8\u6a21\u6001\u5339\u914d\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002  \n\u25c6 \u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u591a\u5730\u56fe\u5bf9\u9f50\u6d41\u7a0b\uff0c\u5305\u542b\u52a8\u6001\u5904\u7406\u3001\u95ed\u73af\u68c0\u6d4b\u4e0e\u591a\u9636\u6bb5\u4f4d\u59ff\u56fe\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u573a\u666f\u3002|\n",
    "2507.21742": "|2025-07-29|Adversarial Reconstruction Feedback for Robust Fine-grained Generalization|Shijie Wang\u7b49|[2507.21742](http://arxiv.org/pdf/2507.21742)|\u65e0|\u25c6 \u63d0\u51faAdvRF\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u91cd\u5efa\u53cd\u9988\u673a\u5236\u5b66\u4e60\u4e0e\u7c7b\u522b\u65e0\u5173\u7684\u5dee\u5f02\u8868\u5f81\uff0c\u89e3\u51b3\u73b0\u6709\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5bf9\u9884\u5b9a\u4e49\u7c7b\u522b\u7684\u8bed\u4e49\u4f9d\u8d56\u95ee\u9898\u3002  \n\u25c6 \u5c06\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u89c9\u5dee\u5f02\u91cd\u5efa\u4efb\u52a1\uff0c\u7ed3\u5408\u68c0\u7d22\u6a21\u578b\u7684\u7c7b\u522b\u611f\u77e5\u5dee\u5f02\u5b9a\u4f4d\u4e0e\u91cd\u5efa\u6a21\u578b\u7684\u7c7b\u522b\u65e0\u5173\u7279\u5f81\u5b66\u4e60\uff0c\u5b9e\u73b0\u53cc\u5411\u4f18\u5316\u3002  \n\u25c6 \u901a\u8fc7\u91cd\u5efa\u6a21\u578b\u63ed\u793a\u68c0\u7d22\u6a21\u578b\u5ffd\u7565\u7684\u6b8b\u5dee\u5f02\uff0c\u8feb\u4f7f\u68c0\u7d22\u6a21\u578b\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u68c0\u7d22\u6a21\u578b\u7684\u4f18\u5316\u4fe1\u53f7\u6307\u5bfc\u91cd\u5efa\u6a21\u578b\u6539\u8fdb\u91cd\u5efa\u80fd\u529b\u3002  \n\u25c6 \u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u5c06\u91cd\u5efa\u6a21\u578b\u751f\u6210\u7684\u7c7b\u522b\u65e0\u5173\u5dee\u5f02\u8868\u5f81\u8fc1\u79fb\u5230\u68c0\u7d22\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002  \n\u25c6 \u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86AdvRF\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u5747\u663e\u793a\u5176\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2507.22791": "|2025-07-30|Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques|Weide Liu\u7b49|[2507.22791](http://arxiv.org/pdf/2507.22791)|\u65e0|\u25c6 \u5168\u9762\u7efc\u8ff0\u4e86\u5355\u6a21\u6001\u4e0e\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u6280\u672f\uff0c\u6db5\u76d6RGB\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u50cf\u30013D\u70b9\u4e91\u3001LiDAR\u626b\u63cf\u3001\u533b\u5b66\u56fe\u50cf\u53ca\u89c6\u89c9-\u8bed\u8a00\u4ea4\u4e92\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7cfb\u7edf\u6027\u603b\u7ed3\u7684\u7a7a\u767d\u3002  \n\u25c6 \u5bf9\u6bd4\u5206\u6790\u4e86\u4f20\u7edf\u624b\u5de5\u65b9\u6cd5\uff08\u5982Harris\u89d2\u70b9\u3001SIFT\u548cORB\u63cf\u8ff0\u7b26\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982SuperPoint\u548cLoFTR\uff09\u7684\u4f18\u52a3\uff0c\u6307\u51fa\u540e\u8005\u5728\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002  \n\u25c6 \u91cd\u70b9\u4ecb\u7ecd\u4e86\u6a21\u6001\u611f\u77e5\u6280\u672f\u8fdb\u5c55\uff0c\u5305\u62ec\u9488\u5bf9\u6df1\u5ea6\u56fe\u50cf\u7684\u51e0\u4f55\u4e0e\u6df1\u5ea6\u4e13\u7528\u63cf\u8ff0\u7b26\u30013D\u70b9\u4e91\u7684\u7a00\u758f\u4e0e\u7a20\u5bc6\u5b66\u4e60\u65b9\u6cd5\u3001LiDAR\u626b\u63cf\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u53ca\u533b\u5b66\u56fe\u50cf\u5339\u914d\u7684MIND\u63cf\u8ff0\u7b26\u7b49\u521b\u65b0\u65b9\u6848\u3002  \n\u25c6 \u6df1\u5165\u63a2\u8ba8\u8de8\u6a21\u6001\u5e94\u7528\u573a\u666f\uff0c\u5982\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u548c\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u5339\u914d\u6280\u672f\u5728\u5904\u7406\u591a\u6837\u5316\u6570\u636e\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u4e0e\u53d1\u5c55\u8d8b\u52bf\u3002  \n\u25c6 \u7cfb\u7edf\u603b\u7ed3\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u5411\u66f4\u590d\u6742\u3001\u66f4\u5b9e\u7528\u7684\u573a\u666f\u62d3\u5c55\u3002|\n",
    "2507.23629": "|2025-07-31|DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching|Yewei Huang\u7b49|[2507.23629](http://arxiv.org/pdf/2507.23629)|\u65e0|\u25c6 \u63d0\u51faDRACo-SLAM2\u6846\u67b6\uff0c\u6539\u8fdb\u539f\u6709\u7cfb\u7edf\uff0c\u4e13\u4e3a\u914d\u5907\u591a\u6ce2\u675f\u6210\u50cf\u58f0\u7eb3\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u56e2\u961f\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0fSLAM\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u58f0\u7eb3\u5730\u56fe\u8868\u793a\u4e3a\u5bf9\u8c61\u56fe\uff0c\u901a\u8fc7\u5bf9\u8c61\u56fe\u5339\u914d\u5b9e\u73b0\u9ad8\u6548\u8de8\u673a\u5668\u4eba\u95ed\u73af\u68c0\u6d4b\uff0c\u65e0\u9700\u4f9d\u8d56\u5148\u9a8c\u51e0\u4f55\u4fe1\u606f\u3002  \n\u25c6 \u9488\u5bf9\u6c34\u4e0b\u626b\u63cf\u5339\u914d\u7279\u70b9\uff0c\u63d0\u51fa\u589e\u91cf\u5f0f\u7fa4\u7ec4\u4e00\u81f4\u6d4b\u91cf\u96c6\u6700\u5927\u5316\uff08GCM\uff09\u65b9\u6cd5\uff0c\u6539\u8fdb\u539f\u6709PCM\u7b97\u6cd5\uff0c\u6709\u6548\u5904\u7406\u76f8\u90bb\u8de8\u673a\u5668\u4eba\u95ed\u73af\u5171\u4eab\u76f8\u4f3c\u914d\u51c6\u8bef\u5dee\u7684\u573a\u666f\u3002  \n\u25c6 \u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u5bf9\u6bd4\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u5b9e\u7528\u6027\u3002  \n\u25c6 \u6846\u67b6\u5728\u901a\u4fe1\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u7279\u522b\u9002\u5408\u6c34\u4e0b\u673a\u5668\u4eba\u56e2\u961f\u534f\u4f5c\u5efa\u56fe\u4e0e\u5b9a\u4f4d\u9700\u6c42\u3002|\n",
    "2507.23569": "|2025-07-31|Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization|Maxime Pietrantoni\u7b49|[2507.23569](http://arxiv.org/pdf/2507.23569)|\u65e0|\u25c6 \u63d0\u51fa\u9ad8\u65af\u6cfc\u6e85\u7279\u5f81\u573a\uff08GSFFs\uff09\uff0c\u5c06\u663e\u5f0f\u51e0\u4f55\u6a21\u578b\uff083DGS\uff09\u4e0e\u9690\u5f0f\u7279\u5f81\u573a\u7ed3\u5408\uff0c\u7528\u4e8e\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u3002  \n\u25c6 \u5229\u75283DGS\u7684\u5bc6\u96c6\u51e0\u4f55\u4fe1\u606f\u548c\u53ef\u5fae\u5206\u5149\u6805\u5316\u7b97\u6cd5\uff0c\u5b66\u4e60\u57fa\u4e8e3D\u7a7a\u95f4\u7684\u9c81\u68d2\u7279\u5f81\u8868\u793a\u3002  \n\u25c6 \u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c063D\u5c3a\u5ea6\u611f\u77e5\u7279\u5f81\u573a\u4e0e2D\u7279\u5f81\u7f16\u7801\u5668\u5bf9\u9f50\u5230\u540c\u4e00\u5d4c\u5165\u7a7a\u95f4\uff0c\u63d0\u5347\u7279\u5f81\u4e00\u81f4\u6027\u3002  \n\u25c6 \u5f15\u51653D\u7ed3\u6784\u611f\u77e5\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u6b63\u5219\u5316\u8868\u5f81\u5b66\u4e60\u5e76\u751f\u6210\u53ef\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u573a\u666f\u5206\u5272\u7ed3\u679c\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u56fe\u6216\u5206\u5272\u56fe\u5bf9\u9f50\u7684\u4f4d\u59ff\u4f18\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u548c\u975e\u9690\u79c1\u4fdd\u62a4\u4e24\u79cd\u5b9a\u4f4d\u6d41\u7a0b\u3002  \n\u25c6 \u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5148\u8fdb\u6027\uff0c\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u5b9a\u4f4d\u6027\u80fd\u3002|\n",
    "2507.22938": "|2025-07-25|A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents|Sumit Soman\u7b49|[2507.22938](http://arxiv.org/pdf/2507.22938)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u5c06\u6d41\u7a0b\u56fe\u8f6c\u5316\u4e3a\u56fe\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6587\u672c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u56fe\u50cf\u95ee\u7b54\u7684\u75db\u70b9\u3002  \n\u25c6 \u9996\u6b21\u5c06\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b(VLM)\u751f\u6210\u7684\u6d41\u7a0b\u56fe\u56fe\u8868\u793a\u4e0e\u6587\u672c\u5d4c\u5165\u7ba1\u9053\u7ed3\u5408\uff0c\u5b9e\u73b0\u7535\u4fe1\u9886\u57df\u591a\u6a21\u6001\u95ee\u7b54\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u6280\u672f\u6587\u6863\u5904\u7406\u3001\u56fe\u50cf\u7c7b\u578b\u5206\u7c7b\u3001\u56fe\u8868\u793a\u6784\u5efa\u7b49\u5173\u952e\u6b65\u9aa4\uff0c\u5f62\u6210\u53ef\u843d\u5730\u7684\u7cfb\u7edf\u67b6\u6784\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u5fae\u8c03\u540e\u7684VLM\u751f\u6210\u7684\u56fe\u8868\u793a\u4e0e\u771f\u5b9e\u503c\u7f16\u8f91\u8ddd\u79bb\u66f4\u5c0f\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9\u6d41\u7a0b\u56fe\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5728\u63a8\u7406\u9636\u6bb5\u65e0\u9700\u4f7f\u7528VLM\uff0c\u4ec5\u9700\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u3002  \n\u25c6 \u5728\u7535\u4fe1\u4ea7\u54c1\u6587\u6863\u6784\u5efa\u7684QA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u7535\u4fe1\u9886\u57df\u9002\u914d\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002|\n",
    "2508.02034": "|2025-08-04|Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure|Ziling Wang\u7b49|[2508.02034](http://arxiv.org/pdf/2508.02034)|\u65e0|\u25c6\u63d0\u51faProtego\uff0c\u4e00\u79cd\u7528\u6237\u4e2d\u5fc3\u5316\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc73D\u9762\u90e8\u7b7e\u540d\u751f\u6210\u59ff\u6001\u4e0d\u53d8\u76842D\u8868\u793a\uff0c\u52a8\u6001\u53d8\u5f62\u4e3a\u81ea\u71363D\u9762\u5177\uff0c\u9002\u914d\u7528\u6237\u4efb\u610f\u59ff\u6001\u8868\u60c5\u7684\u56fe\u50cf\uff0c\u5728\u5206\u4eab\u524d\u8fdb\u884c\u4fdd\u62a4\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u589e\u5f3aFR\u6a21\u578b\u654f\u611f\u6027\uff0c\u4f7f\u53d7\u4fdd\u62a4\u56fe\u50cf\u65e0\u6cd5\u76f8\u4e92\u5339\u914d\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u4ec5\u9632\u5fa1\u5916\u90e8\u67e5\u8be2\u7684\u5c40\u9650\u3002  \n\u25c6\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u79cd\u9ed1\u76d2FR\u6a21\u578b\u4e0b\u663e\u8457\u964d\u4f4e\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u6027\u80fd\u81f3\u5c11\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd52\u500d\u3002  \n\u25c6\u9996\u6b21\u5b9e\u73b0\u89c6\u9891\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u8fde\u8d2f\u6027\u4fdd\u62a4\uff0c\u6ee1\u8db3\u52a8\u6001\u5185\u5bb9\u5bf9\u4e00\u81f4\u6027\u548c\u81ea\u7136\u5916\u89c2\u7684\u9ad8\u8981\u6c42\u3002  \n\u25c6\u4e3a\u5bf9\u6297FR\u6280\u672f\u6ee5\u7528\uff08\u5982\u5927\u89c4\u6a21\u76d1\u63a7\u4e0e\u975e\u81ea\u613f\u8eab\u4efd\u8ffd\u8e2a\uff09\u63d0\u4f9b\u5b9e\u7528\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u7528\u6237\u4e3b\u52a8\u9632\u62a4\u7684\u6280\u672f\u7a7a\u767d\u3002  \n\u25c6\u901a\u8fc73D\u52302D\u7684\u5c01\u88c5\u4e0e\u52a8\u6001\u9002\u914d\u673a\u5236\uff0c\u517c\u987e\u5f3a\u9690\u79c1\u4fdd\u62a4\u4e0e\u89c6\u89c9\u81ea\u7136\u5ea6\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u6613\u88ab\u68c0\u6d4b\u6216\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u7684\u75db\u70b9\u3002|\n",
    "2508.03494": "|2025-08-05|Prototype-Enhanced Confidence Modeling for Cross-Modal Medical Image-Report Retrieval|Shreyank N Gowda\u7b49|[2508.03494](http://arxiv.org/pdf/2508.03494)|\u65e0|\u25c6 \u63d0\u51faPrototype-Enhanced Confidence Modeling (PECM)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u539f\u578b\u5efa\u6a21\u89e3\u51b3\u533b\u5b66\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u8bed\u4e49\u6a21\u7cca\u6027\u95ee\u9898\u3002  \n\u25c6 \u9996\u6b21\u5728\u533b\u5b66\u56fe\u50cf-\u62a5\u544a\u68c0\u7d22\u4e2d\u5f15\u5165\u53cc\u6d41\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u673a\u5236\uff0c\u7ed3\u5408\u539f\u578b\u76f8\u4f3c\u5ea6\u5206\u5e03\u81ea\u9002\u5e94\u8c03\u6574\u9ad8\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u7684\u5f71\u54cd\u6743\u91cd\u3002  \n\u25c6 \u8bbe\u8ba1\u591a\u6a21\u6001\u539f\u578b\u5b66\u4e60\u6a21\u5757\uff0c\u5206\u522b\u6355\u6349\u56fe\u50cf\u548c\u6587\u672c\u7684\u5c42\u6b21\u5316\u8bed\u4e49\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5f00\u53d1\u81ea\u9002\u5e94\u52a0\u6743\u7b56\u7565\uff0c\u52a8\u6001\u5e73\u8861\u4e0d\u540c\u7f6e\u4fe1\u5ea6\u6837\u672c\u5728\u68c0\u7d22\u6392\u5e8f\u4e2d\u7684\u8d21\u732e\uff0c\u6539\u5584\u4e34\u5e8a\u590d\u6742\u573a\u666f\u4e0b\u7684\u7ed3\u679c\u53ef\u9760\u6027\u3002  \n\u25c6 \u5728\u5b8c\u5168\u76d1\u7763\u548c\u96f6\u6837\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6700\u9ad810.17%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u591a\u4e2a\u653e\u5c04\u5b66\u6570\u636e\u96c6\u4e0a\u5237\u65b0\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u9a8c\u8bc1\u539f\u578b\u589e\u5f3a\u65b9\u6cd5\u5bf9\u533b\u5b66\u6570\u636e\u56fa\u6709\u6b67\u4e49\u6027\u7684\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u4e34\u5e8a\u8de8\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2508.04801": "|2025-08-06|ACM Multimedia Grand Challenge on ENT Endoscopy Analysis|Trong-Thuan Nguyen\u7b49|[2508.04801](http://arxiv.org/pdf/2508.04801)|\u65e0|\u25c6 \u63d0\u51fa\u4e86ENTRep\u6311\u6218\u8d5b\uff0c\u9996\u6b21\u5c06\u8033\u9f3b\u5589\u5185\u7aa5\u955c\u5206\u6790\u7684\u7ec6\u7c92\u5ea6\u89e3\u5256\u5206\u7c7b\u4e0e\u8de8\u6a21\u6001\u68c0\u7d22\uff08\u56fe\u50cf\u5230\u56fe\u50cf\u3001\u6587\u672c\u5230\u56fe\u50cf\uff09\u7ed3\u5408\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u516c\u5171\u57fa\u51c6\u7684\u7a7a\u767d\u3002  \n\u25c6 \u6784\u5efa\u4e86\u9996\u4e2a\u652f\u6301\u53cc\u8bed\uff08\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\uff09\u4e34\u5e8a\u63cf\u8ff0\u7684\u4e13\u4e1a\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684\u89e3\u5256\u533a\u57df\u3001\u6b63\u5e38/\u5f02\u5e38\u72b6\u6001\u53ca\u53cc\u8bed\u8a00\u53d9\u4e8b\u6587\u672c\uff0c\u589e\u5f3a\u4e86\u6570\u636e\u591a\u6837\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e09\u4e2a\u6807\u51c6\u5316\u8bc4\u6d4b\u4efb\u52a1\uff08\u5206\u7c7b\u3001\u56fe\u50cf\u68c0\u7d22\u3001\u6587\u672c\u68c0\u7d22\uff09\uff0c\u5e76\u5efa\u7acb\u670d\u52a1\u5668\u7aef\u8bc4\u5206\u673a\u5236\uff0c\u786e\u4fdd\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u4e0e\u53ef\u590d\u73b0\u6027\u3002  \n\u25c6 \u5f15\u5165\u516c\u5f00\u548c\u79c1\u6709\u6d4b\u8bd5\u96c6\u7684\u53cc\u8f68\u8bc4\u4f30\u7b56\u7565\uff0c\u517c\u987e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0e\u4e34\u5e8a\u5b9e\u9645\u9700\u6c42\u3002  \n\u25c6 \u901a\u8fc7\u5206\u6790\u4f18\u80dc\u56e2\u961f\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u8bed\u8a00\u5bf9\u9f50\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002|\n",
    "2508.04790": "|2025-08-06|Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization|MD Shaikh Rahman\u7b49|[2508.04790](http://arxiv.org/pdf/2508.04790)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u9488\u5bf9BIRADS\u4e94\u5206\u7c7b\u4e73\u817a\u56fe\u50cf\u68c0\u7d22\u7684\u7efc\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u4e2d\u6837\u672c\u4e0d\u8db3\u3001\u6570\u636e\u5212\u5206\u4e0d\u5f53\u548c\u7edf\u8ba1\u9a8c\u8bc1\u4e0d\u8db3\u7684\u65b9\u6cd5\u5b66\u5c40\u9650\u3002  \n\u25c6 \u7cfb\u7edf\u6bd4\u8f83\u4e86DenseNet121\u3001ResNet50\u548cVGG16\u67b6\u6784\uff0c\u7ed3\u5408\u5dee\u5f02\u5316\u5fae\u8c03\u3001\u5ea6\u91cf\u5b66\u4e60\u548c\u8d85\u96c6\u6210\u4f18\u5316\u7b49\u5148\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u5176\u4e2d\u5dee\u5f02\u5316\u5fae\u8c03\u4f7fDenseNet121\u548cResNet50\u7684precision@10\u63d0\u534719.6%\u3002  \n\u25c6 \u91c7\u7528\u4e25\u683c\u5206\u5c42\u6570\u636e\u5212\u5206\uff0850%/20%/30%\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\uff09\u548c1000\u6b21bootstrap\u7f6e\u4fe1\u533a\u95f4\u9a8c\u8bc1\uff0c\u6d4b\u8bd5\u96c6\u5305\u542b602\u4f8b\u67e5\u8be2\uff0c\u786e\u4fdd\u7ed3\u679c\u4e34\u5e8a\u53ef\u9760\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u63d0\u51fa\u8d85\u96c6\u6210\u4f18\u5316\u65b9\u6cd5\uff0c\u6574\u5408\u4e92\u8865\u67b6\u6784\u5b9e\u73b036.33%\u7684precision@10\uff0895% CI: 34.78%-37.88%\uff09\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534724.93%\uff0c\u6bcf\u67e5\u8be2\u8fd4\u56de3.6\u4e2a\u76f8\u5173\u75c5\u4f8b\u3002  \n\u25c6 \u901a\u8fc7\u7edf\u8ba1\u9a8c\u8bc1\u663e\u793a\u4e0d\u540c\u4f18\u5316\u7b56\u7565\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08p<0.001\uff0cCohen's d>0.8\uff09\uff0c\u540c\u65f6\u4fdd\u63012.8\u6beb\u79d2\u7684\u5b9e\u65f6\u68c0\u7d22\u6548\u7387\uff0c\u8fdc\u8d85\u6587\u732e\u4e2d5\u7c7b\u533b\u5b66\u68c0\u7d2220-25%\u7684\u6027\u80fd\u9884\u671f\u3002  \n\u25c6 \u5efa\u7acb\u4e34\u5e8a\u90e8\u7f72\u7684\u5faa\u8bc1\u67b6\u6784\u9009\u62e9\u6307\u5357\uff0c\u4e3a\u8bca\u65ad\u652f\u6301\u548c\u8d28\u91cf\u63a7\u5236\u5e94\u7528\u63d0\u4f9b\u65b0\u6027\u80fd\u57fa\u51c6\u3002|\n",
    "2508.04476": "|2025-08-06|Metric Learning in an RKHS|Gokcan Tatli\u7b49|[2508.04476](http://arxiv.org/pdf/2508.04476)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\u4e2d\u8fdb\u884c\u5ea6\u91cf\u5b66\u4e60\u7684\u901a\u7528\u6846\u67b6\uff0c\u7a81\u7834\u4e86\u4ee5\u5f80\u4ec5\u9650\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff08\u211d\u1d48\uff09\u7684\u7406\u8bba\u5c40\u9650\u3002  \n\u25c6 \u9996\u6b21\u4e3a\u57fa\u4e8e\u6838\u65b9\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u5ea6\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u7406\u8bba\u652f\u6491\u7684\u7a7a\u767d\u3002  \n\u25c6 \u63a8\u5bfc\u4e86\u65b0\u7684\u6cdb\u5316\u8bef\u5dee\u754c\u548c\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u754c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u6240\u9700\u7684\u6570\u636e\u91cf\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002  \n\u25c6 \u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002  \n\u25c6 \u5c06\u4e09\u5143\u7ec4\u6bd4\u8f83\uff08\u5982\u201ch\u66f4\u63a5\u8fd1i\u8fd8\u662fj\uff1f\u201d\uff09\u7684\u5f31\u76d1\u7763\u4fe1\u53f7\u4e0eRKHS\u7ed3\u5408\uff0c\u6269\u5c55\u4e86\u5ea6\u91cf\u5b66\u4e60\u5728\u56fe\u50cf\u68c0\u7d22\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u573a\u666f\u7684\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2508.04424": "|2025-08-06|Composed Object Retrieval: Object-level Retrieval via Composed Expressions|Tong Wang\u7b49|[2508.04424](http://arxiv.org/pdf/2508.04424)|\u65e0|\u25c6 \u63d0\u51fa\u5168\u65b0\u4efb\u52a1Composed Object Retrieval (COR)\uff0c\u7a81\u7834\u73b0\u6709\u56fe\u50cf\u7ea7\u7ec4\u5408\u68c0\u7d22\u5c40\u9650\uff0c\u5b9e\u73b0\u57fa\u4e8e\u53c2\u8003\u5bf9\u8c61+\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u8c61\u7ea7\u7cbe\u786e\u68c0\u7d22\u4e0e\u5206\u5272\u3002  \n\u25c6 \u63ed\u793aCOR\u4efb\u52a1\u7684\u6838\u5fc3\u6311\u6218\uff1a\u9700\u5728\u590d\u6742\u573a\u666f\u4e2d\u7cbe\u51c6\u5b9a\u4f4d\u7b26\u5408\u7ec4\u5408\u8bed\u4e49\u7684\u4efb\u610f\u5bf9\u8c61\uff0c\u540c\u65f6\u6392\u9664\u8bed\u4e49\u76f8\u4f3c\u4f46\u65e0\u5173\u7684\u5e72\u6270\u5bf9\u8c61\u3002  \n\u25c6 \u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21COR\u57fa\u51c6\u6570\u636e\u96c6COR127K\uff0c\u5305\u542b408\u4e2a\u7c7b\u522b\u300112.7\u4e07\u7ec4\u68c0\u7d22\u4e09\u5143\u7ec4\uff0c\u8986\u76d6\u591a\u6837\u5316\u8bed\u4e49\u53d8\u6362\u573a\u666f\u3002  \n\u25c6 \u8bbe\u8ba1\u7edf\u4e00\u7aef\u5230\u7aef\u6a21\u578bCORE\uff0c\u521b\u65b0\u6027\u6574\u5408\u53c2\u8003\u533a\u57df\u7f16\u7801\u3001\u81ea\u9002\u5e94\u89c6\u89c9-\u6587\u672c\u4ea4\u4e92\u548c\u533a\u57df\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u4e09\u5927\u5173\u952e\u6280\u672f\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eCORE\u5728\u57fa\u7c7b\u548c\u65b0\u7c7b\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u68c0\u7d22\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\u3002  \n\u25c6 \u9996\u6b21\u5b9e\u73b0\u4ece\"\u56fe\u50cf\u7ea7\u5339\u914d\"\u5230\"\u5bf9\u8c61\u7ea7\u5b9a\u4f4d\"\u7684\u8de8\u8d8a\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u7cfb\u7edf\u5bf9\u7528\u6237\u610f\u56fe\u7684\u7cbe\u7ec6\u5316\u7406\u89e3\u3002|\n",
    "2508.04335": "|2025-08-06|RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization|Yanyan Li\u7b49|[2508.04335](http://arxiv.org/pdf/2508.04335)|\u65e0|\u25c6 \u63d0\u51faRiemanLine\uff0c\u4e00\u79cd\u57fa\u4e8e\u9ece\u66fc\u6d41\u5f62\u76843D\u76f4\u7ebf\u7edf\u4e00\u6700\u5c0f\u8868\u793a\u6cd5\uff0c\u53ef\u540c\u65f6\u5904\u7406\u72ec\u7acb\u76f4\u7ebf\u548c\u5e73\u884c\u7ebf\u7ec4\uff0c\u89e3\u51b3\u4e86\u4eba\u9020\u73af\u5883\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7ed3\u6784\u89c4\u5f8b\u6027\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u76f4\u7ebf\u5730\u6807\u89e3\u8026\u4e3a\u5168\u5c40\u548c\u5c40\u90e8\u7ec4\u4ef6\uff1a\u5728\u5355\u4f4d\u7403\u9762S\u00b2\u4e0a\u4f18\u5316\u7684\u5171\u4eab\u6d88\u5931\u65b9\u5411\uff0c\u4ee5\u53ca\u5728\u6b63\u4ea4\u5b50\u7a7a\u95f4\u4e0a\u7ea6\u675f\u7684\u7f29\u653e\u6cd5\u5411\u91cf\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u89c4\u5f8b\u7684\u7d27\u51d1\u7f16\u7801\u3002  \n\u25c6 \u5bf9\u4e8en\u6761\u5e73\u884c\u7ebf\uff0c\u5c06\u53c2\u6570\u7a7a\u95f4\u4ece4n\uff08\u6b63\u4ea4\u5f62\u5f0f\uff09\u51cf\u5c11\u52302n+2\uff0c\u65e0\u9700\u663e\u5f0f\u7ea6\u675f\u5373\u53ef\u81ea\u7136\u5d4c\u5165\u5e73\u884c\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u53c2\u6570\u7ef4\u5ea6\u3002  \n\u25c6 \u5c06\u8be5\u53c2\u6570\u5316\u65b9\u6cd5\u96c6\u6210\u5230\u56e0\u5b50\u56fe\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u65b9\u5411\u5bf9\u9f50\u548c\u5c40\u90e8\u91cd\u6295\u5f71\u4f18\u5316\u7684\u7edf\u4e00\u57fa\u4e8e\u6d41\u5f62\u7684\u675f\u8c03\u6574\u3002  \n\u25c6 \u5728ICL-NUIM\u3001TartanAir\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u4f30\u8ba1\u548c\u76f4\u7ebf\u91cd\u5efa\u65b9\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6539\u5584\u4e86\u6536\u655b\u7a33\u5b9a\u6027\u3002|\n",
    "2508.05661": "|2025-07-31|Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace|Andre Rusli\u7b49|[2508.05661](http://arxiv.org/pdf/2508.05661)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u6837\u672c\u68c0\u7d22\u7684\u53ef\u6269\u5c55\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\uff0c\u9002\u7528\u4e8eC2C\u7535\u5546\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u975e\u7ed3\u6784\u5316\u5546\u54c1\u5217\u8868\u7684\u641c\u7d22\u96be\u9898\u3002  \n\u25c6 \u9996\u6b21\u5728Mercari\u5e73\u53f0\u4e2d\u5bf9\u6bd4\u4e86\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u53d1\u73b0\u591a\u8bed\u8a00SigLIP\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0cnDCG@5\u6307\u6807\u6bd4\u539f\u6709\u5fae\u8c03\u57fa\u7ebf\u63d0\u534713.3%\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u5b9e\u65f6\u63a8\u7406\u4e0e\u540e\u53f0\u7d22\u5f15\u76f8\u7ed3\u5408\u7684\u5de5\u4f5c\u6d41\uff0c\u5e76\u901a\u8fc7\u964d\u7ef4\u4f18\u5316\u7edf\u4e00\u5d4c\u5165\u7ba1\u9053\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u90e8\u7f72\u3002  \n\u25c6 \u901a\u8fc7\u7ebf\u4e0aA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5b9e\u9645\u6548\u679c\uff0c\u5b9e\u9a8c\u7ec4\u7528\u6237\u901a\u8fc7\u56fe\u50cf\u641c\u7d22\u7684\u6210\u4ea4\u7387\u63d0\u5347\u9ad8\u8fbe40.9%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u8f6c\u5316\u7387\u3002  \n\u25c6 \u8bc1\u660e\u4e86\u96f6\u6837\u672c\u6a21\u578b\u53ef\u4f5c\u4e3a\u751f\u4ea7\u73af\u5883\u7684\u5f3a\u57fa\u7ebf\u65b9\u6848\uff0c\u65e2\u80fd\u5feb\u901f\u90e8\u7f72\uff0c\u53c8\u4fdd\u7559\u4e86\u672a\u6765\u5fae\u8c03\u7684\u7075\u6d3b\u6027\uff0c\u5927\u5e45\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002|\n",
    "2508.09105": "|2025-08-13|SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling|Shixuan Sun\u7b49|[2508.09105](http://arxiv.org/pdf/2508.09105)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u9762\u5411\u534a\u9ed1\u76d2\u68c0\u7d22\u63a7\u5236\u73af\u5883\u7684\u6765\u6e90\u611f\u77e5\u6210\u5458\u5ba1\u8ba1\u6846\u67b6(SMA)\uff0c\u5b9e\u73b0\u751f\u6210\u5185\u5bb9\u7ec6\u7c92\u5ea6\u6765\u6e90\u8ffd\u8e2a\uff08\u9884\u8bad\u7ec3/\u5916\u90e8\u68c0\u7d22/\u7528\u6237\u8f93\u5165\uff09\uff0c\u89e3\u51b3\u4f20\u7edf\u6210\u5458\u63a8\u7406\u65b9\u6cd5\u5728RAG\u7cfb\u7edf\u4e2d\u5931\u6548\u7684\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u57fa\u4e8e\u96f6\u9636\u4f18\u5316\u7684\u5f52\u56e0\u4f30\u8ba1\u673a\u5236\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6270\u52a8\u91c7\u6837\u548c\u5cad\u56de\u5f52\u5efa\u6a21\uff0c\u5728\u534a\u9ed1\u76d2\u7ea6\u675f\u4e0b\u9c81\u68d2\u8fd1\u4f3c\u8f93\u5165\u4ee4\u724c\u5bf9\u8f93\u51fa\u7684\u771f\u5b9e\u5f71\u54cd\u3002  \n\u25c6 \u9996\u521b\u8de8\u6a21\u6001\u5f52\u56e0\u6280\u672f\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u5c06\u56fe\u50cf\u8f93\u5165\u6295\u5f71\u4e3a\u6587\u672c\u63cf\u8ff0\uff0c\u5b9e\u73b0\u6587\u672c\u6a21\u6001\u7684\u4ee4\u724c\u7ea7\u5f52\u56e0\uff0c\u9996\u6b21\u652f\u6301MRAG\u7cfb\u7edf\u4e2d\u56fe\u50cf\u68c0\u7d22\u75d5\u8ff9\u7684\u6210\u5458\u63a8\u7406\u3002  \n\u25c6 \u5c06\u6210\u5458\u63a8\u7406\u7684\u7814\u7a76\u7126\u70b9\u4ece\"\u6570\u636e\u662f\u5426\u88ab\u8bb0\u5fc6\"\u8f6c\u5411\"\u5185\u5bb9\u6765\u6e90\u4f55\u5904\"\uff0c\u4e3a\u590d\u6742\u751f\u6210\u7cfb\u7edf\u7684\u6570\u636e\u6eaf\u6e90\u5ba1\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002  \n\u25c6 \u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u7d22\u4e0e\u591a\u6a21\u6001\u878d\u5408\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u63a7\u5236\u68c0\u7d22\u8fc7\u7a0b\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u9690\u79c1\u6cc4\u9732\u8d23\u4efb\u8ba4\u5b9a\u3002|\n",
    "2508.08917": "|2025-08-12|A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition|Jintao Cheng\u7b49|[2508.08917](http://arxiv.org/pdf/2508.08917)|\u65e0|\u25c6\u63d0\u51fa\u57fa\u4e8e\u4f2a\u5168\u5c40\u878d\u5408\u8303\u5f0f\u7684\u8de8\u89c6\u89d2\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5206\u652f\u534f\u540c\u5b66\u4e60\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\u7279\u5f81\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7279\u5f81\u7a7a\u95f4\u5185\u5728\u7ed3\u6784\u7684\u95ee\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u5f15\u5165\u4f2a\u5168\u5c40\u4fe1\u606f\u5f15\u5bfc\u673a\u5236\uff0c\u6709\u6548\u534f\u8c03\u4e0d\u540c\u6a21\u6001\u5206\u652f\u7684\u7279\u5f81\u8868\u8fbe\uff0c\u589e\u5f3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u573a\u666f\u8bc6\u522b\u80fd\u529b\u3002  \n\u25c6\u63d0\u51fa\u6d41\u5f62\u9002\u5e94\u4e0e\u6210\u5bf9\u65b9\u5dee-\u5c40\u90e8\u6027\u5b66\u4e60\u5ea6\u91cf\u65b9\u6cd5\uff0c\u6784\u5efa\u5bf9\u79f0\u6b63\u5b9a(SPD)\u77e9\u9635\u8ba1\u7b97\u9a6c\u6c0f\u8ddd\u79bb\uff0c\u53d6\u4ee3\u4f20\u7edf\u6b27\u6c0f\u8ddd\u79bb\u5ea6\u91cf\u3002  \n\u25c6\u901a\u8fc7\u51e0\u4f55\u5316\u5efa\u6a21\u51c6\u786e\u523b\u753b\u7279\u5f81\u7a7a\u95f4\u5185\u6570\u636e\u672c\u8d28\u5206\u5e03\uff0c\u6355\u6349\u590d\u6742\u7684\u7c7b\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u65f6\u53d8\u573a\u666f\u4e0b\u7684\u8bc6\u522b\u9c81\u68d2\u6027\u3002  \n\u25c6\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u5177\u6709\u7ade\u4e89\u4f18\u52bf\uff0c\u5c24\u5176\u5728GPS\u62d2\u6b62\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u548c\u95ed\u73af\u68c0\u6d4b\u4efb\u52a1\u8868\u73b0\u7a81\u51fa\u3002  \n\u25c6\u6574\u4f53\u6846\u67b6\u7a81\u7834\u4e86\u6b27\u5f0f\u7a7a\u95f4\u7ebf\u6027\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6fc0\u5149\u96f7\u8fbe\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u975e\u7ebf\u6027\u7279\u5f81\u5b66\u4e60\u8303\u5f0f\u3002|\n",
    "2508.09241": "|2025-08-12|FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents|Fengxian Ji\u7b49|[2508.09241](http://arxiv.org/pdf/2508.09241)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u7ec6\u7c92\u5ea6GUI\u4ee3\u7406\u63a7\u5236\u8bc4\u4f30\u6807\u51c6FineState-Bench\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u4ec5\u5173\u6ce8\u7c97\u7c92\u5ea6\u4efb\u52a1\u5b8c\u6210\u7684\u7a7a\u767d  \n\u25c6 \u6784\u5efa\u4e86\u8de8\u5e73\u53f0\uff08\u684c\u9762/\u7f51\u9875/\u79fb\u52a8\u7aef\uff09\u76842257\u9879\u4efb\u52a1\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\u6a21\u5757\u548c\u56db\u9636\u6bb5\u8bc4\u4f30\u6307\u6807  \n\u25c6 \u521b\u65b0\u5f00\u53d1\u4e86\u5373\u63d2\u5373\u7528\u7684\u89c6\u89c9\u8bca\u65ad\u52a9\u624bVDA\uff0c\u9996\u6b21\u5b9e\u73b0\u611f\u77e5\u4e0e\u5b9a\u4f4d\u80fd\u529b\u7684\u91cf\u5316\u89e3\u8026\u5206\u6790  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u4e2d\u4ec5\u8fbe32.8%\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u57fa\u51c6\u7684\u6709\u6548\u6027  \n\u25c6 \u9996\u6b21\u8bc1\u5b9e\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u662f\u5f53\u524dGUI\u4ee3\u7406\u7684\u4e3b\u8981\u74f6\u9888\uff08\u7406\u60f3\u89c6\u89c9\u53ef\u4f7fGemini\u6a21\u578b\u6210\u529f\u7387\u63d0\u534714.9%\uff09  \n\u25c6 \u5b8c\u6574\u5f00\u6e90\u8bc4\u4f30\u6846\u67b6\u4e0e\u6570\u636e\u96c6\uff0c\u4e3aGUI\u4ee3\u7406\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u73af\u5883\u4e0e\u8bca\u65ad\u5de5\u5177|\n",
    "2508.11272": "|2025-08-15|Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering|Jun Li\u7b49|[2508.11272](http://arxiv.org/pdf/2508.11272)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPMTFR\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u91d1\u5b57\u5854\u5339\u914d\u6a21\u578b\u4e0e\u65e0\u8bad\u7ec3\u7cbe\u70bc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76d1\u7763\u5f0f\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u7684\u6027\u80fd\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u91d1\u5b57\u5854\u4fee\u8865\u6a21\u5757\uff08Pyramid Patcher\uff09\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u7406\u89e3\u589e\u5f3a\u6a21\u578b\u5bf9\u53c2\u8003\u56fe\u50cf\u548c\u4fee\u6539\u6307\u4ee4\u7684\u8054\u5408\u89e3\u6790\u80fd\u529b\u3002  \n\u25c6 \u9996\u6b21\u5c06\u8868\u793a\u5de5\u7a0b\uff08Representation Engineering\uff09\u5f15\u5165CIR\u4efb\u52a1\uff0c\u5229\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u6570\u636e\u63d0\u53d6\u8868\u5f81\u5e76\u6ce8\u5165\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6392\u5e8f\u6a21\u578b\u5373\u53ef\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5728\u76d1\u7763\u5f0fCIR\u4e2d\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7cbe\u70bc\u8303\u5f0f\uff0c\u6446\u8131\u4f20\u7edf\u65b9\u6cd5\u5bf9\u663e\u5f0f\u6587\u672c\u63a8\u7406\u6216\u590d\u6742\u63d0\u793a\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u4ec5\u901a\u8fc7\u8868\u5f81\u6ce8\u5165\u5373\u53ef\u63d0\u5347\u5206\u6570\u3002  \n\u25c6 \u5728\u4e3b\u6d41CIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u4e0e\u53ef\u6269\u5c55\u6027\u3002|\n",
    "2508.10933": "|2025-08-12|Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications|Yoli Shavit\u7b49|[2508.10933](http://arxiv.org/pdf/2508.10933)|\u65e0|\u25c6 \u5c06\u76f8\u673a\u4f4d\u59ff\u81ea\u7f16\u7801\u5668\uff08PAE\uff09\u4ece\u7edd\u5bf9\u4f4d\u59ff\u56de\u5f52\uff08APR\uff09\u6269\u5c55\u5230\u76f8\u5bf9\u4f4d\u59ff\u56de\u5f52\uff08RPR\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8ePAE\u7684RPR\u65b9\u6cd5\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u5b58\u50a8\u56fe\u50cf\u6216\u4f4d\u59ff\u6570\u636e\u7684\u91cd\u5b9a\u4f4d\u65b9\u6848\uff0c\u901a\u8fc7PAE-based RPR\u5bf9APR\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u5728\u540c\u7b49\u67b6\u6784\u4e0b\uff0c\u9a8c\u8bc1\u4e86PAE-based RPR\u76f8\u6bd4\u4f20\u7edf\u56fe\u50cf\u57faRPR\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002  \n\u25c6 \u5728\u5ba4\u5185\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5bf9APR\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002  \n\u25c6 \u4ec5\u970030%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u96f6\u552e\u573a\u666f\u90e8\u7f72\u4e2d\u7684\u6570\u636e\u6536\u96c6\u8d1f\u62c5\uff0c\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u3002  \n\u25c6 \u5f00\u6e90\u4e86\u4ee3\u7801\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u4e0e\u5e94\u7528\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002|\n",
    "2508.12290": "|2025-08-17|CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval|Chor Boon Tan\u7b49|[2508.12290](http://arxiv.org/pdf/2508.12290)|\u65e0|\u25c6 \u63d0\u51faCLAIR\u65b9\u6cd5\uff0c\u5229\u7528CLIP\u751f\u6210\u7684\u566a\u58f0\u4f2a\u6807\u7b7e\u8fdb\u884c\u5f31\u76d1\u7763\u96f6\u6837\u672c\u8de8\u57df\u56fe\u50cf\u68c0\u7d22\uff08WSZS-CDIR\uff09\uff0c\u66ff\u4ee3\u4f20\u7edf\u65e0\u76d1\u7763\u65b9\u6cd5\u3002  \n\u25c6 \u901a\u8fc7CLIP\u6587\u672c\u4e0e\u56fe\u50cf\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7f6e\u4fe1\u5206\u6570\uff0c\u6709\u6548\u4f18\u5316\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u3002  \n\u25c6 \u8bbe\u8ba1\u7c7b\u611f\u77e5\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\u673a\u5236\uff0c\u7ed3\u5408\u5b9e\u4f8b\u95f4\u548c\u7c07\u95f4\u5bf9\u6bd4\u635f\u5931\uff0c\u63d0\u5347\u7279\u5f81\u533a\u5206\u5ea6\u3002  \n\u25c6 \u63d0\u51fa\u8de8\u57df\u5bf9\u6bd4\u635f\u5931\u51cf\u5c11\u57df\u5dee\u5f02\uff0c\u5e76\u521b\u65b0\u6027\u5730\u901a\u8fc7\u95ed\u5f0f\u89e3\u5b66\u4e60\u8de8\u57df\u6620\u5c04\u51fd\u6570\uff0c\u4ec5\u7528CLIP\u6587\u672c\u5d4c\u5165\u5b9e\u73b0\u7279\u5f81\u5bf9\u9f50\u3002  \n\u25c6 \u5f15\u5165\u53ef\u5b66\u4e60\u63d0\u793a\u8bcd\u589e\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u65b0\u7c7b\u522b\u68c0\u7d22\u3002  \n\u25c6 \u5728TUBerlin\u3001Sketchy\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CLAIR\u7684\u4f18\u8d8a\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002|\n",
    "2508.13843": "|2025-08-19|UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion|Zihan Liang\u7b49|[2508.13843](http://arxiv.org/pdf/2508.13843)|\u65e0|\u25c6 \u63d0\u51fa\u4e86UniECS\u7edf\u4e00\u591a\u6a21\u6001\u7535\u5546\u641c\u7d22\u6846\u67b6\uff0c\u80fd\u7075\u6d3b\u5904\u7406\u56fe\u50cf\u3001\u6587\u672c\u53ca\u5176\u4efb\u610f\u7ec4\u5408\u7684\u68c0\u7d22\u573a\u666f\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fa\u5b9a\u6a21\u6001\u914d\u5bf9\u7684\u9650\u5236\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u65b0\u578b\u95e8\u63a7\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u6574\u5408\u4e0d\u540c\u6a21\u6001\u8868\u5f81\u5e76\u5904\u7406\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u7efc\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u5bf9\u9f50\u635f\u5931\u3001\u5c40\u90e8\u5bf9\u9f50\u635f\u5931\u3001\u6a21\u6001\u5185\u5bf9\u6bd4\u635f\u5931\u548c\u81ea\u9002\u5e94\u635f\u5931\u52a0\u6743\uff0c\u4f18\u5316\u6a21\u578b\u5b66\u4e60\u6548\u679c\u3002  \n\u25c6 \u6784\u5efa\u4e86M-BEER\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b5\u4e07\u5546\u54c1\u5bf9\uff0c\u4e3a\u591a\u6a21\u6001\u7535\u5546\u68c0\u7d22\u63d0\u4f9b\u5168\u9762\u8bc4\u4f30\u6807\u51c6\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5982\u6587\u672c\u641c\u56fe\u4efb\u52a1R@10\u63d0\u534728%\uff09\uff0c\u53c2\u6570\u91cf\u4ec50.2B\uff0c\u6548\u7387\u8fdc\u8d85\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002  \n\u25c6 \u6210\u529f\u90e8\u7f72\u4e8e\u5feb\u624b\u7535\u5546\u641c\u7d22\u5e73\u53f0\uff0c\u70b9\u51fb\u7387\u63d0\u53472.74%\u3001\u6536\u5165\u589e\u957f8.33%\uff0c\u9a8c\u8bc1\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002|\n",
    "2508.13488": "|2025-08-19|ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments|Jingwen Yu\u7b49|[2508.13488](http://arxiv.org/pdf/2508.13488)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u91cd\u590d\u73af\u5883\u4e2d\u8fdb\u884c\u9c81\u68d2\u56de\u73af\u95ed\u5408\u9a8c\u8bc1\u7684\u65b9\u6cd5ROVER\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u7ea6\u675f\u6765\u63d0\u5347\u9a8c\u8bc1\u53ef\u9760\u6027\u3002  \n\u25c6 \u9996\u6b21\u5c06\u673a\u5668\u4eba\u7684\u65f6\u7a7a\u8fd0\u52a8\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u5f15\u5165\u56de\u73af\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5916\u89c2\u7279\u5f81\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u63d0\u51fa\u901a\u8fc7\u4f4d\u59ff\u56fe\u4f18\u5316\u751f\u6210\u5019\u9009\u56de\u73af\u5bf9\u5e94\u7684\u8f68\u8ff9\uff0c\u5e76\u8bbe\u8ba1\u8bc4\u5206\u673a\u5236\u8bc4\u4f30\u8be5\u8f68\u8ff9\u4e0e\u65e0\u56de\u73af\u5148\u9a8c\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\u3002  \n\u25c6 \u5728\u5b58\u5728\u9ad8\u5ea6\u76f8\u4f3c\u7ed3\u6784\u7684\u91cd\u590d\u73af\u5883\u4e2d\u80fd\u6709\u6548\u62d2\u7edd\u9519\u8bef\u56de\u73af\uff0c\u663e\u8457\u964d\u4f4eSLAM\u7cfb\u7edf\u7684\u8bef\u68c0\u98ce\u9669\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u81f3\u73b0\u6709\u5148\u8fdbSLAM\u7cfb\u7edf\u4e2d\u3002|\n",
    "2508.15297": "|2025-08-21|DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding|Zhu Wang\u7b49|[2508.15297](http://arxiv.org/pdf/2508.15297)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DesignCLIP\uff0c\u4e00\u4e2a\u57fa\u4e8eCLIP\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u8bbe\u8ba1\u4e13\u5229\u7684\u7406\u89e3\u4e0e\u5206\u6790\u3002\u5176\u6838\u5fc3\u8d21\u732e\u4e0e\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9\u7f8e\u56fd\u8bbe\u8ba1\u4e13\u5229\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4e3a\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\n\u25c6 \u63d0\u51fa\u7c7b\u611f\u77e5\u5206\u7c7b\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u9002\u5e94\u4e13\u5229\u6570\u636e\u7684\u62bd\u8c61\u548c\u7ed3\u6784\u6027\u7279\u70b9\u3002\n\u25c6 \u5229\u7528\u751f\u6210\u5f0f\u8be6\u7ec6\u6807\u6ce8\u548c\u4e13\u5229\u56fe\u50cf\u7684\u591a\u89c6\u89d2\u5b66\u4e60\uff0c\u589e\u5f3a\u4e86\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u8bed\u4e49\u5bf9\u9f50\u3002\n\u25c6 \u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u4e13\u5229\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u53caSOTA\u6a21\u578b\u3002\n\u25c6 \u63a2\u7d22\u4e86\u591a\u6a21\u6001\u4e13\u5229\u68c0\u7d22\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u8bbe\u8ba1\u521b\u65b0\u63d0\u4f9b\u66f4\u591a\u6837\u5316\u7684\u7075\u611f\u6765\u6e90\u3002|\n",
    "2508.18242": "|2025-08-25|GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations|Fadi Khatib\u7b49|[2508.18242](http://arxiv.org/pdf/2508.18242)|\u65e0|GSVisLoc\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u573a\u666f\u8868\u793a\u8bbe\u8ba1\u7684\u901a\u7528\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u3002  \n\u25c6 \u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u4efb\u4f55\u4fee\u6539\u6216\u91cd\u8bad\u7ec3\uff0c\u76f4\u63a5\u5229\u7528\u539f\u59cb3DGS\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u5b9a\u4f4d\u3002  \n\u25c6 \u901a\u8fc7\u4e0b\u91c7\u6837\u548c\u7f16\u78013D\u9ad8\u65af\u6765\u63d0\u53d6\u573a\u666f\u7279\u5f81\uff0c\u5e76\u4e0e\u67e5\u8be2\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u9c81\u68d2\u5339\u914d\u3002  \n\u25c6 \u91c7\u7528\u7531\u7c97\u5230\u7cbe\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u7c97\u5339\u914d\u3001\u7cbe\u7ec6\u5339\u914d\u548c\u59ff\u6001\u4f18\u5316\uff0c\u786e\u4fdd\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u3002  \n\u25c6 \u5728\u5ba4\u5185\u5916\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e3DGS\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002  \n\u25c6 \u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u5168\u65b0\u573a\u666f\u3002|\n",
    "2508.17972": "|2025-08-25|SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization|Junyuan Deng\u7b49|[2508.17972](http://arxiv.org/pdf/2508.17972)|\u65e0|SAIL-Recon\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u7684\u524d\u9988Transformer\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u573a\u666f\u56de\u5f52\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5927\u91cf\u8f93\u5165\u56fe\u50cf\u7684\u95ee\u9898\u3002\n\n\u25c6 \u6838\u5fc3\u521b\u65b0\u662f\u5c06\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u878d\u5165\u573a\u666f\u56de\u5f52\u7f51\u7edc\uff0c\u901a\u8fc7\u5f15\u5165\u951a\u70b9\u56fe\u50cf\u5b50\u96c6\u6765\u6784\u5efa\u795e\u7ecf\u573a\u666f\u8868\u793a\u3002\n\u25c6 \u8be5\u65b9\u6cd5\u9996\u5148\u4ece\u951a\u70b9\u56fe\u50cf\u8ba1\u7b97\u51fa\u4e00\u4e2a\u7d27\u51d1\u7684\u795e\u7ecf\u573a\u666f\u8868\u793a\uff0c\u4f5c\u4e3a\u5168\u5c40\u573a\u666f\u5148\u9a8c\u3002\n\u25c6 \u56de\u5f52\u7f51\u7edc\u968f\u540e\u4ee5\u8be5\u795e\u7ecf\u8868\u793a\u4e3a\u6761\u4ef6\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u80fd\u591f\u9ad8\u6548\u5730\u91cd\u5efa\u6240\u6709\u8f93\u5165\u56fe\u50cf\u7684\u76f8\u673a\u4f4d\u59ff\u548c3D\u7ed3\u6784\u3002\n\u25c6 \u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u573a\u666f\u56de\u5f52\u65b9\u6cd5\u5e94\u5bf9\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u6210\u529f\u5c06\u5176\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u573a\u666f\u3002\n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728TUM-RGBD\u3001CO3Dv2\u548cTanks & Temples\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|\n",
    "2508.17416": "|2025-08-24|Data Leakage in Visual Datasets|Patrick Ramos\u7b49|[2508.17416](http://arxiv.org/pdf/2508.17416)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u53ca\u5176\u5bf9\u6a21\u578b\u8bc4\u4f30\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002  \n\u25c6\u9996\u6b21\u5bf9\u89c6\u89c9\u6570\u636e\u6cc4\u6f0f\u8fdb\u884c\u4e86\u591a\u7ef4\u5ea6\u5206\u7c7b\uff0c\u4f9d\u636e\u6a21\u6001\u3001\u8986\u76d6\u8303\u56f4\u548c\u7a0b\u5ea6\u5212\u5206\u6cc4\u6f0f\u7c7b\u578b\u3002  \n\u25c6\u91c7\u7528\u56fe\u50cf\u68c0\u7d22\u6280\u672f\u5b9e\u8bc1\u68c0\u9a8c\u4e86\u591a\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6240\u6709\u88ab\u5206\u6790\u6570\u636e\u96c6\u5747\u5b58\u5728\u4e0d\u540c\u5f62\u5f0f\u7684\u6cc4\u6f0f\u3002  \n\u25c6\u8bc1\u660e\u4e86\u5404\u7c7b\u6cc4\u6f0f\uff08\u4ece\u4e25\u91cd\u5230\u8f7b\u5fae\uff09\u5747\u4f1a\u635f\u5bb3\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6a21\u578b\u8bc4\u4f30\u7684\u516c\u6b63\u6027\u3002  \n\u25c6\u63ed\u793a\u4e86\u4e92\u8054\u7f51\u6570\u636e\u6e90\u4e0e\u516c\u5f00\u57fa\u51c6\u5e76\u5b58\u5bfc\u81f4\u7684\u6cc4\u6f0f\u5fc5\u7136\u6027\uff0c\u547c\u5401\u5b66\u754c\u5173\u6ce8\u6570\u636e\u6784\u5efa\u89c4\u8303\u3002  \n\u7814\u7a76\u7ed3\u679c\u5bf9\u89c6\u89c9\u9886\u57df\u57fa\u51c6\u6784\u5efa\u548c\u6a21\u578b\u8bc4\u4f30\u5b9e\u8df5\u5177\u6709\u91cd\u8981\u8b66\u793a\u610f\u4e49\u3002|\n",
    "2508.16707": "|2025-08-22|Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval|Jonghyun Song\u7b49|[2508.16707](http://arxiv.org/pdf/2508.16707)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u7a00\u758f-\u5bc6\u96c6\u68c0\u7d22\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u6027\u80fd\u3002  \n\u25c6 \u901a\u8fc7\u81ea\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u7a00\u758f\u4e0e\u5bc6\u96c6\u8868\u793a\u7684\u53cc\u5411\u534f\u540c\u5b66\u4e60\uff0c\u7a81\u7834\u4ee5\u5f80\u5355\u5411\u84b8\u998f\u6216\u72ec\u7acb\u8bad\u7ec3\u7684\u9650\u5236\u3002  \n\u25c6 \u63d0\u51fa\u878d\u5408\u76f8\u4f3c\u5ea6\u5206\u6570\uff08\u7a00\u758f\u4e0e\u5bc6\u96c6\u5f97\u5206\u7684\u52a0\u6743\u548c\uff09\u4f5c\u4e3a\u5171\u4eab\u6559\u5e08\u4fe1\u53f7\uff0c\u540c\u6b65\u4f18\u5316\u4e24\u79cd\u8868\u793a\u3002  \n\u25c6 \u4ec5\u5fae\u8c03\u5bc6\u96c6\u7f16\u7801\u5668\u6700\u540e\u4e00\u5c42\u548c\u7a00\u758f\u6295\u5f71\u5934\uff0c\u65e0\u9700\u5168\u6a21\u578b\u91cd\u8bad\u7ec3\uff0c\u9ad8\u6548\u517c\u5bb9\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4f7f\u7a00\u758f\u68c0\u7d22\u5668\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u57fa\u7ebf\uff0c\u751a\u81f3\u8fbe\u5230\u6216\u8d85\u8d8a\u5bc6\u96c6\u6a21\u578b\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u7a00\u758f\u6a21\u578b\u7684\u9ad8\u6548\u4e0e\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u3002|\n",
    "2508.18971": "|2025-08-26|Can we make NeRF-based visual localization privacy-preserving?|Maxime Pietrantoni\u7b49|[2508.18971](http://arxiv.org/pdf/2508.18971)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u57fa\u4e8eNeRF\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5b58\u5728\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u7cfb\u7edf\u68c0\u9a8cNeRF\u8868\u793a\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u79fb\u9664\u989c\u8272\u9884\u6d4b\u5934\uff0c\u5176\u51e0\u4f55\u8868\u793a\u4ecd\u4f1a\u5b58\u50a8\u654f\u611f\u7ec6\u8282\u3002  \n\u25c6 \u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5c06RGB\u56fe\u50cf\u8f6c\u6362\u4e3a\u5206\u5272\u6807\u7b7e\u4f5c\u4e3a\u8bad\u7ec3\u76d1\u7763\uff0c\u907f\u514d\u76f4\u63a5\u4f7f\u7528\u539f\u59cb\u56fe\u50cf\u6570\u636e\u3002  \n\u25c6 \u6784\u5efa\u4e86ppNeSF\uff08\u9690\u79c1\u4fdd\u62a4\u795e\u7ecf\u5206\u5272\u573a\uff09\uff0c\u4ee5\u5206\u5272\u6807\u7b7e\u66ff\u4ee3RGB\u8fdb\u884c\u8bad\u7ec3\uff0c\u786e\u4fdd\u573a\u666f\u8868\u793a\u65e2\u7c97\u7cd9\u65e0\u6cd5\u8fd8\u539f\u7ec6\u8282\uff0c\u53c8\u4fdd\u7559\u8db3\u591f\u7684\u5224\u522b\u6027\u7528\u4e8e\u5b9a\u4f4d\u3002  \n\u25c6 \u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u89c6\u89c9\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u3002  \n\u25c6 \u6574\u4f53\u5de5\u4f5c\u9996\u6b21\u7cfb\u7edf\u63ed\u793a\u4e86NeRF\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u66ff\u4ee3\u7684\u9690\u79c1\u4fdd\u62a4\u8303\u5f0f\u3002|\n",
    "2508.18904": "|2025-08-26|Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025|Thien-Phuc Tran\u7b49|[2508.18904](http://arxiv.org/pdf/2508.18904)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63a8\u51fa\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4e8b\u4ef6\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6311\u6218EVENTA\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u6790\u4e2d\u5ffd\u89c6\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u6df1\u5ea6\u7684\u95ee\u9898\u3002\n\n\u25c6 \u9996\u521b\u5927\u89c4\u6a21\u4e8b\u4ef6\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u57fa\u51c6\uff0c\u7a81\u7834\u4f20\u7edf\u56fe\u50cf\u63cf\u8ff0\u4e0e\u68c0\u7d22\u7684\u8868\u5c42\u8bc6\u522b\u5c40\u9650\u3002\n\u25c6 \u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u3001\u65f6\u5e8f\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6784\u5efa\u201c\u4eba\u7269\u3001\u65f6\u95f4\u3001\u5730\u70b9\u3001\u4e8b\u4ef6\u3001\u539f\u56e0\u201d\u4e94\u7ef4\u4e8b\u4ef6\u7406\u89e3\u6846\u67b6\u3002\n\u25c6 \u57fa\u4e8eOpenEvents V1\u6570\u636e\u96c6\u8bbe\u8ba1\u53cc\u8d5b\u9053\uff1a\u4e8b\u4ef6\u589e\u5f3a\u56fe\u50cf\u68c0\u7d22\u4e0e\u63cf\u8ff0\uff0c\u4ee5\u53ca\u4e8b\u4ef6\u9a71\u52a8\u56fe\u50cf\u68c0\u7d22\u3002\n\u25c6 \u5efa\u7acb\u5305\u542b45\u652f\u56fd\u9645\u56e2\u961f\u53c2\u4e0e\u7684\u516c\u5e73\u8bc4\u4f30\u4f53\u7cfb\uff0c\u901a\u8fc7\u516c\u5f00\u548c\u79c1\u6709\u6d4b\u8bd5\u9636\u6bb5\u786e\u4fdd\u7ed3\u679c\u53ef\u590d\u73b0\u6027\u3002\n\u25c6 \u4e3a\u53d9\u4e8b\u9a71\u52a8\u591a\u5a92\u4f53AI\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8\u65b0\u95fb\u3001\u5a92\u4f53\u5206\u6790\u3001\u6587\u5316\u5b58\u6863\u7b49\u9886\u57df\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5e94\u7528\u53d1\u5c55\u3002|\n",
    "2508.19714": "|2025-08-27|Addressing Deepfake Issue in Selfie banking through camera based authentication|Subhrojyoti Mukherjee\u7b49|[2508.19714](http://arxiv.org/pdf/2508.19714)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u76f8\u673a\u6210\u50cf\u7279\u5f81\u6765\u9632\u5fa1\u81ea\u62cd\u94f6\u884c\u4e2d\u6df1\u5ea6\u4f2a\u9020\u653b\u51fb\u7684\u65b0\u578b\u8ba4\u8bc1\u65b9\u6cd5\u3002\n\n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u539f\u672c\u7528\u4e8e\u56fe\u50cf\u6eaf\u6e90\uff08\u5982\u56fe\u7247\u76f8\u673a\u5b9a\u4f4d\uff09\u7684\u53d6\u8bc1\u8bc6\u522b\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u9886\u57df\uff0c\u5b9e\u73b0\u4e86\u6280\u672f\u5e94\u7528\u7684\u8de8\u754c\u8fc1\u79fb\u3002\n\u25c6 \u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u5229\u7528\u76f8\u673a\u672c\u8eab\u7684\u786c\u4ef6\u7f3a\u9677\uff08\u5982\u955c\u5934\u5149\u5b66\u7279\u6027\u3001\u4f20\u611f\u5668\u566a\u58f0\u6a21\u5f0f\uff09\u4f5c\u4e3a\u751f\u7269\u7279\u5f81\u4e4b\u5916\u7684\u8f85\u52a9\u8ba4\u8bc1\u56e0\u7d20\uff0c\u56e0\u4e3a\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u96be\u4ee5\u5b8c\u7f8e\u590d\u5236\u8fd9\u4e9b\u7269\u7406\u5c42\u9762\u7684\u7ec6\u5fae\u7279\u5f81\u3002\n\u25c6 \u901a\u8fc7\u5206\u6790\u56fe\u50cf\u4e2d\u5d4c\u5165\u7684\u76f8\u673a\u56fa\u6709\u201c\u6307\u7eb9\u201d\u6765\u533a\u5206\u771f\u5b9e\u62cd\u6444\u7684\u7167\u7247\u4e0eAI\u751f\u6210\u7684\u4f2a\u9020\u56fe\u50cf\uff0c\u4e3a\u73b0\u6709\u7684\u9762\u90e8\u8bc6\u522b\u751f\u7269\u7cfb\u7edf\u589e\u52a0\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5b89\u5168\u5c42\u3002\n\u25c6 \u4e3a\u89e3\u51b3\u81ea\u62cd\u94f6\u884c\u7b49\u91d1\u878d\u573a\u666f\u4e0b\u9762\u4e34\u7684\u65e5\u76ca\u4e25\u5cfb\u7684\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u80fd\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2508.20322": "|2025-08-27|Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)|Zhi Li\u7b49|[2508.20322](http://arxiv.org/pdf/2508.20322)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSLiCS\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u8026\u89c6\u89c9-\u8bed\u8a00\u5171\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u5176\u6838\u5fc3\u8d21\u732e\u662f\u901a\u8fc7\u7a00\u758f\u7ebf\u6027\u6982\u5ff5\u5b50\u7a7a\u95f4\u5b9e\u73b0\u5d4c\u5165\u5411\u91cf\u7684\u7ed3\u6784\u5316\u5206\u89e3\u3002  \n\u25c6 \u63d0\u51fa\u4e00\u79cd\u76d1\u7763\u5f0f\u5b57\u5178\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5d4c\u5165\u5411\u91cf\u5206\u89e3\u4e3a\u591a\u4e2a\u6982\u5ff5\u7279\u5b9a\u7684\u6210\u5206\uff0c\u6bcf\u4e2a\u6210\u5206\u7531\u5b57\u5178\u4e2d\u4e00\u7ec4\u7a00\u758f\u975e\u8d1f\u7684\u539f\u5b50\u5411\u91cf\u7ebf\u6027\u7ec4\u5408\u800c\u6210\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u4fdd\u8bc1\u6536\u655b\u6027\uff0c\u5e76\u80fd\u5b66\u4e60\u5177\u6709\u5206\u7ec4\u7ed3\u6784\u7684\u5b57\u5178\uff0c\u5176\u7ec4\u6d3b\u52a8\u4e0e\u591a\u6807\u7b7e\u4fe1\u606f\u5339\u914d\u3002  \n\u25c6 \u5229\u7528\u6587\u672c\u5171\u5d4c\u5165\u7279\u6027\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u5b57\u5178\u5b66\u4e60\uff1a\u901a\u8fc7\u6982\u5ff5\u6807\u7b7e\u7684\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u81ea\u52a8\u751f\u6210\u5b9e\u4f8b\u7ea7\u591a\u6807\u7b7e\u3002  \n\u25c6 \u80fd\u591f\u4e3a\u6bcf\u4e2a\u6982\u5ff5\u5b50\u7a7a\u95f4\u627e\u5230\u8bed\u4e49\u660e\u786e\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002  \n\u8be5\u65b9\u6cd5\u5728\u6982\u5ff5\u8fc7\u6ee4\u56fe\u50cf\u68c0\u7d22\u548c\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8eCLIP\u3001TiTok\u548cDINOv2\u7b49\u591a\u79cd\u5d4c\u5165\u7a7a\u95f4\u3002|\n",
    "2508.20209": "|2025-08-27|Low-exposure, high-quality multimodal speckle X-ray imaging via an intrinsic gradient-flow approach|Jayvan Liu\u7b49|[2508.20209](http://arxiv.org/pdf/2508.20209)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u6d41\u65b9\u6cd5\u7684\u65b0\u578b\u591a\u6a21\u6001\u6563\u6591X\u5c04\u7ebf\u6210\u50cf\u6280\u672f\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5f00\u53d1\u4e86\u68af\u5ea6\u6d41MIST\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u50cf\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002  \n\u25c6 \u9996\u6b21\u5c06\u68af\u5ea6\u6d41\u65b9\u6cd5\u5f15\u5165\u6563\u6591\u6210\u50cf\u9886\u57df\uff0c\u901a\u8fc7\u6c42\u89e3\u798f\u514b-\u666e\u6717\u514b\u65b9\u7a0b\u540c\u6b65\u83b7\u53d6\u8870\u51cf\u3001\u76f8\u79fb\u548c\u6697\u573a\u4e09\u79cd\u4e92\u8865\u6210\u50cf\u6a21\u5f0f  \n\u25c6 \u5927\u5e45\u51cf\u5c11\u6210\u50cf\u6240\u9700\u6570\u636e\u91cf\uff0c\u964d\u4f4e\u5b9e\u9a8c\u4e2d\u5bf9\u66dd\u5149\u91cf\u548c\u91c7\u6837\u6570\u91cf\u7684\u8981\u6c42  \n\u25c6 \u5728\u4fdd\u6301X\u5c04\u7ebf\u798f\u514b-\u666e\u6717\u514b\u65b9\u7a0b\u5b8c\u6574\u901a\u7528\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u7a81\u7834\u4f20\u7edf\u7b97\u6cd5\u7684\u5c40\u9650\u6027  \n\u25c6 \u663e\u8457\u63d0\u5347\u6697\u573a\u56fe\u50cf\u8d28\u91cf\uff0c\u80fd\u6709\u6548\u663e\u793a\u4f4e\u4e8e\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u4e9a\u50cf\u7d20\u7ed3\u6784\u4fe1\u606f  \n\u25c6 \u901a\u8fc7\u6fb3\u5927\u5229\u4e9a\u540c\u6b65\u8f90\u5c04\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u76f8\u4f4d\u886c\u5ea6\u548c\u6697\u573a\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u7b80\u5316\u5b9e\u9a8c\u6d41\u7a0b\u7684\u573a\u666f\u3002|\n",
    "2508.20188": "|2025-08-27|Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study|Max Torop\u7b49|[2508.20188](http://arxiv.org/pdf/2508.20188)|\u65e0|\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u5982\u4f55\u5229\u7528\u5b9a\u91cf\u76ae\u80a4\u5c5e\u6027\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e0e\u5b9a\u91cf\u76ae\u80a4\u5c5e\u6027\uff08\u5982\u75c5\u7076\u9762\u79ef\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u8bca\u65ad\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u3002  \n\u25c6 \u63d0\u51fa\u901a\u8fc7\u5fae\u8c03MLLMs\uff0c\u4f7f\u5176\u80fd\u591f\u4ece\u76ae\u80a4\u56fe\u50cf\u4e2d\u9884\u6d4b\u8fd9\u4e9b\u5b9a\u91cf\u5c5e\u6027\u503c\uff0c\u4ece\u800c\u5b9e\u73b0\u6a21\u578b\u5d4c\u5165\u7a7a\u95f4\u4e0e\u4e34\u5e8a\u5c5e\u6027\u7684\u5bf9\u9f50\u3002  \n\u25c6 \u91c7\u7528\u57fa\u4e8e\u5185\u5bb9\u7684\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff0c\u5728SLICE-3D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5d4c\u5165\u7a7a\u95f4\u4e0e\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u8054\u6027\u3002  \n\u25c6 \u4e3a\u6a21\u578b\u8bca\u65ad\u7ed3\u679c\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u89c6\u89c9\u4f9d\u636e\uff0c\u4f7f\u6a21\u578b\u8f93\u51fa\u66f4\u5177\u53ef\u4fe1\u5ea6\u548c\u4ea4\u4e92\u6027\u3002  \n\u8fd9\u4e00\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u533b\u7597\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u65b9\u6cd5\u57fa\u7840\u3002|\n",
    "2508.21539": "|2025-08-29|HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones|Hao Ruan\u7b49|[2508.21539](http://arxiv.org/pdf/2508.21539)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u65e0\u4eba\u673a\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u6311\u6218\uff0c\u63d0\u51fa\u4e86HCCM\u5206\u5c42\u8de8\u7c92\u5ea6\u5bf9\u6bd4\u4e0e\u5339\u914d\u5b66\u4e60\u6846\u67b6\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5982\u4e0b\uff1a\n\u25c6 \u63d0\u51fa\u533a\u57df-\u5168\u5c40\u56fe\u50cf\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\uff08RG-ITC\uff09\uff0c\u65e0\u9700\u7cbe\u786e\u573a\u666f\u5212\u5206\u5373\u53ef\u6355\u83b7\u4ece\u5c40\u90e8\u5230\u5168\u5c40\u7684\u5206\u5c42\u8bed\u4e49\u5bf9\u9f50\uff1b\n\u25c6 \u8bbe\u8ba1\u533a\u57df-\u5168\u5c40\u56fe\u50cf\u6587\u672c\u5339\u914d\uff08RG-ITM\uff09\uff0c\u901a\u8fc7\u8bc4\u4f30\u5168\u5c40\u8de8\u6a21\u6001\u8868\u5f81\u4e2d\u7684\u5c40\u90e8\u8bed\u4e49\u4e00\u81f4\u6027\u6765\u589e\u5f3a\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff1b\n\u25c6 \u5f15\u5165\u52a8\u91cf\u5bf9\u6bd4\u4e0e\u84b8\u998f\u673a\u5236\uff08MCD\uff09\uff0c\u6709\u6548\u7f13\u89e3\u65e0\u4eba\u673a\u6587\u672c\u63cf\u8ff0\u4e0d\u5b8c\u6574\u6216\u6a21\u7cca\u5e26\u6765\u7684\u5bf9\u9f50\u4e0d\u7a33\u5b9a\u95ee\u9898\uff1b\n\u25c6 \u5728GeoText-1652\u548cERA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u68c0\u7d22\u7cbe\u5ea6\u7a81\u7834\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5c55\u73b0\u5f3a\u5927\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2509.02129": "|2025-09-02|Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time|Jintao Cheng\u7b49|[2509.02129](http://arxiv.org/pdf/2509.02129)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u7684\u96f6\u6837\u672c\u65b0\u6846\u67b6\uff0c\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u8de8\u57df\u8bc6\u522b\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u63d0\u51fa\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\uff0c\u901a\u8fc7\u57fa\u4e8e\u5f15\u5bfc\u7684\u65b9\u6cd5\u76f4\u63a5\u8fdb\u884c\u76f8\u4f3c\u6027\u8bc4\u5206\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5fae\u8c03\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u3002\n\u25c6 \u91c7\u7528\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u957f\u5ea6\u53ef\u63a7\u7684JSON\u8f93\u51fa\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u590d\u6742\u7684\u591a\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff0c\u7b80\u5316\u4e86\u6d41\u7a0b\u3002\n\u25c6 \u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u81ea\u4e00\u81f4\u6027\uff08UASC\uff09\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u5b9e\u65f6\u81ea\u9002\u5e94\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6210\u672c\uff0c\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u3002\n\u25c6 \u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe210\u500d\u3002\n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6781\u5927\u63d0\u5347\u4e86\u6548\u7387\u4e0e\u9002\u5e94\u6027\u3002|\n",
    "2509.01968": "|2025-09-02|Ensemble-Based Event Camera Place Recognition Under Varying Illumination|Therese Joseph\u7b49|[2509.01968](http://arxiv.org/pdf/2509.01968)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5f0f\u4e8b\u4ef6\u76f8\u673a\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u5267\u70c8\u5149\u7167\u53d8\u5316\u4e0b\u7684\u73af\u5883\u9c81\u68d2\u6027\u3002  \n\u25c6 \u91c7\u7528\u591a\u4e8b\u4ef6\u91cd\u5efa\u3001\u591a\u7279\u5f81\u63d0\u53d6\u4e0e\u591a\u65f6\u5e8f\u5206\u8fa8\u7387\u7684\u96c6\u6210\u878d\u5408\u7b56\u7565\uff0c\u7a81\u7834\u4e86\u4ee5\u5f80\u4ec5\u878d\u5408\u65f6\u5e8f\u4fe1\u606f\u7684\u5c40\u9650\u3002  \n\u25c6 \u5728\u8de8\u65e5-\u591c\u5149\u7167\u53d8\u5316\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86Recall@1\u6307\u680757%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u8868\u73b0\u51fa\u6781\u5f3a\u7684\u5149\u7167\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728\u957f\u8fbe8\u516c\u91cc\u7684\u5b9e\u9645\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u672a\u8fdb\u884c\u964d\u91c7\u6837\uff0c\u4fdd\u7559\u4e86\u771f\u5b9e\u4e8b\u4ef6\u5bc6\u5ea6\u53d8\u5316\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u5bf9\u5e8f\u5217\u5339\u914d\u6846\u67b6\u7684\u6539\u8fdb\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u4e0b\u7684\u8bc6\u522b\u6027\u80fd\u3002  \n\u25c6 \u7cfb\u7edf\u5206\u6790\u4e86\u4e8b\u4ef6\u8868\u5f81\u3001\u91cd\u5efa\u65b9\u6cd5\u548c\u7279\u5f81\u63d0\u53d6\u7b49\u5173\u952e\u8bbe\u8ba1\u9009\u9879\u7684\u5f71\u54cd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002|\n",
    "2509.01360": "|2025-09-01|M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision|Che Liu\u7b49|[2509.01360](http://arxiv.org/pdf/2509.01360)|\u65e0|M3Ret\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u7684\u788e\u7247\u5316\u95ee\u9898\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6df7\u5408\u6a21\u6001\u533b\u5b66\u6570\u636e\u96c6\uff0c\u5305\u542b86\u4e07\u4f59\u6837\u672c\uff0c\u6db5\u76d62D\u30013D\u53ca\u89c6\u9891\u6570\u636e\n\u25c6\u9996\u6b21\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u6a21\u6001\u5b9a\u5236\u5316\u8bbe\u8ba1\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89c6\u89c9\u7f16\u7801\u5668\n\u25c6\u878d\u5408\u751f\u6210\u5f0f(MAE)\u4e0e\u5bf9\u6bd4\u5f0f(SimDINO)\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u89c6\u89c9\u8868\u793a\n\u25c6\u5728\u96f6\u6837\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u5168\u9762\u8d85\u8d8aDINOv3\u548cBMC-CLIP\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\n\u25c6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8de8\u6a21\u6001\u68c0\u7d22\n\u25c6\u9996\u6b21\u8bc1\u660e\u7eaf\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60\u53ef\u6cdb\u5316\u81f3\u672a\u89c1\u6a21\u6001\uff08\u5982\u672a\u8bad\u7ec3\u7684MRI\u6570\u636e\uff09\uff0c\u4e3a\u533b\u5b66\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411|\n",
    "2509.01259": "|2025-09-01|ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization|Thinh-Phuc Nguyen\u7b49|[2509.01259](http://arxiv.org/pdf/2509.01259)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ReCap\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u65e0\u6cd5\u6355\u6349\u4e8b\u4ef6\u7ea7\u8bed\u4e49\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u76f8\u5173\u6587\u7ae0\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u751f\u6210\u53d9\u4e8b\u4e30\u5bcc\u4e14\u4e8b\u5b9e\u51c6\u786e\u7684\u63cf\u8ff0\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6587\u7ae0\u68c0\u7d22\u7cfb\u7edf\uff0c\u7ed3\u5408DINOv2\u5168\u5c40\u7279\u5f81\u76f8\u4f3c\u5ea6\u521d\u9009\u548c\u5c40\u90e8\u5757\u4e92\u8fd1\u90bb\u76f8\u4f3c\u5ea6\u91cd\u6392\u5e8f\uff0c\u63d0\u5347\u4e8b\u4ef6\u76f8\u5173\u6587\u7ae0\u7684\u68c0\u7d22\u7cbe\u5ea6\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u4e0a\u4e0b\u6587\u63d0\u53d6\u6846\u67b6\uff0c\u7efc\u5408\u6587\u7ae0\u6458\u8981\u3001\u901a\u7528\u63cf\u8ff0\u548c\u6e90\u6570\u636e\u4fe1\u606f\uff0c\u4e3a\u751f\u6210\u63cf\u8ff0\u63d0\u4f9b\u591a\u7ef4\u5ea6\u8bed\u4e49\u652f\u6301\u3002  \n\u25c6 \u5f15\u5165\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63cf\u8ff0\u751f\u6210\u673a\u5236\uff0c\u5e76\u91c7\u7528\u8bed\u4e49\u9ad8\u65af\u5f52\u4e00\u5316\u6280\u672f\uff0c\u589e\u5f3a\u751f\u6210\u6587\u672c\u7684\u6d41\u7545\u6027\u548c\u76f8\u5173\u6027\u3002  \n\u5728EVENTA 2025\u6311\u6218\u8d5b\u4e2d\uff0cReCap\u5728OpenEvents V1\u6570\u636e\u96c6\u4e0a\u53d6\u5f970.54666\u7684\u7efc\u5408\u8bc4\u5206\uff0c\u6392\u540d\u7b2c\u4e8c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002  \n\u8be5\u7cfb\u7edf\u4e3a\u65b0\u95fb\u5b58\u6863\u7b49\u9ad8\u8981\u6c42\u9886\u57df\u63d0\u4f9b\u4e86\u89c6\u89c9\u611f\u77e5\u4e0e\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u878d\u5408\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.00798": "|2025-09-03|Multimodal Iterative RAG for Knowledge Visual Question Answering|Changin Choi\u7b49|[2509.00798](http://arxiv.org/pdf/2509.00798)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MI-RAG\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u8fed\u4ee3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u77e5\u8bc6\u5bc6\u96c6\u578b\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5728\u4e8e\uff1a\n\n\u25c6 \u91c7\u7528\u8fed\u4ee3\u5f0f\u68c0\u7d22-\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u9010\u6b65\u5b8c\u5584\u5916\u90e8\u77e5\u8bc6\u7684\u83b7\u53d6\u4e0e\u7406\u89e3\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5355\u6b21\u68c0\u7d22\u77e5\u8bc6\u4e0d\u8db3\u7684\u5c40\u9650\u3002\n\n\u25c6 \u5229\u7528\u7d2f\u79ef\u7684\u63a8\u7406\u8bb0\u5f55\u52a8\u6001\u751f\u6210\u591a\u67e5\u8be2\uff0c\u9a71\u52a8\u5bf9\u5305\u542b\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u7684\u5f02\u6784\u77e5\u8bc6\u5e93\u8fdb\u884c\u8054\u5408\u641c\u7d22\u3002\n\n\u25c6 \u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u77e5\u8bc6\u878d\u5408\u4e0e\u63a8\u7406\u66f4\u65b0\uff0c\u5c06\u65b0\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u5408\u6210\u5230\u63a8\u7406\u8bb0\u5f55\u4e2d\uff0c\u8fdb\u884c\u6e10\u8fdb\u5f0f\u7684\u7cbe\u5316\u7406\u89e3\u3002\n\n\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u53ec\u56de\u7387\u548c\u7b54\u6848\u51c6\u786e\u7387\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u89c6\u89c9\u95ee\u7b54\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ec4\u5408\u63a8\u7406\u65b9\u6848\u3002|\n",
    "2509.00752": "|2025-08-31|Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification|Y Hop Nguyen\u7b49|[2509.00752](http://arxiv.org/pdf/2509.00752)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u7684\u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u4e09\u7c7b\u4e34\u5e8a\u4efb\u52a1\u7684\u9ad8\u6548\u534f\u540c\u5904\u7406\u3002  \n\u25c6 \u91c7\u7528CLIP ViT-B/16\u4e3b\u5e72\u7f51\u7edc\u5e76\u5f15\u5165\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\uff0c\u5b9e\u73b0\u6709\u9650\u533b\u7597\u6570\u636e\u4e0b\u7684\u9ad8\u6548\u5fae\u8c03\u3002  \n\u25c6 \u63d0\u51fa\u591a\u7ea7CLS\u4ee4\u724c\u805a\u5408\u673a\u5236\uff0c\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\u7684\u591a\u6837\u6027\u548c\u8868\u5f81\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1\u7403\u9762\u7279\u5f81\u63d2\u503c\u65b9\u6cd5\uff0c\u4f18\u5316\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u6548\u679c\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u7c7b\u522b\u7279\u5b9a\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u5c06\u8bca\u65ad\u6587\u672c\u4e0a\u4e0b\u6587\u4e0e\u89c6\u89c9\u7279\u5f81\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u76d1\u7763\u5206\u7c7b\u8054\u5408\u8bad\u7ec3\u76ee\u6807\u8fdb\u884c\u878d\u5408\u3002  \n\u8be5\u6846\u67b6\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8fbe\u5230S\u6027\u80fd\uff08\u5206\u7c7b\u51c6\u786e\u738795%\uff0c\u68c0\u7d22Recall@1\u8d850.92\uff09\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u533b\u7597\u573a\u666f\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u591a\u6a21\u6001\u7406\u89e3\u65b9\u6848\u3002|\n",
    "2509.00751": "|2025-08-31|EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions|Dinh-Khoi Vo\u7b49|[2509.00751](http://arxiv.org/pdf/2509.00751)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u590d\u6742\u4e8b\u4ef6\u63cf\u8ff0\u7684\u591a\u6a21\u6001\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edfEVENT-Retriever\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u901a\u8fc7\u591a\u9636\u6bb5\u6846\u67b6\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5bf9\u9690\u542b\u4e8b\u4ef6\u8bed\u4e49\u548c\u957f\u6587\u672c\u63cf\u8ff0\u7684\u68c0\u7d22\u74f6\u9888\u3002  \n\u25c6 \u7ed3\u5408\u5bc6\u96c6\u6587\u6863\u68c0\u7d22\u3001\u4e8b\u4ef6\u611f\u77e5\u8bed\u8a00\u6a21\u578b\u91cd\u6392\u5e8f\u548c\u9ad8\u6548\u56fe\u50cf\u6536\u96c6\u7684\u591a\u9636\u6bb5\u68c0\u7d22\u67b6\u6784  \n\u25c6 \u5229\u7528Qwen3\u7cfb\u5217\u6a21\u578b\u5b9e\u73b0\u6587\u7ae0\u641c\u7d22\u3001\u4e0a\u4e0b\u6587\u5bf9\u9f50\u548c\u7cbe\u51c6\u56fe\u50cf\u8bc4\u5206\u7684\u5206\u5c42\u5904\u7406  \n\u25c6 \u5f15\u5165\u57fa\u4e8e\u6807\u9898\u7684\u8bed\u4e49\u5339\u914d\u4e0e\u6392\u5e8f\u611f\u77e5\u9009\u62e9\u673a\u5236\u589e\u5f3a\u4e8b\u4ef6\u5173\u8054\u6027  \n\u25c6 \u91c7\u7528 Reciprocal Rank Fusion \u878d\u5408\u591a\u914d\u7f6e\u8f93\u51fa\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027  \n\u8be5\u7cfb\u7edf\u5728EVENTA 2025\u6311\u6218\u8d5bTrack 2\u79c1\u6709\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u68c0\u7d22\u7ed3\u5408\u5bf9\u590d\u6742\u73b0\u5b9e\u56fe\u50cf\u7406\u89e3\u7684\u6709\u6548\u6027\u3002|\n",
    "2509.00177": "|2025-08-29|Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders|Faizan Farooq Khan\u7b49|[2509.00177](http://arxiv.org/pdf/2509.00177)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u7c7b\u522b\u7ea7\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6a21\u6001\u5dee\u5f02\u7684\u521b\u65b0\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u878d\u5408\u751f\u6210\u6a21\u578b\u4e0e\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u3002\n\n\u25c6 \u63d0\u51fa\u4e24\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u6a21\u578b\u5c06\u6587\u672c\u67e5\u8be2\u8f6c\u6362\u4e3a\u89c6\u89c9\u67e5\u8be2\uff0c\u518d\u7528\u89c6\u89c9\u6a21\u578b\u8ba1\u7b97\u56fe\u50cf\u95f4\u76f8\u4f3c\u5ea6\u3002\n\u25c6 \u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5c06\u6587\u672c\u6a21\u6001\u8f6c\u5316\u4e3a\u89c6\u89c9\u6a21\u6001\uff0c\u6709\u6548\u7f29\u5c0f\u6587\u672c\u4e0e\u56fe\u50cf\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5dee\u8ddd\u3002\n\u25c6 \u8bbe\u8ba1\u805a\u5408\u7f51\u7edc\u6574\u5408\u591a\u4e2a\u751f\u6210\u56fe\u50cf\u7684\u5411\u91cf\u8868\u793a\uff0c\u5f62\u6210\u5355\u4e00\u4e14\u9c81\u68d2\u7684\u67e5\u8be2\u8868\u5f81\u3002\n\u25c6 \u521b\u65b0\u6027\u5730\u878d\u5408\u6587\u672c\u548c\u751f\u6210\u56fe\u50cf\u53cc\u6a21\u6001\u7684\u76f8\u4f3c\u5ea6\u8bc4\u5206\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u3002\n\u8be5\u65b9\u6cd5\u7efc\u5408\u8fd0\u7528\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6269\u6563\u751f\u6210\u6a21\u578b\u548c\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u6587\u672c\u67e5\u8be2\u7684\u68c0\u7d22\u65b9\u6cd5\u3002|\n",
    "2509.04351": "|2025-09-05|Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking|Dror Aiger\u7b49|[2509.04351](http://arxiv.org/pdf/2509.04351)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e0e\u4e3b\u6d41\u76f8\u53cd\u7684\u201c\u5c40\u90e8\u5230\u5168\u5c40\u201d\u56fe\u50cf\u68c0\u7d22\u65b0\u8303\u5f0f\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u201c\u5168\u5c40\u5230\u5c40\u90e8\u201d\u65b9\u6cd5\u3002  \n\u25c6 \u5229\u7528\u65b0\u5174\u7684\u9ad8\u6548\u5c40\u90e8\u7279\u5f81\u641c\u7d22\u6280\u672f\uff0c\u9996\u5148\u8fdb\u884c\u5927\u89c4\u6a21\u7cbe\u7ec6\u7684\u5c40\u90e8\u5339\u914d\uff0c\u4ee5\u627e\u5230\u5168\u5c40\u7279\u5f81\u5bb9\u6613\u9057\u6f0f\u7684\u5c40\u90e8\u76f8\u4f3c\u56fe\u50cf\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u68c0\u7d22\u76f8\u4f3c\u6027\u7684\u5168\u5c40\u7279\u5f81\u5373\u65f6\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u5728\u91cd\u6392\u5e8f\u9636\u6bb5\u624d\u52a8\u6001\u751f\u6210\u5168\u5c40\u7279\u5f81\u3002  \n\u25c6 \u91c7\u7528\u591a\u7ef4\u7f29\u653e\u6280\u672f\uff0c\u5c06\u5c40\u90e8\u7279\u5f81\u68c0\u7d22\u83b7\u5f97\u7684\u76f8\u4f3c\u6027\u5173\u7cfb\u5d4c\u5165\u5230\u5168\u5c40\u7279\u5f81\u8868\u793a\u4e2d\uff0c\u4f7f\u5168\u5c40\u7279\u5f81\u80fd\u591f\u5c0a\u91cd\u5c40\u90e8\u5339\u914d\u7684\u7ed3\u679c\u3002  \n\u8fd9\u79cd\u7ed3\u5408\u4f7f\u5f97\u91cd\u6392\u5e8f\u8fc7\u7a0b\u65e2\u4fdd\u6301\u4e86\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u53c8\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u7cbe\u5ea6\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Revisited Oxford\u548cParis\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002|\n",
    "2509.04193": "|2025-09-04|DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval|Ruohong Yang\u7b49|[2509.04193](http://arxiv.org/pdf/2509.04193)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u76d1\u7763\u8de8\u57df\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5DUDE\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u89e3\u51b3\u8de8\u57df\u68c0\u7d22\u4e2d\u5bf9\u8c61\u7279\u5f81\u4e0e\u57df\u98ce\u683c\u7ea0\u7f20\u7684\u96be\u9898\u3002  \n\u25c6 \u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u5bf9\u8c61\u7279\u5f81\u4e0e\u57df\u7279\u5b9a\u98ce\u683c\u7684\u89e3\u8026\uff0c\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u7684\u7eaf\u51c0\u6027\u3002  \n\u25c6 \u63d0\u51fa\u6e10\u8fdb\u5f0f\u8de8\u57df\u4e92\u8fd1\u90bb\u5bf9\u9f50\u673a\u5236\uff0c\u901a\u8fc7\u57df\u5185\u5230\u57df\u95f4\u7684\u9010\u6b65\u5bf9\u9f50\u63d0\u5347\u7279\u5f81\u5339\u914d\u53ef\u9760\u6027\u3002  \n\u25c6 \u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u6db5\u76d613\u4e2a\u57df\uff09\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002  \n\u8be5\u65b9\u6cd5\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u8de8\u57df\u7cbe\u51c6\u68c0\u7d22\uff0c\u4e3a\u65e0\u76d1\u7763\u57df\u9002\u5e94\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2509.04948": "|2025-09-05|Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)|Emanuela Boros|[2509.04948](http://arxiv.org/pdf/2509.04948)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u79fb\u52a8\u673a\u5668\u4eba\u5728\u529e\u516c\u5ba4\u73af\u5883\u4e2d\u7684\u62d3\u6251\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u9760\u5355\u76ee\u5f69\u8272\u76f8\u673a\u56fe\u50cf\u3001\u4e0d\u4f9d\u8d56\u65f6\u5e8f\u8fde\u7eed\u6027\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u3002  \n\u25c6 \u7cfb\u7edf\u6027\u5730\u5b9a\u91cf\u6bd4\u8f83\u4e86\u591a\u79cd\u5148\u8fdb\u89c6\u89c9\u63cf\u8ff0\u7b26\uff08\u5982\u989c\u8272\u76f4\u65b9\u56fe\u3001SIFT\u3001ASIFT\u3001RGB-SIFT\u53ca\u8bcd\u888b\u6a21\u578b\uff09\u7684\u6027\u80fd\u3002  \n\u25c6 \u6df1\u5165\u5206\u6790\u4e86\u4e0d\u540c\u7279\u5f81\u63cf\u8ff0\u7b26\u3001\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u548c\u5206\u7c7b\u5668\u7684\u7ec4\u5408\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u548c\u53ef\u89c6\u5316\u65b9\u6cd5\u6269\u5c55\u4e86\u5df2\u6709\u5b9e\u9a8c\u3002  \n\u25c6 \u5728ImageCLEF\u8bc4\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u914d\u7f6e\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u65b0\u56fe\u50cf\u5e8f\u5217\u7684\u6700\u53ef\u80fd\u4f4d\u7f6e\u8bc6\u522b\u3002  \n\u8bba\u6587\u4e3a\u5916\u89c2\u63cf\u8ff0\u7b26\u3001\u76f8\u4f3c\u6027\u5ea6\u91cf\u4e0e\u5206\u7c7b\u5668\u7684\u5408\u7406\u914d\u7f6e\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u5b9e\u65f6\u5b9a\u4f4d\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002|\n",
    "2509.04772": "|2025-09-05|FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph|Zhangding Liu\u7b49|[2509.04772](http://arxiv.org/pdf/2509.04772)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86FloodVision\uff0c\u4e00\u79cd\u7ed3\u5408\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u7684\u96f6\u6837\u672c\u6d2a\u6c34\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u6d2a\u6c34\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u5f3a\u5927\u7684\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578bGPT-4o\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u4e0e\u7ed3\u6784\u5316\u7684\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6df1\u5ea6\u4f30\u8ba1\u3002\n\u25c6 \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8f66\u8f86\u3001\u884c\u4eba\u7b49\u5e38\u89c1\u57ce\u5e02\u7269\u4f53\u771f\u5b9e\u5c3a\u5bf8\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u4e3a\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7269\u7406\u73b0\u5b9e\u4f9d\u636e\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\u3002\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u5957\u52a8\u6001\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u53c2\u8003\u7269\u4f53\u8bc6\u522b\u3001\u6df9\u6ca1\u6bd4\u4f8b\u4f30\u7b97\u548c\u7edf\u8ba1\u79bb\u7fa4\u503c\u8fc7\u6ee4\uff0c\u4ee5\u8ba1\u7b97\u51fa\u7cbe\u786e\u7684\u6df1\u5ea6\u503c\u3002\n\u25c6 \u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5176\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4f4e\u81f38.17\u5398\u7c73\uff0c\u8f83GPT-4o\u57fa\u7ebf\u63d0\u5347\u4e8620.5%\uff0c\u5e76\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684CNN\u65b9\u6cd5\uff0c\u4e14\u5177\u5907\u8fd1\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002|\n",
    "2509.06566": "|2025-09-08|Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval|Emil Demi\u0107\u7b49|[2509.06566](http://arxiv.org/pdf/2509.06566)|\u65e0|\u672c\u6587\u9488\u5bf9\u573a\u666f\u7ea7\u8349\u56fe\u68c0\u7d22\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u8c03\u8349\u56fe\u56fa\u6709\u6a21\u7cca\u6027\u548c\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8bad\u7ec3\u65b9\u6848\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u91cd\u65b0\u5ba1\u89c6\u4e86\u573a\u666f\u7ea7\u8349\u56fe\u68c0\u7d22\u95ee\u9898\uff0c\u5f3a\u8c03\u771f\u5b9e\u624b\u7ed8\u8349\u56fe\u7684\u6a21\u7cca\u6027\u548c\u566a\u58f0\u7279\u6027\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u6a21\u578b\u7ed3\u6784\u6539\u8fdb\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u663e\u5f0f\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u4e13\u95e8\u9488\u5bf9\u8349\u56fe\u7684\u9ad8\u53d8\u5f02\u6027\u8fdb\u884c\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\n\u25c6 \u901a\u8fc7\u7ed3\u5408\u9002\u5f53\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3001\u7f16\u7801\u5668\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\uff0c\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\n\u25c6 \u5728FS-COCO\u548cSketchyCOCO\u7b49\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u8bad\u7ec3\u8bbe\u8ba1\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002\n\u25c6 \u6307\u51fa\u4e86\u6539\u8fdb\u573a\u666f\u7ea7\u8349\u56fe\u68c0\u7d22\u8bc4\u4f30\u573a\u666f\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002|\n",
    "2509.07362": "|2025-09-09|Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark|Yandi Yang\u7b49|[2509.07362](http://arxiv.org/pdf/2509.07362)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u6784\u5efa\u4e86\u4e00\u4e2a\u89e3\u51b3\u7a7a\u5730\u8de8\u6a21\u6001\u5b9a\u4f4d\u6311\u6218\u7684\u7efc\u5408\u57fa\u51c6\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u7a7a\u5730\u8de8\u6a21\u6001\u6570\u636e\u96c6\uff0c\u96c6\u6210\u4e86\u6765\u81ea\u79fb\u52a8\u6d4b\u91cf\u7cfb\u7edf\u7684\u5730\u9762\u56fe\u50cf\u548c\u4e09\u4e2a\u57ce\u5e02\uff08\u6b66\u6c49\u3001\u9999\u6e2f\u3001\u65e7\u91d1\u5c71\uff09\u7684\u673a\u8f7d\u6fc0\u5149\u626b\u63cf\u70b9\u4e91\u3002\n\u25c6 \u89e3\u51b3\u4e86\u8be5\u9886\u57df\u5e73\u53f0\u591a\u6837\u5316\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u4e3a\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u57fa\u7840\u3002\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u7684\u53ef\u9760\u5730\u9762\u771f\u503c\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u8be5\u73af\u8282\u7684\u7f3a\u5931\u95ee\u9898\u3002\n\u25c6 \u9996\u6b21\u5728\u7a7a\u5730\u8de8\u5e73\u53f0\u8bbe\u7f6e\u4e0b\u5bf9\u73b0\u6709\u7684\u56fe\u50cf\u5230\u70b9\u4e91\u914d\u51c6\u7b97\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002\n\u25c6 \u4e3a\u63d0\u5347\u5728\u7eb9\u7406\u7f3a\u5931\u3001\u5927\u89c6\u89d2\u53d8\u5316\u7b49\u6311\u6218\u4e0b\u7684\u89c6\u89c9\u5b9a\u4f4d\u7cbe\u5ea6\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002|\n",
    "2509.09306": "|2025-09-11|Listening for \"You\": Enhancing Speech Image Retrieval via Target Speaker Extraction|Wenhao Yang\u7b49|[2509.09306](http://arxiv.org/pdf/2509.09306)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76ee\u6807\u8bf4\u8bdd\u4eba\u8bed\u97f3-\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u53ca\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8bf4\u8bdd\u4eba\u573a\u666f\u4e0b\u8bed\u97f3\u56fe\u50cf\u68c0\u7d22\u7684\u96be\u9898\u3002  \n\u25c6 \u9996\u6b21\u5f15\u5165\u76ee\u6807\u8bf4\u8bdd\u4eba\u63d0\u53d6\u6280\u672f\uff0c\u4ece\u6df7\u5408\u8bed\u97f3\u4e2d\u5206\u79bb\u5e76\u8bc6\u522b\u76ee\u6807\u8bf4\u8bdd\u4eba\u7684\u6307\u4ee4\u3002  \n\u25c6 \u7ed3\u5408\u81ea\u76d1\u7763\u97f3\u9891\u7f16\u7801\u5668\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u901a\u8fc7\u76ee\u6807\u8bf4\u8bdd\u4eba\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u7aef\u5230\u7aef\u7684\u8054\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u8bf4\u8bdd\u4eba\u63d0\u53d6\u4e0e\u68c0\u7d22\u4efb\u52a1\u7edf\u4e00\u4f18\u5316\uff0c\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u3002  \n\u5728\u4e8c\u8bf4\u8bdd\u4eba\u548c\u4e09\u8bf4\u8bdd\u4eba\u6df7\u5408\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u5347\u660e\u663e\u3002  \n\u8be5\u6280\u672f\u4e3a\u8f85\u52a9\u673a\u5668\u4eba\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.11862": "|2025-09-15|Bridging Vision Language Models and Symbolic Grounding for Video Question Answering|Haodi Ma\u7b49|[2509.11862](http://arxiv.org/pdf/2509.11862)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSG-VLM\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7b26\u53f7\u5316\u573a\u666f\u56fe\u589e\u5f3a\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u65f6\u7a7a\u4e0e\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002\n\n\u25c6 \u5f15\u5165\u7b26\u53f7\u5316\u573a\u666f\u56fe\uff08SGs\uff09\u4f5c\u4e3a\u89c6\u9891\u95ee\u7b54\u7684\u4e2d\u95f4 grounding \u4fe1\u53f7\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u5bf9\u8c61-\u5173\u7cfb\u8868\u793a\u4ee5\u8865\u5145\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6574\u4f53\u63a8\u7406\u3002\n\u25c6 \u63d0\u51faSG-VLM\u6846\u67b6\uff0c\u5c06\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e0e\u573a\u666f\u56fe grounding \u673a\u5236\u901a\u8fc7\u63d0\u793a\u548c\u89c6\u89c9\u5b9a\u4f4d\u8fdb\u884c\u96c6\u6210\u3002\n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08NExT-QA\u3001iVQA\u3001ActivityNet-QA\uff09\u548c\u4e0d\u540cVLMs\uff08QwenVL\u3001InternVL\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\n\u25c6 \u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u548c\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u9700\u8981\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u5173\u7cfb\u7684\u89c6\u9891\u95ee\u7b54\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\n\u25c6 \u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u7b26\u53f7 grounding \u65b9\u6cd5\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u878d\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u6df7\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002|\n",
    "2509.11301": "|2025-09-14|UnLoc: Leveraging Depth Uncertainties for Floorplan Localization|Matthias W\u00fcest\u7b49|[2509.11301](http://arxiv.org/pdf/2509.11301)|\u65e0|UnLoc\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u5e8f\u5217\u5316\u76f8\u673a\u5e73\u9762\u56fe\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u5f15\u5165\u6982\u7387\u5316\u6df1\u5ea6\u9884\u6d4b\u6a21\u578b\uff0c\u5c06\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7ed3\u679c\u8868\u793a\u4e3a\u663e\u5f0f\u6982\u7387\u5206\u5e03\uff0c\u4ece\u800c\u6709\u6548\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002  \n\u25c6 \u6446\u8131\u5bf9\u6bcf\u573a\u666f\u5b9a\u5236\u6df1\u5ea6\u7f51\u7edc\u7684\u4f9d\u8d56\uff0c\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\uff0c\u5927\u5e45\u63d0\u5347\u65b9\u6cd5\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u3002  \n\u25c6 \u5728\u957f\u5e8f\u5217\u4e0e\u77ed\u5e8f\u5217\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u7a81\u7834\uff0c\u5728LaMAR HGE\u6570\u636e\u96c6\u4e0a\u957f\u5e8f\u5217\u5b9a\u4f4d\u53ec\u56de\u7387\u63d0\u53472.7\u500d\uff0c\u77ed\u5e8f\u5217\u63d0\u534716.7\u500d\u3002  \n\u25c6 \u901a\u8fc7\u878d\u5408\u6613\u4e8e\u83b7\u53d6\u4e14\u957f\u671f\u7a33\u5b9a\u7684\u5e73\u9762\u56fe\u6570\u636e\uff0c\u514b\u670d\u89c6\u89c9\u5916\u89c2\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u3002  \n\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u4e3a\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.12824": "|2025-09-17|DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep Hashing Image Retrieval|Zechao Liu\u7b49|[2509.12824](http://arxiv.org/pdf/2509.12824)|\u65e0|\u25c6 Deep hashing models have been widely adopted to tackle the challenges of large-scale image retrieval.\n\u25c6 However, these approaches face serious security risks due to their vulnerability to adversarial examples.\n\u25c6 Despite the increasing exploration of targeted attacks on deep hashing models, existing approaches still suffer from a lack of multimodal guidance, reliance on labeling information and dependence on pixel-level operations for attacks.|\n",
    "2509.14104": "|2025-09-17|CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts|Leonard Hackel\u7b49|[2509.14104](http://arxiv.org/pdf/2509.14104)|\u65e0|\u25c6 Self-supervised learning through masked autoencoders has attracted great attention for remote sensing (RS) foundation model (FM) development, enabling improved representation learning across diverse sensors and downstream tasks.\n\u25c6 However, existing RS FMs often either suffer from substantial computational complexity during both training and inference or exhibit limited representational capacity.\n\u25c6 These issues restrict their practical applicability in RS.|\n",
    "2509.13474": "|2025-09-16|Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization|Yujia Lin\u7b49|[2509.13474](http://arxiv.org/pdf/2509.13474)|\u65e0|\u25c6 Ensuring accurate localization of robots in environments without GPS capability is a challenging task.\n\u25c6 Visual Place Recognition (VPR) techniques can potentially achieve this goal, but existing RGB-based methods are sensitive to changes in illumination, weather, and other seasonal changes.\n\u25c6 Existing cross-modal localization methods leverage the geometric properties of RGB images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.|\n",
    "2509.13414": "|2025-09-18|MapAnything: Universal Feed-Forward Metric 3D Reconstruction|Nikhil Keetha\u7b49|[2509.13414](http://arxiv.org/pdf/2509.13414)|\u65e0|\u25c6 We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras.\n\u25c6 MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame.\n\u25c6 Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more.|\n",
    "2509.14985": "|2025-09-18|PRISM: Product Retrieval In Shopping Carts using Hybrid Matching|Arda Kabadayi\u7b49|[2509.14985](http://arxiv.org/pdf/2509.14985)|\u65e0|\u25c6 Compared to traditional image retrieval tasks, product retrieval in retail settings is even more challenging.\n\u25c6 Products of the same type from different brands may have highly similar visual appearances, and the query image may be taken from an angle that differs significantly from view angles of the stored catalog images.\n\u25c6 Foundational models, such as CLIP and SigLIP, often struggle to distinguish these subtle but important local differences.|\n",
    "2509.14746": "|2025-09-18|Chain-of-Thought Re-ranking for Image Retrieval Tasks|Shangrong Wu\u7b49|[2509.14746](http://arxiv.org/pdf/2509.14746)|\u65e0|\u25c6 Image retrieval remains a fundamental yet challenging problem in computer vision.\n\u25c6 While recent advances in Multimodal Large Language Models (MLLMs) have demonstrated strong reasoning capabilities, existing methods typically employ them only for evaluation, without involving them directly in the ranking process.\n\u25c6 As a result, their rich multimodal reasoning abilities remain underutilized, leading to suboptimal performance.|\n",
    "2509.14565": "|2025-09-18|DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising|Li Gao\u7b49|[2509.14565](http://arxiv.org/pdf/2509.14565)|\u65e0|\u25c6 Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap.\n\u25c6 Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS.\n\u25c6 Although GPS is readily available, it suffers from multipath errors in urban environments.|\n",
    "2509.14516": "|2025-09-18|Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods|Adam D. Hines\u7b49|[2509.14516](http://arxiv.org/pdf/2509.14516)|\u65e0|\u25c6 Event-based localization research and datasets are a rapidly growing area of interest, with a tenfold increase in the cumulative total number of published papers on this topic over the past 10 years.\n\u25c6 Whilst the rapid expansion in the field is exciting, it brings with it an associated challenge: a growth in the variety of required code and package dependencies as well as data formats, making comparisons difficult and cumbersome for researchers to implement reliably.\n\u25c6 To address this challenge, we present Event-LAB: a new and unified framework for running several event-based localization methodologies across multiple datasets.|\n",
    "2509.14427": "|2025-09-17|Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models|Ilyass Moummad\u7b49|[2509.14427](http://arxiv.org/pdf/2509.14427)|\u65e0|\u25c6 Information retrieval with compact binary embeddings, also referred to as hashing, is crucial for scalable fast search applications, yet state-of-the-art hashing methods require expensive, scenario-specific training.\n\u25c6 In this work, we introduce Hashing-Baseline, a strong training-free hashing method leveraging powerful pretrained encoders that produce rich pretrained embeddings.\n\u25c6 We revisit classical, training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization, to produce a strong baseline for hashing.|\n",
    "2509.15472": "|2025-09-25|Efficient Multimodal Dataset Distillation via Generative Models|Zhenghao Zhao\u7b49|[2509.15472](http://arxiv.org/pdf/2509.15472)|\u65e0|\u25c6 Dataset distillation aims to synthesize a small dataset from a large dataset, enabling the model trained on it to perform well on the original dataset.\n\u25c6 With the blooming of large language models and multimodal large language models, the importance of multimodal datasets, particularly image-text datasets, has grown significantly.\n\u25c6 However, existing multimodal dataset distillation methods are constrained by the Matching Training Trajectories algorithm, which significantly increases the computing resource requirement, and takes days to process the distillation.|\n",
    "2509.15432": "|2025-09-18|SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models|Thong Nguyen\u7b49|[2509.15432](http://arxiv.org/pdf/2509.15432)|\u65e0|\u25c6 Visual Document Retrieval (VDR) typically operates as text-to-image retrieval using specialized bi-encoders trained to directly embed document images.\n\u25c6 We revisit a zero-shot generate-and-encode pipeline: a vision-language model first produces a detailed textual description of each document image, which is then embedded by a standard text encoder.\n\u25c6 On the ViDoRe-v2 benchmark, the method reaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual document encoder.|\n",
    "2509.17049": "|2025-09-21|Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization|Peng Wang\u7b49|[2509.17049](http://arxiv.org/pdf/2509.17049)|\u65e0|\u25c6 Fine-grained hashing has become a powerful solution for rapid and efficient image retrieval, particularly in scenarios requiring high discrimination between visually similar categories.\n\u25c6 To enable each hash bit to correspond to specific visual attributes, we propoe a novel method that harnesses learnable queries for attribute-aware hash codes learning.\n\u25c6 This method deploys a tailored set of queries to capture and represent nuanced attribute-level information within the hashing process, thereby enhancing both the interpretability and relevance of each hash bit.|\n",
    "2509.19203": "|2025-09-23|Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions|Ioanna Ntinou\u7b49|[2509.19203](http://arxiv.org/pdf/2509.19203)|\u65e0|\u25c6 Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have become the standard approach for learning discriminative vision-language representations.\n\u25c6 However, these models often exhibit shallow language understanding, manifesting bag-of-words behaviour.\n\u25c6 These limitations are reinforced by their dual-encoder design, which induces a modality gap.|\n",
    "2509.18350": "|2025-09-22|OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata|Oussema Dhaouadi\u7b49|[2509.18350](http://arxiv.org/pdf/2509.18350)|\u65e0|\u25c6 Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations.\n\u25c6 In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical.\n\u25c6 Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union).|\n",
    "2509.20271": "|2025-09-24|A Versatile Foundation Model for AI-enabled Mammogram Interpretation|Fuxiang Huang\u7b49|[2509.20271](http://arxiv.org/pdf/2509.20271)|\u65e0|\u25c6 Breast cancer is the most commonly diagnosed cancer and the leading cause of cancer-related mortality in women globally.\n\u25c6 Mammography is essential for the early detection and diagnosis of breast lesions.\n\u25c6 Despite recent progress in foundation models (FMs) for mammogram analysis, their clinical translation remains constrained by several fundamental limitations, including insufficient diversity in training data, limited model generalizability, and a lack of comprehensive evaluation across clinically relevant tasks.|\n",
    "2509.20401": "|2025-09-23|SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment|Binod Singh\u7b49|[2509.20401](http://arxiv.org/pdf/2509.20401)|\u65e0|\u25c6 Aligning 3D scene graphs is a crucial initial step for several applications in robot navigation and embodied perception.\n\u25c6 Current methods in 3D scene graph alignment often rely on single-modality point cloud data and struggle with incomplete or noisy input.\n\u25c6 We introduce SGAligner++, a cross-modal, language-aided framework for 3D scene graph alignment.|\n",
    "2509.22307": "|2025-09-26|Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation|Jinpeng Lu\u7b49|[2509.22307](http://arxiv.org/pdf/2509.22307)|\u65e0|\u25c6 Lightweight 3D medical image segmentation remains constrained by a fundamental \"efficiency / robustness conflict\", particularly when processing complex anatomical structures and heterogeneous modalities.\n\u25c6 In this paper, we study how to redesign the framework based on the characteristics of high-dimensional 3D images, and explore data synergy to overcome the fragile representation of lightweight methods.\n\u25c6 Our approach, VeloxSeg, begins with a deployable and extensible dual-stream CNN-Transformer architecture composed of Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC).|\n",
    "2509.24477": "|2025-09-29|Performance-Efficiency Trade-off for Fashion Image Retrieval|Julio Hurtado\u7b49|[2509.24477](http://arxiv.org/pdf/2509.24477)|\u65e0|\u25c6 The fashion industry has been identified as a major contributor to waste and emissions, leading to an increased interest in promoting the second-hand market.\n\u25c6 Machine learning methods play an important role in facilitating the creation and expansion of second-hand marketplaces by enabling the large-scale valuation of used garments.\n\u25c6 We contribute to this line of work by addressing the scalability of second-hand image retrieval from databases.|\n",
    "2509.24094": "|2025-09-28|Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras|Vignesh Ramanathan\u7b49|[2509.24094](http://arxiv.org/pdf/2509.24094)|\u65e0|\u25c6 Visual Place Recognition (VPR) enables systems to identify previously visited locations within a map, a fundamental task for autonomous navigation.\n\u25c6 Prior works have developed VPR solutions using event cameras, which asynchronously measure per-pixel brightness changes with microsecond temporal resolution.\n\u25c6 However, these approaches rely on dense representations of the inherently sparse camera output and require tens to hundreds of milliseconds of event data to predict a place.|\n",
    "2509.26604": "|2025-09-30|Video Object Segmentation-Aware Audio Generation|Ilpo Viertola\u7b49|[2509.26604](http://arxiv.org/pdf/2509.26604)|\u65e0|\u25c6 Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows.\n\u25c6 In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects.\n\u25c6 To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps.|\n",
    "2509.26330": "|2025-09-30|SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval|Ren-Di Wu\u7b49|[2509.26330](http://arxiv.org/pdf/2509.26330)|\u65e0|\u25c6 Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of a reference image while incorporating user-specified textual modifications.\n\u25c6 Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging.\n\u25c6 In this paper, we present SQUARE, a novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR.|\n",
    "2509.26012": "|2025-09-30|SETR: A Two-Stage Semantic-Enhanced Framework for Zero-Shot Composed Image Retrieval|Yuqi Xiao\u7b49|[2509.26012](http://arxiv.org/pdf/2509.26012)|\u65e0|\u25c6 Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve a target image given a reference image and a relative text, without relying on costly triplet annotations.\n\u25c6 Existing CLIP-based methods face two core challenges: (1) union-based feature fusion indiscriminately aggregates all visual cues, carrying over irrelevant background details that dilute the intended modification, and (2) global cosine similarity from CLIP embeddings lacks the ability to resolve fine-grained semantic relations.\n\u25c6 To address these issues, we propose SETR (Semantic-enhanced Two-Stage Retrieval).|\n",
    "2509.25723": "|2025-09-30|SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition|Shunpeng Chen\u7b49|[2509.25723](http://arxiv.org/pdf/2509.25723)|\u65e0|\u25c6 Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation.\n\u25c6 Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training.\n\u25c6 We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining.|\n",
    "2509.25520": "|2025-09-29|Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity|Tu-Hoa Pham\u7b49|[2509.25520](http://arxiv.org/pdf/2509.25520)|\u65e0|\u25c6 We consider the problem of vision-based 6-DoF object pose estimation in the context of the notional Mars Sample Return campaign, in which a robotic arm would need to localize multiple objects of interest for low-clearance pickup and insertion, under severely constrained hardware.\n\u25c6 We propose a novel localization algorithm leveraging a custom renderer together with a new template matching metric tailored to the edge domain to achieve robust pose estimation using only low-fidelity, textureless 3D models as inputs.\n\u25c6 Extensive evaluations on synthetic datasets as well as from physical testbeds on Earth and in situ Mars imagery shows that our method consistently beats the state of the art in compute and memory-constrained localization, both in terms of robustness and accuracy, in turn enabling new possibilities for cheap and reliable localization on general-purpose hardware.|\n",
    "2510.01183": "|2025-10-01|EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory|Jiahao Wang\u7b49|[2510.01183](http://arxiv.org/pdf/2510.01183)|\u65e0|\u25c6 Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced.\n\u25c6 Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration.\n\u25c6 Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory.|\n",
    "2510.00978": "|2025-10-01|A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features|Axel Barroso-Laguna\u7b49|[2510.00978](http://arxiv.org/pdf/2510.00978)|\u65e0|\u25c6 Visually localizing an image, i.e., estimating its camera pose, requires building a scene representation that serves as a visual map.\n\u25c6 The representation we choose has direct consequences towards the practicability of our system.\n\u25c6 Even when starting from mapping images with known camera poses, state-of-the-art approaches still require hours of mapping time in the worst case, and several minutes in the best.|\n",
    "2510.00783": "|2025-10-01|Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions|Thanh Nguyen Canh\u7b49|[2510.00783](http://arxiv.org/pdf/2510.00783)|\u65e0|\u25c6 Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of research within robotics and computer vision, focusing on the simultaneous localization of robotic systems and associating semantic information to construct the most accurate and complete comprehensive model of the surrounding environment.\n\u25c6 Since the first foundational work in Semantic SLAM appeared more than two decades ago, this field has received increasing attention across various scientific communities.\n\u25c6 Despite its significance, the field lacks comprehensive surveys encompassing recent advances and persistent challenges.|\n",
    "2510.02874": "|2025-10-03|Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping|Charith Premachandra\u7b49|[2510.02874](http://arxiv.org/pdf/2510.02874)|\u65e0|\u25c6 Traditional exteroceptive sensors in mobile robots, such as LiDARs and cameras often struggle to perceive the environment in poor visibility conditions.\n\u25c6 Recently, radar technologies, such as ultra-wideband (UWB) have emerged as potential alternatives due to their ability to see through adverse environmental conditions (e.g.\n\u25c6 dust, smoke and rain).|\n",
    "2510.02728": "|2025-10-03|Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4|Lingfeng Zhang\u7b49|[2510.02728](http://arxiv.org/pdf/2510.02728)|\u65e0|\u25c6 Cross-modal drone navigation remains a challenging task in robotics, requiring efficient retrieval of relevant images from large-scale databases based on natural language descriptions.\n\u25c6 The RoboSense 2025 Track 4 challenge addresses this challenge, focusing on robust, natural language-guided cross-view image retrieval across multiple platforms (drones, satellites, and ground cameras).\n\u25c6 Current baseline methods, while effective for initial retrieval, often struggle to achieve fine-grained semantic matching between text queries and visual content, especially in complex aerial scenes.|\n",
    "2510.04282": "|2025-10-05|Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition|Yu Kiu\u7b49|[2510.04282](http://arxiv.org/pdf/2510.04282)|\u65e0|\u25c6 Sequential Visual Place Recognition (Seq-VPR) leverages transformers to capture spatio-temporal features effectively; however, existing approaches prioritize performance at the expense of flexibility and efficiency.\n\u25c6 In practice, a transformer-based Seq-VPR model should be flexible to the number of frames per sequence (seq-length), deliver fast inference, and have low memory usage to meet real-time constraints.\n\u25c6 To our knowledge, no existing transformer-based Seq-VPR method achieves both flexibility and efficiency.|\n",
    "2510.03751": "|2025-10-04|The Overlooked Value of Test-time Reference Sets in Visual Place Recognition|Mubariz Zaffar\u7b49|[2510.03751](http://arxiv.org/pdf/2510.03751)|\u65e0|\u25c6 Given a query image, Visual Place Recognition (VPR) is the task of retrieving an image of the same place from a reference database with robustness to viewpoint and appearance changes.\n\u25c6 Recent works show that some VPR benchmarks are solved by methods using Vision-Foundation-Model backbones and trained on large-scale and diverse VPR-specific datasets.\n\u25c6 Several benchmarks remain challenging, particularly when the test environments differ significantly from the usual VPR training datasets.|\n",
    "2510.05586": "|2025-10-07|CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval|Bin Kang\u7b49|[2510.05586](http://arxiv.org/pdf/2510.05586)|\u65e0|\u25c6 Existing Visual Language Models (VLMs) suffer structural limitations where a few low contribution tokens may excessively capture global semantics, dominating the information aggregation process and suppressing the discriminative features in text-driven image retrieval tasks.\n\u25c6 To address this, we introduce \\textbf{CalibCLIP}, a training-free method designed to calibrate the suppressive effect of dominant tokens.\n\u25c6 Specifically, in the visual space, we propose the Contrastive Visual Enhancer (CVE), which decouples visual features into target and low information regions.|\n",
    "2510.05411": "|2025-10-06|Personalizing Retrieval using Joint Embeddings or \"the Return of Fluffy\"|Bruno Korbar\u7b49|[2510.05411](http://arxiv.org/pdf/2510.05411)|\u65e0|\u25c6 The goal of this paper is to be able to retrieve images using a compound query that combines object instance information from an image, with a natural text description of what that object is doing or where it is.\n\u25c6 For example, to retrieve an image of \"Fluffy the unicorn (specified by an image) on someone's head\".\n\u25c6 To achieve this we design a mapping network that can \"translate\" from a local image embedding (of the object instance) to a text token, such that the combination of the token and a natural language query is suitable for CLIP style text encoding, and image retrieval.|\n",
    "2510.06868": "|2025-10-08|Multi-hop Deep Joint Source-Channel Coding with Deep Hash Distillation for Semantically Aligned Image Retrieval|Didrik Bergstr\u00f6m\u7b49|[2510.06868](http://arxiv.org/pdf/2510.06868)|\u65e0|\u25c6 We consider image transmission via deep joint source-channel coding (DeepJSCC) over multi-hop additive white Gaussian noise (AWGN) channels by training a DeepJSCC encoder-decoder pair with a pre-trained deep hash distillation (DHD) module to semantically cluster images, facilitating security-oriented applications through enhanced semantic consistency and improving the perceptual reconstruction quality.\n\u25c6 We train the DeepJSCC module to both reduce mean square error (MSE) and minimize cosine distance between DHD hashes of source and reconstructed images.\n\u25c6 Significantly improved perceptual quality as a result of semantic alignment is illustrated for different multi-hop settings, for which classical DeepJSCC may suffer from noise accumulation, measured by the learned perceptual image patch similarity (LPIPS) metric.|\n",
    "2510.08094": "|2025-10-09|DarkHash: A Data-Free Backdoor Attack Against Deep Hashing|Ziqi Zhou\u7b49|[2510.08094](http://arxiv.org/pdf/2510.08094)|\u65e0|\u25c6 Benefiting from its superior feature learning capabilities and efficiency, deep hashing has achieved remarkable success in large-scale image retrieval.\n\u25c6 Recent studies have demonstrated the vulnerability of deep hashing models to backdoor attacks.\n\u25c6 Although these studies have shown promising attack results, they rely on access to the training dataset to implant the backdoor.|\n",
    "2510.08003": "|2025-10-09|CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning|Weihuang Lin\u7b49|[2510.08003](http://arxiv.org/pdf/2510.08003)|\u65e0|\u25c6 Composed Image Retrieval (CIR), which aims to find a target image from a reference image and a modification text, presents the core challenge of performing unified reasoning across visual and semantic modalities.\n\u25c6 While current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown progress, they predominantly function as ``black boxes.\" This inherent opacity not only prevents users from understanding the retrieval rationale but also restricts the models' ability to follow complex, fine-grained instructions.\n\u25c6 To overcome these limitations, we introduce CIR-CoT, the first end-to-end retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT) reasoning.|\n",
    "2510.07703": "|2025-10-09|Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision|Xiaoxu Ma\u7b49|[2510.07703](http://arxiv.org/pdf/2510.07703)|\u65e0|\u25c6 Deep hashing has been widely adopted for large-scale image retrieval, with numerous strategies proposed to optimize hash function learning.\n\u25c6 Pairwise-based methods are effective in learning hash functions that preserve local similarity relationships, whereas center-based methods typically achieve superior performance by more effectively capturing global data distributions.\n\u25c6 However, the strength of center-based methods in modeling global structures often comes at the expense of underutilizing important local similarity information.|\n",
    "2510.08976": "|2025-10-10|Hierarchical Scheduling for Multi-Vector Image Retrieval|Maoliang Li\u7b49|[2510.08976](http://arxiv.org/pdf/2510.08976)|\u65e0|\u25c6 To effectively leverage user-specific data, retrieval augmented generation (RAG) is employed in multimodal large language model (MLLM) applications.\n\u25c6 However, conventional retrieval approaches often suffer from limited retrieval accuracy.\n\u25c6 Recent advances in multi-vector retrieval (MVR) improve accuracy by decomposing queries and matching against segmented images.|\n",
    "2510.12014": "|2025-10-13|Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval|Eric He\u7b49|[2510.12014](http://arxiv.org/pdf/2510.12014)|\u65e0|\u25c6 Text--image retrieval is necessary for applications such as product recommendation.\n\u25c6 Embedding-based approaches like CLIP enable efficient large-scale retrieval via vector similarity search, but they are primarily trained on literal caption-like text--image pairs and often fail to capture abstract or persona-driven attributes common in product recommendation applications (e.g., ``a gift for a mother who loves gardening'').\n\u25c6 In contrast, state-of-the-art vision--language models (vLLMs) can align text with images in a flexible manner, but their limited context window prevents them from directly handling retrieval over large catalogs.|\n",
    "2510.13464": "|2025-10-15|Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition|Emily Miller\u7b49|[2510.13464](http://arxiv.org/pdf/2510.13464)|\u65e0|\u25c6 Visual Place Recognition (VPR) enables robots and autonomous vehicles to identify previously visited locations by matching current observations against a database of known places.\n\u25c6 However, VPR systems face significant challenges when deployed across varying visual environments, lighting conditions, seasonal changes, and viewpoints changes.\n\u25c6 Failure-critical VPR applications, such as loop closure detection in simultaneous localization and mapping (SLAM) pipelines, require robust estimation of place matching uncertainty.|\n",
    "2510.14535": "|2025-10-16|Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval|Keima Abe\u7b49|[2510.14535](http://arxiv.org/pdf/2510.14535)|\u65e0|\u25c6 Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification.\n\u25c6 Domain harmonization is thus a critical research focus.\n\u25c6 Recent approaches encode brain images $\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then disentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and $\\boldsymbol{z_d}$ (domain-specific), achieving strong results.|\n",
    "2510.17739": "|2025-10-20|Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition|Timur Ismagilov\u7b49|[2510.17739](http://arxiv.org/pdf/2510.17739)|\u65e0|\u25c6 We address multi-reference visual place recognition (VPR), where reference sets captured under varying conditions are used to improve localisation performance.\n\u25c6 While deep learning with large-scale training improves robustness, increasing data diversity and model complexity incur extensive computational cost during training and deployment.\n\u25c6 Descriptor-level fusion via voting or aggregation avoids training, but often targets multi-sensor setups or relies on heuristics with limited gains under appearance and viewpoint change.|\n",
    "2510.18437": "|2025-10-21|Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection|Ji Du\u7b49|[2510.18437](http://arxiv.org/pdf/2510.18437)|\u65e0|\u25c6 At the core of Camouflaged Object Detection (COD) lies segmenting objects from their highly similar surroundings.\n\u25c6 Previous efforts navigate this challenge primarily through image-level modeling or annotation-based optimization.\n\u25c6 Despite advancing considerably, this commonplace practice hardly taps valuable dataset-level contextual information or relies on laborious annotations.|\n",
    "2510.18433": "|2025-10-21|ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization|Yuanhe Guo\u7b49|[2510.18433](http://arxiv.org/pdf/2510.18433)|\u65e0|\u25c6 We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences.\n\u25c6 We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations.\n\u25c6 Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images.|\n",
    "2510.18218": "|2025-10-21|DualHash: A Stochastic Primal-Dual Algorithm with Theoretical Guarantee for Deep Hashing|Luxuan Li\u7b49|[2510.18218](http://arxiv.org/pdf/2510.18218)|\u65e0|\u25c6 Deep hashing converts high-dimensional feature vectors into compact binary codes, enabling efficient large-scale retrieval.\n\u25c6 A fundamental challenge in deep hashing stems from the discrete nature of quantization in generating the codes.\n\u25c6 W-type regularizations, such as $|\n",
    "2510.18890": "|2025-10-18|Small Language Models Offer Significant Potential for Science Community|Jian Zhang|[2510.18890](http://arxiv.org/pdf/2510.18890)|\u65e0|\u25c6 Recent advancements in natural language processing, particularly with large language models (LLMs), are transforming how scientists engage with the literature.\n\u25c6 While the adoption of LLMs is increasing, concerns remain regarding potential information biases and computational costs.\n\u25c6 Rather than LLMs, I developed a framework to evaluate the feasibility of precise, rapid, and cost-effective information retrieval from extensive geoscience literature using freely available small language models (MiniLMs).|\n",
    "2510.20095": "|2025-10-24|BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models|Ziheng Zhang\u7b49|[2510.20095](http://arxiv.org/pdf/2510.20095)|\u65e0|\u25c6 This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models.\n\u25c6 Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits.\n\u25c6 Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations.|\n",
    "2510.23224": "|2025-10-27|Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment|Hongyi Wang\u7b49|[2510.23224](http://arxiv.org/pdf/2510.23224)|\u65e0|\u25c6 The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows.\n\u25c6 Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education.\n\u25c6 However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content.|\n",
    "2510.22868": "|2025-10-26|Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models|Yang Zhang\u7b49|[2510.22868](http://arxiv.org/pdf/2510.22868)|\u65e0|\u25c6 Wind turbine blades operate in harsh environments, making timely damage detection essential for preventing failures and optimizing maintenance.\n\u25c6 Drone-based inspection and deep learning are promising, but typically depend on large, labeled datasets, which limit their ability to detect rare or evolving damage types.\n\u25c6 To address this, we propose a zero-shot-oriented inspection framework that integrates Retrieval-Augmented Generation (RAG) with Vision-Language Models (VLM).|\n",
    "2510.22754": "|2025-10-26|TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments|Chunyu Li\u7b49|[2510.22754](http://arxiv.org/pdf/2510.22754)|\u65e0|\u25c6 Multi-agent cooperative SLAM often encounters challenges in similar indoor environments characterized by repetitive structures, such as corridors and rooms.\n\u25c6 These challenges can lead to significant inaccuracies in shared location identification when employing point cloud-based techniques.\n\u25c6 To mitigate these issues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that integrates text semantics and WiFi signal features to enhance location identification and loop closure detection.|\n",
    "2510.22736": "|2025-10-30|Cross-view Localization and Synthesis -- Datasets, Challenges and Opportunities|Ningli Xu\u7b49|[2510.22736](http://arxiv.org/pdf/2510.22736)|\u65e0|\u25c6 Cross-view localization and synthesis are two fundamental tasks in cross-view visual understanding, which deals with cross-view datasets: overhead (satellite or aerial) and ground-level imagery.\n\u25c6 These tasks have gained increasing attention due to their broad applications in autonomous navigation, urban planning, and augmented reality.\n\u25c6 Cross-view localization aims to estimate the geographic position of ground-level images based on information provided by overhead imagery while cross-view synthesis seeks to generate ground-level images based on information from the overhead imagery.|\n",
    "2510.22571": "|2025-10-26|STATUS Bench: A Rigorous Benchmark for Evaluating Object State Understanding in Vision-Language Models|Mahiro Ukai\u7b49|[2510.22571](http://arxiv.org/pdf/2510.22571)|\u65e0|\u25c6 Object state recognition aims to identify the specific condition of objects, such as their positional states (e.g., open or closed) and functional states (e.g., on or off).\n\u25c6 While recent Vision-Language Models (VLMs) are capable of performing a variety of multimodal tasks, it remains unclear how precisely they can identify object states.\n\u25c6 To alleviate this issue, we introduce the STAte and Transition UnderStanding Benchmark (STATUS Bench), the first benchmark for rigorously evaluating the ability of VLMs to understand subtle variations in object states in diverse situations.|\n",
    "2510.22529": "|2025-10-26|Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing|Xiang Fei\u7b49|[2510.22529](http://arxiv.org/pdf/2510.22529)|\u65e0|\u25c6 Loop closure is critical in Simultaneous Localization and Mapping (SLAM) systems to reduce accumulative drift and ensure global mapping consistency.\n\u25c6 However, conventional methods struggle in perceptually aliased environments, such as narrow pipes, due to vector quantization, feature sparsity, and repetitive textures, while existing solutions often incur high computational costs.\n\u25c6 This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure detection method that achieves superior precision-recall, robustness, and computational efficiency.|\n",
    "2510.25387": "|2025-10-29|Instance-Level Composed Image Retrieval|Bill Psomas\u7b49|[2510.25387](http://arxiv.org/pdf/2510.25387)|\u65e0|\u25c6 The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data.\n\u25c6 We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition.\n\u25c6 The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries.|\n",
    "2510.24813": "|2025-10-28|DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts|Binbin Li\u7b49|[2510.24813](http://arxiv.org/pdf/2510.24813)|\u65e0|\u25c6 Recent lightweight retrieval-augmented image caption models often utilize retrieved data solely as text prompts, thereby creating a semantic gap by leaving the original visual features unenhanced, particularly for object details or complex scenes.\n\u25c6 To address this limitation, we propose $DualCap$, a novel approach that enriches the visual representation by generating a visual prompt from retrieved similar images.\n\u25c6 Our model employs a dual retrieval mechanism, using standard image-to-text retrieval for text prompts and a novel image-to-image retrieval to source visually analogous scenes.|\n",
    "2510.26795": "|2025-10-30|Scaling Image Geo-Localization to Continent Level|Philipp Lindenberger\u7b49|[2510.26795](http://arxiv.org/pdf/2510.26795)|\u65e0|\u25c6 Determining the precise geographic location of an image at a global scale remains an unsolved challenge.\n\u25c6 Standard image retrieval techniques are inefficient due to the sheer volume of images (>100M) and fail when coverage is insufficient.\n\u25c6 Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions.|\n",
    "2510.27243": "|2025-10-31|Approximate Diverse $k$-nearest Neighbor Search in Vector Database|Jiachen Zhao\u7b49|[2510.27243](http://arxiv.org/pdf/2510.27243)|\u65e0|\u25c6 Approximate $k$-nearest neighbor search (A$k$-NNS) is a core operation in vector databases, underpinning applications such as retrieval-augmented generation (RAG) and image retrieval.\n\u25c6 In these scenarios, users often prefer diverse result sets to minimize redundancy and enhance information value.\n\u25c6 However, existing greedy-based diverse methods frequently yield sub-optimal results, failing to adequately approximate the optimal similarity score under certain diversification level.|\n",
    "2510.26861": "|2025-11-03|Evaluating Perspectival Biases in Cross-Modal Retrieval|Teerapol Saengsukhiran\u7b49|[2510.26861](http://arxiv.org/pdf/2510.26861)|\u65e0|\u25c6 Multimodal retrieval systems are expected to operate in a semantic space, agnostic to the language or cultural origin of the query.\n\u25c6 In practice, however, retrieval outcomes systematically reflect perspectival biases: deviations shaped by linguistic prevalence and cultural associations.\n\u25c6 We study two such biases.|\n",
    "2511.02489": "|2025-11-04|Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization|Tao Liu\u7b49|[2511.02489](http://arxiv.org/pdf/2511.02489)|\u65e0|\u25c6 With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems.\n\u25c6 However, in GNSS-denied areas, satellite-based localization methods are prone to failure.\n\u25c6 This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching.|\n",
    "2511.02371": "|2025-11-04|LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment|Rohan Wandre\u7b49|[2511.02371](http://arxiv.org/pdf/2511.02371)|\u65e0|\u25c6 Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for grounding large language model outputs in verifiable evidence.\n\u25c6 However, as modern AI agents transition from static knowledge bases to continuous multimodal streams encompassing text, images, video, and audio, two critical challenges arise: maintaining index freshness without prohibitive re-indexing costs, and preserving cross-modal semantic consistency across heterogeneous embedding spaces.\n\u25c6 We present LUMA-RAG, a lifelong multimodal agent architecture featuring three key innovations: (i) a streaming, multi-tier memory system that dynamically spills embeddings from a hot HNSW tier to a compressed IVFPQ tier under strict memory budgets; (ii) a streaming CLAP->CLIP alignment bridge that maintains cross-modal consistency through incremental orthogonal Procrustes updates; and (iii) stability-aware retrieval telemetry providing Safe@k guarantees by jointly bounding alignment drift and quantization error.|\n",
    "2511.01390": "|2025-11-03|SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment|Xinyu Mao\u7b49|[2511.01390](http://arxiv.org/pdf/2511.01390)|\u65e0|\u25c6 Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications.\n\u25c6 Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities.\n\u25c6 Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities.|\n",
    "2511.00925": "|2025-11-02|Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval|Hanwen Su\u7b49|[2511.00925](http://arxiv.org/pdf/2511.00925)|\u65e0|\u25c6 The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved increasing attention due to its wide applications, e.g.\n\u25c6 e-commerce.\n\u25c6 Despite progress made in this field, previous works suffer from using imbalanced samples of modalities and inconsistent low-quality information during training, resulting in sub-optimal performance.|\n",
    "2511.00635": "|2025-11-01|Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles|Hyungtae Lim\u7b49|[2511.00635](http://arxiv.org/pdf/2511.00635)|\u65e0|\u25c6 As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted.\n\u25c6 Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions.\n\u25c6 In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration.|\n",
    "2511.04384": "|2025-11-06|Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA|Itbaan Safwan\u7b49|[2511.04384](http://arxiv.org/pdf/2511.04384)|\u65e0|\u25c6 We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding.\n\u25c6 The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks.\n\u25c6 This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable.|\n",
    "2511.04232": "|2025-11-06|An Efficient Algorithm for Learning-Based Visual Localization|Jindi Zhong\u7b49|[2511.04232](http://arxiv.org/pdf/2511.04232)|\u65e0|\u25c6 This paper addresses the visual localization problem in Global Positioning System (GPS)-denied environments, where computational resources are often limited.\n\u25c6 To achieve efficient and robust performance under these constraints, we propose a novel algorithm.\n\u25c6 The algorithm stems from the optimal control principle (OCP).|\n",
    "2511.05404": "|2025-11-07|Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments|Laura Alejandra Encinar Gonzalez\u7b49|[2511.05404](http://arxiv.org/pdf/2511.05404)|\u65e0|\u25c6 Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration.\n\u25c6 In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity.\n\u25c6 This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments.|\n",
    "2511.05020": "|2025-11-07|DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval|Yawei Cai\u7b49|[2511.05020](http://arxiv.org/pdf/2511.05020)|\u65e0|\u25c6 Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text.\n\u25c6 Most existing methods rely on a single model to perform feature fusion and similarity matching.\n\u25c6 However, this paradigm faces two major challenges.|\n",
    "2511.07412": "|2025-11-10|TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research|Han Zhang\u7b49|[2511.07412](http://arxiv.org/pdf/2511.07412)|\u65e0|\u25c6 Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation.\n\u25c6 However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings.\n\u25c6 Digital twins provide high-fidelity, risk-free environments for exploration and training.|\n",
    "2511.07078": "|2025-11-10|LeCoT: revisiting network architecture for two-view correspondence pruning|Luanyuan Dai\u7b49|[2511.07078](http://arxiv.org/pdf/2511.07078)|\u65e0|\u25c6 Two-view correspondence pruning aims to accurately remove incorrect correspondences (outliers) from initial ones and is widely applied to various computer vision tasks.\n\u25c6 Current popular strategies adopt multilayer perceptron (MLP) as the backbone, supplemented by additional modules to enhance the network ability to handle context information, which is a known limitation of MLPs.\n\u25c6 In contrast, we introduce a novel perspective for capturing correspondence context information without extra design modules.|\n",
    "2511.06749": "|2025-11-10|Semi-distributed Cross-modal Air-Ground Relative Localization|Weining Lu\u7b49|[2511.06749](http://arxiv.org/pdf/2511.06749)|\u65e0|\u25c6 Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks.\n\u25c6 However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy.\n\u25c6 To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework.|\n",
    "2511.06422": "|2025-11-09|DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization|Tao Liu\u7b49|[2511.06422](http://arxiv.org/pdf/2511.06422)|\u65e0|\u25c6 With the rapid growth of the low-altitude economy, unmanned aerial vehicles (UAVs) have become key platforms for measurement and tracking in intelligent patrol systems.\n\u25c6 However, in GNSS-denied environments, localization schemes that rely solely on satellite signals are prone to failure.\n\u25c6 Cross-view image retrieval-based localization is a promising alternative, yet substantial geometric and appearance domain gaps exist between oblique UAV views and nadir satellite orthophotos.|\n",
    "2511.06024": "|2025-11-08|Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era|Feng Lu\u7b49|[2511.06024](http://arxiv.org/pdf/2511.06024)|\u65e0|\u25c6 Visual place recognition (VPR) is typically regarded as a specific image retrieval task, whose core lies in representing images as global descriptors.\n\u25c6 Over the past decade, dominant VPR methods (e.g., NetVLAD) have followed a paradigm that first extracts the patch features/tokens of the input image using a backbone, and then aggregates these patch features into a global descriptor via an aggregator.\n\u25c6 This backbone-plus-aggregator paradigm has achieved overwhelming dominance in the CNN era and remains widely used in transformer-based models.|\n",
    "2511.10591": "|2025-11-13|Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering|Bavana Durgapraveen\u7b49|[2511.10591](http://arxiv.org/pdf/2511.10591)|\u65e0|\u25c6 The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently.\n\u25c6 The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images.\n\u25c6 In this work, we present two complementary approaches developed for the English track.|\n",
    "2511.10518": "|2025-11-13|SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation|Wei Li\u7b49|[2511.10518](http://arxiv.org/pdf/2511.10518)|\u65e0|\u25c6 Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions.\n\u25c6 In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation.\n\u25c6 Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2.|\n",
    "2511.10424": "|2025-11-13|Domain Adaptation for Camera-Specific Image Characteristics using Shallow Discriminators|Maximiliane Gruber\u7b49|[2511.10424](http://arxiv.org/pdf/2511.10424)|\u65e0|\u25c6 Each image acquisition setup leads to its own camera-specific image characteristics degrading the image quality.\n\u25c6 In learning-based perception algorithms, characteristics occurring during the application phase, but absent in the training data, lead to a domain gap impeding the performance.\n\u25c6 Previously, pixel-level domain adaptation through unpaired learning of the pristine-to-distorted mapping function has been proposed.|\n",
    "2511.10390": "|2025-11-13|MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns|Jiarui Zhang\u7b49|[2511.10390](http://arxiv.org/pdf/2511.10390)|\u65e0|\u25c6 Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis.\n\u25c6 However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems.\n\u25c6 We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline.|\n",
    "2511.10387": "|2025-11-13|Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery|Prince Mensah\u7b49|[2511.10387](http://arxiv.org/pdf/2511.10387)|\u65e0|\u25c6 Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management.\n\u25c6 In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data.\n\u25c6 Unlike previous hybrid approaches that require real satellite images for self-supevised training.|\n",
    "2511.10301": "|2025-11-13|Rethinking Visual Information Processing in Multimodal LLMs|Dongwan Kim\u7b49|[2511.10301](http://arxiv.org/pdf/2511.10301)|\u65e0|\u25c6 Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities.\n\u25c6 We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder.\n\u25c6 To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations.|\n",
    "2511.10260": "|2025-11-13|H3Former: Hypergraph-based Semantic-Aware Aggregation via Hyperbolic Hierarchical Contrastive Loss for Fine-Grained Visual Classification|Yongji Zhang\u7b49|[2511.10260](http://arxiv.org/pdf/2511.10260)|\u65e0|\u25c6 Fine-Grained Visual Classification (FGVC) remains a challenging task due to subtle inter-class differences and large intra-class variations.\n\u25c6 Existing approaches typically rely on feature-selection mechanisms or region-proposal strategies to localize discriminative regions for semantic analysis.\n\u25c6 However, these methods often fail to capture discriminative cues comprehensively while introducing substantial category-agnostic redundancy.|\n",
    "2511.10241": "|2025-11-13|TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding|Jinxuan Li\u7b49|[2511.10241](http://arxiv.org/pdf/2511.10241)|\u65e0|\u25c6 Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video.\n\u25c6 This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning.\n\u25c6 Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps.|\n",
    "2511.10212": "|2025-11-13|Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization|Ashutosh Anshul\u7b49|[2511.10212](http://arxiv.org/pdf/2511.10212)|\u65e0|\u25c6 Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets.\n\u25c6 However, such approaches that target generalization require pretraining over real samples.\n\u25c6 Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment.|\n",
    "2511.10182": "|2025-11-13|Beyond the Black Box: Demystifying Multi-Turn LLM Reasoning with VISTA|Yiran Zhang\u7b49|[2511.10182](http://arxiv.org/pdf/2511.10182)|\u65e0|\u25c6 Recent research has increasingly focused on the reasoning capabilities of Large Language Models (LLMs) in multi-turn interactions, as these scenarios more closely mirror real-world problem-solving.\n\u25c6 However, analyzing the intricate reasoning processes within these interactions presents a significant challenge due to complex contextual dependencies and a lack of specialized visualization tools, leading to a high cognitive load for researchers.\n\u25c6 To address this gap, we present VISTA, an web-based Visual Interactive System for Textual Analytics in multi-turn reasoning tasks.|\n",
    "2511.11552": "|2025-11-14|DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding|Dawei Zhu\u7b49|[2511.11552](http://arxiv.org/pdf/2511.11552)|\u65e0|\u25c6 Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs).\n\u25c6 Existing approaches falter on a fundamental challenge: evidence localization.\n\u25c6 They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination.|\n",
    "2511.11528": "|2025-11-14|STEM EBIC as a Quantitative Probe of Semiconductor Devices|Sebastian Schneider\u7b49|[2511.11528](http://arxiv.org/pdf/2511.11528)|\u65e0|\u25c6 Electron beam-induced current (EBIC) imaging in the scanning transmission electron microscope (STEM), STEM-EBIC, provides direct access to carrier transport at the nanoscale.\n\u25c6 While well established in bulk SEM geometries, its application to thin TEM lamellae remains largely unexplored.\n\u25c6 Here, we present a systematic STEM-EBIC study of silicon photodiode lamellae prepared by gallium and xenon focused ion beam (FIB) milling.|\n",
    "2511.11526": "|2025-11-14|Bridging Hidden States in Vision-Language Models|Benjamin Fein-Ashley\u7b49|[2511.11526](http://arxiv.org/pdf/2511.11526)|\u65e0|\u25c6 Vision-Language Models (VLMs) are a new family of models that align image content with natural language.\n\u25c6 Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings.\n\u25c6 Many methods also tie fusion to an autoregressive decoder.|\n",
    "2511.11427": "|2025-11-14|Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs|Francisco Nogueira\u7b49|[2511.11427](http://arxiv.org/pdf/2511.11427)|\u65e0|\u25c6 Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions.\n\u25c6 Research on the area remains predominantly English-centric, despite increasing global deployment demands.\n\u25c6 This work addresses multilingual REC through two main contributions.|\n",
    "2511.11422": "|2025-11-14|Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment|Lukun Wu\u7b49|[2511.11422](http://arxiv.org/pdf/2511.11422)|\u65e0|\u25c6 Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach.\n\u25c6 We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG's inherent noise and signal degradation, vs.\n\u25c6 vision's high-fidelity features) and a Semantic Gap (arising from EEG's shallow conceptual representation, vs.|\n",
    "2511.11403": "|2025-11-14|Bidimensional measurements of photon statistics within a multimodal temporal framework|C. Hainaut\u7b49|[2511.11403](http://arxiv.org/pdf/2511.11403)|\u65e0|\u25c6 Ultrafast imaging of photon statistics in two dimensions is a powerful tool for probing non-equilibrium and transient optical phenomena, yet it remains experimentally challenging due to the simultaneous need for high temporal resolution and statistical fidelity.\n\u25c6 In this work, we demonstrate spatially resolved single-shot measurements of photon number distributions using difference-frequency generation (DFG) in a nonlinear BBO crystal.\n\u25c6 We show that our platform can discriminate between coherent and thermal photon statistics across two spatial dimensions with picosecond resolution.|\n",
    "2511.11401": "|2025-11-14|GRANITE: High-Resolution Imaging and Electrical Qualification of Large-Area TPC Electrodes|Shumit A. Mitra\u7b49|[2511.11401](http://arxiv.org/pdf/2511.11401)|\u65e0|\u25c6 Next-generation dual-phase time projection chambers (TPCs) for rare event searches will require large-scale, high-precision electrodes.\n\u25c6 To meet the stringent requirements for high-voltage performance of such an experiment, we have developed a scanning setup for comprehensive electrode quality assurance.\n\u25c6 The system is built around the GRANITE (Granular Robotic Assay for Novel Integrated TPC Electrodes) facility: a gantry robot on top of a $2.5\\,\\text{m}\\times1.8\\,\\text{m}$ granite table, equipped with a suite of non-contact metrology devices.|\n",
    "2511.11320": "|2025-11-14|StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks|Jiaqi Lin\u7b49|[2511.11320](http://arxiv.org/pdf/2511.11320)|\u65e0|\u25c6 Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation.\n\u25c6 Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible.\n\u25c6 Equilibrium Propagation (EP) provides a more local and biologically grounded alternative.|\n",
    "2511.11313": "|2025-11-14|DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding|Tanveer Hannan\u7b49|[2511.11313](http://arxiv.org/pdf/2511.11313)|\u65e0|\u25c6 Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents.\n\u25c6 However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices.\n\u25c6 We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources.|\n",
    "2511.11305": "|2025-11-14|MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising|Chenghan Fu\u7b49|[2511.11305](http://arxiv.org/pdf/2511.11305)|\u65e0|\u25c6 We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications.\n\u25c6 MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on.\n\u25c6 The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement.|\n",
    "2511.13586": "|2025-11-17|Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images|Yinuo Xu\u7b49|[2511.13586](http://arxiv.org/pdf/2511.13586)|\u65e0|\u25c6 Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease.\n\u25c6 Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity.\n\u25c6 In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.|\n",
    "2511.13575": "|2025-11-17|Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification|Linhan Zhou\u7b49|[2511.13575](http://arxiv.org/pdf/2511.13575)|\u65e0|\u25c6 Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I).\n\u25c6 Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment.\n\u25c6 Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance.|\n",
    "2511.13494": "|2025-11-17|Language-Guided Invariance Probing of Vision-Language Models|Jae Joong Lee|[2511.13494](http://arxiv.org/pdf/2511.13494)|\u65e0|\u25c6 Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations.\n\u25c6 We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching.\n\u25c6 Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.|\n",
    "2511.13415": "|2025-11-17|Attention Grounded Enhancement for Visual Document Retrieval|Wanqing Cui\u7b49|[2511.13415](http://arxiv.org/pdf/2511.13415)|\u65e0|\u25c6 Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs.\n\u25c6 Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance.\n\u25c6 However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match.|\n",
    "2511.13348": "|2025-11-17|Stray Light Correction for the Helioseismic and Magnetic Imager|A. A. Norton\u7b49|[2511.13348](http://arxiv.org/pdf/2511.13348)|\u65e0|\u25c6 We report a point spread function (PSF) and deconvolution procedure to remove stray light from the Helioseismic and Magnetic Imager (HMI) data.\n\u25c6 Pre-launch calibration observations, post-launch Venus transit and lunar transit data were used to develop the PSF and evaluate how well it reproduced the observed scattering.\n\u25c6 The PSF reported differs from previous stray light removal efforts since we do not use Gaussians as the central mathematical component.|\n",
    "2511.13243": "|2025-11-17|Uncovering and Mitigating Transient Blindness in Multimodal Model Editing|Xiaoqi Han\u7b49|[2511.13243](http://arxiv.org/pdf/2511.13243)|\u65e0|\u25c6 Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models.\n\u25c6 Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting.\n\u25c6 We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits.|\n",
    "2511.13216": "|2025-11-17|GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry|Chiyun Noh\u7b49|[2511.13216](http://arxiv.org/pdf/2511.13216)|\u65e0|\u25c6 Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms.\n\u25c6 In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping.\n\u25c6 Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation.|\n",
    "2511.13189": "|2025-11-17|Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework|Diego Ortego\u7b49|[2511.13189](http://arxiv.org/pdf/2511.13189)|\u65e0|\u25c6 Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC).\n\u25c6 Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance.\n\u25c6 Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures.|\n",
    "2511.13170": "|2025-11-17|THIR: Topological Histopathological Image Retrieval|Zahra Tabatabaei\u7b49|[2511.13170](http://arxiv.org/pdf/2511.13170)|\u65e0|\u25c6 According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020.\n\u25c6 Early diagnosis and accurate clinical decision making are critical in reducing this global burden.\n\u25c6 In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns.|\n",
    "2511.13168": "|2025-11-17|SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration|Haodong Wang\u7b49|[2511.13168](http://arxiv.org/pdf/2511.13168)|\u65e0|\u25c6 Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics.\n\u25c6 Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory.\n\u25c6 Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences.|\n",
    "2511.14712": "|2025-11-18|FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation|Yunfeng Wu\u7b49|[2511.14712](http://arxiv.org/pdf/2511.14712)|\u65e0|\u25c6 The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive.\n\u25c6 Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation.\n\u25c6 At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail.|\n",
    "2511.14687": "|2025-11-18|Overcoming global sensitivity limitations: using active subspaces to explore discrepancies between global and local parameter sensitivities|Huiyan Zou\u7b49|[2511.14687](http://arxiv.org/pdf/2511.14687)|\u65e0|\u25c6 Global sensitivity metrics are essential tools for assessing parameter importance in complex models, particularly when precise information about parameter values is unavailable.\n\u25c6 In many cases, such metrics are used to provide parameter rankings that allow for necessary dimension reduction in moderate-to-high dimensional systems.\n\u25c6 However, globally-derived sensitivity results may obscure localized variability in parameter sensitivities, resulting in misleading conclusions about parameter importance and ensuing consequences for subsequent tasks such as model calibration and surrogate model construction.|\n",
    "2511.14638": "|2025-11-18|A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases|Tao Yang\u7b49|[2511.14638](http://arxiv.org/pdf/2511.14638)|\u65e0|\u25c6 Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years.\n\u25c6 Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations.\n\u25c6 We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval.|\n",
    "2511.14544": "|2025-11-18|Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction|Jaume Ros\u7b49|[2511.14544](http://arxiv.org/pdf/2511.14544)|\u65e0|\u25c6 Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane.\n\u25c6 However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions.\n\u25c6 Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection.|\n",
    "2511.14518": "|2025-11-18|D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images|Taifour Yousra Nabila\u7b49|[2511.14518](http://arxiv.org/pdf/2511.14518)|\u65e0|\u25c6 Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks.\n\u25c6 However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development.\n\u25c6 While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details.|\n",
    "2511.14504": "|2025-11-18|Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations|Jan Quenzel\u7b49|[2511.14504](http://arxiv.org/pdf/2511.14504)|\u65e0|\u25c6 Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings.\n\u25c6 The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location.\n\u25c6 Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.|\n",
    "2511.14449": "|2025-11-18|DIR-TIR: Dialog-Iterative Refinement for Text-to-Image Retrieval|Zongwei Zhen\u7b49|[2511.14449](http://arxiv.org/pdf/2511.14449)|\u65e0|\u25c6 This paper addresses the task of interactive, conversational text-to-image retrieval.\n\u25c6 Our DIR-TIR framework progressively refines the target image search through two specialized modules: the Dialog Refiner Module and the Image Refiner Module.\n\u25c6 The Dialog Refiner actively queries users to extract essential information and generate increasingly precise descriptions of the target image.|\n",
    "2511.14446": "|2025-11-18|Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding|Hong Gao\u7b49|[2511.14446](http://arxiv.org/pdf/2511.14446)|\u65e0|\u25c6 Video understanding requires not only visual recognition but also complex reasoning.\n\u25c6 While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement.\n\u25c6 While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training.|\n",
    "2511.14386": "|2025-11-18|Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving|Kangqiao Zhao\u7b49|[2511.14386](http://arxiv.org/pdf/2511.14386)|\u65e0|\u25c6 Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception.\n\u25c6 Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored.\n\u25c6 To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving.|\n",
    "2511.14368": "|2025-11-18|O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model|Rishi Gupta\u7b49|[2511.14368](http://arxiv.org/pdf/2511.14368)|\u65e0|\u25c6 While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited.\n\u25c6 Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually.\n\u25c6 We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions.|\n",
    "2511.15705": "|2025-11-19|GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization|Yikun Wang\u7b49|[2511.15705](http://arxiv.org/pdf/2511.15705)|\u65e0|\u25c6 Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models.\n\u25c6 In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning.\n\u25c6 Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models.|\n",
    "2511.15700": "|2025-11-19|First Frame Is the Place to Go for Video Content Customization|Jingxi Chen\u7b49|[2511.15700](http://arxiv.org/pdf/2511.15700)|\u65e0|\u25c6 What role does the first frame play in video generation models?\n\u25c6 Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation.\n\u25c6 In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation.|\n",
    "2511.15633": "|2025-11-19|Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning|Tao Hu\u7b49|[2511.15633](http://arxiv.org/pdf/2511.15633)|\u65e0|\u25c6 Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge.\n\u25c6 Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL.\n\u25c6 However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like \"dog\" subsumes fine-grained categories such as \"Labrador\" and \"Golden Retriever,\" and each category entails its images.|\n",
    "2511.15515": "|2025-11-19|Multi-Text Guided Few-Shot Semantic Segmentation|Qiang Jiao\u7b49|[2511.15515](http://arxiv.org/pdf/2511.15515)|\u65e0|\u25c6 Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class).\n\u25c6 However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories.\n\u25c6 Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality.|\n",
    "2511.15464": "|2025-11-19|SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome|Dabin Jeong\u7b49|[2511.15464](http://arxiv.org/pdf/2511.15464)|\u65e0|\u25c6 Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles.\n\u25c6 However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization.\n\u25c6 To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales.|\n",
    "2511.15435": "|2025-11-19|HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation|Linyin Luo\u7b49|[2511.15435](http://arxiv.org/pdf/2511.15435)|\u65e0|\u25c6 Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues.\n\u25c6 Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents.\n\u25c6 However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components.|\n",
    "2511.15370": "|2025-11-19|The Empowerment of Science of Science by Large Language Models: New Tools and Methods|Guoqiang Liang\u7b49|[2511.15370](http://arxiv.org/pdf/2511.15370)|\u65e0|\u25c6 Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race.\n\u25c6 This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning.\n\u25c6 Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain.|\n",
    "2511.15333": "|2025-11-19|C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models|Nayoung Oh\u7b49|[2511.15333](http://arxiv.org/pdf/2511.15333)|\u65e0|\u25c6 Space grounding refers to localizing a set of spatial references described in natural language instructions.\n\u25c6 Traditional methods often fail to account for complex reasoning -- such as distance, geometry, and inter-object relationships -- while vision-language models (VLMs), despite strong reasoning abilities, struggle to produce a fine-grained region of outputs.\n\u25c6 To overcome these limitations, we propose C2F-Space, a novel coarse-to-fine space-grounding framework that (i) estimates an approximated yet spatially consistent region using a VLM, then (ii) refines the region to align with the local environment through superpixelization.|\n",
    "2511.15201": "|2025-11-19|Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval|Qing Wang\u7b49|[2511.15201](http://arxiv.org/pdf/2511.15201)|\u65e0|\u25c6 This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem.\n\u25c6 As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment.\n\u25c6 Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions.|\n",
    "2511.15118": "|2025-11-19|Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation|Jin Wang\u7b49|[2511.15118](http://arxiv.org/pdf/2511.15118)|\u65e0|\u25c6 Few-shot segmentation has garnered significant attention.\n\u25c6 Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task.\n\u25c6 With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation.|\n",
    "2511.16671": "|2025-11-20|Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation|Ziyu Guo\u7b49|[2511.16671](http://arxiv.org/pdf/2511.16671)|\u65e0|\u25c6 Recent advances in visual generation have increasingly explored the integration of reasoning capabilities.\n\u25c6 They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself.\n\u25c6 In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process.|\n",
    "2511.16654": "|2025-11-20|Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems|Elias Lumer\u7b49|[2511.16654](http://arxiv.org/pdf/2511.16654)|\u65e0|\u25c6 Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents.\n\u25c6 However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering.\n\u25c6 To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space).|\n",
    "2511.16635": "|2025-11-20|SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction|Guolin Huang\u7b49|[2511.16635](http://arxiv.org/pdf/2511.16635)|\u65e0|\u25c6 Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption.\n\u25c6 While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases.\n\u25c6 We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction.|\n",
    "2511.16567": "|2025-11-20|POMA-3D: The Point Map Way to 3D Scene Understanding|Ye Mao\u7b49|[2511.16567](http://arxiv.org/pdf/2511.16567)|\u65e0|\u25c6 In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps.\n\u25c6 Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models.\n\u25c6 To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed.|\n",
    "2511.16566": "|2025-11-20|NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening|Misaal Khan\u7b49|[2511.16566](http://arxiv.org/pdf/2511.16566)|\u65e0|\u25c6 Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention.\n\u25c6 In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance.\n\u25c6 In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings.|\n",
    "2511.16527": "|2025-11-20|Contrastive vision-language learning with paraphrasing and negation|Kwun Ho Ngan\u7b49|[2511.16527](http://arxiv.org/pdf/2511.16527)|\u65e0|\u25c6 Contrastive vision-language models continue to be the dominant approach for image and text retrieval.\n\u25c6 Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space.\n\u25c6 Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning.|\n",
    "2511.16524": "|2025-11-20|BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization|Rahul Kumar\u7b49|[2511.16524](http://arxiv.org/pdf/2511.16524)|\u65e0|\u25c6 Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments.\n\u25c6 In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing.\n\u25c6 The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes.|\n",
    "2511.16521": "|2025-11-20|YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras|Fan Yang\u7b49|[2511.16521](http://arxiv.org/pdf/2511.16521)|\u65e0|\u25c6 Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications.\n\u25c6 However, registering CMCs to the target scene layout presents a challenging task.\n\u25c6 While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists.|\n",
    "2511.16423": "|2025-11-20|TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models|Li Zhang\u7b49|[2511.16423](http://arxiv.org/pdf/2511.16423)|\u65e0|\u25c6 Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning.\n\u25c6 Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks.\n\u25c6 Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive.|\n",
    "2511.16349": "|2025-11-20|CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering|Joni Vanherck\u7b49|[2511.16349](http://arxiv.org/pdf/2511.16349)|\u65e0|\u25c6 Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content.\n\u25c6 Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure.\n\u25c6 This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud.|\n",
    "2511.17442": "|2025-11-21|REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing|Binger Chen\u7b49|[2511.17442](http://arxiv.org/pdf/2511.17442)|\u65e0|\u25c6 Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping.\n\u25c6 These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data.\n\u25c6 They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering.|\n",
    "2511.17384": "|2025-11-21|IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation|Yifan Li\u7b49|[2511.17384](http://arxiv.org/pdf/2511.17384)|\u65e0|\u25c6 While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning.\n\u25c6 Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity.\n\u25c6 To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning.|\n",
    "2511.17364": "|2025-11-21|SVRecon: Sparse Voxel Rasterization for Surface Reconstruction|Seunghun Oh\u7b49|[2511.17364](http://arxiv.org/pdf/2511.17364)|\u65e0|\u25c6 We extend the recently proposed sparse voxel rasterization paradigm to the task of high-fidelity surface reconstruction by integrating Signed Distance Function (SDF), named SVRecon.\n\u25c6 Unlike 3D Gaussians, sparse voxels are spatially disentangled from their neighbors and have sharp boundaries, which makes them prone to local minima during optimization.\n\u25c6 Although SDF values provide a naturally smooth and continuous geometric field, preserving this smoothness across independently parameterized sparse voxels is nontrivial.|\n",
    "2511.17322": "|2025-11-21|NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior|Dongbo Shi\u7b49|[2511.17322](http://arxiv.org/pdf/2511.17322)|\u65e0|\u25c6 In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.\n\u25c6 Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios.\n\u25c6 To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation.|\n",
    "2511.17300": "|2025-11-21|MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning|Wenrui Zhang\u7b49|[2511.17300](http://arxiv.org/pdf/2511.17300)|\u65e0|\u25c6 Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations.\n\u25c6 This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains.\n\u25c6 However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements.|\n",
    "2511.17282": "|2025-11-21|Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation|Chuancheng Shi\u7b49|[2511.17282](http://arxiv.org/pdf/2511.17282)|\u65e0|\u25c6 Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized.\n\u25c6 Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency.\n\u25c6 We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts.|\n",
    "2511.17255": "|2025-11-21|A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback|Bulat Khaertdinov\u7b49|[2511.17255](http://arxiv.org/pdf/2511.17255)|\u65e0|\u25c6 Large vision-language models (VLMs) enable intuitive visual search using natural language queries.\n\u25c6 However, improving their performance often requires fine-tuning and scaling to larger model variants.\n\u25c6 In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback.|\n",
    "2511.17246": "|2025-11-21|Mixed Reality Scenic Live Streaming for Cultural Heritage: Visual Interactions in a Historic Landscape|Zeyu Huang\u7b49|[2511.17246](http://arxiv.org/pdf/2511.17246)|\u65e0|\u25c6 Scenic Live Streams (SLS), capturing real-world scenic sites from fixed cameras without streamers, have gained increasing popularity recently.\n\u25c6 They afford unique real-time lenses into remote sites for viewers' synchronous and collective engagement.\n\u25c6 Foregrounding its lack of dynamism and interactivity, we aim to maximize the potential of SLS by making it interactive.|\n",
    "2511.17207": "|2025-11-21|SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors|Kunyi Li\u7b49|[2511.17207](http://arxiv.org/pdf/2511.17207)|\u65e0|\u25c6 Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis.\n\u25c6 To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework.\n\u25c6 The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications.|\n",
    "2511.17183": "|2025-11-21|Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition|Aditya Mishra\u7b49|[2511.17183](http://arxiv.org/pdf/2511.17183)|\u65e0|\u25c6 Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving.\n\u25c6 Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets.\n\u25c6 Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively.|\n",
    "2511.19400": "|2025-11-24|Wigner and Gabor phase-space analysis of propagators for evolution equations|Elena Cordero\u7b49|[2511.19400](http://arxiv.org/pdf/2511.19400)|\u65e0|\u25c6 We study the Wigner kernel and the Gabor matrix associated with the propagators of a broad class   of linear evolution equations, including the complex heat, wave,   and Hermite equations.\n\u25c6 Within the framework of time-frequency analysis, we derive   explicit expressions for the Wigner kernels of Fourier multipliers and establish quantitative   decay estimates for the corresponding Gabor matrices.\n\u25c6 These results are obtained under symbol   regularity conditions formulated in the Gelfand-Shilov scale and ensure exponential off-diagonal   decay or quasi-diagonality of the matrix representation.|\n",
    "2511.19396": "|2025-11-24|Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments|Jorge Ortigoso-Narro\u7b49|[2511.19396](http://arxiv.org/pdf/2511.19396)|\u65e0|\u25c6 Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics.\n\u25c6 This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments.\n\u25c6 The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects.|\n",
    "2511.19226": "|2025-11-24|In-vivo imaging with a low-cost MRI scanner and cloud data processing in low-resource settings|Teresa Guallart-Naval\u7b49|[2511.19226](http://arxiv.org/pdf/2511.19226)|\u65e0|\u25c6 Purpose: To demonstrate in-vivo imaging with a low-cost, low-field MRI scanner built and operated in Africa, and to show how systematic hardware and software improvements can mitigate the main operational limitations encountered in low-resource environments.\n\u25c6 Methods: A 46 mT Halbach scanner located at the Mbarara University of Science and Technology (Uganda) was upgraded through a complete reorganization of grounding and shielding, installation of new control electronics and open-source user-interface software.\n\u25c6 Noise performance was quantified using a standardized protocol and in-vivo brain images were acquired with three-dimensional RARE sequences.|\n",
    "2511.19200": "|2025-11-24|Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?|Itay Cohen\u7b49|[2511.19200](http://arxiv.org/pdf/2511.19200)|\u65e0|\u25c6 Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception.\n\u25c6 One subtle ability is to judge whether an image looks like a given object without being an instance of that object.\n\u25c6 We study whether vision-language models such as CLIP capture this distinction.|\n",
    "2511.19149": "|2025-11-24|From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation|Moazzam Umer Gondal\u7b49|[2511.19149](http://arxiv.org/pdf/2511.19149)|\u65e0|\u25c6 This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting.\n\u25c6 The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization.\n\u25c6 The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index.|\n",
    "2511.19105": "|2025-11-24|Graph-based 3D Human Pose Estimation using WiFi Signals|Jichao Chen\u7b49|[2511.19105](http://arxiv.org/pdf/2511.19105)|\u65e0|\u25c6 WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods.\n\u25c6 However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints.\n\u25c6 In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE.|\n",
    "2511.19080": "|2025-11-24|Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach|Fan Nie\u7b49|[2511.19080](http://arxiv.org/pdf/2511.19080)|\u65e0|\u25c6 The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes.\n\u25c6 Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection.\n\u25c6 Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection.|\n",
    "2511.19057": "|2025-11-24|LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space|Hai Wu\u7b49|[2511.19057](http://arxiv.org/pdf/2511.19057)|\u65e0|\u25c6 Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding.\n\u25c6 However, datasets tailored for 3D LAA perception remain scarce.\n\u25c6 To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles.|\n",
    "2511.19031": "|2025-11-24|Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors|Haihang Wu\u7b49|[2511.19031](http://arxiv.org/pdf/2511.19031)|\u65e0|\u25c6 Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera.\n\u25c6 While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization.\n\u25c6 To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses.|\n",
    "2511.19021": "|2025-11-24|Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting|Qiyang Yu\u7b49|[2511.19021](http://arxiv.org/pdf/2511.19021)|\u65e0|\u25c6 Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details.\n\u25c6 Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation.\n\u25c6 To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity.|\n",
    "2511.20609": "|2025-11-25|Adaptive Hopfield Network: Rethinking Similarities in Associative Memory|Shurong Wang\u7b49|[2511.20609](http://arxiv.org/pdf/2511.20609)|\u65e0|\u25c6 Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability.\n\u25c6 However, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness.\n\u25c6 We reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process.|\n",
    "2511.20544": "|2025-11-25|New York Smells: A Large Multimodal Dataset for Olfaction|Ege Ozguroglu\u7b49|[2511.20544](http://arxiv.org/pdf/2511.20544)|\u65e0|\u25c6 While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines.\n\u25c6 One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings.\n\u25c6 We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\\times$ more objects than existing olfactory datasets.|\n",
    "2511.20472": "|2025-11-25|Wide Area Surface Dosimetry with Conformal Scintillator Array for External Beam Radiotherapy|Roman Vasyltsiv\u7b49|[2511.20472](http://arxiv.org/pdf/2511.20472)|\u65e0|\u25c6 Background: In vivo dosimetry is essential for treatment verification in modern radiotherapy, but existing techniques are limited by spatiotemporal resolution and performance on non-uniform anatomy.\n\u25c6 Scintillation imaging dosimetry shows potential to address several of these limitations.\n\u25c6 Here, translation to conventional photon external beam radiotherapy was examined using a novel wide-area imaging and sensing technique.|\n",
    "2511.20467": "|2025-11-25|Power-Efficient Autonomous Mobile Robots|Liangkai Liu\u7b49|[2511.20467](http://arxiv.org/pdf/2511.20467)|\u65e0|\u25c6 This paper presents pNav, a novel power-management system that significantly enhances the power/energy-efficiency of Autonomous Mobile Robots (AMRs) by jointly optimizing their physical/mechanical and cyber subsystems.\n\u25c6 By profiling AMRs' power consumption, we identify three challenges in achieving CPS (cyber-physical system) power-efficiency that involve both cyber (C) and physical (P) subsystems: (1) variabilities of system power consumption breakdown, (2) environment-aware navigation locality, and (3) coordination of C and P subsystems.\n\u25c6 pNav takes a multi-faceted approach to achieve power-efficiency of AMRs.|\n",
    "2511.20462": "|2025-11-25|STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow|Jiatao Gu\u7b49|[2511.20462](http://arxiv.org/pdf/2511.20462)|\u65e0|\u25c6 Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation.\n\u25c6 Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models.\n\u25c6 In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation.|\n",
    "2511.20460": "|2025-11-25|Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search|Yunqi Zhou\u7b49|[2511.20460](http://arxiv.org/pdf/2511.20460)|\u65e0|\u25c6 With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread.\n\u25c6 However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details.\n\u25c6 In this context, guiding the model look where it matters before prediction becomes crucial.|\n",
    "2511.20391": "|2025-11-25|Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi|Anton Ivashkevich\u7b49|[2511.20391](http://arxiv.org/pdf/2511.20391)|\u65e0|\u25c6 We describe a prototype of a fully capable Ethereum Proof-of-Work (PoW) blockchain network running on multiple Raspberry Pi (RPi) computers.\n\u25c6 The prototype is easy to set up and is intended to function as a completely standalone system, using a local WiFi router for connectivity.\n\u25c6 It features LCD screens for visualization of the local state of blockchain ledgers on each RPi, making it ideal for educational purposes and to demonstrate fundamental blockchain concepts to a wide audience.|\n",
    "2511.20306": "|2025-11-25|TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection|Han Guo\u7b49|[2511.20306](http://arxiv.org/pdf/2511.20306)|\u65e0|\u25c6 Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images.\n\u25c6 Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions.\n\u25c6 Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies.|\n",
    "2511.20295": "|2025-11-25|Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations|Chao Wang\u7b49|[2511.20295](http://arxiv.org/pdf/2511.20295)|\u65e0|\u25c6 Counterfactual explanations (CFEs) are minimal and semantically meaningful modifications of the input of a model that alter the model predictions.\n\u25c6 They highlight the decisive features the model relies on, providing contrastive interpretations for classifiers.\n\u25c6 State-of-the-art visual counterfactual explanation methods are designed to explain image classifiers.|\n",
    "2511.20280": "|2025-11-25|Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement|Yang Liu\u7b49|[2511.20280](http://arxiv.org/pdf/2511.20280)|\u65e0|\u25c6 Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles.\n\u25c6 To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation.\n\u25c6 Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality.|\n",
    "2511.21663": "|2025-11-26|Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models|Naifu Zhang\u7b49|[2511.21663](http://arxiv.org/pdf/2511.21663)|\u65e0|\u25c6 In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly.\n\u25c6 However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches.\n\u25c6 To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space.|\n",
    "2511.21647": "|2025-11-26|Fast 3D Ultrasound Localization Microscopy via Projection-based Processing Framework|Jingke Zhang\u7b49|[2511.21647](http://arxiv.org/pdf/2511.21647)|\u65e0|\u25c6 Three-dimensional ultrasound localization microscopy (ULM) enables comprehensive visualization of the vasculature, thereby improving diagnostic reliability.\n\u25c6 Nevertheless, its clinical translation remains challenging, as the exponential growth in voxel count for full 3D reconstruction imposes heavy computational demands and extensive post-processing time.\n\u25c6 In this row-column array (RCA)-based 3D in vivo pig kidney ULM study, we reformulate each step of the full 3D ULM pipeline, including beamforming, clutter filtering, motion estimation, microbubble separation and localization into a series of computational-efficient 2D operations, substantially reducing the number of voxels to be processed while maintaining comparable accuracy.|\n",
    "2511.21631": "|2025-11-26|Qwen3-VL Technical Report|Shuai Bai\u7b49|[2511.21631](http://arxiv.org/pdf/2511.21631)|\u65e0|\u25c6 We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks.\n\u25c6 It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video.\n\u25c6 The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs.|\n",
    "2511.21579": "|2025-11-26|Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy|Teng Hu\u7b49|[2511.21579](http://arxiv.org/pdf/2511.21579)|\u65e0|\u25c6 The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment.\n\u25c6 Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization.\n\u25c6 To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization.|\n",
    "2511.21389": "|2025-11-26|FITRep: Attention-Guided Item Representation via MLLMs|Guoxiao Zhang\u7b49|[2511.21389](http://arxiv.org/pdf/2511.21389)|\u65e0|\u25c6 Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text.\n\u25c6 While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs.\n\u25c6 auxiliary elements), leading to local structural collapse problem.|\n",
    "2511.21375": "|2025-11-26|Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning|Xin Gu\u7b49|[2511.21375](http://arxiv.org/pdf/2511.21375)|\u65e0|\u25c6 Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions.\n\u25c6 Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders.\n\u25c6 To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications.|\n",
    "2511.21317": "|2025-11-26|HTTM: Head-wise Temporal Token Merging for Faster VGGT|Weitian Wang\u7b49|[2511.21317](http://arxiv.org/pdf/2511.21317)|\u65e0|\u25c6 The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass.\n\u25c6 However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views.\n\u25c6 For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck.|\n",
    "2511.21311": "|2025-11-26|Low-dose Chemically Specific Bioimaging via Deep-UV Lensless Holographic Microscopy on a Standard Camera|Piotr Arcab\u7b49|[2511.21311](http://arxiv.org/pdf/2511.21311)|\u65e0|\u25c6 Deep-ultraviolet (DUV) microscopy can provide label-free biochemical contrast by exploiting the intrinsic absorption of nucleic acids, proteins and lipids, offering chemically specific morphological information that complements structural optical thickness contrast from phase-sensitive imaging.\n\u25c6 However, existing DUV microscopes typically rely on specialized optics and DUV-sensitive cameras, which restrict field of view, increase system complexity and cost, and often require high illumination doses that risk photodamage.\n\u25c6 Here, we report a low-dose deep-UV lensless holographic microscopy platform that uses standard board-level CMOS sensors designed for visible light, eliminating all imaging optics and dedicated DUV detectors.|\n",
    "2511.21271": "|2025-11-26|Adaptive Lighting Control in Visible Light Systems: An Integrated Sensing, Communication, and Illumination Framework|Xinyan Xie\u7b49|[2511.21271](http://arxiv.org/pdf/2511.21271)|\u65e0|\u25c6 Indoor visible light communication (VLC) is a promising sixth-generation (6G) technology, as its directional and sensitive optical signals are naturally suited for integrated sensing and communication (ISAC).\n\u25c6 However, current research mainly focuses on maximizing data rates and sensing accuracy, creating a conflict between high performance, high energy consumption, and user visual comfort.\n\u25c6 This paper proposes an adaptive integrated sensing, communication, and illumination (ISCI) framework that resolves this conflict by treating energy savings as a primary objective.|\n",
    "2511.21202": "|2025-11-26|Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition|Baoli Sun\u7b49|[2511.21202](http://arxiv.org/pdf/2511.21202)|\u65e0|\u25c6 Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories.\n\u25c6 However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time.\n\u25c6 In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions.|\n",
    "2511.23377": "|2025-11-28|DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline|Rui Zhang\u7b49|[2511.23377](http://arxiv.org/pdf/2511.23377)|\u65e0|\u25c6 Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize.\n\u25c6 Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content.\n\u25c6 We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images.|\n",
    "2511.23292": "|2025-11-28|FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting|Tianhao Xie\u7b49|[2511.23292](http://arxiv.org/pdf/2511.23292)|\u65e0|\u25c6 Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering.\n\u25c6 Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness.\n\u25c6 However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity.|\n",
    "2511.23221": "|2025-11-28|Robust 3DGS-based SLAM via Adaptive Kernel Smoothing|Shouhe Zhang\u7b49|[2511.23221](http://arxiv.org/pdf/2511.23221)|\u65e0|\u25c6 In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy.\n\u25c6 We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking.\n\u25c6 To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM.|\n",
    "2511.23170": "|2025-11-28|PowerCLIP: Powerset Alignment for Contrastive Pre-Training|Masaki Kawamura\u7b49|[2511.23170](http://arxiv.org/pdf/2511.23170)|\u65e0|\u25c6 Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks.\n\u25c6 Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding.\n\u25c6 However, it remains challenging to capture compositional semantics that span multiple image regions.|\n",
    "2511.23127": "|2025-11-28|DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation|Hongfei Zhang\u7b49|[2511.23127](http://arxiv.org/pdf/2511.23127)|\u65e0|\u25c6 This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation.\n\u25c6 Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness.\n\u25c6 DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences.|\n",
    "2511.23029": "|2025-11-28|Geodiffussr: Generative Terrain Texturing with Elevation Fidelity|Tai Inui\u7b49|[2511.23029](http://arxiv.org/pdf/2511.23029)|\u65e0|\u25c6 Large-scale terrain generation remains a labor-intensive task in computer graphics.\n\u25c6 We introduce Geodiffussr, a flow-matching pipeline that synthesizes text-guided texture maps while strictly adhering to a supplied Digital Elevation Map (DEM).\n\u25c6 The core mechanism is multi-scale content aggregation (MCA): DEM features from a pretrained encoder are injected into UNet blocks at multiple resolutions to enforce global-to-local elevation consistency.|\n",
    "2511.22958": "|2025-11-28|Contrastive Heliophysical Image Pretraining for Solar Dynamics Observatory Records|Shiyu Shen\u7b49|[2511.22958](http://arxiv.org/pdf/2511.22958)|\u65e0|\u25c6 Deep learning has revolutionized solar image analysis, yet most approaches train task-specific encoders from scratch or rely on natural-image pretraining that ignores the unique characteristics of Solar Dynamics Observatory (SDO) data.\n\u25c6 We introduce SolarCHIP, a family of contrastively pretrained visual backbones tailored to multi-instrument SDO observations.\n\u25c6 SolarCHIP addresses three key challenges in solar imaging: multimodal sensing across AIA and HMI instruments, weak inter-class separability due to slow temporal evolution, and strong intra-class variability with sparse activity signals.|\n",
    "2511.22906": "|2025-11-28|See, Rank, and Filter: Important Word-Aware Clip Filtering via Scene Understanding for Moment Retrieval and Highlight Detection|YuEun Lee\u7b49|[2511.22906](http://arxiv.org/pdf/2511.22906)|\u65e0|\u25c6 Video moment retrieval (MR) and highlight detection (HD) with natural language queries aim to localize relevant moments and key highlights in a video clips.\n\u25c6 However, existing methods overlook the importance of individual words, treating the entire text query and video clips as a black-box, which hinders contextual understanding.\n\u25c6 In this paper, we propose a novel approach that enables fine-grained clip filtering by identifying and prioritizing important words in the query.|\n",
    "2511.22860": "|2025-11-28|MARVO: Marine-Adaptive Radiance-aware Visual Odometry|Sacchin Sundar\u7b49|[2511.22860](http://arxiv.org/pdf/2511.22860)|\u65e0|\u25c6 Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise.\n\u25c6 We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization.\n\u25c6 At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity.|\n",
    "2511.22843": "|2025-11-28|Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering|Dosung Lee\u7b49|[2511.22843](http://arxiv.org/pdf/2511.22843)|\u65e0|\u25c6 Existing Multimodal Knowledge-Based Visual Question Answering (MKB-VQA) benchmarks suffer from \"visual shortcuts\", as the query image typically matches the primary subject entity of the target document.\n\u25c6 We demonstrate that models can exploit these shortcuts, achieving comparable results using visual cues alone.\n\u25c6 To address this, we introduce Relational Entity Text-Image kNowledge Augmented (RETINA) benchmark, automatically constructed using an LLM-driven pipeline, consisting of 120k training and 2k human-curated test set.|\n",
    "2512.01979": "|2025-12-01|Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback|Aiden Yiliu Li\u7b49|[2512.01979](http://arxiv.org/pdf/2512.01979)|\u65e0|\u25c6 GUI grounding aims to align natural language instructions with precise regions in complex user interfaces.\n\u25c6 Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts.\n\u25c6 These limitations arise from limited grounding capacity and from underuse of existing reasoning potential.|\n",
    "2512.01908": "|2025-12-01|SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception|Gurmeher Khurana\u7b49|[2512.01908](http://arxiv.org/pdf/2512.01908)|\u65e0|\u25c6 Contact-rich robotic manipulation requires representations that encode local geometry.\n\u25c6 Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues.\n\u25c6 Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information.|\n",
    "2512.01889": "|2025-12-01|KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM|Zaid Nasser\u7b49|[2512.01889](http://arxiv.org/pdf/2512.01889)|\u65e0|\u25c6 We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.\n\u25c6 Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.\n\u25c6 KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).|\n",
    "2512.01636": "|2025-12-01|Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval|Xin Wang\u7b49|[2512.01636](http://arxiv.org/pdf/2512.01636)|\u65e0|\u25c6 Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification.\n\u25c6 While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions.\n\u25c6 The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap.|\n",
    "2512.01608": "|2025-12-01|Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track|Mo Chen|[2512.01608](http://arxiv.org/pdf/2512.01608)|\u65e0|\u25c6 This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller.\n\u25c6 The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization.\n\u25c6 The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization.|\n",
    "2512.01589": "|2025-12-01|Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation|Thao Thi Phuong Dao\u7b49|[2512.01589](http://arxiv.org/pdf/2512.01589)|\u65e0|\u25c6 Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly.\n\u25c6 Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention.\n\u25c6 In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses.|\n",
    "2512.01525": "|2025-12-01|Near-infrared polarimetric imaging with nonlinear flat-optics|Evgenii Menshikov\u7b49|[2512.01525](http://arxiv.org/pdf/2512.01525)|\u65e0|\u25c6 A compact and broadband polarimetric imaging platform is presented, based on second-harmonic generation (SHG) in nonlinear flat-optics.\n\u25c6 The system employs periodic all-dielectric AlGaAs gratings to induce polarization-dependent SH emission, enabling pixel by pixel direct retrieval of the full Stokes vector from an input intensity distribution in the near-infrared range.\n\u25c6 By engineering the geometry and orientation of the polarimetric units, sensitivity to linear and circular polarization components is achieved.|\n",
    "2512.01519": "|2025-12-01|QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions|Can Polat\u7b49|[2512.01519](http://arxiv.org/pdf/2512.01519)|\u65e0|\u25c6 Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs.\n\u25c6 Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems.\n\u25c6 We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter.|\n",
    "2512.01498": "|2025-12-01|Winning Solutions for the Rayan AI Contest: Compositional Retrieval, Zero-Shot Anomaly Detection, and Backdoor Detection|Ali Nafisi\u7b49|[2512.01498](http://arxiv.org/pdf/2512.01498)|\u65e0|\u25c6 This report presents solutions to three machine learning challenges: compositional image retrieval, zero-shot anomaly detection, and backdoored model detection.\n\u25c6 In compositional image retrieval, we developed a system that processes visual and textual inputs to retrieve relevant images, achieving 95.38\\% accuracy and ranking first with a clear margin over the second team.\n\u25c6 For zero-shot anomaly detection, we designed a model that identifies and localizes anomalies in images without prior exposure to abnormal examples, securing 1st place with 73.14\\% accuracy.|\n",
    "2512.01419": "|2025-12-01|Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries|Tushar Pranav\u7b49|[2512.01419](http://arxiv.org/pdf/2512.01419)|\u65e0|\u25c6 Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA).\n\u25c6 To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries.\n\u25c6 RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories.|\n",
    "2512.03046": "|2025-12-02|MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues|Zichen Liu\u7b49|[2512.03046](http://arxiv.org/pdf/2512.03046)|\u65e0|\u25c6 We propose MagicQuill V2, a novel system that introduces a \\textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software.\n\u25c6 While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance.\n\u25c6 To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette.|\n",
    "2512.03040": "|2025-12-02|Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation|Zeqi Xiao\u7b49|[2512.03040](http://arxiv.org/pdf/2512.03040)|\u65e0|\u25c6 We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data.\n\u25c6 To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks.\n\u25c6 We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning.|\n",
    "2512.02906": "|2025-12-02|MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding|Fan Yang\u7b49|[2512.02906](http://arxiv.org/pdf/2512.02906)|\u65e0|\u25c6 Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs).\n\u25c6 Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model.\n\u25c6 The most relevant crops are then selected to localize the target object and suppress irrelevant information.|\n",
    "2512.02897": "|2025-12-02|Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models|Pierpaolo Serio\u7b49|[2512.02897](http://arxiv.org/pdf/2512.02897)|\u65e0|\u25c6 This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model.\n\u25c6 We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself.\n\u25c6 Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy.|\n",
    "2512.02817": "|2025-12-02|BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion|Sai Koneru\u7b49|[2512.02817](http://arxiv.org/pdf/2512.02817)|\u65e0|\u25c6 The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge.\n\u25c6 Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities.\n\u25c6 To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning.|\n",
    "2512.02792": "|2025-12-02|HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval|Zhiwei Chen\u7b49|[2512.02792](http://arxiv.org/pdf/2512.02792)|\u65e0|\u25c6 Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video.\n\u25c6 The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning.\n\u25c6 Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality.|\n",
    "2512.02737": "|2025-12-02|Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone|Tristan Amadei\u7b49|[2512.02737](http://arxiv.org/pdf/2512.02737)|\u65e0|\u25c6 Image-based localization in GNSS-denied environments is critical for UAV autonomy.\n\u25c6 Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training.\n\u25c6 Such data are costly to acquire and often unavailable, limiting their applicability.|\n",
    "2512.02727": "|2025-12-02|DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions|Yifan Zhou\u7b49|[2512.02727](http://arxiv.org/pdf/2512.02727)|\u65e0|\u25c6 Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE).\n\u25c6 To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene).\n\u25c6 However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context.|\n",
    "2512.02713": "|2025-12-02|Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs|Theodoros Aivalis\u7b49|[2512.02713](http://arxiv.org/pdf/2512.02713)|\u65e0|\u25c6 As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified.\n\u25c6 Understanding how specific training data contributes to a model's output is critical.\n\u25c6 We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs).|\n",
    "2512.02697": "|2025-12-02|GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization|Zixuan Song\u7b49|[2512.02697](http://arxiv.org/pdf/2512.02697)|\u65e0|\u25c6 Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image.\n\u25c6 However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable.\n\u25c6 It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image).|\n",
    "2512.04040": "|2025-12-03|RELIC: Interactive Video World Model with Long-Horizon Memory|Yicong Hong\u7b49|[2512.04040](http://arxiv.org/pdf/2512.04040)|\u65e0|\u25c6 A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control.\n\u25c6 However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance.\n\u25c6 In this work, we present RELIC, a unified framework that tackles these three challenges altogether.|\n",
    "2512.03981": "|2025-12-03|DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment|Sheng-Hao Liao\u7b49|[2512.03981](http://arxiv.org/pdf/2512.03981)|\u65e0|\u25c6 Drag-based image editing using generative models provides intuitive control over image structures.\n\u25c6 However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision.\n\u25c6 Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts.|\n",
    "2512.03734": "|2025-12-03|Revealing Nanoscale Molecular Organization in Liquid Crystals via Cryogenic Atom Probe Tomograph|Kuan Meng\u7b49|[2512.03734](http://arxiv.org/pdf/2512.03734)|\u65e0|\u25c6 While liquid crystals (LCs) have been extensively studied, obtaining a comprehensive nanoscale picture of their molecular organization remains challenging, as conventional techniques face an intrinsic trade-off between spatial and chemical resolution.\n\u25c6 Here, cryogenic atom probe tomography (cryo-APT) is introduced as a new analytical approach for LC materials, using 4'-Pentyl-4-cyanobiphenyl (5CB) and 4'-Octyl-4-cyanobiphenyl (8CB) as representative model compounds.\n\u25c6 This was enabled by a tailored cryogenic focused ion beam (cryo-FIB) protocol optimized for small organic molecules.|\n",
    "2512.03715": "|2025-12-03|DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction|Kaichen Zhang\u7b49|[2512.03715](http://arxiv.org/pdf/2512.03715)|\u65e0|\u25c6 This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.\n\u25c6 The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.\n\u25c6 DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.|\n",
    "2512.03684": "|2025-12-03|A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection|Shahid Ansari\u7b49|[2512.03684](http://arxiv.org/pdf/2512.03684)|\u65e0|\u25c6 This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping.\n\u25c6 The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting.\n\u25c6 For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination.|\n",
    "2512.03663": "|2025-12-03|Multi-Scale Visual Prompting for Lightweight Small-Image Classification|Salim Khazem|[2512.03663](http://arxiv.org/pdf/2512.03663)|\u65e0|\u25c6 Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space.\n\u25c6 However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet.\n\u25c6 In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting.|\n",
    "2512.03514": "|2025-12-03|M3DR: Towards Universal Multilingual Multimodal Document Retrieval|Adithya S Kolavi\u7b49|[2512.03514](http://arxiv.org/pdf/2512.03514)|\u65e0|\u25c6 Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search.\n\u25c6 However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts.\n\u25c6 In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts.|\n",
    "2512.03454": "|2025-12-03|Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles|Haicheng Liao\u7b49|[2512.03454](http://arxiv.org/pdf/2512.03454)|\u65e0|\u25c6 Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\n\u25c6 Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution.\n\u25c6 Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions.|\n",
    "2512.03445": "|2025-12-03|Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation|Xieji Li\u7b49|[2512.03445](http://arxiv.org/pdf/2512.03445)|\u65e0|\u25c6 Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations.\n\u25c6 However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts.\n\u25c6 To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining.|\n",
    "2512.03438": "|2025-12-03|Multimodal Reinforcement Learning with Agentic Verifier for AI Agents|Reuben Tan\u7b49|[2512.03438](http://arxiv.org/pdf/2512.03438)|\u65e0|\u25c6 Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers.\n\u25c6 Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance.\n\u25c6 However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too.|\n",
    "2512.05111": "|2025-12-04|ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning|Shengyuan Ding\u7b49|[2512.05111](http://arxiv.org/pdf/2512.05111)|\u65e0|\u25c6 Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks.\n\u25c6 We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring.\n\u25c6 This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models.|\n",
    "2512.05091": "|2025-12-04|Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark|Haobo Yuan\u7b49|[2512.05091](http://arxiv.org/pdf/2512.05091)|\u65e0|\u25c6 Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering.\n\u25c6 However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result.\n\u25c6 This contrasts with human intelligence, which naturally operates through a chain of visual reasoning.|\n",
    "2512.05039": "|2025-12-04|Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding|Abhigyan Bhattacharya\u7b49|[2512.05039](http://arxiv.org/pdf/2512.05039)|\u65e0|\u25c6 Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration.\n\u25c6 Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors.\n\u25c6 In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis.|\n",
    "2512.05007": "|2025-12-04|Revealing stimulus-dependent dynamics through statistical complexity|Edson V. de Paula\u7b49|[2512.05007](http://arxiv.org/pdf/2512.05007)|\u65e0|\u25c6 Advances in large-scale neural recordings have expanded our ability to describe the activity of distributed brain circuits.\n\u25c6 However, understanding how neural population dynamics differ across regions and behavioral contexts remains challenging.\n\u25c6 Here, we surveyed neuronal population dynamics across multiple mouse brain areas (visual cortex, hippocampus, thalamus, and midbrain) using spike data from local ensembles.|\n",
    "2512.04989": "|2025-12-04|Influence of Object Affordance on Action Language Understanding: Evidence from Dynamic Causal Modeling Analysis|Supriya Bordoloi\u7b49|[2512.04989](http://arxiv.org/pdf/2512.04989)|\u65e0|\u25c6 This study investigates the causal neural dynamics by which affordance representations influence action language comprehension.\n\u25c6 In this study, 18 participants observed stimuli displayed in two conditions during the experiment: text-only (e.g., `Hit with a hammer') and video+text (visual clips with matching phrases).\n\u25c6 EEG data were recorded from 32 channels and analyzed for event-related potentials and source localization using LORETA, which identified four left-hemisphere regions of interest: the Lateral Occipital Cortex (LOC), Posterior Superior Temporal Gyrus (pSTG), Ventral Premotor Cortex (PMv), and Inferior Parietal Lobule (IPL).|\n",
    "2512.04939": "|2025-12-04|LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging|Zhijian Shu\u7b49|[2512.04939](http://arxiv.org/pdf/2512.04939)|\u65e0|\u25c6 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception.\n\u25c6 However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images.\n\u25c6 To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes.|\n",
    "2512.04783": "|2025-12-04|Terahertz Fourier Ptychographic Imaging|Pitambar Mukherjee\u7b49|[2512.04783](http://arxiv.org/pdf/2512.04783)|\u65e0|\u25c6 High-resolution imaging in the terahertz (THz) spectral range remains fundamentally constrained by the limited numerical apertures of currently existing state-of-the-art imagers, which restricts its applicability across many fields, such as imaging in complex media or nondestructive testing.\n\u25c6 To address this challenge, we introduce a proof-of-concept implementation of THz Fourier Ptychographic imaging to enhance spatial resolution without requiring extensive hardware modifications.\n\u25c6 Our method employs a motorized kinematic mirror to generate a sequence of controlled, multi-angle plane-wave illuminations, with each resulting oblique-illumination intensity image encoding a limited portion of the spatial-frequency content of the target imaging sample.|\n",
    "2512.04772": "|2025-12-04|TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards|Mauro Martini\u7b49|[2512.04772](http://arxiv.org/pdf/2512.04772)|\u65e0|\u25c6 In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation.\n\u25c6 However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials.\n\u25c6 The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions.|\n",
    "2512.04763": "|2025-12-04|MemLoRA: Distilling Expert Adapters for On-Device Memory Systems|Massimo Bini\u7b49|[2512.04763](http://arxiv.org/pdf/2512.04763)|\u65e0|\u25c6 Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context.\n\u25c6 Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private.\n\u25c6 However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment.|\n",
    "2512.04662": "|2025-12-04|Spectral micro-CT for quantitative analysis of calcification in fibrocartilage|Vittoria Mazzini\u7b49|[2512.04662](http://arxiv.org/pdf/2512.04662)|\u65e0|\u25c6 This work introduces a quantitative method for assessing calcification in fibrocartilage using spectral micro-computed tomography ($\u03bc$CT).\n\u25c6 Tissue samples of hip acetabular labrum from patients with osteoarthritis and femoroacetabular impingement were imaged with a laboratory-based spectral $\u03bc$CT system equipped with a small-pixel photon-counting detector.\n\u25c6 The detector operated with two energy thresholds, allowing the simultaneous acquisition of two CT datasets at different X-ray energies.|\n",
    "2512.07338": "|2025-12-08|Generalized Referring Expression Segmentation on Aerial Photos|Lu\u00eds Marnoto\u7b49|[2512.07338](http://arxiv.org/pdf/2512.07338)|\u65e0|\u25c6 Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions.\n\u25c6 Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions.\n\u25c6 This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types.|\n",
    "2512.06865": "|2025-12-07|Spatial Retrieval Augmented Autonomous Driving|Xiaosong Jia\u7b49|[2512.06865](http://arxiv.org/pdf/2512.06865)|\u65e0|\u25c6 Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception.\n\u25c6 However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain.\n\u25c6 In contrast, human drivers are able to recall road structure even under poor visibility.|\n",
    "2512.06255": "|2025-12-06|Language-driven Fine-grained Retrieval|Shijie Wang\u7b49|[2512.06255](http://arxiv.org/pdf/2512.06255)|\u65e0|\u25c6 Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision.\n\u25c6 While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories.\n\u25c6 To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs).|\n",
    "2512.06147": "|2025-12-05|GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers|Hochul Hwang\u7b49|[2512.06147](http://arxiv.org/pdf/2512.06147)|\u65e0|\u25c6 While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare.\n\u25c6 To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking.\n\u25c6 After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people.|\n",
    "2511.22253": "|2025-11-27|UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries|Hoang-Bao Le\u7b49|[2511.22253](http://arxiv.org/pdf/2511.22253)|\u65e0|\u25c6 Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images.\n\u25c6 This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR).\n\u25c6 In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt.|\n",
    "2512.09903": "|2025-12-10|YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos|Ryan Meegan\u7b49|[2512.09903](http://arxiv.org/pdf/2512.09903)|\u65e0|\u25c6 Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning.\n\u25c6 However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive.\n\u25c6 We address the problem of visual navigation when exploration videos of a large environment are available.|\n",
    "2512.09071": "|2025-12-09|Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics|Nick Trinh\u7b49|[2512.09071](http://arxiv.org/pdf/2512.09071)|\u65e0|\u25c6 Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications.\n\u25c6 This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic.\n\u25c6 Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves.|\n",
    "2512.10596": "|2025-12-11|Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval|J. Xiao\u7b49|[2512.10596](http://arxiv.org/pdf/2512.10596)|\u65e0|\u25c6 Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \\textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts.\n\u25c6 While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context.\n\u25c6 To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image.|\n"
  },
  "Keypoint Detection": {
    "2505.02787": "|2025-05-05|Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration|David Rivas-Villar\u7b49|[2505.02787](http://arxiv.org/pdf/2505.02787)|\u65e0|\u25c6\u63d0\u51fa\u9996\u4e2a\u4e0d\u4f9d\u8d56\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u65e0\u76d1\u7763\u63cf\u8ff0\u7b26\u5b66\u4e60\u65b9\u6cd5\uff0c\u7a81\u7834\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u9886\u57df\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u5b9e\u73b0\u63cf\u8ff0\u7b26\u7f51\u7edc\u4e0e\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u7684\u89e3\u8026\uff0c\u4f7f\u6a21\u578b\u80fd\u9002\u914d\u4efb\u610f\u68c0\u6d4b\u5668\uff0c\u63d0\u5347\u4e34\u5e8a\u5e94\u7528\u7075\u6d3b\u6027\u3002  \n\u25c6\u5728\u6807\u51c6\u89c6\u7f51\u819c\u914d\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u9a8c\u8bc1\uff0c\u8bc1\u660e\u65e0\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\u5ab2\u7f8e\u6709\u76d1\u7763\u65b9\u6cd5\u3002  \n\u25c6\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u65b0\u578b\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5bf9\u4e0d\u540c\u68c0\u6d4b\u5668\u7684\u5f3a\u9c81\u68d2\u6027\u3002  \n\u25c6\u4e3a\u533b\u5b66\u9886\u57df\u65e0\u76d1\u7763\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8303\u4f8b\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u7a00\u7f3a\u7684\u6838\u5fc3\u75db\u70b9\u3002  \n\u25c6\u901a\u8fc7\u7aef\u5230\u7aef\u65e0\u76d1\u7763\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u7684\u6280\u672f\u95e8\u69db\u548c\u5b9e\u73b0\u6210\u672c\u3002|\n",
    "2505.02779": "|2025-05-05|Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance|David Rivas-Villar\u7b49|[2505.02779](http://arxiv.org/pdf/2505.02779)|\u65e0|\u25c6\u63d0\u51fa\u9996\u4e2a\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u914d\u51c6\u6d41\u7a0b\uff0c\u65e0\u9700\u4efb\u4f55\u6807\u6ce8\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u9886\u57df\u6807\u6ce8\u7a00\u7f3a\u7684\u96be\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u98a0\u8986\u4f20\u7edf\u601d\u8def\uff0c\u901a\u8fc7\u63cf\u8ff0\u5b50\u6027\u80fd\u53cd\u63a8\u5173\u952e\u70b9\u68c0\u6d4b\uff08\u63cf\u8ff0\u5b50\u9a71\u52a8\u68c0\u6d4b\u5668\uff09\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5173\u952e\u70b9\u9a71\u52a8\u63cf\u8ff0\u5b50\u5b66\u4e60\u3002  \n\u25c6\u5f00\u53d1\u4e86\u65e0\u9700\u5173\u952e\u70b9\u68c0\u6d4b\u6216\u6807\u7b7e\u7684\u63cf\u8ff0\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u76f4\u63a5\u4e3a\u89c6\u7f51\u819c\u56fe\u50cf\u4efb\u610f\u4f4d\u7f6e\u751f\u6210\u9ad8\u8d28\u91cf\u63cf\u8ff0\u7b26\u3002  \n\u25c6\u8bbe\u8ba1\u4e86\u65b0\u578b\u65e0\u6807\u7b7e\u5173\u952e\u70b9\u68c0\u6d4b\u7f51\u7edc\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u8f93\u5165\u56fe\u50cf\u9884\u6d4b\u63cf\u8ff0\u5b50\u5339\u914d\u6027\u80fd\u6765\u5b9a\u4f4d\u5173\u952e\u70b9\u3002  \n\u25c6\u5728\u56db\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u8868\u660e\uff0c\u65e0\u76d1\u7763\u63cf\u8ff0\u5b50\u8d85\u8d8a\u6709\u76d1\u7763SOTA\u65b9\u6cd5\uff0c\u65e0\u76d1\u7763\u68c0\u6d4b\u5668\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u68c0\u6d4b\u65b9\u6cd5\u3002  \n\u25c6\u6574\u4e2a\u914d\u51c6\u6d41\u7a0b\u6027\u80fd\u5ab2\u7f8e\u4e3b\u6d41\u6709\u76d1\u7763\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u7279\u6027\u4f7f\u5176\u53ef\u76f4\u63a5\u8fc1\u79fb\u5230\u5176\u4ed6\u9886\u57df\u548c\u6a21\u6001\u3002|\n",
    "2505.02161": "|**2025-05-04**|**Focus What Matters: Matchability-Based Reweighting for Local Feature Matching**|Dongyue Li et.al.|[2505.02161](http://arxiv.org/abs/2505.02161)|null|\n",
    "2505.02049": "|**2025-05-04**|**Enhancing Lidar Point Cloud Sampling via Colorization and Super-Resolution of Lidar Imagery**|Sier Ha et.al.|[2505.02049](http://arxiv.org/abs/2505.02049)|null|\n",
    "2505.08013": "|2025-06-19|RDD: Robust Feature Detector and Descriptor using Deformable Transformer|Gonglin Chen\u7b49|[2505.08013](http://arxiv.org/pdf/2505.08013)|\u65e0|\u25c6 \u63d0\u51faRDD\uff08Robust Deformable Detector\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62Transformer\u7684\u65b0\u578b\u5173\u952e\u70b9\u68c0\u6d4b\u4e0e\u63cf\u8ff0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u51e0\u4f55\u4e0d\u53d8\u6027\u3002  \n\u25c6 \u5229\u7528\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u5173\u952e\u4f4d\u7f6e\uff0c\u663e\u8457\u964d\u4f4e\u641c\u7d22\u7a7a\u95f4\u590d\u6742\u5ea6\u5e76\u6709\u6548\u5efa\u6a21\u51e0\u4f55\u53d8\u6362\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u957f\u7a0b\u89c6\u89c9\u5173\u7cfb\u7684\u95ee\u9898\u3002  \n\u25c6 \u7ed3\u5408\u6807\u51c6MegaDepth\u6570\u636e\u96c6\u4e0e\u81ea\u5efa\u7684Air-to-Ground\uff08\u7a7a\u5bf9\u5730\uff09\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u589e\u5f3a\u6a21\u578b\u5728\u8de8\u89c6\u89d2\u548c\u8de8\u5c3a\u5ea6\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728\u7a00\u758f\u5339\u914d\u4efb\u52a1\u4e2d\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5e76\u5177\u5907\u534a\u7a20\u5bc6\u5339\u914d\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u5f15\u5165\u4e24\u4e2a\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff1a\u4e00\u4e2a\u9488\u5bf9\u5927\u89c6\u89d2\u4e0e\u5c3a\u5ea6\u53d8\u5316\uff0c\u53e6\u4e00\u4e2a\u4e3a\u7a7a\u5bf9\u5730\u573a\u666f\uff0c\u586b\u8865\u4e86\u8de8\u9ad8\u5ea63D\u91cd\u5efa\u8bc4\u4f30\u7684\u7a7a\u767d\u3002|\n",
    "2505.07306": "|2025-05-12|Enabling Privacy-Aware AI-Based Ergonomic Analysis|Sander De Coninck\u7b49|[2505.07306](http://arxiv.org/pdf/2505.07306)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u611f\u77e5\u7684AI\u5de5\u6548\u5b66\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5f00\u53d1\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u89c6\u9891\u6570\u636e\u4e2d\u6a21\u7cca\u9690\u79c1\u4fe1\u606f\uff0c\u4ec5\u4fdd\u7559\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6240\u9700\u5173\u952e\u7279\u5f81\u3002  \n\u25c6 \u91c7\u7528\u6570\u636e\u6df7\u6dc6\u6280\u672f\u786e\u4fdd\u4e0e\u6807\u51c6\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\u517c\u5bb9\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u9ad8\u7cbe\u5ea6\u5206\u6790\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6444\u50cf\u5934\u7cfb\u7edf\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u6df7\u6dc6\u540e\u7684\u6570\u636e\u4f20\u8f93\u81f3\u4e2d\u592e\u670d\u52a1\u5668\u5904\u7406\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u878d\u5408\u6280\u672f\u91cd\u5efa3D\u5173\u952e\u70b9\uff0c\u5b9e\u73b0\u8fdc\u7a0b\u9ad8\u7cbe\u5ea6\u5de5\u6548\u5b66\u8bc4\u4f30\u3002  \n\u25c6 \u6574\u5408REBA\uff08\u5feb\u901f\u5168\u8eab\u8bc4\u4f30\uff09\u65b9\u6cd5\u5bf93D\u59ff\u6001\u8fdb\u884c\u5de5\u6548\u5b66\u98ce\u9669\u91cf\u5316\uff0c\u5f62\u6210\u4ece\u6570\u636e\u91c7\u96c6\u5230\u98ce\u9669\u8bc4\u4f30\u7684\u5b8c\u6574\u95ed\u73af\u7cfb\u7edf\u3002  \n\u25c6 \u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u9996\u6b21\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u4e0e\u5de5\u6548\u5b66\u76d1\u6d4b\u7684\u5e73\u8861\uff0c\u4e3a\u5236\u9020\u4e1a\u63d0\u4f9b\u517c\u987e\u5b89\u5168\u6027\u4e0e\u5408\u89c4\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u7cfb\u7edf\u8bbe\u8ba1\u8f7b\u91cf\u5316\u4e14\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5de5\u4e1a\u73af\u5883\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u4f18\u52bf\u3002|\n",
    "2505.06436": "|2025-05-09|My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing|Jingrui He\u7b49|[2505.06436](http://arxiv.org/pdf/2505.06436)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u578b\u7684\u65b0\u635f\u5931\u51fd\u6570\uff08HFLD\u635f\u5931\uff09\uff0c\u7528\u4e8e\u89e3\u51b3StyleGAN/2\u6f5c\u5728\u7a7a\u95f4\u7f16\u8f91\u4e2d\u7684\u8868\u60c5\u7ea0\u7f20\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u5728\u73b0\u6709\u6a21\u578b\u635f\u5931\u51fd\u6570\u4e2d\u589e\u52a0HFLD\u635f\u5931\uff0c\u6709\u6548\u9650\u5236\u4e86\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u5bf9\u9762\u90e8\u8868\u60c5\u7684\u5e72\u6270\uff0c\u5b9e\u9a8c\u663e\u793a\u60c5\u7eea\u53d8\u5316\u51cf\u5c11\u9ad8\u8fbe49%\u3002  \n\u25c6 \u9996\u6b21\u5c06\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u6280\u672f\u4e0eGAN\u6f5c\u5728\u7a7a\u95f4\u7f16\u8f91\u7ed3\u5408\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8868\u60c5\u4e00\u81f4\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\u3002  \n\u25c6 \u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u5728\u56fa\u5b9a\u8868\u60c5\u4e0b\u53d8\u6362\u5916\u8c8c\u7279\u5f81\u7684\u80fd\u529b\uff0c\u4e3a\u624b\u52bf\u548c\u8868\u60c5\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u589e\u5f3a\u624b\u6bb5\u3002  \n\u25c6 \u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9762\u90e8\u8868\u60c5\u7684\u540c\u65f6\u7f16\u8f91\u5176\u4ed6\u5c5e\u6027\uff08\u5982\u6027\u522b\u3001\u5e74\u9f84\uff09\u7684\u6548\u679c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u3002  \n\u25c6 \u4e3a\u9762\u90e8\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6280\u672f\u8def\u5f84\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u7ea6\u675f\u76f4\u63a5\u89e3\u51b3\u7279\u5f81\u89e3\u8026\u95ee\u9898\uff0c\u800c\u975e\u4f9d\u8d56\u9690\u5f0f\u5b66\u4e60\u3002|\n",
    "2505.12246": "|2025-05-18|SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving|Muleilan Pei\u7b49|[2505.12246](http://arxiv.org/pdf/2505.12246)|\u65e0|\u25c6 \u63d0\u51faSEPT\u6846\u67b6\uff0c\u5229\u7528\u6807\u51c6\u5b9a\u4e49\u5730\u56fe\uff08SD\u5730\u56fe\uff09\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u611f\u77e5\u4e0e\u62d3\u6251\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u9ad8\u7cbe\u5730\u56fe\u7684\u4f9d\u8d56\u3002  \n\u25c6 \u8bbe\u8ba1\u6df7\u5408\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u7ed3\u5408SD\u5730\u56fe\u4e0e\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7279\u5f81\uff0c\u540c\u65f6\u5904\u7406\u6805\u683c\u5316\u548c\u77e2\u91cf\u5316\u8868\u793a\uff0c\u89e3\u51b3\u4e24\u8005\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165\u57fa\u4e8eSD\u5730\u56fe\u7684\u8f85\u52a9\u4efb\u52a1\u2014\u2014\u4ea4\u53c9\u8def\u53e3\u611f\u77e5\u5173\u952e\u70b9\u68c0\u6d4b\uff0c\u63d0\u5347\u957f\u8ddd\u79bb\u548c\u906e\u6321\u573a\u666f\u4e0b\u7684\u7406\u89e3\u6027\u80fd\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728OpenLane-V2\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660eSD\u5730\u56fe\u5148\u9a8c\u7684\u6709\u6548\u6027\u3002  \n\u25c6 \u6574\u4f53\u6846\u67b6\u517c\u987e\u611f\u77e5\u4e0e\u63a8\u7406\uff0c\u4e3a\u65e0\u9ad8\u7cbe\u5730\u56fe\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u5728\u7ebf\u73af\u5883\u7406\u89e3\u65b9\u6848\u3002|\n",
    "2505.12130": "|2025-05-17|Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation|Niaz Ahmad\u7b49|[2505.12130](http://arxiv.org/pdf/2505.12130)|\u65e0|\u25c6 \u63d0\u51faKeypoints as Dynamic Centroid (KDC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8d28\u5fc3\u8868\u793a\u7edf\u4e00\u89e3\u51b3\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u5173\u8282\u91cd\u53e0\u6216\u5feb\u901f\u8fd0\u52a8\u65f6\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u8303\u5f0f\u751f\u6210\u5173\u952e\u70b9\u70ed\u56fe\uff0c\u5e76\u5f15\u5165KeyCentroids\uff08\u57fa\u4e8e\u5173\u952e\u70b9\u78c1\u76d8\uff09\u63d0\u5347\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6\u548c\u7f6e\u4fe1\u5ea6\u5f97\u5206\u3002  \n\u25c6 \u5229\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u5173\u952e\u70b9\u4f5c\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u52a8\u6001\u8d28\u5fc3\uff08MaskCentroids\uff09\uff0c\u5b9e\u73b0\u5feb\u901f\u8fd0\u52a8\u4e0b\u50cf\u7d20\u5230\u4eba\u4f53\u5b9e\u4f8b\u7684\u9ad8\u6548\u805a\u7c7b\u3002  \n\u25c6 \u5728CrowdPose\u3001OCHuman\u548cCOCO\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86KDC\u7684\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u8868\u73b0\u7a81\u51fa\u3002  \n\u25c6 \u901a\u8fc7\u52a8\u6001\u8d28\u5fc3\u673a\u5236\u6709\u6548\u5904\u7406\u5b9e\u4f8b\u7ea7\u5206\u5272\u4e2d\u7684\u906e\u6321\u548c\u59ff\u6001\u5feb\u901f\u53d8\u5316\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2505.11110": "|2025-05-16|Deepfake Forensic Analysis: Source Dataset Attribution and Legal Implications of Synthetic Media Manipulation|Massimiliano Cassia\u7b49|[2505.11110](http://arxiv.org/pdf/2505.11110)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684GAN\u751f\u6210\u56fe\u50cf\u6eaf\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7279\u5f81\u5206\u6790\u51c6\u786e\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u96c6\uff08\u5982CelebA\u6216FFHQ\uff09\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u878d\u5408\u9891\u57df\u53d8\u6362\uff08\u5085\u91cc\u53f6/DCT\uff09\u3001\u8272\u5f69\u5206\u5e03\u5ea6\u91cf\u548c\u5c40\u90e8\u7279\u5f81\u63cf\u8ff0\u7b26\uff08SIFT\uff09\uff0c\u63d0\u53d6\u5408\u6210\u56fe\u50cf\u4e2d\u7684 discriminative \u7edf\u8ba1\u7279\u5f81\u3002  \n\u25c6\u76d1\u7763\u5206\u7c7b\u5668\uff08\u968f\u673a\u68ee\u6797\u3001SVM\u3001XGBoost\uff09\u5728\u4e8c\u5143\u5206\u7c7b\uff08\u771f\u5b9evs\u5408\u6210\uff09\u548c\u591a\u7c7b\u6570\u636e\u96c6\u6eaf\u6e90\u4efb\u52a1\u4e2d\u8fbe\u523098-99%\u51c6\u786e\u7387\uff0c\u8986\u76d6\u591a\u79cd\u4e3b\u6d41GAN\u67b6\u6784\uff08\u5982StyleGAN\u7cfb\u5217\uff09\u3002  \n\u25c6\u5b9e\u9a8c\u8bc1\u660e\u9891\u57df\u7279\u5f81\uff08DCT/FFT\uff09\u5bf9\u6355\u6349\u6570\u636e\u96c6\u7279\u5f02\u6027\u4f2a\u5f71\uff08\u5982\u4e0a\u91c7\u6837\u6a21\u5f0f\u3001\u9891\u8c31\u5f02\u5e38\uff09\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u8272\u5f69\u76f4\u65b9\u56fe\u5219\u80fd\u63ed\u793aGAN\u8bad\u7ec3\u7684\u9690\u5f0f\u6b63\u5219\u5316\u7b56\u7565\u3002  \n\u25c6\u9996\u6b21\u7cfb\u7edf\u63a2\u8ba8\u4e86\u5408\u6210\u5a92\u4f53\u6570\u636e\u96c6\u6eaf\u6e90\u7684\u6cd5\u5f8b\u5e94\u7528\u573a\u666f\uff0c\u5305\u62ec\u7248\u6743\u4fb5\u6743\u3001\u9690\u79c1\u6570\u636e\u6ee5\u7528\uff08\u5982GDPR\u5408\u89c4\uff09\u53ca\u52a0\u5ddeAB 602\u6cd5\u6848\u7b49\u76d1\u7ba1\u5e94\u5bf9\u65b9\u6848\u3002  \n\u25c6\u8be5\u6846\u67b6\u4e3a\u751f\u6210\u6a21\u578b\u7684\u95ee\u8d23\u5236\u6cbb\u7406\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6491\uff0c\u53ef\u5e94\u7528\u4e8e\u6570\u5b57\u53d6\u8bc1\u3001\u5185\u5bb9\u5ba1\u6838\u548c\u77e5\u8bc6\u4ea7\u6743\u8bc9\u8bbc\u7b49\u5b9e\u9645\u9886\u57df\u3002|\n",
    "2505.23475": "|2025-05-29|TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning|Ron Shapira Weber\u7b49|[2505.23475](http://arxiv.org/pdf/2505.23475)|[\u4ee3\u7801](https://github.com/bgu-cs-vil/timepoint)|\u25c6\u63d0\u51faTimePoint\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4ece\u5408\u6210\u6570\u636e\u4e2d\u63d0\u53d6\u5173\u952e\u70b9\u548c\u63cf\u8ff0\u7b26\uff0c\u663e\u8457\u52a0\u901f\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u7684\u5bf9\u9f50\u8fc7\u7a0b\uff0c\u540c\u65f6\u63d0\u9ad8\u5bf9\u9f50\u7cbe\u5ea6\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u5c062D\u5173\u952e\u70b9\u68c0\u6d4b\u601d\u60f3\u9002\u914d\u52301D\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u4e00\u7ef4\u5fae\u5206\u540c\u80da\u6a21\u578b\u751f\u6210\u903c\u771f\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u6a21\u62df\u975e\u7ebf\u6027\u65f6\u95f4\u626d\u66f2\u3002  \n\u25c6\u91c7\u7528\u5168\u5377\u79ef\u548c\u5c0f\u6ce2\u5377\u79ef\u67b6\u6784\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u7684\u7a00\u758f\u8868\u793a\uff0c\u4f7fDTW\u5728\u7a00\u758f\u6570\u636e\u4e0a\u8fd0\u884c\u65f6\u83b7\u5f97\u6570\u91cf\u7ea7\u52a0\u901f\uff0c\u4e14\u7cbe\u5ea6\u901a\u5e38\u4f18\u4e8e\u539f\u59cb\u4fe1\u53f7\u4e0a\u7684\u6807\u51c6DTW\u3002  \n\u25c6\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5373\u53ef\u5728\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u4e0a\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002  \n\u25c6\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cTimePoint\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6DTW\uff0c\u4e3a\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2505.18652": "|2025-05-24|Why Not Replace? Sustaining Long-Term Visual Localization via Handcrafted-Learned Feature Collaboration on CPU|Yicheng Lin\u7b49|[2505.18652](http://arxiv.org/pdf/2505.18652)|[\u4ee3\u7801](https://github.com/linyicheng1/orb_slam3_localization)|\u25c6 \u63d0\u51fa\u624b\u5de5-\u5b66\u4e60\u7279\u5f81\u534f\u4f5c\u673a\u5236\uff1a\u9996\u6b21\u7cfb\u7edf\u8bba\u8bc1\u624b\u5de5\u7279\u5f81\uff08\u9002\u5408\u8fde\u7eed\u8ddf\u8e2a\uff09\u4e0e\u5b66\u4e60\u7279\u5f81\uff08\u64c5\u957f\u5bbd\u57fa\u7ebf\u5339\u914d\uff09\u7684\u529f\u80fd\u4e92\u8865\u6027\uff0c\u6253\u7834\u4f20\u7edf\"\u66ff\u4ee3\"\u601d\u7ef4\uff0c\u5efa\u7acb\u534f\u540c\u6846\u67b6\u3002  \n\u25c6 \u8bbe\u8ba1CPU\u53cb\u597d\u7684\u5206\u5c42\u5b9a\u4f4d\u67b6\u6784\uff1a\u5b9e\u65f6\u5c42\u91c7\u7528\u624b\u5de5\u7279\u5f81\u8fdb\u884c\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5f02\u6b65\u5c42\u9009\u62e9\u6027\u8c03\u7528\u5b66\u4e60\u7279\u5f81\u8fdb\u884c\u7edd\u5bf9\u5b9a\u4f4d\uff0c\u5b9e\u73b0\u4ec5\u9700CPU\u7684\u957f\u671f\u7a33\u5b9a\u8fd0\u884c\u3002  \n\u25c6 \u521b\u65b0\u5173\u952e\u5e27\u4f18\u5316\u7b56\u7565\uff1a\u901a\u8fc7\u52a8\u6001\u7b5b\u9009\u673a\u5236\u5e73\u8861\u5b66\u4e60\u7279\u5f81\u7684\u8ba1\u7b97\u5f00\u9500\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4f7f\u7cfb\u7edf\u5728\u5149\u7167\u53d8\u5316\u4e0b\u4fdd\u630147%\u7684\u5e73\u5747\u8bef\u5dee\u964d\u4f4e\u3002  \n\u25c6 \u5b9e\u73b0\u5168\u65f6\u6bb5\u73af\u5883\u9002\u5e94\u6027\uff1a\u901a\u8fc7\u7279\u5f81\u534f\u4f5c\u6709\u6548\u5e94\u5bf9\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5b63\u8282\u66f4\u66ff\u3001\u663c\u591c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\uff0c\u5b9a\u4f4d\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u3002  \n\u25c6 \u63d0\u4f9b\u5b8c\u6574\u5f00\u6e90\u5b9e\u73b0\uff1a\u516c\u5f00\u4ee3\u7801\u5305\u542b\u7279\u5f81\u4e92\u8865\u6027\u5206\u6790\u3001\u8ba1\u7b97\u5ef6\u8fdf\u5256\u6790\u5230\u7cfb\u7edf\u7ea7\u9a8c\u8bc1\u7684\u5168\u5957\u5b9e\u9a8c\u6570\u636e\uff0c\u63a8\u52a8\u5de5\u4e1a\u5e94\u7528\u843d\u5730\u3002  \n\u25c6 \u5efa\u7acb\u4e09\u9636\u6bb5\u9a8c\u8bc1\u4f53\u7cfb\uff1a\u4ece\u7279\u5f81\u7279\u6027\u5bf9\u6bd4\u3001CPU\u5e73\u53f0\u7b97\u529b\u5256\u6790\u5230\u771f\u5b9e\u5149\u7167\u53d8\u5316\u6d4b\u8bd5\uff0c\u5f62\u6210\u4e25\u8c28\u7684\u6280\u672f\u9a8c\u8bc1\u94fe\u8def\u3002|\n",
    "2506.22336": "|2025-06-27|MatChA: Cross-Algorithm Matching with Feature Augmentation|Paula Carb\u00f3 Cubero\u7b49|[2506.22336](http://arxiv.org/pdf/2506.22336)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u9996\u4e2a\u89e3\u51b3\u8de8\u7279\u5f81\u68c0\u6d4b\u5668\u573a\u666f\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u95ee\u9898\u7684\u65b9\u6cd5MatChA\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u5fc5\u987b\u4f7f\u7528\u76f8\u540c\u68c0\u6d4b\u5668\u7684\u9650\u5236\u3002  \n\u25c6 \u901a\u8fc7\u7279\u5f81\u63cf\u8ff0\u7b26\u589e\u5f3a\u6280\u672f\u63d0\u5347\u8de8\u68c0\u6d4b\u5668\u7279\u5f81\u5339\u914d\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u70b9\u91cd\u590d\u7387\u4f4e\u548c\u63cf\u8ff0\u7b26\u533a\u5206\u5ea6\u4e0d\u8db3\u7684\u96be\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u589e\u5f3a\u540e\u7684\u7279\u5f81\u8f6c\u6362\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u7b97\u6cd5\u751f\u6210\u63cf\u8ff0\u7b26\u7684\u6709\u6548\u5339\u914d\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u7279\u5f81\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u5339\u914d\u548c\u89c6\u89c9\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6848\u4f9d\u8d56\u5171\u540c\u5173\u952e\u70b9\u7684\u5047\u8bbe\uff0c\u66f4\u8d34\u5408\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u540c\u8bbe\u5907\u4f7f\u7528\u4e0d\u540c\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\u7684\u590d\u6742\u573a\u666f\u3002|\n",
    "2506.21945": "|2025-06-27|SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images|Naftaly Wambugu\u7b49|[2506.21945](http://arxiv.org/pdf/2506.21945)|\u65e0|\u25c6 \u63d0\u51fa\u5806\u53e0\u5f0f\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff08SDRNet\uff09\uff0c\u901a\u8fc7\u53cc\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u540c\u65f6\u6355\u83b7\u957f\u7a0b\u8bed\u4e49\u5e76\u4fdd\u7559\u7a7a\u95f4\u7ec6\u8282\uff0c\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u5206\u5272\u4e2d\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002  \n\u25c6 \u5728\u7f16\u7801\u5668\u4e0e\u89e3\u7801\u5668\u4e4b\u95f4\u5f15\u5165\u81a8\u80c0\u6b8b\u5dee\u5757\uff08DRB\uff09\uff0c\u589e\u5f3a\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u80fd\u529b\uff0c\u6709\u6548\u5e94\u5bf9\u5730\u7269\u7c7b\u522b\u5dee\u5f02\u548c\u906e\u6321\u5bfc\u81f4\u7684\u7279\u5f81\u63d0\u53d6\u6311\u6218\u3002  \n\u25c6 \u901a\u8fc7\u591a\u4e0a\u4e0b\u6587\u7279\u5f81\u5b66\u4e60\u673a\u5236\uff0c\u8986\u76d6\u4e0d\u540c\u5c3a\u5bf8\u5730\u7269\u76ee\u6807\uff0c\u7f13\u89e3\u56e0\u7269\u4f53\u5c3a\u5bf8\u53d8\u5316\u5bfc\u81f4\u7684\u7ec6\u5206\u4e0d\u51c6\u95ee\u9898\u3002  \n\u25c6 \u7ed3\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u7ec6\u5c0f\u5730\u7269\u548c\u590d\u6742\u8fb9\u754c\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u514b\u670d\u4f20\u7edf\u6df1\u5ea6\u7f51\u7edc\u4e0b\u91c7\u6837\u5bfc\u81f4\u7684\u8fb9\u754c\u6a21\u7cca\u7f3a\u9677\u3002  \n\u25c6 \u5728ISPRS Vaihingen\u548cPotsdam\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u4f18\u8d8a\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u5730\u7269\u5206\u7c7b\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.07077": "|2025-07-09|Reading a Ruler in the Wild|Yimu Pan\u7b49|[2507.07077](http://arxiv.org/pdf/2507.07077)|\u65e0|\u25c6 \u63d0\u51faRulerNet\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u6807\u5c3a\u8bfb\u6570\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7edf\u4e00\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea7\u6570\u53c2\u6570\u8868\u793a\u6807\u5c3a\u523b\u5ea6\uff0c\u5b9e\u73b0\u900f\u89c6\u53d8\u6362\u4e0b\u7684\u9c81\u68d2\u6027\u6d4b\u91cf\u3002  \n\u25c6 \u91c7\u7528\u6297\u7578\u53d8\u6807\u6ce8\u548c\u8bad\u7ec3\u7b56\u7565\u76f4\u63a5\u5b9a\u4f4d\u5398\u7c73\u523b\u5ea6\uff0c\u6446\u8131\u4f20\u7edf\u65b9\u6cd5\u5bf9\u624b\u5de5\u9608\u503c\u6216\u56fa\u5b9a\u6d41\u7a0b\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u4e0d\u540c\u6807\u5c3a\u7c7b\u578b\u548c\u6210\u50cf\u6761\u4ef6\u7684\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u5f00\u53d1\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u56fe\u5f62\u5316\u6807\u5c3a\u751f\u6210\u4e0eControlNet\u6280\u672f\u6dfb\u52a0\u903c\u771f\u80cc\u666f\uff0c\u6709\u6548\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u5e76\u589e\u5f3a\u8bad\u7ec3\u591a\u6837\u6027\u3002  \n\u25c6 \u63d0\u51fa\u8f7b\u91cf\u7ea7DeepGP\u7f51\u7edc\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u6807\u8bb0\u56de\u5f52\u51e0\u4f55\u7ea7\u6570\u53c2\u6570\uff0c\u66ff\u4ee3\u4f20\u7edf\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u79fb\u52a8\u6216\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5c3a\u5ea6\u4f30\u8ba1\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eRulerNet\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u80fd\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u3001\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u5c3a\u5ea6\u4f30\u8ba1\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u3001\u6cd5\u533b\u7b49\u9886\u57df\u7684\u81ea\u52a8\u5316\u6d4b\u91cf\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.06662": "|2025-07-09|MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning|Yifan Yang\u7b49|[2507.06662](http://arxiv.org/pdf/2507.06662)|\u65e0|\u25c6 \u63d0\u51faMK-Pose\u6846\u67b6\uff0c\u9996\u6b21\u878d\u5408RGB\u56fe\u50cf\u3001\u70b9\u4e91\u6570\u636e\u548c\u7c7b\u522b\u7ea7\u6587\u672c\u63cf\u8ff0\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u63d0\u5347\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u81ea\u76d1\u7763\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u67e5\u8be2\u3001\u8f6f\u70ed\u56fe\u5339\u914d\u548c\u56fe\u5173\u7cfb\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u906e\u6321\u548c\u8de8\u5b9e\u4f8b\u6cdb\u5316\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165\u56fe\u589e\u5f3a\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u6574\u5408\u5c40\u90e8\u51e0\u4f55\u4fe1\u606f\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u589e\u5f3a\u5bf9\u590d\u6742\u573a\u666f\u7684\u5efa\u6a21\u80fd\u529b\u3002  \n\u25c6 \u5728CAMERA25\u548cREAL275\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6027\u80fd\uff0c\u65e0\u9700\u5f62\u72b6\u5148\u9a8c\u5373\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff08IoU\u548c\u5e73\u5747\u7cbe\u5ea6\u6307\u6807\uff09\u3002  \n\u25c6 \u989d\u5916\u6d4b\u8bd5\u8de8\u6570\u636e\u96c6\u80fd\u529b\uff08HouseCat6D\uff09\uff0c\u8bc1\u660e\u6a21\u578b\u5177\u5907\u5f3a\u6cdb\u5316\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u5e76\u63d0\u4f9b\u5b8c\u6574\u5b9e\u73b0\uff0c\u63a8\u52a8\u9886\u57df\u7814\u7a76\u4e0e\u5e94\u7528\u843d\u5730\u3002|\n",
    "2507.07994": "|2025-07-27|Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection|Subhajit Maity\u7b49|[2507.07994](http://arxiv.org/pdf/2507.07994)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u8349\u56fe\u7684\u5c0f\u6837\u672c\u5173\u952e\u70b9\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u4eba\u7c7b\u624b\u7ed8\u8349\u56fe\u4f5c\u4e3a\u65e0\u6e90\u6570\u636e\u66ff\u4ee3\u65b9\u6848\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u67e5\u8be2\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u65f6\u7684\u56f0\u5883\u3002  \n\u25c6 \u8bbe\u8ba1\u8de8\u6a21\u6001\u5d4c\u5165\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u6865\u63a5\u8349\u56fe\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\uff0c\u5b9e\u73b0\u8349\u56fe\u5230\u5173\u952e\u70b9\u7684\u7cbe\u51c6\u6620\u5c04\u3002  \n\u25c6 \u5f15\u5165\u7f51\u683c\u5316\u5b9a\u4f4d\u5668\uff08grid-based locator\uff09\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u7ed3\u5408\u539f\u578b\u7f51\u7edc\u4f18\u5316\u5173\u952e\u70b9\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u91c7\u7528\u539f\u578b\u57df\u9002\u5e94\u6280\u672f\uff08prototypical domain adaptation\uff09\uff0c\u81ea\u9002\u5e94\u6d88\u9664\u7528\u6237\u624b\u7ed8\u98ce\u683c\u7684\u4e2a\u4f53\u5dee\u5f02\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u5728\u8de8\u7c7b\u522b\u3001\u8de8\u5173\u952e\u70b9\u4efb\u52a1\u4e2d\u7684\u5c0f\u6837\u672c\u5feb\u901f\u6536\u655b\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u5e94\u7528\u8fb9\u754c\u3002|\n",
    "2507.11102": "|2025-07-15|KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model|Jie Yang\u7b49|[2507.11102](http://arxiv.org/pdf/2507.11102)|\u65e0|\u25c6 \u63d0\u51faKptLLM++\uff0c\u9996\u4e2a\u4e13\u7528\u4e8e\u901a\u7528\u5173\u952e\u70b9\u7406\u89e3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u6307\u4ee4\u6574\u5408\u591a\u6837\u5316\u8f93\u5165\u6a21\u6001\uff0c\u586b\u8865\u4e86MLLMs\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u6355\u6349\u4e0a\u7684\u7a7a\u767d\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\"\u5148\u8bc6\u522b\u540e\u68c0\u6d4b\"\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236\uff0c\u5148\u89e3\u6790\u5173\u952e\u70b9\u8bed\u4e49\u518d\u7cbe\u786e\u5b9a\u4f4d\uff0c\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u7406\u89e3\u80fd\u529b\u3002  \n\u25c6 \u6784\u5efa\u8d8550\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u6837\u7269\u4f53\u3001\u5173\u952e\u70b9\u7c7b\u522b\u3001\u56fe\u50cf\u98ce\u683c\u53ca\u906e\u6321\u573a\u666f\uff0c\u663e\u8457\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u6027\u3002  \n\u25c6 \u5b9e\u73b0\u8de8\u573a\u666f\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5efa\u7acb\u9ad8\u6548\u4eba\u673a\u534f\u4f5c\u63a5\u53e3\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u6790\u3001\u7269\u4f53\u68c0\u7d22\u548c\u884c\u4e3a\u8bc6\u522b\u7b49\u5e94\u7528\u3002  \n\u25c6 \u5728\u591a\u4e2a\u5173\u952e\u70b9\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u7edf\u4e00\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002  \n\u25c6 \u4e3aAI\u7406\u89e3\u7ed3\u6784\u5316\u50cf\u7d20\u7ea7\u8bed\u4e49\u4fe1\u606f\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u5bf9\u63a8\u52a8\u4eba\u673a\u4ea4\u4e92\u53d8\u9769\u5177\u6709\u91cd\u8981\u542f\u793a\u610f\u4e49\u3002|\n",
    "2507.11077": "|2025-07-15|GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft|Weizhao Ma\u7b49|[2507.11077](http://arxiv.org/pdf/2507.11077)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u5173\u952e\u70b9\u7f51\u7edcGKNet\uff0c\u5229\u7528\u5173\u952e\u70b9\u95f4\u7684\u51e0\u4f55\u7ea6\u675f\u5173\u7cfb\u63d0\u5347\u975e\u5408\u4f5c\u822a\u5929\u5668\u5355\u76ee\u59ff\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002  \n\u25c6 \u9488\u5bf9\u822a\u5929\u5668\u7ed3\u6784\u5bf9\u79f0\u6027\u548c\u5c40\u90e8\u906e\u6321\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u7f51\u7edc\u5efa\u6a21\u5173\u952e\u70b9\u62d3\u6251\u5173\u7cfb\uff0c\u589e\u5f3a\u68c0\u6d4b\u9c81\u68d2\u6027\u3002  \n\u25c6 \u6784\u5efa\u4e2d\u7b49\u89c4\u6a21\u822a\u5929\u5668\u5173\u952e\u70b9\u68c0\u6d4b\u6570\u636e\u96c6SKD\uff0c\u5305\u542b3\u79cd\u822a\u5929\u5668\u76ee\u6807\u30019\u4e07\u5f20\u4eff\u771f\u56fe\u50cf\u53ca\u9ad8\u7cbe\u5ea6\u6807\u6ce8\uff0c\u586b\u8865\u9886\u57df\u6570\u636e\u7a7a\u767d\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eGKNet\u5728\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u7a7a\u95f4\u573a\u666f\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u4e3a\u5728\u8f68\u670d\u52a1\u4efb\u52a1\uff08\u5982\u536b\u661f\u7ef4\u62a4\u3001\u592a\u7a7a\u788e\u7247\u6e05\u7406\uff09\u63d0\u4f9b\u53ef\u9760\u6280\u672f\u652f\u6491\u3002|\n",
    "2507.10770": "|2025-07-14|FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching|Ionu\u0163 Grigore\u7b49|[2507.10770](http://arxiv.org/pdf/2507.10770)|\u65e0|\u25c6\u63d0\u51faFPC-Net\uff0c\u901a\u8fc7\u7279\u5f81\u91d1\u5b57\u5854\u548c\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u9690\u5f0f\u5339\u914d\u5b9e\u73b0\u65e0\u63cf\u8ff0\u7b26\u7684\u5173\u952e\u70b9\u68c0\u6d4b\uff0c\u91cd\u65b0\u6539\u8fdb\u4e86SuperPoint\u65b9\u6cd5\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u5728\u5173\u952e\u70b9\u68c0\u6d4b\u9636\u6bb5\u76f4\u63a5\u5efa\u7acb\u5173\u8054\u6027\uff0c\u7701\u53bb\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u63cf\u8ff0\u7b26\u7684\u8ba1\u7b97\u3001\u5b58\u50a8\u3001\u4f20\u8f93\u548c\u5339\u914d\u6b65\u9aa4\u3002  \n\u25c6\u5c3d\u7ba1\u5339\u914d\u7cbe\u5ea6\u7565\u4f4e\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5b8c\u5168\u6d88\u9664\u4e86\u63cf\u8ff0\u7b26\u9700\u6c42\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u5b9a\u4f4d\u7cfb\u7edf\u7684\u5185\u5b58\u5360\u7528\u3002  \n\u25c6\u901a\u8fc7\u7279\u5f81\u91d1\u5b57\u5854\u548c\u4e00\u81f4\u6027\u5339\u914d\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8f7b\u91cf\u5316\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u4e0e\u5339\u914d\u6d41\u7a0b\u3002  \n\u25c6\u5728\u5b9e\u9a8c\u4e2d\u5bf9\u6bd4\u4e86\u4f20\u7edf\u624b\u5de5\u65b9\u6cd5\u548c\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002  \n\u25c6\u4e3a\u51e0\u4f55\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u6d01\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u3002|\n",
    "2507.13145": "|2025-07-17|DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model|Maulana Bisyir Azhari\u7b49|[2507.13145](http://arxiv.org/pdf/2507.13145)|\u65e0|\u25c6 \u63d0\u51faDINO-VO\u7cfb\u7edf\uff0c\u9996\u6b21\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578bDINOv2\u7684\u9c81\u68d2\u8bed\u4e49\u7279\u5f81\u5e94\u7528\u4e8e\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff08VO\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5b66\u4e60\u578bVO\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u9488\u5bf9DINOv2\u7279\u5f81\u7c92\u5ea6\u7c97\u7cd9\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e13\u7528\u663e\u8457\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u6709\u6548\u63d0\u5347\u7a00\u758f\u7279\u5f81\u5339\u914d\u7684\u7cbe\u5ea6\u3002  \n\u25c6 \u7ed3\u5408DINOv2\u7684\u8bed\u4e49\u7279\u5f81\u4e0e\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7279\u5f81\uff0c\u751f\u6210\u517c\u5177\u9c81\u68d2\u6027\u548c\u5c40\u90e8\u5316\u80fd\u529b\u7684\u6df7\u5408\u7279\u5f81\u8868\u793a\u3002  \n\u25c6 \u91c7\u7528\u57fa\u4e8eTransformer\u7684\u5339\u914d\u5668\u548c\u53ef\u5fae\u5206\u4f4d\u59ff\u4f30\u8ba1\u5c42\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u4f18\u5316\u7279\u5f81\u5339\u914d\u4e0e\u8fd0\u52a8\u4f30\u8ba1\u3002  \n\u25c6 \u5728TartanAir\u3001KITTI\u7b49\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4f20\u7edf\u5e27\u95f4VO\u65b9\u6cd5\uff08\u5982SuperPoint\uff09\uff0c\u5e76\u5728\u5ba4\u5916\u9a7e\u9a76\u573a\u666f\u4e2d\u4e0e\u89c6\u89c9SLAM\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u630172 FPS\u5b9e\u65f6\u6027\u3002  \n\u25c6 \u7cfb\u7edf\u5185\u5b58\u5360\u7528\u4f4e\u4e8e1GB\uff0c\u5c55\u73b0\u4e86\u9ad8\u6548\u90e8\u7f72\u6f5c\u529b\uff0c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u65f6\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2507.17327": "|2025-07-23|CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits|Chao He\u7b49|[2507.17327](http://arxiv.org/pdf/2507.17327)|\u65e0|\u25c6 \u63d0\u51faCartoonAlive\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4ece\u5355\u5f20\u8096\u50cf\u7167\u7247\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cfLive2D\u5361\u901a\u6a21\u578b\uff0c\u8017\u65f6\u4e0d\u8db330\u79d2\u3002  \n\u25c6 \u521b\u65b0\u5730\u5c063D\u4eba\u8138\u5efa\u6a21\u4e2d\u7684\u5f62\u72b6\u57fa\u6982\u5ff5\u5f15\u51652D\u9886\u57df\uff0c\u6784\u5efa\u9002\u7528\u4e8eLive2D\u7684\u9762\u90e8\u6df7\u5408\u5f62\u72b6\u7cfb\u7edf\u3002  \n\u25c6 \u901a\u8fc7\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u81ea\u52a8\u63a8\u65ad\u6df7\u5408\u5f62\u72b6\u6743\u91cd\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8868\u60c5\u9a71\u52a8\u3002  \n\u25c6 \u91c7\u7528\u5206\u5c42\u5206\u5272\u6280\u672f\u6a21\u62df3D\u8fd0\u52a8\u6548\u679c\uff0c\u5728\u4fdd\u63012D\u5361\u901a\u98ce\u683c\u7684\u540c\u65f6\u5b9e\u73b0\u7c7b\u4f3c3D\u7684\u5b9e\u65f6\u52a8\u6001\u64cd\u63a7\u3002  \n\u25c6 \u76f8\u6bd4\u4f20\u7edf3D\u5efa\u6a21\u65b9\u6848\u5927\u5e45\u964d\u4f4e\u5236\u4f5c\u6210\u672c\uff0c\u76f8\u6bd42D\u89c6\u9891\u65b9\u6848\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u7075\u6d3b\u6027\u3002  \n\u25c6 \u4e3a\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u62d3\u5c55\u4e86\u865a\u62df\u89d2\u8272\u52a8\u753b\u7684\u5e94\u7528\u573a\u666f\u3002|\n",
    "2507.16850": "|2025-07-21|Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors|Mohamed Adjel|[2507.16850](http://arxiv.org/pdf/2507.16850)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5355\u76ee3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u54082D\u5173\u952e\u70b9\u68c0\u6d4b\u4e0e\u51e0\u4f55\u611f\u77e5\u76842D\u52303D\u63d0\u5347\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u65e0\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002  \n\u25c6 \u663e\u5f0f\u5229\u7528\u76f8\u673a\u5185\u53c2\u548c\u4e2a\u6027\u5316\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u81ea\u6821\u51c6\u548c\u751f\u7269\u529b\u5b66\u7ea6\u675f\u7684\u53cd\u5411\u8fd0\u52a8\u5b66\u589e\u5f3a\u6a21\u578b\u7cbe\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u4ece\u52a8\u4f5c\u6355\u6349\u548c\u5408\u6210\u6570\u636e\u96c6\u4e2d\u751f\u6210\u5927\u89c4\u6a21\u5408\u7406\u76842D-3D\u8bad\u7ec3\u5bf9\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002  \n\u25c6 \u6846\u67b6\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u3001\u4e2a\u6027\u5316\u7684\u9ad8\u7cbe\u5ea63D\u59ff\u6001\u4f30\u8ba1\uff0c\u5177\u6709\u5f3a\u90e8\u7f72\u9002\u5e94\u6027\u3002  \n\u25c6 \u878d\u5408\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u4e0e\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u63d0\u5347\u51c6\u786e\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u80fd\u529b\u3002|\n",
    "2507.18551": "|2025-07-24|A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration|Daniil Morozov\u7b49|[2507.18551](http://arxiv.org/pdf/2507.18551)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b3D\u8de8\u6a21\u6001\u5173\u952e\u70b9\u63cf\u8ff0\u7b26\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3MRI\u4e0e\u5b9e\u65f6\u8d85\u58f0(iUS)\u4e4b\u95f4\u7684\u914d\u51c6\u96be\u9898\uff0c\u514b\u670d\u4e86\u4e24\u79cd\u6a21\u6001\u5728\u5916\u89c2\u3001\u5206\u8fa8\u7387\u548c\u89c6\u91ce\u4e0a\u7684\u663e\u8457\u5dee\u5f02\u3002  \n\u25c6\u91c7\u7528\u60a3\u8005\u7279\u5f02\u6027\u7684\u5408\u6210\u5339\u914d\u65b9\u6cd5\uff0c\u4ece\u672f\u524dMRI\u751f\u6210\u5408\u6210iUS\u4f53\u79ef\uff0c\u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u5171\u4eab\u63cf\u8ff0\u7b26\u7a7a\u95f4\uff0c\u589e\u5f3a\u4e86\u8de8\u6a21\u6001\u5339\u914d\u80fd\u529b\u3002  \n\u25c6\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6982\u7387\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u7b56\u7565\uff0c\u80fd\u591f\u8bc6\u522b\u89e3\u5256\u5b66\u663e\u8457\u4e14\u6a21\u6001\u4e00\u81f4\u7684\u4f4d\u7f6e\uff0c\u63d0\u9ad8\u4e86\u5173\u952e\u70b9\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002  \n\u25c6\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u8bfe\u7a0b\u5f0f\u4e09\u5143\u7ec4\u635f\u5931\u548c\u52a8\u6001\u96be\u8d1f\u6837\u672c\u6316\u6398\uff0c\u4f7f\u63cf\u8ff0\u7b26\u5bf9iUS\u4f2a\u5f71\uff08\u5982\u6591\u70b9\u566a\u58f0\u548c\u6709\u9650\u8986\u76d6\uff09\u5177\u6709\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u65cb\u8f6c\u4e0d\u53d8\u6027\u3002  \n\u25c6\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u901a\u8fc7\u7a00\u758f\u5339\u914d\u5b9e\u73b0\u521a\u6027\u914d\u51c6\uff0c\u65e0\u9700\u4eba\u5de5\u521d\u59cb\u5316\uff0c\u4e14\u5728ReMIND\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u5e73\u5747\u5339\u914d\u7cbe\u5ea6\u8fbe69.8%\uff0c\u914d\u51c6\u8bef\u5dee\u4f4e\u81f32.39 mm\u3002  \n\u25c6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5bf9iUS\u89c6\u91ce\u53d8\u5316\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002|\n",
    "2507.19118": "|2025-07-25|Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching|Abu Sadat Mohammad Salehin Amit\u7b49|[2507.19118](http://arxiv.org/pdf/2507.19118)|\u65e0|\u25c6\u63d0\u51fa\u8de8\u65f6\u7a7a\u878d\u5408\u6ce8\u610f\u529b\u673a\u5236(CSTF)\uff0c\u901a\u8fc7\u72ec\u7acb\u68c0\u6d4b\u53c2\u8003\u56fe\u50cf\u548c\u67e5\u8be2\u56fe\u50cf\u4e2d\u7684\u5c3a\u5ea6\u4e0d\u53d8\u5173\u952e\u70b9\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u95f4\u51e0\u4f55\u548c\u8f90\u5c04\u5dee\u5f02\u5927\u7684\u95ee\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u6784\u5efa\u5bf9\u5e94\u5173\u7cfb\u56fe\uff0c\u540c\u65f6\u5229\u7528\u591a\u4e2a\u56fe\u50cf\u533a\u57df\u7684\u4fe1\u606f\uff0c\u6709\u6548\u6355\u6349\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\uff0c\u514b\u670d\u4f20\u7edf\u5168\u8fde\u63a5\u5c42\u7279\u5f81\u63d0\u53d6\u7684\u5c40\u9650\u6027\u3002  \n\u25c6\u5c06\u76f8\u4f3c\u6027\u5339\u914d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed3\u5408SoftMax\u548c\u5168\u5377\u79ef\u7f51\u7edc(FCN)\u5c42\uff0c\u5728\u4fdd\u6301\u5c40\u90e8\u7279\u5f81\u654f\u611f\u6027\u7684\u540c\u65f6\u6574\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002  \n\u25c6\u5728HRSC2016\u548cDOTA\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5e73\u5747mAP\u5206\u522b\u8fbe\u523090.99%\u548c90.86%\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002  \n\u25c6\u4fdd\u630112.5 FPS\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u7cbe\u5ea6\u7684\u540c\u65f6\u5177\u5907\u5b9e\u9645\u5e94\u7528\u7684\u9ad8\u6548\u6027\u3002  \n\u25c6\u9a8c\u8bc1\u4e86\u6539\u8fdb\u7684\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u80fd\u76f4\u63a5\u63d0\u5347\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u9065\u611f\u5206\u6790\u63d0\u4f9b\u65b0\u601d\u8def\u3002|\n",
    "2507.23461": "|2025-07-31|Mitigating Resolution-Drift in Federated Learning: Case of Keypoint Detection|Taeheon Lim\u7b49|[2507.23461](http://arxiv.org/pdf/2507.23461)|\u65e0|\u25c6 \u9996\u6b21\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u63d0\u51fa\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\"\u5206\u8fa8\u7387\u6f02\u79fb\"\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5206\u8fa8\u7387\u5dee\u5f02\u4f5c\u4e3a\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u65b0\u7ef4\u5ea6\u5bf9\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u7684\u91cd\u8981\u5f71\u54cd\u3002  \n\u25c6 \u63d0\u51fa\u5206\u8fa8\u7387\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60\uff08RAF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u70ed\u56fe\u7684\u591a\u5206\u8fa8\u7387\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u6559\u5e08\u6a21\u578b\u548c\u4f4e\u5206\u8fa8\u7387\u5b66\u751f\u6a21\u578b\u95f4\u4f20\u9012\u77e5\u8bc6\uff0c\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u5206\u8fa8\u7387\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u9ad8\u4f4e\u5206\u8fa8\u7387\u53cc\u5411\u84b8\u998f\u7b56\u7565\uff0c\u65e2\u907f\u514d\u4e86\u4f4e\u5206\u8fa8\u7387\u5ba2\u6237\u7aef\u8fc7\u62df\u5408\uff0c\u53c8\u4fdd\u7559\u4e86\u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u7ec6\u8282\u7279\u5f81\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u975e\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u3002  \n\u25c6 \u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660eRAF\u4e0d\u4ec5\u80fd\u663e\u8457\u7f13\u89e3\u5206\u8fa8\u7387\u6f02\u79fb\uff08\u6700\u9ad8\u63d0\u534723.6%\u51c6\u786e\u7387\uff09\uff0c\u8fd8\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5177\u6709\u5f3a\u5b9e\u7528\u6027\u3002  \n\u25c6 \u901a\u8fc7t-SNE\u53ef\u89c6\u5316\u5206\u6790\uff0c\u9996\u6b21\u63ed\u793a\u4e86\u5206\u7c7b\u4efb\u52a1\u4e0e\u9ad8\u5206\u8fa8\u7387\u8868\u5f81\u4efb\u52a1\u5728\u7279\u5f81\u5206\u5e03\u4e0a\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u4e3aRAF\u65b9\u6cd5\u6269\u5c55\u5230\u5176\u4ed6\u9700\u8981\u4fdd\u6301\u7a7a\u95f4\u7ec6\u8282\u7684\u4efb\u52a1\uff08\u5982\u533b\u7597\u5f71\u50cf\u5206\u6790\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002  \n\u25c6 \u5f00\u8f9f\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7b49\u975e\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u89e3\u51b3\u8de8\u8bbe\u5907\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5206\u8fa8\u7387\u5f02\u6784\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2508.05514": "|2025-08-07|Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking|Zewei Wu\u7b49|[2508.05514](http://arxiv.org/pdf/2508.05514)|\u65e0|\u25c6 \u63d0\u51fa\u878d\u5408\u76ee\u6807\u68c0\u6d4b\u5668\u56de\u5f52\u5206\u652f\u548c\u5206\u7c7b\u5206\u652f\u7279\u5f81\u7684\u53cc\u91cd\u7279\u5f81\u589e\u5f3a\u7b56\u7565\uff0c\u5c06\u7a7a\u95f4\u4f4d\u7f6e\u4fe1\u606f\u76f4\u63a5\u5d4c\u5165\u7279\u5f81\u8868\u793a\uff0c\u63d0\u5347\u5916\u89c2\u5efa\u6a21\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165\u5934\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u5229\u7528\u5934\u90e8\u4e0d\u6613\u88ab\u906e\u6321\u7684\u7279\u6027\uff0c\u6709\u6548\u7f13\u89e3\u5bc6\u96c6\u573a\u666f\u4e2d\u5168\u8eab\u7279\u5f81\u4e22\u5931\u5bfc\u81f4\u7684\u8ddf\u8e2a\u5931\u6548\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u8fed\u4ee3\u5f0f\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fd0\u52a8\u6a21\u578b\uff0c\u7a81\u7834\u4f20\u7edf\u7ebf\u6027\u5300\u901f\u5047\u8bbe\uff0c\u7ed3\u54083D\u573a\u666f\u5148\u9a8c\u77e5\u8bc6\u5b9e\u73b0\u590d\u6742\u906e\u6321\u4e0b\u7684\u8f68\u8ff9\u8865\u5168\u3002  \n\u25c6 \u9996\u6b21\u5c06\u68c0\u6d4b\u4efb\u52a1\u7684\u591a\u7ef4\u5ea6\u7279\u5f81\uff08\u5206\u7c7b/\u56de\u5f52/\u5173\u952e\u70b9\uff09\u4e0e\u6539\u8fdb\u8fd0\u52a8\u6a21\u578b\u8054\u5408\u4f18\u5316\uff0c\u5f62\u6210\u5916\u89c2-\u8fd0\u52a8\u534f\u540c\u589e\u5f3a\u7684\u8ddf\u8e2a\u6846\u67b6\u3002  \n\u25c6 \u9488\u5bf9\u4e25\u91cd\u906e\u6321\u573a\u666f\uff0c\u901a\u8fc7\u5934\u90e8\u5b9a\u4f4d\u4e0e\u5168\u8eab\u68c0\u6d4b\u7684\u5f02\u6784\u7279\u5f81\u4e92\u8865\uff0c\u663e\u8457\u63d0\u5347\u5bc6\u96c6\u4eba\u7fa4\u7684\u8f68\u8ff9\u8fde\u7eed\u6027\u548cID\u4fdd\u6301\u80fd\u529b\u3002  \n\u25c6 \u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5bf9\u91cd\u53e0\u7387\u8d85\u8fc770%\u7684\u6781\u7aef\u906e\u6321\u60c5\u51b5\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edfRe-ID\u65b9\u6848\u7684\u8ddf\u8e2a\u7a33\u5b9a\u6027\u3002|\n",
    "2508.07112": "|2025-08-16|AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation|Nikolai Warner\u7b49|[2508.07112](http://arxiv.org/pdf/2508.07112)|\u65e0|\u25c6 \u63d0\u51faAugLift\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u4f46\u6709\u6548\u7684\u8f93\u5165\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e2D\u5173\u952e\u70b9\u63d0\u5347\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u4f20\u611f\u5668\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5728\u6807\u51c62D\u5173\u952e\u70b9\u5750\u6807(x,y)\u57fa\u7840\u4e0a\uff0c\u7a00\u758f\u5730\u589e\u52a0\u4e86\u5173\u952e\u70b9\u68c0\u6d4b\u7f6e\u4fe1\u5ea6c\u548c\u5bf9\u5e94\u6df1\u5ea6\u4f30\u8ba1d\u4e24\u4e2a\u4fe1\u53f7\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u8fd9\u4e9b\u4fe1\u53f7\uff0c\u7ee7\u627f\u4e86\u5b83\u4eec\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u65b9\u6cd5\u5177\u6709\u6a21\u5757\u5316\u7279\u6027\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684\u5404\u79cd\u63d0\u5347\u67b6\u6784\u4e2d\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u3002  \n\u25c6 \u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAugLift\u5c06\u672a\u89c1\u6570\u636e\u96c6\u7684\u8de8\u6570\u636e\u96c6\u6027\u80fd\u5e73\u5747\u63d0\u534710.1%\uff0c\u540c\u65f6\u5c06\u5206\u5e03\u5185\u6027\u80fd\u63d0\u53474.0%\u3002  \n\u25c6 \u5206\u6790\u8868\u660e\uff0c\u8fd9\u4e9b\u7a00\u758f\u7684\u5173\u952e\u70b9\u5bf9\u9f50\u7ebf\u7d22\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u5e27\u7ea7\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e3a\u63d0\u5347\u4efb\u4f55\u57fa\u4e8e\u63d0\u5347\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002  \n\u25c6 \u6240\u6709\u4ee3\u7801\u5c06\u516c\u5f00\uff0c\u4fbf\u4e8e\u7814\u7a76\u793e\u533a\u4f7f\u7528\u548c\u590d\u73b0\u3002|\n",
    "2508.09949": "|2025-08-13|Stable Diffusion Models are Secretly Good at Visual In-Context Learning|Trevine Oorloff\u7b49|[2508.09949](http://arxiv.org/pdf/2508.09949)|\u65e0|\u25c6 \u63ed\u793a\u4e86\u73b0\u6210Stable Diffusion\u6a21\u578b\u5177\u5907\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u6f5c\u529b\uff0c\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002  \n\u25c6 \u63d0\u51fa\u539f\u4f4d\u6ce8\u610f\u529b\u91cd\u8ba1\u7b97\u673a\u5236\uff0c\u901a\u8fc7\u6539\u9020\u81ea\u6ce8\u610f\u529b\u5c42\u663e\u5f0f\u878d\u5408\u67e5\u8be2\u4e0e\u793a\u4f8b\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002  \n\u25c6 \u9996\u6b21\u5b9e\u73b0\u5355\u4e00\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u516d\u5927\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u524d\u666f\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\u7b49\uff09\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u9700\u5b9a\u5236\u5316\u8bad\u7ec3\u7684\u5c40\u9650\u3002  \n\u25c6 \u5728Pascal-5i\u6570\u636e\u96c6\u4e0a\uff0c\u524d\u666f\u5206\u5272\u4efb\u52a1mIoU\u6307\u6807\u5206\u522b\u8d85\u8d8aVisual Prompting\u548cIMProv\u65b9\u6cd58.9%\u548c3.2%\u3002  \n\u25c6 \u901a\u8fc7\u96c6\u6210\u591a\u63d0\u793a\u6837\u672c\u63d0\u5347\u4efb\u52a1\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u4e0a\u4e0b\u6587\u793a\u4f8b\u63d0\u5347\u6027\u80fd\u3002  \n\u25c6 \u4e3a\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u8f7b\u91cf\u5316\u65b0\u8303\u5f0f\uff0c\u4ec5\u9700\u4fee\u6539\u6ce8\u610f\u529b\u673a\u5236\u4e14\u4fdd\u6301\u6a21\u578b\u6743\u91cd\u51bb\u7ed3\uff0c\u663e\u8457\u63d0\u5347\u901a\u7528\u6027\u3002|\n",
    "2508.10942": "|2025-08-13|Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram|Liming Xu\u7b49|[2508.10942](http://arxiv.org/pdf/2508.10942)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7279\u5f81\u63cf\u8ff0\u7b26\u2014\u2014\u65b9\u5411\u76f4\u65b9\u56fe\u5f62\u72b6\uff08shape of orientation histogram\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0Artcode\u7684\u901a\u7528\u62d3\u6251\u7ed3\u6784\u3002  \n\u25c6 \u5c06Artcode\u8bc6\u522b\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3aArtcode\u63d0\u6848\u68c0\u6d4b\u4efb\u52a1\uff0c\u5c06\u62d3\u6251\u76f8\u4f3c\u4f46\u51e0\u4f55\u548c\u8bed\u4e49\u4e0d\u540c\u7684\u5bf9\u8c61\u5f52\u4e3a\u540c\u4e00\u7c7b\u522b\u3002  \n\u25c6 \u6784\u5efa\u4e86\u4e13\u95e8\u7684\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7279\u5f81\u5411\u91cf\u5728\u8868\u793a\u62d3\u6251\u7ed3\u6784\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u57fa\u4e8e\u8be5\u7279\u5f81\u5411\u91cf\u7684Artcode\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u68c0\u6d4b\u6548\u679c\u663e\u8457\u3002  \n\u25c6 \u9996\u6b21\u5c1d\u8bd5\u5f00\u53d1\u57fa\u4e8e\u7279\u5f81\u7684\u62d3\u6251\u5bf9\u8c61\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4e3aArtcode\u7b49\u62d3\u6251\u5bf9\u8c61\u7684\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002  \n\u25c6 \u8be5\u7814\u7a76\u4e3a\u865a\u5b9e\u4ea4\u4e92\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\uff0c\u5e76\u5c55\u793a\u4e86\u62d3\u6251\u5bf9\u8c61\u68c0\u6d4b\u7684\u6f5c\u5728\u5e94\u7528\u524d\u666f\u3002|\n",
    "2508.12216": "|2025-08-17|Splat Feature Solver|Butian Xiong\u7b49|[2508.12216](http://arxiv.org/pdf/2508.12216)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u4e0e\u6838\u51fd\u6570\u53ca\u7279\u5f81\u65e0\u5173\u7684\u7279\u5f81\u63d0\u5347\u95ee\u9898\u7a00\u758f\u7ebf\u6027\u9006\u95ee\u9898\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u53ef\u901a\u8fc7\u95ed\u5f0f\u89e3\u9ad8\u6548\u6c42\u89e3\u3002  \n\u25c6 \u5728\u51f8\u635f\u5931\u51fd\u6570\u4e0b\u63d0\u4f9b\u4e86\u5168\u5c40\u6700\u4f18\u8bef\u5dee\u7684\u53ef\u8bc1\u660e\u4e0a\u754c\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u7279\u5f81\u63d0\u5347\u7ed3\u679c\u3002  \n\u25c6 \u5f15\u5165\u4e24\u79cd\u4e92\u8865\u7684\u6b63\u5219\u5316\u7b56\u7565\uff08Tikhonov Guidance\u548cPost-Lifting Aggregation\uff09\u4ee5\u89e3\u51b3\u591a\u89c6\u89d2\u89c2\u6d4b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002  \n\u25c6 Tikhonov Guidance\u901a\u8fc7\u8f6f\u5bf9\u89d2\u4f18\u52bf\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027\uff0cPost-Lifting Aggregation\u901a\u8fc7\u7279\u5f81\u805a\u7c7b\u8fc7\u6ee4\u566a\u58f0\u8f93\u5165\u3002  \n\u25c6 \u5728\u5f00\u653e\u8bcd\u6c473D\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u8bad\u7ec3\u3001\u5206\u7ec4\u548c\u542f\u53d1\u5f0f\u7684\u524d\u6cbf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u7279\u5f81\u63d0\u5347\u4ec5\u9700\u6570\u5206\u949f\u5b8c\u6210\u3002|\n",
    "2508.15300": "|2025-08-21|Mag-Match: Magnetic Vector Field Features for Map Matching and Registration|William McDonald\u7b49|[2508.15300](http://arxiv.org/pdf/2508.15300)|\u65e0|Mag-Match\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e09\u7ef4\u78c1\u573a\u77e2\u91cf\u7279\u5f81\u8fdb\u884c\u5730\u56fe\u5339\u914d\u4e0e\u6ce8\u518c\u7684\u65b0\u65b9\u6cd5\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u78c1\u529b\u8ba1\u6570\u636e\u63d0\u53d6\u9ad8\u9636\u5bfc\u6570\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u89c6\u89c9\u6216\u6fc0\u5149\u4f20\u611f\u5668\u5931\u6548\u7684\u70df\u96fe\u3001\u7c89\u5c18\u7b49\u6076\u52a3\u73af\u5883\u5177\u6709\u9c81\u68d2\u6027\u3002  \n\u25c6 \u6240\u63d0\u51fa\u7684\u78c1\u573a\u7279\u5f81\u63cf\u8ff0\u5b50\u5177\u6709\u5168\u5c40\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u91cd\u529b\u65b9\u5411\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540c\u5730\u56fe\u6216\u4e0d\u540c\u673a\u5668\u4eba\u6570\u636e\u4e4b\u95f4\u7684\u914d\u51c6\u7075\u6d3b\u6027\u3002  \n\u25c6 \u91c7\u7528\u7269\u7406\u4fe1\u606f\u9ad8\u65af\u8fc7\u7a0b\u8fdb\u884c\u6982\u7387\u63a8\u7406\uff0c\u80fd\u591f\u4ece\u79bb\u6563\u70b9\u4e91\u6570\u636e\u4e2d\u9ad8\u6548\u3001\u9012\u5f52\u5730\u63a8\u65ad\u6574\u4e2a\u5730\u56fe\u7684\u78c1\u573a\u53ca\u5176\u9ad8\u9636\u5bfc\u6570\uff0c\u5b9e\u73b0\u4e86\u5bf9\u78c1\u573a\u573a\u7684\u8fde\u7eed\u5efa\u6a21\u3002  \n\u25c6 \u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5730\u56fe-\u5730\u56fe\u3001\u673a\u5668\u4eba-\u5730\u56fe\u548c\u673a\u5668\u4eba-\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u7cbe\u786e\u53d8\u6362\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8eSIFT\u7684\u4f20\u7edf\u65b9\u6cd5\u3002|\n",
    "2508.16465": "|2025-08-25|HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images|Anilkumar Swamy\u7b49|[2508.16465](http://arxiv.org/pdf/2508.16465)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u540d\u4e3aHOSt3R\u7684\u65e0\u9700\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u624b-\u7269\u4f53\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u6446\u8131\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u7684\u3001\u76f4\u63a5\u4ece\u5355\u76ee\u8fd0\u52a8\u89c6\u9891\u4f30\u8ba1\u624b-\u7269\u4f53\u4e09\u7ef4\u53d8\u6362\u7684\u9c81\u68d2\u65b9\u6cd5\u3002\n\u25c6 \u6446\u8131\u4e86\u5bf9\u9884\u626b\u63cf\u7269\u4f53\u6a21\u677f\u6216\u5df2\u77e5\u76f8\u673a\u5185\u53c2\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u66f4\u901a\u7528\u548c\u65e0\u7ea6\u675f\u7684\u5e94\u7528\u3002\n\u25c6 \u5c06\u6240\u63d0\u51fa\u7684\u53d8\u6362\u4f30\u8ba1\u65b9\u6cd5\u4e0e\u591a\u89c6\u56fe\u91cd\u5efa\u6d41\u7a0b\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u624b-\u7269\u4f53\u4e09\u7ef4\u5f62\u72b6\u6062\u590d\u3002\n\u25c6 \u5728SHOWMe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u624b-\u7269\u4f53\u4e09\u7ef4\u53d8\u6362\u548c\u5f62\u72b6\u4f30\u8ba1\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\n\u25c6 \u5728HO3D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u522b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002|\n",
    "2508.17746": "|2025-08-25|DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning|Seo-Bin Hwang\u7b49|[2508.17746](http://arxiv.org/pdf/2508.17746)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDroneKey\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u5e8f\u5217\u4e2d\u7cbe\u786e\u4f30\u8ba1\u65e0\u4eba\u673a\u7684\u4e09\u7ef4\u4f4d\u59ff\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5173\u952e\u70b9\u68c0\u6d4b\u4e2d\u56e0\u87ba\u65cb\u6868\u89c6\u89c9\u76f8\u4f3c\u6027\u9ad8\u548c\u59ff\u6001\u591a\u6837\u6027\u5927\u800c\u5bfc\u81f4\u7684\u96be\u9898\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e8c\u7ef4\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u548c\u4e09\u7ef4\u4f4d\u59ff\u4f30\u8ba1\u5668\u7684\u4e13\u7528\u6846\u67b6\uff0c\u9488\u5bf9\u65e0\u4eba\u673a\u7279\u6027\u8fdb\u884c\u4f18\u5316\u3002  \n\u25c6 \u5728\u5173\u952e\u70b9\u68c0\u6d4b\u9636\u6bb5\uff0c\u521b\u65b0\u6027\u5730\u4ece\u6bcf\u4e2aTransformer\u7f16\u7801\u5668\u5c42\u63d0\u53d6\u4e24\u79cd\u5173\u952e\u8868\u793a\uff08\u4e2d\u95f4\u8868\u793a\u548c\u7d27\u51d1\u8868\u793a\uff09\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u6c42\u548c\u8fdb\u884c\u6700\u4f18\u878d\u5408\u3002  \n\u25c6 \u5f15\u5165\u4e86\u59ff\u6001\u81ea\u9002\u5e94\u7684\u9a6c\u6c0f\u8ddd\u79bb\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6781\u7aef\u59ff\u6001\u4e0b\u5173\u952e\u70b9\u9884\u6d4b\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002  \n\u25c6 \u6784\u5efa\u5e76\u516c\u5f00\u4e86\u65b0\u7684\u65e0\u4eba\u673a\u4e8c\u7ef4\u5173\u952e\u70b9\u53ca\u4e09\u7ef4\u4f4d\u59ff\u6570\u636e\u96c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002  \n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5173\u952e\u70b9\u68c0\u6d4b\u4e2d\u8fbe\u5230\u4e8699.68%\u7684AP\uff08OKS\u6307\u6807\uff09\uff0c\u4e09\u7ef4\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u6781\u4f4e\uff08\u89d2\u5ea6MAE\u4e3a10.62\u5ea6\uff0c\u4f4d\u7f6eRMSE\u4e3a0.221\u7c73\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u4e8644 FPS\u7684\u5b9e\u65f6\u5904\u7406\u901f\u5ea6\u3002|\n",
    "2508.20830": "|2025-08-28|Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation|Krit Duangprom\u7b49|[2508.20830](http://arxiv.org/pdf/2508.20830)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u624b\u672f\u5de5\u5177\u4e8c\u7ef4\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u8fdb\u884c\u5fae\u8c03\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5728\u5c0f\u578b\u533b\u7597\u6570\u636e\u96c6\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u5957\u7cbe\u5fc3\u6784\u5efa\u7684\u63d0\u793a\u6a21\u677f\uff0c\u6784\u5efa\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u4e49\u5173\u952e\u70b9\u63cf\u8ff0\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002  \n\u25c6 \u4ec5\u9700\u4e24\u4e2a\u8bad\u7ec3\u5468\u671f\u5373\u53ef\u663e\u8457\u8d85\u8d8a\u4f20\u7edfCNN\u6216Transformer\u57fa\u7ebf\u6a21\u578b\uff0c\u663e\u793a\u51fa\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u6027\u80fd\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u4e3a\u540e\u7eed\u4e09\u7ef4\u624b\u672f\u5668\u68b0\u53ca\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u62d3\u5c55\u4e86VLM\u5728\u533b\u7597\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2509.08712": "|2025-09-10|Computational Imaging for Enhanced Computer Vision|Humera Shaikh\u7b49|[2509.08712](http://arxiv.org/pdf/2509.08712)|\u65e0|\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u8ba1\u7b97\u6210\u50cf\u6280\u672f\u5982\u4f55\u63d0\u5347\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002  \n\u25c6 \u7cfb\u7edf\u68b3\u7406\u4e86\u8ba1\u7b97\u6210\u50cf\u6280\u672f\uff08\u5982\u5149\u573a\u6210\u50cf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u6210\u50cf\u3001\u53bb\u6a21\u7cca\u3001\u9ad8\u901f\u6210\u50cf\u548c\u7729\u5149\u6291\u5236\uff09\u5728\u63d0\u5347\u56fe\u50cf\u91c7\u96c6\u4e0e\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u7684\u4f5c\u7528\u3002  \n\u25c6 \u6df1\u5165\u5206\u6790\u4e86\u8ba1\u7b97\u6210\u50cf\u6280\u672f\u4e0e\u6838\u5fc3\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u6df1\u5ea6\u4f30\u8ba1\u3001\u5149\u6d41\u4f30\u8ba1\u3001\u4eba\u8138\u8bc6\u522b\u548c\u5173\u952e\u70b9\u68c0\u6d4b\uff09\u4e4b\u95f4\u7684\u534f\u540c\u5173\u7cfb\u3002  \n\u25c6 \u5f3a\u8c03\u4e86\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u6210\u50cf\u6d41\u7a0b\u7684\u6f5c\u529b\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5728\u81ea\u4e3b\u5bfc\u822a\u3001\u76d1\u63a7\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002  \n\u25c6 \u6307\u51fa\u4e86\u8be5\u9886\u57df\u65b0\u5174\u7684\u7814\u7a76\u673a\u9047\u4e0e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002  \n\u8bba\u6587\u901a\u8fc7\u8de8\u9886\u57df\u6574\u5408\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002|\n",
    "2509.09758": "|2025-09-11|A Path Signature Framework for Detecting Creative Fatigue in Digital Advertising|Charles Shaw|[2509.09758](http://arxiv.org/pdf/2509.09758)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u5f84\u7b7e\u540d\u5206\u6790\u7684\u6570\u5b57\u5e7f\u544a\u521b\u610f\u75b2\u52b3\u68c0\u6d4b\u65b0\u6846\u67b6\u3002  \n\u25c6\u9996\u6b21\u5c06\u968f\u673a\u5206\u6790\u4e2d\u7684\u8def\u5f84\u7b7e\u540d\u65b9\u6cd5\u5e94\u7528\u4e8e\u5e7f\u544a\u75b2\u52b3\u68c0\u6d4b\u9886\u57df\uff0c\u5f00\u8f9f\u4e86\u8425\u9500\u5206\u6790\u4e2d\u51e0\u4f55\u65b9\u6cd5\u7684\u65b0\u9014\u5f84\u3002  \n\u25c6\u5c06\u5e7f\u544a\u6027\u80fd\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u4e8c\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\uff0c\u5229\u7528\u5176\u7b7e\u540d\u4f5c\u4e3a\u4e30\u5bcc\u7684\u7279\u5f81\u63cf\u8ff0\u7b26\u4ee5\u6355\u6349\u52a8\u6001\u53d8\u5316\u3002  \n\u25c6\u901a\u8fc7\u8ba1\u7b97\u8fde\u7eed\u65f6\u95f4\u7a97\u53e3\u7b7e\u540d\u95f4\u7684\u8ddd\u79bb\uff0c\u8bc6\u522b\u6027\u80fd\u52a8\u6001\u4e2d\u7edf\u8ba1\u663e\u8457\u7684\u53d8\u66f4\u70b9\uff0c\u7075\u654f\u5ea6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002  \n\u25c6\u5c06\u7edf\u8ba1\u68c0\u6d4b\u7ed3\u679c\u8f6c\u5316\u4e3a\u76f4\u63a5\u8d22\u52a1\u6307\u6807\uff0c\u91cf\u5316\u6301\u7eed\u6295\u8d44\u75b2\u52b3\u521b\u610f\u7684\u673a\u4f1a\u6210\u672c\uff0c\u63d0\u5347\u51b3\u7b56\u5b9e\u7528\u6027\u3002  \n\u25c6\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u548c\u6848\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u6570\u5b66\u539f\u7406\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5e7f\u544a\u75b2\u52b3\u5206\u6790\u63d0\u4f9b\u4e86\u4e92\u8865\u6027\u65b0\u5de5\u5177\u3002|\n",
    "2509.11731": "|2025-09-15|Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference|Yudong Shen\u7b49|[2509.11731](http://arxiv.org/pdf/2509.11731)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u8f68\u8ff9\u6570\u636e\u5730\u56fe\u63a8\u65ad\u4e2d\u7a00\u758f\u533a\u57df\u9053\u8def\u65ad\u88c2\u548c\u5bc6\u96c6\u533a\u57df\u5197\u4f59\u6bb5\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DGMap\u53cc\u89e3\u7801\u6846\u67b6\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u63d0\u51fa\u591a\u5c3a\u5ea6\u7f51\u683c\u7f16\u7801\u65b9\u6cd5\uff0c\u6574\u5408\u5168\u5c40\u8bed\u4e49\u4e0a\u4e0b\u6587\u4e0e\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u63d0\u5347\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6\u3002\n\u25c6 \u8bbe\u8ba1\u63a9\u7801\u589e\u5f3a\u5173\u952e\u70b9\u63d0\u53d6\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u7a00\u758f\u8f68\u8ff9\u533a\u7684\u9053\u8def\u65ad\u88c2\u95ee\u9898\u3002\n\u25c6 \u5f15\u5165\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u5173\u7cfb\u9884\u6d4b\u6a21\u5757\uff0c\u901a\u8fc7\u5efa\u6a21\u957f\u8f68\u8ff9\u6a21\u5f0f\u6291\u5236\u5bc6\u96c6\u533a\u57df\u7684\u9519\u8bef\u8fde\u63a5\u3002\n\u5b9e\u9a8c\u8868\u660e\uff0cDGMap\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0aAPLS\u6307\u6807\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd55%\uff0c\u5c24\u5176\u5728\u6ef4\u6ef4\u5e73\u53f0\u6570\u636e\u4e0a\u8868\u73b0\u7a81\u51fa\u3002|\n",
    "2509.21926": "|2025-09-26|PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning|Jiahao Zhang\u7b49|[2509.21926](http://arxiv.org/pdf/2509.21926)|\u65e0|\u25c6 Visual In-Context Learning (VICL) uses input-output image pairs, referred to as in-context pairs (or examples), as prompts alongside query images to guide models in performing diverse vision tasks.\n\u25c6 However, VICL often suffers from over-reliance on a single in-context pair, which can lead to biased and unstable predictions.\n\u25c6 We introduce PAtch-based $k$-Nearest neighbor visual In-Context Learning (PANICL), a general training-free framework that mitigates this issue by leveraging multiple in-context pairs.|\n",
    "2510.00083": "|2025-09-30|Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks|Hanjiang Hu\u7b49|[2510.00083](http://arxiv.org/pdf/2510.00083)|\u65e0|\u25c6 Deep neural networks have been widely adopted in many vision and robotics applications with visual inputs.\n\u25c6 It is essential to verify its robustness against semantic transformation perturbations, such as brightness and contrast.\n\u25c6 However, current certified training and robustness certification methods face the challenge of over-parameterization, which hinders the tightness and scalability due to the over-complicated neural networks.|\n",
    "2510.17422": "|2025-10-21|DeepDetect: Learning All-in-One Dense Keypoints|Shaharyar Ahmed Khan Tareen\u7b49|[2510.17422](http://arxiv.org/pdf/2510.17422)|\u65e0|\u25c6 Keypoint detection is the foundation of many computer vision tasks, including image registration, structure-from motion, 3D reconstruction, visual odometry, and SLAM.\n\u25c6 Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong performance yet suffer from key limitations: sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding, often failing to prioritize visually important regions.\n\u25c6 We present DeepDetect, an intelligent, all-in-one, dense keypoint detector that unifies the strengths of classical detectors using deep learning.|\n",
    "2510.18539": "|2025-10-21|GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization|Du\u0161an Mali\u0107\u7b49|[2510.18539](http://arxiv.org/pdf/2510.18539)|\u65e0|\u25c6 This technical report outlines the top-ranking solution for RoboSense 2025: Track 3, achieving state-of-the-art performance on 3D object detection under various sensor placements.\n\u25c6 Our submission utilizes GBlobs, a local point cloud feature descriptor specifically designed to enhance model generalization across diverse LiDAR configurations.\n\u25c6 Current LiDAR-based 3D detectors often suffer from a \\enquote{geometric shortcut} when trained on conventional global features (\\ie, absolute Cartesian coordinates).|\n",
    "2510.22618": "|2025-10-26|Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation|Mackenzie Tapp\u7b49|[2510.22618](http://arxiv.org/pdf/2510.22618)|\u65e0|\u25c6 Pose estimation serves as a cornerstone of computer vision for understanding animal posture, behavior, and welfare.\n\u25c6 Yet, agricultural applications remain constrained by the scarcity of large, annotated datasets for livestock, especially dairy cattle.\n\u25c6 This study evaluates the potential and limitations of cross-species transfer learning by adapting ZebraPose - a vision transformer-based model trained on synthetic zebra imagery - for 27-keypoint detection in dairy cows under real barn conditions.|\n",
    "2510.22454": "|2025-10-25|SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET Tomography Using Semi-Supervised Learning|Linhan Wang\u7b49|[2510.22454](http://arxiv.org/pdf/2510.22454)|\u65e0|\u25c6 Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging (SVA) is the only imaging modality capable of resolving protein structures inside cells at molecular resolution.\n\u25c6 Particle picking, the task of localizing and classifying target proteins in 3D CryoET volumes, remains the main bottleneck.\n\u25c6 Due to the reliance on time-consuming manual labels, the vast reserve of unlabeled tomograms remains underutilized.|\n",
    "2511.04126": "|2025-11-06|Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)|Venkata Manikanta Desu\u7b49|[2511.04126](http://arxiv.org/pdf/2511.04126)|\u65e0|\u25c6 This study presents a complete pipeline for automated tennis match analysis.\n\u25c6 Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference.\n\u25c6 Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times.|\n",
    "2511.05893": "|2025-11-08|Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition|Hongxia Li\u7b49|[2511.05893](http://arxiv.org/pdf/2511.05893)|\u65e0|\u25c6 Low-rank sparse regression models have been widely applied in the field of face recognition.\n\u25c6 To further address the challenges caused by complex occlusions and illumination variations, this paper proposes a Hybrid Second-Order Gradient Histogram based Global Low-Rank Sparse Regression (H2H-GLRSR) model.\n\u25c6 Specifically, a novel feature descriptor called the Hybrid Second-Order Gradient Histogram (H2H) is first designed to more effectively characterize the local structural features of facial images.|\n",
    "2504.00139": "|2025-10-01|SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM|Yannick Burkhardt\u7b49|[2504.00139](http://arxiv.org/pdf/2504.00139)|\u65e0|\u25c6 Event-based keypoint detection and matching holds significant potential, enabling the integration of event sensors into highly optimized Visual SLAM systems developed for frame cameras over decades of research.\n\u25c6 Unfortunately, existing approaches struggle with the motion-dependent appearance of keypoints and the complex noise prevalent in event streams, resulting in severely limited feature matching capabilities and poor performance on downstream tasks.\n\u25c6 To mitigate this problem, we propose SuperEvent, a data-driven approach to predict stable keypoints with expressive descriptors.|\n",
    "2511.09506": "|2025-11-12|A thermoinformational framework for the description of neuropsychological systems|George-Rafael Domenikos\u7b49|[2511.09506](http://arxiv.org/pdf/2511.09506)|\u65e0|\u25c6 This work presents a statistical thermodynamics-inspired framework that summarizes multichannel EEG and behavior using macroscopic state variables (entropy, internal energy, temperature, Helmholtz free energy) to quantify stability and reconfiguration in neuropsychological systems.\n\u25c6 Applied to mother-infant EEG dyads performing the A-not-B task, these variables dissociate neural reconfiguration from behavioral success across a large set of model and feature configurations.\n\u25c6 Informational heat increases during environmental switches and decision errors, consistent with increased information exchange with the task context.|\n",
    "2511.09245": "|2025-11-12|Hydrogen permeability prediction in palladium alloys and virtual screening of B2-phase stabilized Pd(100-x-y)CuxMy ternary alloys using machine learning|Eric Kolor\u7b49|[2511.09245](http://arxiv.org/pdf/2511.09245)|\u65e0|\u25c6 We present a forward prediction material screening framework designed to discover Pd-Cu alloys with improved B2 phase stability, thereby unlocking simultaneous $H_2$ generation and utilization.\n\u25c6 First, we trained CatBoost models with literature-derived Pd alloy data to predict $H_2$ permeability from composition and testing conditions.\n\u25c6 We evaluated fractional, composition-based, and physics-informed descriptors, individually and in combination, and showed that sequential Pearson filtering and fold-wise SHAP-based recursive feature elimination with cross-fold aggregation reduced errors while controlling complexity.|\n",
    "2511.08833": "|2025-11-11|Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms|Jiaxun Guo\u7b49|[2511.08833](http://arxiv.org/pdf/2511.08833)|\u65e0|\u25c6 Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations.\n\u25c6 However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures.\n\u25c6 We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries.|\n",
    "2511.06749": "|2025-11-10|Semi-distributed Cross-modal Air-Ground Relative Localization|Weining Lu\u7b49|[2511.06749](http://arxiv.org/pdf/2511.06749)|\u65e0|\u25c6 Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks.\n\u25c6 However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy.\n\u25c6 To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework.|\n",
    "2511.06422": "|2025-11-09|DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization|Tao Liu\u7b49|[2511.06422](http://arxiv.org/pdf/2511.06422)|\u65e0|\u25c6 With the rapid growth of the low-altitude economy, unmanned aerial vehicles (UAVs) have become key platforms for measurement and tracking in intelligent patrol systems.\n\u25c6 However, in GNSS-denied environments, localization schemes that rely solely on satellite signals are prone to failure.\n\u25c6 Cross-view image retrieval-based localization is a promising alternative, yet substantial geometric and appearance domain gaps exist between oblique UAV views and nadir satellite orthophotos.|\n",
    "2511.06024": "|2025-11-08|Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era|Feng Lu\u7b49|[2511.06024](http://arxiv.org/pdf/2511.06024)|\u65e0|\u25c6 Visual place recognition (VPR) is typically regarded as a specific image retrieval task, whose core lies in representing images as global descriptors.\n\u25c6 Over the past decade, dominant VPR methods (e.g., NetVLAD) have followed a paradigm that first extracts the patch features/tokens of the input image using a backbone, and then aggregates these patch features into a global descriptor via an aggregator.\n\u25c6 This backbone-plus-aggregator paradigm has achieved overwhelming dominance in the CNN era and remains widely used in transformer-based models.|\n",
    "2511.05750": "|2025-11-07|From Quantum Annealing to Alloy Discovery: Towards Accelerated Design of High-Entropy Alloys|Diego Ibarra-Hoyos\u7b49|[2511.05750](http://arxiv.org/pdf/2511.05750)|\u65e0|\u25c6 Data scarcity remains a central challenge in materials discovery, where finding meaningful descriptors and tuning models for generalization is critical but inherently a discrete optimization problem prone to multiple local minima confounding the true optimal state.\n\u25c6 Classical methods often get trapped in these minima, while quantum annealing can escape them via quantum fluctuations, including tunneling, that overcome narrow energy barriers.\n\u25c6 We present a quantum-assisted machine-learning (QaML) framework that employs quantum annealing to address these combinatorial optimization challenges through feature selection, support-vector training formulated in QUBO form for classification and regression, and a new QUBO-based neural-network pruning formulation.|\n",
    "2511.05404": "|2025-11-07|Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments|Laura Alejandra Encinar Gonzalez\u7b49|[2511.05404](http://arxiv.org/pdf/2511.05404)|\u65e0|\u25c6 Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration.\n\u25c6 In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity.\n\u25c6 This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments.|\n",
    "2511.04190": "|2025-11-06|Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification|Josef Mayr\u7b49|[2511.04190](http://arxiv.org/pdf/2511.04190)|\u65e0|\u25c6 Covariance descriptors capture second-order statistics of image features.\n\u25c6 They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging.\n\u25c6 We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices.|\n",
    "2511.11284": "|2025-11-14|Interpretable descriptors enable prediction of hydrogen-based superconductors at moderate pressures|Jiawei Chen\u7b49|[2511.11284](http://arxiv.org/pdf/2511.11284)|\u65e0|\u25c6 Room temperature superconductivity remains elusive, and hydrogen-base compounds despite remarkable transition temperatures(Tc) typically require extreme pressures that hinder application.\n\u25c6 To accelerate discovery under moderate pressures, an interpretable framework based on symbolic regression is developed to predict Tc in hydrogen-based superconductors.\n\u25c6 A key descriptor is an integrated density of states (IDOS) within 1 eV of the Fermi level (EF), which exhibits greater robustness than conventional single-point DOS features.|\n",
    "2511.13208": "|2025-11-17|End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer|Yonghui Yu\u7b49|[2511.13208](http://arxiv.org/pdf/2511.13208)|\u65e0|\u25c6 Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation.\n\u25c6 This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency.\n\u25c6 In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations.|\n",
    "2511.13170": "|2025-11-17|THIR: Topological Histopathological Image Retrieval|Zahra Tabatabaei\u7b49|[2511.13170](http://arxiv.org/pdf/2511.13170)|\u65e0|\u25c6 According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020.\n\u25c6 Early diagnosis and accurate clinical decision making are critical in reducing this global burden.\n\u25c6 In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns.|\n",
    "2511.13168": "|2025-11-17|SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration|Haodong Wang\u7b49|[2511.13168](http://arxiv.org/pdf/2511.13168)|\u65e0|\u25c6 Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics.\n\u25c6 Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory.\n\u25c6 Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences.|\n",
    "2511.12268": "|2025-11-15|Multimodal RGB-HSI Feature Fusion with Patient-Aware Incremental Heuristic Meta-Learning for Oral Lesion Classification|Rupam Mukherjee\u7b49|[2511.12268](http://arxiv.org/pdf/2511.12268)|\u65e0|\u25c6 Early detection of oral cancer and potentially malignant disorders is challenging in low-resource settings due to limited annotated data.\n\u25c6 We present a unified four-class oral lesion classifier that integrates deep RGB embeddings, hyperspectral reconstruction, handcrafted spectral-textural descriptors, and demographic metadata.\n\u25c6 A pathologist-verified subset of oral cavity images was curated and processed using a fine-tuned ConvNeXt-v2 encoder, followed by RGB-to-HSI reconstruction into 31-band hyperspectral cubes.|\n",
    "2511.12241": "|2025-11-15|AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos|Junhyuk Seo\u7b49|[2511.12241](http://arxiv.org/pdf/2511.12241)|\u65e0|\u25c6 Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death.\n\u25c6 Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data.\n\u25c6 We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset.|\n",
    "2511.14335": "|2025-11-23|Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors|Jeryes Danial\u7b49|[2511.14335](http://arxiv.org/pdf/2511.14335)|\u65e0|\u25c6 Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera.\n\u25c6 Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive.\n\u25c6 Monocular SLAM also faces scale ambiguities, which affect its accuracy.|\n",
    "2511.14210": "|2025-11-19|Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution|N Dinesh Reddy\u7b49|[2511.14210](http://arxiv.org/pdf/2511.14210)|\u65e0|\u25c6 We introduce Orion, a visual agent framework that can take in any modality and generate any modality.\n\u25c6 Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results.\n\u25c6 Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows.|\n",
    "2511.14109": "|2025-11-18|$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors|Zhenyu Li\u7b49|[2511.14109](http://arxiv.org/pdf/2511.14109)|\u65e0|\u25c6 Visual Place Recognition (VPR) aims to match query images against a database using visual cues.\n\u25c6 State-of-the-art methods aggregate features from deep backbones to form global descriptors.\n\u25c6 Optimal transport-based aggregation methods reformulate feature-to-cluster assignment as a transport problem, but the standard Sinkhorn algorithm symmetrically treats source and target marginals, limiting effectiveness when image features and cluster centers exhibit substantially different distributions.|\n",
    "2511.13960": "|2025-11-17|Classification of Aortic Shape with Topographical Pair Correlation Functions|Cooper Bruno\u7b49|[2511.13960](http://arxiv.org/pdf/2511.13960)|\u65e0|\u25c6 Quantitative descriptors convert high-dimensional medical images into low-dimensional features capable of differentiating organ shapes that correlate with injury or disease progression for diagnostic purposes.\n\u25c6 An important example is aortic dissections, which can be imaged using high-resolution CT scans and for which the shape of the true and false lumens of the aorta has long been used to predict disease state and the potential for positive surgical outcomes (namely thoracic endovascular repair or TEVAR).\n\u25c6 Here we present a method for calculating the topographical pair correlation function (TPCF), a descriptor of the spatial correlation of point estimates for Gaussian curvature, mean curvature, shape index, and bending ratio constrained to the surface of a meshed image.|\n",
    "2511.15963": "|2025-11-20|PySERA: Open-Source Standardized Python Library for Automated, Scalable, and Reproducible Handcrafted and Deep Radiomics|Mohammad R. Salmanpour\u7b49|[2511.15963](http://arxiv.org/pdf/2511.15963)|\u65e0|\u25c6 Radiomics enables the extraction of quantitative biomarkers from medical images for precision modeling, but reproducibility and scalability remain limited due to heterogeneous software implementations and incomplete adherence to standards.\n\u25c6 Existing tools also lack unified support for deep learning based radiomics.\n\u25c6 To address these limitations, we introduce PySERA, an open source, Python native, standardized radiomics framework designed for automation, reproducibility, and seamless AI integration.|\n",
    "2511.17164": "|2025-11-21|Teager-Kaiser Energy Methods For EEG Feature Extraction In Biomedical Applications|Ioanna Chourdaki\u7b49|[2511.17164](http://arxiv.org/pdf/2511.17164)|\u65e0|\u25c6 Electroencephalography (EEG) signals are inherently non-linear, non-stationary, and vulnerable to noise sources, making the extraction of discriminative features a long-standing challenge.\n\u25c6 In this work, we investigate the non-linear Teager-Kaiser Energy Operator (TKEO) for modeling the underlying energy dynamics of EEG in three representative tasks: motor imagery, emotion recognition, and epilepsy detection.\n\u25c6 To accommodate the narrowband nature of the operator, we employ Gabor filterbanks to isolate canonical frequency bands, followed by the Energy Separation Algorithm to decompose the TKEO output into amplitude envelope and instantaneous frequency components.|\n",
    "2511.17158": "|2025-11-21|Exploring the added value of pretherapeutic MR descriptors in predicting breast cancer pathologic complete response to neoadjuvant chemotherapy|Caroline Malhaire\u7b49|[2511.17158](http://arxiv.org/pdf/2511.17158)|\u65e0|\u25c6 Objectives: To evaluate the association between pretreatment MRI descriptors and breast cancer (BC) pathological complete response (pCR) to neoadjuvant chemotherapy (NAC).\n\u25c6 Materials \\& Methods: Patients with BC treated by NAC with a breast MRI between 2016 and 2020 were included in this retrospective observational single-center study.\n\u25c6 MR studies were described using the standardized BI-RADS and breast edema score on T2-weighted MRI.|\n",
    "2511.20469": "|2025-11-25|Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features|Ben Hamscher\u7b49|[2511.20469](http://arxiv.org/pdf/2511.20469)|\u65e0|\u25c6 Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories.\n\u25c6 Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns.\n\u25c6 This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos.|\n",
    "2511.20250": "|2025-11-25|Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation|Daniel Kienzle\u7b49|[2511.20250](http://arxiv.org/pdf/2511.20250)|\u65e0|\u25c6 Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world.\n\u25c6 This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video.\n\u25c6 To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task.|\n",
    "2511.21666": "|2025-11-26|Uncertainty Quantification for Visual Object Pose Estimation|Lorenzo Shaikewitz\u7b49|[2511.21666](http://arxiv.org/pdf/2511.21666)|\u65e0|\u25c6 Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning.\n\u25c6 Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions.\n\u25c6 We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting.|\n",
    "2511.21452": "|2025-11-26|Semantic-Enhanced Feature Matching with Learnable Geometric Verification for Cross-Modal Neuron Registration|Wenwei Li\u7b49|[2511.21452](http://arxiv.org/pdf/2511.21452)|\u65e0|\u25c6 Accurately registering in-vivo two-photon and ex-vivo fluorescence micro-optical sectioning tomography images of individual neurons is critical for structure-function analysis in neuroscience.\n\u25c6 This task is profoundly challenging due to a significant cross-modality appearance gap, the scarcity of annotated data and severe tissue deformations.\n\u25c6 We propose a novel deep learning framework to address these issues.|\n",
    "2511.20956": "|2025-11-26|BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model|Rawa Mohammed\u7b49|[2511.20956](http://arxiv.org/pdf/2511.20956)|\u65e0|\u25c6 Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models.\n\u25c6 We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision.\n\u25c6 BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations.|\n",
    "2511.23169": "|2025-11-28|Quantum spectroscopy of topological dynamics via a supersymmetric Hamiltonian|Hiroshi Yamauchi\u7b49|[2511.23169](http://arxiv.org/pdf/2511.23169)|\u65e0|\u25c6 Topological data analysis (TDA) characterizes complex dynamics through global invariants, but classical computation becomes prohibitive for high-dimensional data.\n\u25c6 We reinterpret time-domain dynamics as the eigenvalue spectrum of a supersymmetric (SUSY) Hamiltonian and thereby estimate topological descriptors through quantum spectroscopy.\n\u25c6 While zero modes correspond to Betti numbers, we show that low-lying excited states quantify the stability of topological features.|\n",
    "2511.22739": "|2025-11-27|All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning|Amir Mohammad Ezzati\u7b49|[2511.22739](http://arxiv.org/pdf/2511.22739)|\u65e0|\u25c6 Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers.\n\u25c6 Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources.\n\u25c6 However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations.|\n",
    "2511.22256": "|2025-11-27|UMind-VL: A Generalist Ultrasound Vision-Language Model for Unified Grounded Perception and Comprehensive Interpretation|Dengbo Chen\u7b49|[2511.22256](http://arxiv.org/pdf/2511.22256)|\u65e0|\u25c6 Despite significant strides in medical foundation models, the ultrasound domain lacks a comprehensive solution capable of bridging low-level Ultrasound Grounded Perception (e.g., segmentation, localization) and high-level Ultrasound Comprehensive Interpretation (e.g., diagnosis, reasoning).\n\u25c6 To bridge this gap, we propose UMind-VL, a unified foundation model designed to synergize pixel-level structural understanding with complex clinical reasoning.\n\u25c6 We first introduce UMind-DS, a large-scale multimodal dataset comprising 1.2 million ultrasound image-text pairs across 16 anatomical regions, enriching standard data with pixel-level annotations and clinician-validated rationales.|\n",
    "2511.22195": "|2025-11-27|3D Affordance Keypoint Detection for Robotic Manipulation|Zhiyang Liu\u7b49|[2511.22195](http://arxiv.org/pdf/2511.22195)|\u65e0|\u25c6 This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality.\n\u25c6 The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question.\n\u25c6 To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent.|\n",
    "2511.21760": "|2025-11-24|fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding|Yuxiang Wei\u7b49|[2511.21760](http://arxiv.org/pdf/2511.21760)|\u65e0|\u25c6 Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored.\n\u25c6 Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations.\n\u25c6 To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework.|\n",
    "2512.01611": "|2025-12-01|Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager|Fengfeng Li\u7b49|[2512.01611](http://arxiv.org/pdf/2512.01611)|\u65e0|\u25c6 In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction.\n\u25c6 This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm.\n\u25c6 The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment.|\n",
    "2512.01479": "|2025-12-01|Non-Markovian dynamics in ice nucleation|Pablo Montero de Hijes\u7b49|[2512.01479](http://arxiv.org/pdf/2512.01479)|\u65e0|\u25c6 In simulation studies of crystallisation, the size of the largest crystalline nucleus is often used as a reaction coordinate to monitor the progress of the nucleation process.\n\u25c6 Here, we investigate, for the case of homogeneous ice nucleation, whether the nucleus size exhibits Markovian dynamics, as assumed in classical nucleation theory.\n\u25c6 Using 300 independent nucleation trajectories generated by molecular dynamics, we evaluate the mean recurrence time required to reach selected values of the largest nucleus size.|\n",
    "2512.00927": "|2025-11-30|LAHNet: Local Attentive Hashing Network for Point Cloud Registration|Wentao Qu\u7b49|[2512.00927](http://arxiv.org/pdf/2512.00927)|\u65e0|\u25c6 Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features.\n\u25c6 However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness.\n\u25c6 In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors.|\n",
    "2512.00563": "|2025-11-29|Explainable Multi-Modal Deep Learning for Automatic Detection of Lung Diseases from Respiratory Audio Signals|S M Asiful Islam Saky\u7b49|[2512.00563](http://arxiv.org/pdf/2512.00563)|\u65e0|\u25c6 Respiratory diseases remain major global health challenges, and traditional auscultation is often limited by subjectivity, environmental noise, and inter-clinician variability.\n\u25c6 This study presents an explainable multimodal deep learning framework for automatic lung-disease detection using respiratory audio signals.\n\u25c6 The proposed system integrates two complementary representations: a spectral-temporal encoder based on a CNN-BiLSTM Attention architecture, and a handcrafted acoustic-feature encoder capturing physiologically meaningful descriptors such as MFCCs, spectral centroid, spectral bandwidth, and zero-crossing rate.|\n",
    "2512.00521": "|2025-11-29|Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction|Sabrina Islam\u7b49|[2512.00521](http://arxiv.org/pdf/2512.00521)|\u65e0|\u25c6 In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role.\n\u25c6 Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound.\n\u25c6 To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings.|\n",
    "2512.00292": "|2025-11-29|Interpretable Graph Neural Networks for Classifying Structure and Magnetism in Delafossite Compounds|Jovin Ryan Joseph\u7b49|[2512.00292](http://arxiv.org/pdf/2512.00292)|\u65e0|\u25c6 Delafossites (ABC2, where A and B are metals and C is a chalcogen) are a versatile family of quantum materials and layered oxides/chalcogenides whose properties are highly sensitive to atomic composition and stacking geometry.\n\u25c6 Their broad chemical tunability makes them an ideal platform for large-scale combinatorial exploration and high-throughput computational screening with desirable quantum properties.\n\u25c6 In this work, we employ a Concept Whitening Graph Neural Network, a gray-box AI model, to classify delafossite structures by stacking sequence and magnetic states.|\n",
    "2512.02783": "|2025-12-02|Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces|Bj\u00f6rn \u00de\u00f3r J\u00f3nsson\u7b49|[2512.02783](http://arxiv.org/pdf/2512.02783)|\u65e0|\u25c6 Digital sound synthesis presents the opportunity to explore vast parameter spaces containing millions of configurations.\n\u25c6 Quality diversity (QD) evolutionary algorithms offer a promising approach to harness this potential, yet their success hinges on appropriate sonic feature representations.\n\u25c6 Existing QD methods predominantly employ handcrafted descriptors or supervised classifiers, potentially introducing unintended exploration biases and constraining discovery to familiar sonic regions.|\n",
    "2512.03880": "|2025-12-03|Leveraging topological data analysis to estimate bone strength from micro-CT as a surrogate for advanced imaging|John Rick Manzanares\u7b49|[2512.03880](http://arxiv.org/pdf/2512.03880)|\u65e0|\u25c6 Accurate bone strength prediction is essential for assessing fracture risk, particularly in aging populations and individuals with osteoporosis.\n\u25c6 Bone imaging has evolved from X-rays and DXA to clinical computed tomography (CT), and now to advanced modalities such as high-resolution peripheral quantitative CT and synchrotron radiation CT, which offer unprecedented resolution of bone microarchitecture.\n\u25c6 However, analytical methods have not kept pace with these imaging advances.|\n",
    "2512.03715": "|2025-12-03|DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction|Kaichen Zhang\u7b49|[2512.03715](http://arxiv.org/pdf/2512.03715)|\u65e0|\u25c6 This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.\n\u25c6 The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.\n\u25c6 DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.|\n",
    "2512.03684": "|2025-12-03|A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection|Shahid Ansari\u7b49|[2512.03684](http://arxiv.org/pdf/2512.03684)|\u65e0|\u25c6 This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping.\n\u25c6 The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting.\n\u25c6 For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination.|\n",
    "2512.03660": "|2025-12-03|Linking Aneurysmal Geometry and Hemodynamics Using Computational Fluid Dynamics|Spyridon C. Katsoudas\u7b49|[2512.03660](http://arxiv.org/pdf/2512.03660)|\u65e0|\u25c6 The development and progression of abdominal aortic aneurysms (AAA) are related to complex flow patterns and wall-shear-driven mechanobiological stimuli, yet the quantitative relationship between aneurysmal geometry and hemodynamics remains poorly defined.\n\u25c6 In this study, we conducted a comprehensive hemodynamic analysis of 74 patient-specific abdominal aortas, representing one of the largest Computational Fluid Dynamics (CFD) cohorts reported to date.\n\u25c6 A multiscale framework coupling 0D-1D systemic circulation models with 3D stabilized finite-element simulations is used to generate physiologically consistent boundary conditions and high-fidelity flow fields.|\n",
    "2512.03598": "|2025-12-03|Memory-Guided Point Cloud Completion for Dental Reconstruction|Jianan Sun\u7b49|[2512.03598](http://arxiv.org/pdf/2512.03598)|\u65e0|\u25c6 Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures.\n\u25c6 We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines.\n\u25c6 After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding.|\n",
    "2512.04282": "|2025-12-03|Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer|Tasmiah Haque\u7b49|[2512.04282](http://arxiv.org/pdf/2512.04282)|\u65e0|\u25c6 Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty.\n\u25c6 To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods.\n\u25c6 While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity.|\n",
    "2512.07712": "|2025-12-08|UnCageNet: Tracking and Pose Estimation of Caged Animal|Sayak Dutta\u7b49|[2512.07712](http://arxiv.org/pdf/2512.07712)|\u65e0|\u25c6 Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions.\n\u25c6 We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames.\n\u25c6 Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods.|\n"
  },
  "Image Matching": {
    "2504.19458": "|**2025-05-15**|**Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective**|Taoyu Su et.al.|[2504.19458](http://arxiv.org/abs/2504.19458)|**[link](https://github.com/sutaoyu/CDMEA)**|\n",
    "2505.03422": "|2025-05-06|LiftFeat: 3D Geometry-Aware Local Feature Matching|Yepeng Liu\u7b49|[2505.03422](http://arxiv.org/pdf/2505.03422)|[\u4ee3\u7801](https://github.com/lyp-deeplearning/liftfeat)|\u25c6 \u63d0\u51faLiftFeat\u8f7b\u91cf\u7f51\u7edc\uff0c\u901a\u8fc7\u878d\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u751f\u6210\u7684\u4f2a\u8868\u9762\u6cd5\u7ebf\u7279\u5f81\u4e0e\u539f\u59cb2D\u63cf\u8ff0\u7b26\uff0c\u589e\u5f3a\u7279\u5f81\u5339\u914d\u5728\u5149\u7167\u53d8\u5316\u3001\u5f31\u7eb9\u7406\u7b49\u6781\u7aef\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u8bbe\u8ba13D\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u63d0\u5347\u6a21\u5757\uff0c\u5229\u7528\u8868\u9762\u6cd5...|\n",
    "2505.03836": "|2025-05-04|OBD-Finder: Explainable Coarse-to-Fine Te...|Chongsheng Zhang\u7b49|[2505.03836](http://arxiv.org/pdf/2505.03836)|[\u4ee3\u7801](https://github.com/cszhanglmu/obd-finder)|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u7532\u9aa8\u6587\u91cd\u590d\u7247\u53d1\u73b0\u6846\u67b6\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u4f4e\u5c42\u5173\u952e\u70b9\u5339\u914d\u4e0e\u9ad8\u5c42\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\u7684\u5185\u5bb9\u5339\u914d\uff0c\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u7684\u5019\u9009\u6392\u5e8f\u3002\u25c6\u5728\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728Top-5\u548cTop-15\u68c0\u7d22\u7ed3\u679c\u4e2d\u53d6...|\n",
    "2505.02161": "|**2025-05-04**|**Focus What Matters: Matchability-Based Reweighting for Local Feature Matching**|Dongyue Li et.al.|[2505.02161](http://arxiv.org/abs/2505.02161)|null|\n",
    "2505.07375": "|2025-05-12|Boosting Global-Local Feature Matching via Anomaly...|Yuqi Cheng\u7b49|[2505.07375](http://arxiv.org/pdf/2505.07375)|[\u4ee3\u7801](https://github.com/hustCYQ/GLFM-Multi-class-3DAD)|\u25c6\u63d0\u51faGLFM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u5339\u914d\u89e3\u51b3\u591a\u7c7b\u522b\u70b9\u4e91\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u7279\u5f81\u6df7\u6dc6\u95ee\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u5f02\u5e38\u5408\u6210\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3001\u5efa\u7acb\u6297\u6df7\u6dc6\u7684\u5168\u5c40-\u5c40\u90e8\u8bb0\u5fc6\u5e93\u3001\u57fa\u4e8e\u7279\u5f81\u8ddd\u79bb\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e...|\n",
    "2505.11264": "|2025-05-16|Multi-view dense image matching with similarity learning and geometry priors|Mohamed Ali Chebbi\u7b49|[2505.11264](http://arxiv.org/pdf/2505.11264)|\u65e0|\u25c6\u63d0\u51faMV-DeepSimNets\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u591a\u89c6\u56fe\u76f8\u4f3c\u6027\u5b66\u4e60\u4e0e\u6781\u7ebf\u51e0\u4f55\u5148\u9a8c\u7ed3\u5408\uff0c\u65e0\u9700\u7e41\u7410\u7684\u591a\u89c6\u56fe\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u5f15\u5165\u5728\u7ebf\u51e0\u4f55\u5148\u9a8c\uff0c\u901a\u8fc7\u6781\u7ebf\u7ea6\u675f\u6216\u5355\u5e94\u6027\u6821\u6b63\u52a8\u6001\u5efa\u6a21\u50cf\u7d20\u5173\u7cfb\uff0c\u751f\u6210\u51e0\u4f55\u611f\u77e5\u7684\u7279\u5f81\u8868\u793a\u3002  \n\u25c6\u91c7\u7528\u5e73\u9762\u626b\u63cf\u6cd5\u5c06\u51e0\u4f55\u7279\u5f81\u6295\u5f71\u5230\u5019\u9009\u6df1\u5ea6\u5047\u8bbe\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u51e0\u4f55\u6761\u4ef6\u5316\u7279\u5f81\u9002\u914d\uff0c\u63d0\u5347\u591a\u89c6\u56fe\u91cd\u5efa\u7cbe\u5ea6\u3002  \n\u25c6\u901a\u8fc7\u805a\u5408\u5b66\u4e60\u5230\u7684\u76f8\u4f3c\u6027\u6784\u5efa\u5e76\u6b63\u5219\u5316\u4ee3\u4ef7\u4f53\uff0c\u76f8\u6bd4\u4f20\u7edf\u7a20\u5bc6\u5339\u914d\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u53ef\u540c\u65f6\u9002\u7528\u4e8e\u822a\u7a7a\u5f71\u50cf\u548c\u536b\u661f\u5f71\u50cf\uff08\u4e0d\u540c\u5730\u9762\u91c7\u6837\u8ddd\u79bb\uff09\uff0c\u6027\u80fd\u8d85\u8d8a\u4e3b\u6d41\u76f8\u4f3c\u6027\u5b66\u4e60\u7f51\u7edc\u548c\u7aef\u5230\u7aef\u56de\u5f52\u6a21\u578b\u3002  \n\u25c6\u5b8c\u6574\u96c6\u6210\u81f3MicMac\u5f00\u6e90\u8f6f\u4ef6\uff0c\u53ef\u76f4\u63a5\u517c\u5bb9\u6807\u51c6\u591a\u5206\u8fa8\u7387\u5f71\u50cf\u5339\u914d\u6d41\u7a0b\uff0c\u5177\u5907\u5de5\u7a0b\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2505.22458": "|2025-06-05|Universal Domain Adaptation for Semantic Segmentation|Seun-An Choe\u7b49|[2505.22458](http://arxiv.org/pdf/2505.22458)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u901a\u7528\u9886\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\uff08UniDA-SS\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u5ffd\u7565\u7c7b\u522b\u8bbe\u7f6e\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u5176\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51faUniDA-SS\u6846\u67b6\uff0c\u9996\u6b21\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u65e0\u9700\u9884\u77e5\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u7c7b\u522b\u8bbe\u7f6e\u7684\u901a\u7528\u9886\u57df\u81ea\u9002\u5e94\u3002  \n\u25c6 \u8bbe\u8ba1Domain-Specific Prototype-based Distinction\uff08DSPD\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u6bcf\u7c7b\u5212\u5206\u4e3a\u4e24\u4e2a\u57df\u7279\u5b9a\u539f\u578b\uff0c\u589e\u5f3a\u8de8\u57df\u5171\u6709\u7c7b\u522b\u7684\u7279\u5f81\u533a\u5206\u80fd\u529b\u3002  \n\u25c6 \u5f00\u53d1Target-based Image Matching\uff08TIM\uff09\u7b56\u7565\uff0c\u57fa\u4e8e\u76ee\u6807\u57df\u4f2a\u6807\u7b7e\u5339\u914d\u6700\u4f73\u6e90\u57df\u56fe\u50cf\u8fdb\u884c\u6279\u91cf\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u5171\u6709\u7c7b\u522b\u7684\u5b66\u4e60\u6548\u679c\u3002  \n\u25c6 \u6784\u5efa\u65b0\u7684UniDA-SS\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eUniMAP\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002|\n",
    "2505.17973": "|2025-05-23|To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models|Simone Gaisbauer\u7b49|[2505.17973](http://arxiv.org/pdf/2505.17973)|[\u4ee3\u7801](https://github.com/simbauer/to_glue_or_not_to_glue)|\u25c6 \u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4f20\u7edf\u624b\u5de5\u7279\u5f81\u5339\u914d\uff08\u5982SIFT+RANSAC\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u5728\u8bed\u4e493D\u5efa\u7b51\u6a21\u578b\u76f8\u673a\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002  \n\u25c6 \u9488\u5bf9\u79fb\u52a8\u6d4b\u7ed8\u76f8\u673a\u4e0e\u7eb9\u7406\u5316CityGML LoD2\u6a21\u578b\u7684\u5339\u914d\u573a\u666f\uff0c\u63d0\u51fa\u5b9a\u5236\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002  \n\u25c6 \u7ed3\u5408\u6807\u51c6\u6570\u636e\u96c6\uff08HPatches\u3001MegaDepth-1500\uff09\u548c\u81ea\u5efa\u6570\u636e\u96c6\uff08\u542b\u5730\u9762/\u65e0\u4eba\u673a\u62cd\u6444\u7684\u7acb\u9762\u7eb9\u7406\u4e0e\u5bf9\u5e94\u5f71\u50cf\uff09\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u666e\u9002\u6027\u3002  \n\u25c6 \u901a\u8fc7PnP\u7b97\u6cd5\u91cf\u5316\u7edd\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5229\u7528\u5730\u7406\u53c2\u8003\u8f68\u8ff9\u6570\u636e\u751f\u6210\u51e0\u4f55\u771f\u503c\uff0c\u5efa\u7acb\u5ba2\u89c2\u8bc4\u4f30\u57fa\u51c6\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u5b66\u4e60\u5f0f\u7279\u5f81\u5339\u914d\u5728\u6311\u6218\u6027\u573a\u666f\uff08RANSAC\u5185\u70b9\u65700-12\u3001AUC 0-0.16\uff09\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u548c\u9c81\u68d2\u6027\u63d0\u5347\u660e\u663e\u3002  \n\u25c6 \u516c\u5f00\u4ee3\u7801\u5e93\u4fc3\u8fdb\u6a21\u578b\u5316\u89c6\u89c9\u5b9a\u4f4d\u6280\u672f\u53d1\u5c55\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53ef\u590d\u73b0\u57fa\u7840\u3002|\n",
    "2505.24305": "|2025-06-20|SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping|Mingxu Zhang\u7b49|[2505.24305](http://arxiv.org/pdf/2505.24305)|\u65e0|\u25c6\u63d0\u51faSR3D\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u5355\u89c6\u89d2\u7684\u900f\u660e\u4e0e\u955c\u9762\u7269\u4f533D\u91cd\u5efa\u4e0e\u6293\u53d6\uff0c\u7a81\u7834\u4f20\u7edf\u6df1\u5ea6\u611f\u77e5\u9650\u5236\u3002  \n\u25c6\u5229\u7528\u5916\u90e8\u89c6\u89c9\u6a21\u578b\u76f4\u63a5\u4eceRGB\u56fe\u50cf\u751f\u6210\u7269\u4f53\u7f51\u683c\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u5b9e\u73b03D\u573a\u666f\u878d\u5408\uff0c\u907f\u514d\u590d\u6742\u591a\u89c6\u89d2\u91c7\u96c6\u7cfb\u7edf\u3002  \n\u25c6\u521b\u65b0\u6027\u63d0\u51fa\u89c6\u56fe\u5339\u914d\u4e0e\u5173\u952e\u70b9\u5339\u914d\u53cc\u673a\u5236\uff0c\u8054\u54082D\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u4fe1\u606f\u7cbe\u51c6\u5b9a\u4f4d\u7269\u4f53\u4f4d\u59ff\u4e0e\u5c3a\u5ea6\u3002  \n\u25c6\u901a\u8fc7\u5c06\u91cd\u5efa\u7269\u4f53\u9006\u5411\u6620\u5c04\u56de\u539f\u59cb\u6df1\u5ea6\u7f3a\u5931\u573a\u666f\uff0c\u751f\u6210\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u6293\u53d6\u68c0\u6d4b\u6548\u679c\u3002  \n\u25c6\u5728\u4eff\u771f\u4e0e\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u4e3a\u900f\u660e/\u955c\u9762\u7269\u4f53\u6293\u53d6\u63d0\u4f9b\u5b9e\u7528\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u7b80\u5316\u786c\u4ef6\u4f9d\u8d56\u3002|\n",
    "2506.04917": "|2025-06-05|Vanishing arcs for isolated plane curve singularities|Hanwool Bae\u7b49|[2506.04917](http://arxiv.org/pdf/2506.04917)|\u65e0|\u25c6 \u63d0\u51fa\"\u6d88\u5931\u5f27\u96c6\"\u65b0\u6982\u5ff5\uff0c\u4f5c\u4e3a\u4f20\u7edf\u6d88\u5931\u5faa\u73af\u7684\u51e0\u4f55\u5bf9\u5e94\u7269\uff0c\u901a\u8fc7\u51e0\u4f55\u53d8\u5206\u7b97\u5b50\u5c06\u5d4c\u5165\u5f27\u4e0e\u95ed\u66f2\u7ebf\u8054\u7cfb\u8d77\u6765\u3002  \n\u25c6 \u5efa\u7acb\u51e0\u4f55\u53d8\u5206\u7b97\u5b50\u7684\u62d3\u6251\u6846\u67b6\uff0c\u7528\u51e0\u4f55\u5f27\u548c\u95ed\u66f2\u7ebf\u66ff\u4ee3\u540c\u8c03\u5faa\u73af\uff0c\u62d3\u5c55\u4e86\u7ecf\u5178\u8d85\u66f2\u9762\u5947\u70b9\u7406\u8bba\u7684\u5de5\u5177\u96c6\u3002  \n\u25c6 \u7ed9\u51fa\u5224\u5b9a\u5d4c\u5165\u5f27\u88ab\u51e0\u4f55\u53d8\u5206\u7b97\u5b50\u6620\u5c04\u4e3a\u6d88\u5931\u5faa\u73af\u7684\u5145\u8981\u6761\u4ef6\uff0c\u57fa\u4e8e\u5f27\u4e0e\u51e0\u4f55\u5355\u503c\u5316\u6620\u50cf\u7684\u4ea4\u70b9\u6570\u7279\u5f81\u3002  \n\u25c6 \u8bc1\u660e\u5bf9\u4efb\u610f\u7531A'Campo\u5256\u5206\u4ea7\u751f\u7684\u6d88\u5931\u5faa\u73af\u96c6\uff0c\u5b58\u5728\u62d3\u6251\u4f8b\u5916\u5f27\u96c6\u4f7f\u5176\u53d8\u5206\u6620\u50cf\u4e0e\u8be5\u6d88\u5931\u5faa\u73af\u96c6\u5b8c\u5168\u5339\u914d\u3002  \n\u25c6 \u5c06\u51e0\u4f55\u5355\u503c\u5316\u4e0e\u4ea4\u70b9\u6570\u7406\u8bba\u76f8\u7ed3\u5408\uff0c\u4e3a\u5e73\u9762\u66f2\u7ebf\u5947\u70b9\u7684\u62d3\u6251\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u5316\u65b9\u6cd5\u3002|\n",
    "2506.04619": "|2025-06-05|Deep Learning Reforms Image Matching: A Survey and Outlook|Shihua Zhang\u7b49|[2506.04619](http://arxiv.org/pdf/2506.04619)|\u65e0|\u25c6 \u8be5\u8bba\u6587\u9996\u6b21\u4ece\u6df1\u5ea6\u5b66\u4e60\u9010\u6b65\u6539\u9020\u4f20\u7edf\u56fe\u50cf\u5339\u914d\u6d41\u7a0b\u7684\u89c6\u89d2\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u7efc\u8ff0\u6309\u6280\u672f\u5206\u7c7b\u7684\u6846\u67b6\u3002  \n\u25c6 \u63d0\u51fa\u4e0e\u7ecf\u5178\u6d41\u6c34\u7ebf\u9ad8\u5ea6\u5bf9\u9f50\u7684\u65b0\u578b\u5206\u7c7b\u4f53\u7cfb\uff1a\u4e00\u65b9\u9762\u62c6\u89e3\u5404\u73af\u8282\u7684\u53ef\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\uff08\u5982\u53ef\u5b66\u4e60\u68c0\u6d4b-\u63cf\u8ff0\u5b50\u3001\u79bb\u7fa4\u70b9\u8fc7\u6ee4\u5668\uff09\uff0c\u53e6\u4e00\u65b9\u9762\u6574\u5408\u591a\u73af\u8282\u7684\u7aef\u5230\u7aef\u6a21\u5757\uff08\u5982\u4e2d\u7aef\u7a00\u758f\u5339\u914d\u5668\u3001\u7a20\u5bc6\u5339\u914d\u5668\uff09\u3002  \n\u25c6 \u6df1\u5ea6\u5256\u6790\u4e86\u53ef\u5b66\u4e60\u7ec4\u4ef6\u4e0e\u7aef\u5230\u7aef\u6a21\u5757\u7684\u8bbe\u8ba1\u54f2\u5b66\u53ca\u4f18\u52a3\uff0c\u9996\u6b21\u660e\u786e\u63ed\u793a\u4e24\u7c7b\u6280\u672f\u8def\u7ebf\u7684\u4e92\u8865\u6027\u4e0e\u9002\u7528\u8fb9\u754c\u3002  \n\u25c6 \u5728\u76f8\u5bf9\u4f4d\u59ff\u6062\u590d\u3001\u5355\u5e94\u4f30\u8ba1\u7b49\u6838\u5fc3\u4efb\u52a1\u4e0a\u5efa\u7acb\u7edf\u4e00\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5b9a\u91cf\u6bd4\u8f83\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u7684\u6027\u80fd\u7a81\u7834\u4e0e\u73b0\u5b58\u7f3a\u9677\u3002  \n\u25c6 \u524d\u77bb\u6027\u6307\u51fa\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u5339\u914d\u3001\u52a8\u6001\u573a\u666f\u9002\u5e94\u7b49\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u9886\u57df\u53d1\u5c55\u7ed8\u5236\u4e86\u6e05\u6670\u7684\u6280\u672f\u6f14\u8fdb\u5730\u56fe\u3002  \n\u25c6 \u901a\u8fc7\u63ed\u793a\u4f20\u7edf\u6d41\u7a0b\u88ab\u6df1\u5ea6\u5b66\u4e60\"\u89e3\u6784-\u91cd\u6784\"\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u7840\u95ee\u9898\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u5c42\u9762\u7684\u65b0\u8303\u5f0f\u3002|\n",
    "2506.06302": "|2025-05-21|Anti-interrupted sampling repeater jamming via linear canonical Wigner distribution lightweight LFM detection|Jia-Mian Li\u7b49|[2506.06302](http://arxiv.org/pdf/2506.06302)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u7ebf\u6027\u6b63\u5219\u7ef4\u683c\u7eb3\u5206\u5e03\uff08GLWD\uff09\u7684\u6297\u5e72\u6270\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u7406\u8bbe\u7f6e\u53c2\u6570\u83b7\u5f97\u9ad8\u65f6\u9891\u5206\u8fa8\u7387\u548c\u80fd\u91cf\u96c6\u4e2d\u6027\uff0c\u663e\u8457\u63d0\u5347\u4fe1\u53f7\u5206\u79bb\u80fd\u529b\u548c\u4fe1\u566a\u6bd4\u3002  \n\u25c6 \u6539\u8fdb\u73b0\u6709\u79fb\u52a8\u7ebf\u6bb5\u68c0\u6d4b\uff08M-LSD\uff09\u7b97\u6cd5\uff0c\u63d0\u51fa\u79fb\u52a8\u957f\u7ebf\u6bb5\u68c0\u6d4b\uff08M-LSD\uff09\u7b97\u6cd5\uff0c\u589e\u5f3a\u5bf9\u76ee\u6807\u7ebf\u6027\u8c03\u9891\u4fe1\u53f7\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u964d\u4f4e\u5bf9\u5e72\u6270\u4fe1\u53f7\u7684\u654f\u611f\u6027\u3002  \n\u25c6 \u5229\u7528GLWD\u4e0e\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u7684\u6620\u5c04\u5173\u7cfb\u6784\u5efa\u65f6\u9891\u6ee4\u6ce2\u5668\uff0c\u5728STFT\u57df\u8fdb\u884c\u6ee4\u6ce2\u4ee5\u9ad8\u6548\u6291\u5236\u5e72\u6270\u3002  \n\u25c6 \u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u4ecd\u80fd\u6709\u6548\u533a\u5206\u80fd\u91cf\u63a5\u8fd1\u771f\u5b9e\u76ee\u6807\u7684\u5e72\u6270\u91c7\u6837\u8f6c\u53d1\u5e72\u6270\uff08ISRJ\uff09\uff0c\u89e3\u51b3\u4f20\u7edf\u65f6\u9891\u57df\u65b9\u6cd5\u5728\u591a\u5206\u91cf\u4fe1\u53f7\u573a\u666f\u4e2d\u7684\u65f6\u9891\u6df7\u53e0\u95ee\u9898\u3002  \n\u25c6 \u4eff\u771f\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9\u96be\u533a\u5206\u5e72\u6270\u7684\u6709\u6548\u6291\u5236\u80fd\u529b\uff0c\u517c\u5177\u5b9e\u65f6\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u96f7\u8fbe\u6297\u5e72\u6270\u573a\u666f\u3002|\n",
    "2506.09748": "|2025-06-11|Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints|Xiangkai Zhang\u7b49|[2506.09748](http://arxiv.org/pdf/2506.09748)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8de8\u6e90\u56fe\u50cf\u5339\u914d\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u548c\u7ed3\u6784\u7ea6\u675f\u7684\u7c97\u5339\u914d\u6a21\u5757\u4e0e\u8f7b\u91cf\u7ea7\u7ec6\u7c92\u5ea6\u5339\u914d\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7edd\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u3002  \n\u25c6 \u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\uff0c\u5728\u8bed\u4e49\u548c\u7ed3\u6784\u7ea6\u675f\u4e0b\u5efa\u7acb\u533a\u57df\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6e90\u5dee\u5f02\u548c\u65f6\u53d8\u56e0\u7d20\u5bfc\u81f4\u7684\u5339\u914d\u96be\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u7ec6\u7c92\u5ea6\u5339\u914d\u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u53d6\u7cbe\u7ec6\u7279\u5f81\u5efa\u7acb\u50cf\u7d20\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002  \n\u25c6 \u6784\u5efa\u4e86\u4e0d\u4f9d\u8d56\u76f8\u5bf9\u5b9a\u4f4d\u6280\u672f\u7684\u65e0\u4eba\u673a\u7edd\u5bf9\u89c6\u89c9\u5b9a\u4f4d\u6d41\u7a0b\uff0c\u901a\u8fc7\u56fe\u50cf\u68c0\u7d22\u6a21\u5757\u4e0e\u5206\u5c42\u5339\u914d\u6a21\u5757\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u72ec\u7acb\u5b9a\u4f4d\u3002  \n\u25c6 \u5728\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548c\u65b0\u63d0\u51fa\u7684CS-UAV\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002|\n",
    "2506.09369": "|2025-06-11|ScaleLSD: Scalable Deep Line Segment Detection Streamlined|Zeran Ke\u7b49|[2506.09369](http://arxiv.org/pdf/2506.09369)|[\u4ee3\u7801](https://github.com/ant-research/scalelsd)|\u25c6 \u63d0\u51faScaleLSD\uff0c\u9996\u4e2a\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u8d85\u8fc71000\u4e07\u65e0\u6807\u7b7e\u56fe\u50cf\uff09\u8bad\u7ec3\u7684\u9886\u57df\u65e0\u5173\u9c81\u68d2\u7ebf\u68c0\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u81ea\u7136\u56fe\u50cf\u7684\u7ebf\u6bb5\u68c0\u6d4b\u80fd\u529b\u3002  \n\u25c6 \u91cd\u65b0\u8bbe\u8ba1\u5e76\u7b80\u5316\u4e86\u4f20\u7edf\uff08\u6df1\u5ea6\u4e0e\u975e\u6df1\u5ea6\uff09\u7ebf\u6bb5\u68c0\u6d4b\u65b9\u6cd5\u7684\u6838\u5fc3\u67b6\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u6027\u80fd\u7684\u7ebf\u6bb5\u68c0\u6d4b\uff0c\u68c0\u6d4b\u6570\u91cf\u8fdc\u8d85\u7ecf\u5178\u975e\u6df1\u5ea6\u65b9\u6cd5\u3002  \n\u25c6 \u5728\u7ebf\u6bb5\u51e0\u4f55\u8868\u5f81\u4e0a\u66f4\u5b8c\u6574\u4e14\u51c6\u786e\uff0c\u9996\u6b21\u5b9e\u73b0\u6df1\u5ea6\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\uff08\u68c0\u6d4b\u6027\u80fd\u3001\u5355\u89c6\u56fe3D\u51e0\u4f55\u4f30\u8ba1\u7b49\uff09\u5168\u9762\u8d85\u8d8a\u7ecf\u5178\u975e\u6df1\u5ea6LSD\u3002  \n\u25c6 \u901a\u8fc7\u96f6\u6837\u672c\u534f\u8bae\u9a8c\u8bc1\u6a21\u578b\u6cdb\u5316\u6027\uff0c\u5728\u5355\u89c6\u56fe3D\u91cd\u5efa\u3001\u53cc\u89c6\u56fe\u7ebf\u6bb5\u5339\u914d\u3001\u591a\u89c6\u56fe3D\u7ebf\u6bb5\u6620\u5c04\u7b49\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u4e0e\u6a21\u578b\uff0c\u4e3a\u56fe\u50cf\u7ebf\u51e0\u4f55\u7684\u5e7f\u6cdb\u5e94\u7528\uff08\u5982\u4e09\u7ef4\u91cd\u5efa\u3001\u5339\u914d\uff09\u63d0\u4f9b\u66f4\u5f3a\u901a\u7528\u6027\u652f\u6301\uff0c\u5f3a\u5316\u7ebf\u6bb5\u51e0\u4f55\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002|\n",
    "2506.10344": "|2025-06-12|RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration|Mina C. Moghadam\u7b49|[2506.10344](http://arxiv.org/pdf/2506.10344)|\u65e0|\u25c6 \u63d0\u51faRealKeyMorph\uff08RKM\uff09\uff0c\u9996\u4e2a\u65e0\u9700\u56fa\u5b9a\u5206\u8fa8\u7387\u91cd\u91c7\u6837\u7684\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cb\u5206\u8fa8\u7387\u6570\u636e\uff0c\u907f\u514d\u63d2\u503c\u4f2a\u5f71\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u5173\u952e\u70b9\u8f93\u51fa\u4e3a\u626b\u63cf\u4eea\u771f\u5b9e\u4e16\u754c\u5750\u6807\uff08\u800c\u975e\u4f53\u7d20\u5750\u6807\uff09\uff0c\u901a\u8fc7\u5229\u7528\u626b\u63cf\u4eea\u751f\u6210\u7684\u4eff\u5c04\u77e9\u9635\u5b9e\u73b0\u8de8\u5206\u8fa8\u7387\u914d\u51c6\u3002  \n\u25c6 \u6269\u5c55KeyMorph\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u878d\u5165\u771f\u5b9e\u4e16\u754c\u5750\u6807\u8f6c\u6362\uff0c\u4f7f\u5173\u952e\u70b9\u63d0\u53d6\u4e0e\u56fe\u50cf\u5206\u8fa8\u7387\u5b8c\u5168\u89e3\u8026\u3002  \n\u25c6 \u5728\u8179\u90e8MRI\u6b63\u4ea42D\u5806\u6808\u548c\u4e0d\u540c\u5206\u8fa8\u73873D\u8111\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u5176\u5bf9\u5206\u8fa8\u7387\u5dee\u5f02\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u901a\u8fc7\u95ed\u5f0f\u5173\u952e\u70b9\u5339\u914d\u8ba1\u7b97\u53d8\u6362\u53c2\u6570\uff0c\u4fdd\u6301\u4e86KeyMorph\u539f\u6709\u7684\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\uff0c\u540c\u65f6\u7a81\u7834\u5206\u8fa8\u7387\u9650\u5236\u3002|\n",
    "2506.13133": "|2025-06-16|EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints for Visual Place Recognition|Bingxi Liu\u7b49|[2506.13133](http://arxiv.org/pdf/2506.13133)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b80\u5355\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7279\u5f81\uff08MoF\uff09\u65b9\u6cd5\u5728\u5177\u8eab\u7ea6\u675f\u4e0b\u4f18\u5316\u5168\u5c40\u7279\u5f81\uff0c\u63d0\u5347\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u6027\u80fd\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u5177\u8eab\u7ea6\u675f\u5728VPR\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5e76\u6839\u636e\u73b0\u6709\u6570\u636e\u96c6\u5c06\u5176\u5206\u7c7b\u4e3aGPS\u6807\u7b7e\u3001\u65f6\u5e8f\u6233\u3001\u5c40\u90e8\u7279\u5f81\u5339\u914d\u548c\u81ea\u76f8\u4f3c\u77e9\u9635\u7b49\u7c7b\u578b\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684MoF\u6743\u91cd\u8ba1\u7b97\u7b56\u7565\uff0c\u91c7\u7528\u591a\u5ea6\u91cf\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u878d\u5408\u591a\u79cd\u7279\u5f81\u4fe1\u606f\u3002  \n\u25c6 \u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4ec5\u970025 KB\u989d\u5916\u53c2\u6570\u548c\u6bcf\u5e2710\u5fae\u79d2\u5904\u7406\u65f6\u95f4\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\u3002  \n\u25c6 \u5728Pitts-30k\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u57fa\u4e8eDINOv2\u7684\u57fa\u7ebf\u6027\u80fd\u63d0\u53470.9%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002|\n",
    "2506.15180": "|2025-06-18|ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections|Ziling Huang\u7b49|[2506.15180](http://arxiv.org/pdf/2506.15180)|\u65e0|\u25c6 \u63d0\u51faReSeDis\u4efb\u52a1\uff0c\u9996\u6b21\u5c06\u5927\u89c4\u6a21\u56fe\u50cf\u68c0\u7d22\u4e0e\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u7edf\u4e00\uff0c\u8981\u6c42\u6a21\u578b\u6839\u636e\u6587\u672c\u63cf\u8ff0\u5728\u56fe\u50cf\u5e93\u4e2d\u540c\u65f6\u5224\u65ad\u5bf9\u8c61\u662f\u5426\u5b58\u5728\u5e76\u7cbe\u786e\u5b9a\u4f4d\u3002  \n\u25c6 \u6784\u5efa\u9996\u4e2a\u9488\u5bf9\u8be5\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u786e\u4fdd\u6bcf\u4e2a\u63cf\u8ff0\u552f\u4e00\u5bf9\u5e94\u5206\u6563\u5728\u5927\u89c4\u6a21\u591a\u6837\u56fe\u50cf\u5e93\u4e2d\u7684\u5bf9\u8c61\u5b9e\u4f8b\uff0c\u907f\u514d\u8bef\u5339\u914d\u3002  \n\u25c6 \u8bbe\u8ba1\u8054\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u540c\u65f6\u8861\u91cf\u68c0\u7d22\u53ec\u56de\u7387\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u7aef\u5230\u7aef\u6027\u80fd\u63d0\u4f9b\u91cf\u5316\u6807\u51c6\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u51bb\u7ed3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63ed\u793a\u8be5\u4efb\u52a1\u672a\u6765\u7814\u7a76\u7684\u5de8\u5927\u63d0\u5347\u7a7a\u95f4\u3002  \n\u25c6 \u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f25\u8865\u73b0\u6709\u6280\u672f\u4ec5\u4fa7\u91cd\u68c0\u7d22\u6216\u5b9a\u4f4d\u5355\u4e00\u80fd\u529b\u7684\u7f3a\u9677\u3002|\n",
    "2506.20191": "|2025-06-25|Fast entropy-regularized SDP relaxations for permutation synchronization|Michael Lindsey\u7b49|[2506.20191](http://arxiv.org/pdf/2506.20191)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u968f\u673a\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u6392\u5217\u540c\u6b65\u95ee\u9898\uff08PPS\uff09\u7684\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u677e\u5f1b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u56fe\u50cf\u5339\u914d\u7684\u6548\u7387\u3002  \n\u25c6 \u5229\u7528\u71b5\u6b63\u5219\u5316\u6280\u672f\u89e3\u51b3\u4e86\u6807\u51c6\u677e\u5f1b\u4e2d\u4f18\u5316\u89e3\u975e\u552f\u4e00\u6027\u7684\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u4e00\u79cd\u968f\u673a\u6c42\u89e3\u5668\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u5728\u89c2\u6d4b\u5230\u7684\u5bf9\u5e94\u5173\u7cfb\u6570\u91cf\u4e0a\u63a5\u8fd1\u6700\u4f18\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u591a\u79cd\u820d\u5165\u7a0b\u5e8f\uff0c\u80fd\u591f\u4ece\u9690\u5f0f\u8868\u793a\u7684\u539f\u95ee\u9898\u89e3\u53d8\u91cf\u4e2d\u6062\u590d\u7ec4\u5408\u89e3\uff0c\u540c\u65f6\u652f\u6301\u4fdd\u6301\u5faa\u73af\u4e00\u81f4\u6027\u800c\u4e0d\u5f71\u54cd\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002  \n\u25c6 \u5c55\u793a\u4e86\u71b5\u6b63\u5219\u5316SDP\u5728PPS\u95ee\u9898\u4e2d\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\uff0c\u4e3a\u4f20\u7edf\u4f4e\u79e9\u6216\u8c31\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u66ff\u4ee3\u65b9\u6848\u3002|\n",
    "2506.22336": "|2025-06-27|MatChA: Cross-Algorithm Matching with Feature Augmentation|Paula Carb\u00f3 Cubero\u7b49|[2506.22336](http://arxiv.org/pdf/2506.22336)|\u65e0|\u25c6 \u9996\u6b21\u63d0\u51fa\u8de8\u7279\u5f81\u68c0\u6d4b\u5668\u7684\u7279\u5f81\u5339\u914d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u8bbe\u5907\u4f7f\u7528\u4e0d\u540c\u7a00\u758f\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\u65f6\u89c6\u89c9\u5b9a\u4f4d\u5931\u6548\u7684\u95ee\u9898\u3002  \n\u25c6 \u901a\u8fc7\u7279\u5f81\u63cf\u8ff0\u7b26\u589e\u5f3a\u6280\u672f\u63d0\u5347\u8de8\u68c0\u6d4b\u5668\u573a\u666f\u4e0b\u7684\u7279\u5f81\u5339\u914d\u6027\u80fd\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76f8\u540c\u5173\u952e\u70b9\u7684\u9650\u5236\u3002  \n\u25c6 \u5f15\u5165\u7279\u5f81\u8f6c\u6362\u5230\u6f5c\u5728\u7a7a\u95f4\u7684\u7b56\u7565\uff0c\u6709\u6548\u5e94\u5bf9\u5173\u952e\u70b9\u4f4e\u91cd\u590d\u6027\u548c\u63cf\u8ff0\u7b26\u533a\u5206\u5ea6\u4e0d\u8db3\u7684\u6311\u6218\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8de8\u7279\u5f81\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5339\u914d\u548c\u89c6\u89c9\u5b9a\u4f4d\u7684\u51c6\u786e\u7387\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u540c\u63cf\u8ff0\u7b26\u6df7\u5408\u4f7f\u7528\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2506.22139": "|2025-07-22|Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs|Shaojie Zhang\u7b49|[2506.22139](http://arxiv.org/pdf/2506.22139)|\u65e0|\u25c6\u63d0\u51faQ-Frame\u65b9\u6cd5\uff0c\u901a\u8fc7\u67e5\u8be2\u81ea\u9002\u5e94\u7684\u5e27\u9009\u62e9\u7b56\u7565\u89e3\u51b3\u89c6\u9891-\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5173\u952e\u65f6\u7a7a\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u7a81\u7834\u4f20\u7edf\u5747\u5300\u91c7\u6837\u7684\u5c40\u9650\u6027\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u7ed3\u5408CLIP\u7b49\u6587\u672c-\u56fe\u50cf\u5339\u914d\u7f51\u7edc\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u5f0f\u5e27\u9009\u62e9\uff0c\u5229\u7528Gumbel-Max\u6280\u5de7\u63d0\u5347\u9009\u62e9\u6548\u7387\u3002  \n\u25c6\u5f15\u5165\u591a\u5206\u8fa8\u7387\u7f29\u653e\u673a\u5236\uff0c\u6839\u636e\u89c6\u9891\u5185\u5bb9\u548c\u67e5\u8be2\u9700\u6c42\u52a8\u6001\u8c03\u6574\u5e27\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u3002  \n\u25c6\u5728\u4fdd\u6301\u8ba1\u7b97\u8d1f\u8f7d\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u589e\u52a0\u53ef\u5904\u7406\u7684\u5e27\u6570\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u7684\u65f6\u7a7a\u7ec6\u8282\u3002  \n\u25c6\u5728MLVU\u3001LongVideoBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u6db5\u76d6\u591a\u79cd\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002  \n\u25c6\u4e3a\u89c6\u9891-\u5927\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u8f7b\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u8bed\u4e49\u7406\u89e3\u6df1\u5ea6\u7684\u77db\u76fe\u3002|\n",
    "2506.21923": "|2025-06-27|ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction|Juming Xiong\u7b49|[2506.21923](http://arxiv.org/pdf/2506.21923)|\u65e0|\u25c6 \u63d0\u51faZeroReg3D\uff0c\u9996\u4e2a\u9488\u5bf9\u8fde\u7eed\u7ec4\u7ec7\u75c5\u7406\u5207\u72473D\u91cd\u5efa\u7684\u96f6\u6837\u672c\u914d\u51c6\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u76f4\u63a5\u5e94\u7528\u3002  \n\u25c6 \u521b\u65b0\u7ed3\u5408\u96f6\u6837\u672c\u6df1\u5ea6\u5b66\u4e60\u5173\u952e\u70b9\u5339\u914d\u4e0e\u57fa\u4e8e\u4f18\u5316\u7684\u4eff\u5c04/\u975e\u521a\u6027\u914d\u51c6\u6280\u672f\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0e\u6cdb\u5316\u6027\u4e0a\u7684\u77db\u76fe\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u89e3\u51b3\u7ec4\u7ec7\u53d8\u5f62\u3001\u5207\u7247\u4f2a\u5f71\u3001\u67d3\u8272\u5dee\u5f02\u548c\u5149\u7167\u4e0d\u4e00\u81f4\u56db\u5927\u6311\u6218\uff0c\u663e\u8457\u63d0\u53473D\u91cd\u5efa\u7684\u89e3\u5256\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002  \n\u25c6 \u7a81\u7834\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u9650\u5236\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u7b56\u7565\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7684\u9ad8\u9002\u5e94\u6027\u3002  \n\u25c6 \u516c\u5f00\u5b8c\u6574\u4ee3\u7801\u5e93\uff0c\u4e3a\u75c5\u7406\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u53ef\u76f4\u63a5\u90e8\u7f72\u7684\u5f00\u6e90\u5de5\u5177\u3002|\n",
    "2506.23707": "|2025-06-30|Efficient and Accurate Image Provenance Analysis: A Scalable Pipeline for Large-scale Images|Jiewei Lai\u7b49|[2506.23707](http://arxiv.org/pdf/2506.23707)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u56fe\u50cf\u6eaf\u6e90\u5206\u6790\u7ba1\u9053\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u4e24\u5927\u74f6\u9888\u3002  \n\n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u4fee\u6539\u5173\u7cfb\u8ffd\u8e2a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u53d8\u4f53\u7684\u8fc7\u6ee4\u6548\u679c\uff0c\u80fd\u591f\u5168\u9762\u53d1\u73b0\u4e0e\u67e5\u8be2\u56fe\u50cf\u89c6\u89c9\u76f8\u4f3c\u5ea6\u4f4e\u7684\u53d8\u4f53\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u4f4e\u76f8\u4f3c\u5ea6\u800c\u9057\u6f0f\u4e25\u91cd\u4fee\u6539\u56fe\u50cf\u7684\u95ee\u9898\u3002  \n\n\u25c6 \u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u7279\u5f81\u5339\u914d\u548c\u538b\u7f29\u4f2a\u5f71\u6355\u6349\u6280\u672f\uff0c\u589e\u5f3a\u4e86\u65b9\u6cd5\u5bf9\u591a\u6837\u5316\u4fee\u6539\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5206\u6790\u56fe\u50cf\u95f4\u7684\u5173\u8054\u6027\u548c\u4fee\u6539\u65b9\u5411\u3002  \n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7b56\u7565\uff0c\u5e76\u5728\u6784\u5efa\u6709\u5411\u6eaf\u6e90\u56fe\u65f6\u6d88\u9664\u4e86\u5197\u4f59\u7684\u6210\u5bf9\u5206\u6790\uff0c\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u4f4e\u5230\u7ebf\u6027\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u5904\u7406\u3002  \n\n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u5347\u4e8616.7%-56.1%\uff0c\u5e76\u57281000\u4e07\u89c4\u6a21\u56fe\u50cf\u4e0a\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u4ec53\u79d2\uff0c\u8fdc\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u768412\u5206\u949f\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002  \n\n\u25c6 \u6700\u7ec8\u751f\u6210\u7684\u6eaf\u6e90\u56fe\u80fd\u591f\u7cbe\u786e\u523b\u753b\u56fe\u50cf\u7684\u6f14\u5316\u5386\u53f2\uff0c\u4e3a\u6570\u5b57\u6cbb\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u53d6\u8bc1\u5de5\u5177\u3002|\n",
    "2506.23077": "|2025-06-29|Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization|Suofei Zhang\u7b49|[2506.23077](http://arxiv.org/pdf/2506.23077)|\u65e0|\u25c6 \u63d0\u51fa\u4e86Distance-Aware Cross-View Geo-Localization (DACVGL)\u65b0\u95ee\u9898\uff0c\u5f3a\u8c03\u6a21\u578b\u9700\u7efc\u5408\u6355\u6349\u76ee\u6807\u5468\u56f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u964d\u4f4e\u5b9a\u4f4d\u8bef\u5dee\u6210\u672c\u3002  \n\u25c6 \u6784\u5efa\u9996\u4e2a\u591a\u89c6\u89d2\u56fe\u50cf\u4e0e\u7cbe\u786e\u8ddd\u79bb\u6807\u6ce8\u7684\u57fa\u51c6\u6570\u636e\u96c6DA-Campus\uff0c\u6db5\u76d6\u4e09\u79cd\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u652f\u6301\u7cfb\u7edf\u6027\u7814\u7a76\u3002  \n\u25c6 \u5c06DACVGL\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u8de8\u57df\u5206\u5c42\u68c0\u7d22\u4efb\u52a1\uff0c\u63ed\u793a\u4f20\u7edf\u5ea6\u91cf\u5b66\u4e60\u65e0\u6cd5\u89e3\u51b3\u5efa\u7b51\u95f4\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u7684\u95ee\u9898\u3002  \n\u25c6 \u63d0\u51fa\u52a8\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6DyCL\uff0c\u901a\u8fc7\u5206\u5c42\u7a7a\u95f4\u95f4\u9694\u9010\u6b65\u5bf9\u9f50\u7279\u5f81\u8868\u793a\uff0c\u89e3\u51b3\u8de8\u89c6\u89d2\u5c42\u6b21\u5316\u68c0\u7d22\u96be\u9898\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660eDyCL\u4e0e\u73b0\u6709\u591a\u5c3a\u5ea6\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u9ad8\u5ea6\u4e92\u8865\uff0c\u663e\u8457\u63d0\u5347\u5206\u5c42\u68c0\u7d22\u6027\u80fd\u548c\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u7cbe\u5ea6\u3002  \n\u25c6 \u516c\u5f00\u4ee3\u7801\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002|\n",
    "2507.01667": "|2025-07-02|What does really matter in image goal navigation?|Gianluca Monaci\u7b49|[2507.01667](http://arxiv.org/pdf/2507.01667)|\u65e0|\u25c6 \u7814\u7a76\u4e86\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4f9d\u8d56\u4e13\u7528\u56fe\u50cf\u5339\u914d\u6216\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u5757\u7684\u65b9\u6cd5\u3002  \n\u25c6 \u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u5206\u6790\u4e86\u591a\u79cd\u67b6\u6784\u8bbe\u8ba1\uff08\u5982\u5ef6\u8fdf\u878d\u5408\u3001\u901a\u9053\u5806\u53e0\u3001\u7a7a\u95f4\u5230\u6df1\u5ea6\u6295\u5f71\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u5bf9\u5bfc\u822a\u6027\u80fd\u7684\u5f71\u54cd\u3002  \n\u25c6 \u63ed\u793a\u4e86\u4eff\u771f\u73af\u5883\u8bbe\u7f6e\u5bf9\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6307\u51fa\u4eff\u771f\u4e2d\u5b58\u5728\u7684\u6377\u5f84\u95ee\u9898\uff0c\u540c\u65f6\u8bc1\u660e\u90e8\u5206\u80fd\u529b\u53ef\u8fc1\u79fb\u5230\u66f4\u771f\u5b9e\u573a\u666f\u3002  \n\u25c6 \u9996\u6b21\u53d1\u73b0\u5bfc\u822a\u6027\u80fd\u4e0e\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff0c\u8868\u660e\u540e\u8005\u662f\u5bfc\u822a\u4efb\u52a1\u4e2d\u81ea\u7136\u6d8c\u73b0\u7684\u91cd\u8981\u5b50\u6280\u80fd\u3002  \n\u25c6 \u4e3a\u4ec5\u901a\u8fc7\u5bfc\u822a\u5956\u52b1\u4fe1\u53f7\u8bad\u7ec3\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u5668\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\uff0c\u5bf9\u5177\u8eabAI\u53ca\u5176\u4ed6\u9886\u57df\u5177\u6709\u6f5c\u5728\u5f71\u54cd\u3002  \n\u25c6 \u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u667a\u80fd\u4f53\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u573a\u666f\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002|\n",
    "2507.03868": "|2025-07-05|From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM|Xinyi Wu\u7b49|[2507.03868](http://arxiv.org/pdf/2507.03868)|\u65e0|\u25c6 \u63d0\u51faUni-Retrieval\u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u53d6\u67e5\u8be2\u98ce\u683c\u539f\u578b\u5e76\u52a8\u6001\u5339\u914dPrompt Bank\u4e2d\u7684\u6807\u8bb0\uff0c\u89e3\u51b3\u73b0\u6709\u68c0\u7d22\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u6559\u80b2\u573a\u666f\u591a\u6837\u6027\u548c\u6a21\u7cca\u6027\u7684\u95ee\u9898\u3002  \n\u25c6 \u5f15\u5165Prompt Bank\uff0c\u7ed3\u5408MoE-LoRA\u6a21\u5757\u7f16\u7801\u9886\u57df\u77e5\u8bc6\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u9002\u5e94\u672a\u89c1\u67e5\u8be2\u7c7b\u578b\uff0c\u589e\u5f3a\u68c0\u7d22\u7075\u6d3b\u6027\u3002  \n\u25c6 \u5c06Uni-Retrieval\u4e0e\u8f7b\u91cf\u7ea7\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u6784\u5efa\u5b8c\u6574\u7684Uni-RAG\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4ece\u68c0\u7d22\u5230\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7684\u6559\u80b2\u5185\u5bb9\u8f93\u51fa\u3002  \n\u25c6 \u91c7\u7528\u98ce\u683c\u6761\u4ef6\u67e5\u8be2\u673a\u5236\uff0c\u751f\u6210\u7b26\u5408\u5b66\u4e60\u76ee\u6807\u7684\u53ef\u8bfb\u89e3\u91ca\u3001\u53cd\u9988\u6216\u6559\u5b66\u5185\u5bb9\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u6559\u5b66\u6548\u679c\u3002  \n\u25c6 \u5728SER\u7b49\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUni-RAG\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u3002  \n\u25c6 \u4e3aSTEM\u6559\u80b2\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u667a\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u9e3f\u6c9f\uff0c\u652f\u6301\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5b66\u4e60\u8f85\u52a9\u3002|\n",
    "2507.06744": "|2025-07-09|Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching|Yafei Zhang\u7b49|[2507.06744](http://arxiv.org/pdf/2507.06744)|\u65e0|\u25c6 \u63d0\u51fa\u5c40\u90e8-\u5168\u5c40\u53cc\u7c92\u5ea6\u8eab\u4efd\u5173\u8054\u673a\u5236\uff0c\u901a\u8fc7\u6279\u5185\u8de8\u6a21\u6001\u663e\u5f0f\u5173\u8054\u5f3a\u5316\u8eab\u4efd\u7ea6\u675f\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u7ec6\u5fae\u5dee\u5f02\u7684\u6355\u6349\u80fd\u529b\u3002  \n\u25c6 \u6784\u5efa\u4ee5\u89c6\u89c9\u6a21\u6001\u4e3a\u951a\u70b9\u7684\u52a8\u6001\u8de8\u6a21\u6001\u5173\u8054\u7f51\u7edc\uff0c\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u6709\u6548\u589e\u5f3a\u5f31\u5173\u8054\u6837\u672c\u7684\u8bc6\u522b\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6837\u672c\u5bf9\u6784\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e00\u81f4\u6027\u5b66\u4e60\u89e3\u51b3\u56f0\u96be\u6837\u672c\u6316\u6398\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002  \n\u25c6 \u9996\u6b21\u5728\u5f31\u76d1\u7763\u6587\u672c-\u884c\u4eba\u56fe\u50cf\u5339\u914d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u590d\u6742\u4e00\u5bf9\u591a\u8eab\u4efd\u5173\u7cfb\u7684\u5efa\u6a21\uff0c\u7a81\u7834\u6027\u80fd\u74f6\u9888\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8de8\u6a21\u6001\u5339\u914d\u51c6\u786e\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.16201": "|2025-07-22|A Single-step Accurate Fingerprint Registration Method Based on Local Feature Matching|Yuwei Jia\u7b49|[2507.16201](http://arxiv.org/pdf/2507.16201)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5355\u6b65\u6307\u7eb9\u914d\u51c6\u7b97\u6cd5\uff0c\u76f4\u63a5\u901a\u8fc7\u9884\u6d4b\u4e24\u5e45\u6307\u7eb9\u56fe\u50cf\u4e4b\u95f4\u7684\u534a\u5bc6\u96c6\u5339\u914d\u70b9\u5bf9\u5e94\u5173\u7cfb\u6765\u5b9e\u73b0\u5bf9\u9f50\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e24\u6b65\u6cd5\u7684\u590d\u6742\u6027\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u4f4e\u8d28\u91cf\u6307\u7eb9\u56fe\u50cf\u56e0\u7279\u5f81\u70b9\u6570\u91cf\u4e0d\u8db3\u5bfc\u81f4\u7684\u521d\u59cb\u914d\u51c6\u5931\u8d25\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u914d\u51c6\u7684\u9c81\u68d2\u6027\u548c\u6210\u529f\u7387\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u7ed3\u5408\u5168\u5c40-\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u4e24\u5e45\u6307\u7eb9\u56fe\u50cf\u4e4b\u95f4\u7684\u7aef\u5230\u7aef\u50cf\u7d20\u7ea7\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u914d\u51c6\u7cbe\u5ea6\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4ec5\u9700\u5355\u6b65\u914d\u51c6\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5339\u914d\u6027\u80fd\uff0c\u540c\u65f6\u8fd8\u80fd\u4e0e\u5bc6\u96c6\u914d\u51c6\u7b97\u6cd5\u7ed3\u5408\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002  \n\u25c6 \u4e3a\u6307\u7eb9\u8bc6\u522b\u4e2d\u7684\u56fe\u50cf\u5931\u771f\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002|\n",
    "2507.18551": "|2025-07-24|A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration|Daniil Morozov\u7b49|[2507.18551](http://arxiv.org/pdf/2507.18551)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b3D\u8de8\u6a21\u6001\u5173\u952e\u70b9\u63cf\u8ff0\u7b26\uff0c\u7528\u4e8e\u89e3\u51b3MRI\u4e0e\u5b9e\u65f6\u8d85\u58f0(iUS)\u4e4b\u95f4\u7684\u914d\u51c6\u96be\u9898\uff0c\u514b\u670d\u4e86\u6a21\u6001\u95f4\u5916\u89c2\u3001\u5206\u8fa8\u7387\u548c\u89c6\u91ce\u5dee\u5f02\u5927\u7684\u95ee\u9898\u3002  \n\u25c6\u91c7\u7528\u60a3\u8005\u7279\u5f02\u6027\u7684\"\u5408\u6210\u5339\u914d\"\u65b9\u6cd5\uff0c\u4ece\u672f\u524dMRI\u751f\u6210\u5408\u6210iUS\u4f53\u79ef\uff0c\u5b9e\u73b0\u4e86\u6709\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u5171\u4eab\u63cf\u8ff0\u7b26\u7a7a\u95f4\u8bad\u7ec3\u3002  \n\u25c6\u5f00\u53d1\u4e86\u6982\u7387\u5173\u952e\u70b9\u68c0\u6d4b\u7b56\u7565\uff0c\u80fd\u591f\u8bc6\u522b\u89e3\u5256\u5b66\u663e\u8457\u4e14\u6a21\u6001\u4e00\u81f4\u7684\u7279\u5f81\u4f4d\u7f6e\uff0c\u63d0\u9ad8\u4e86\u5339\u914d\u7684\u51c6\u786e\u6027\u3002  \n\u25c6\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u4e09\u5143\u7ec4\u635f\u5931\u51fd\u6570\u548c\u52a8\u6001\u96be\u8d1f\u6837\u672c\u6316\u6398\u6280\u672f\uff0c\u4f7f\u63cf\u8ff0\u7b26\u5177\u6709\u6297iUS\u4f2a\u5f71(\u5982\u6591\u70b9\u566a\u58f0)\u548c\u65cb\u8f6c\u4e0d\u53d8\u6027\u7684\u7279\u70b9\u3002  \n\u25c6\u6574\u4e2a\u6846\u67b6\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u521d\u59cb\u5316\uff0c\u5bf9iUS\u89c6\u91ce\u53d8\u5316\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u5728ReMIND\u6570\u636e\u96c6\u4e0a\u8fbe\u523069.8%\u7684\u5e73\u5747\u5339\u914d\u7cbe\u5ea6\u548c2.39mm\u7684\u914d\u51c6\u8bef\u5dee\u3002  \n\u25c6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u8be5\u65b9\u6848\u9996\u6b21\u5b9e\u73b0\u4e86\u4ece\u5173\u952e\u70b9\u5339\u914d\u5230\u521a\u6027\u914d\u51c6\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5728\u4e34\u5e8a\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2507.19118": "|2025-07-25|Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching|Abu Sadat Mohammad Salehin Amit\u7b49|[2507.19118](http://arxiv.org/pdf/2507.19118)|\u65e0|\u25c6\u63d0\u51fa\u8de8\u65f6\u7a7a\u878d\u5408\u6ce8\u610f\u529b\u673a\u5236(CSTF)\uff0c\u901a\u8fc7\u72ec\u7acb\u68c0\u6d4b\u53c2\u8003\u56fe\u548c\u67e5\u8be2\u56fe\u4e2d\u7684\u5c3a\u5ea6\u4e0d\u53d8\u5173\u952e\u70b9\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u95f4\u51e0\u4f55\u548c\u8f90\u5c04\u5dee\u5f02\u5927\u7684\u95ee\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u6784\u5efa\u5bf9\u5e94\u56fe\uff0c\u540c\u65f6\u5229\u7528\u591a\u56fe\u50cf\u533a\u57df\u4fe1\u606f\uff0c\u63d0\u5347\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u80fd\u529b\u3002  \n\u25c6\u5c06\u76f8\u4f3c\u6027\u5339\u914d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed3\u5408SoftMax\u548c\u5168\u5377\u79ef\u7f51\u7edc(FCN)\u5c42\uff0c\u517c\u987e\u5c40\u90e8\u7279\u5f81\u654f\u611f\u6027\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002  \n\u25c6\u5728HRSC2016\u548cDOTA\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5e73\u5747mAP\u5206\u522b\u8fbe\u523090.99%\u548c90.86%\u3002  \n\u25c6\u4fdd\u630112.5 FPS\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002  \n\u25c6\u9996\u6b21\u8bc1\u660e\u6539\u8fdb\u7684\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u80fd\u76f4\u63a5\u63d0\u5347\u9065\u611f\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\uff09\u7684\u6027\u80fd\u3002|\n",
    "2507.22791": "|2025-07-30|Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques|Weide Liu\u7b49|[2507.22791](http://arxiv.org/pdf/2507.22791)|\u65e0|\u25c6 \u5168\u9762\u7efc\u8ff0\u4e86\u5355\u6a21\u6001\u4e0e\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u6280\u672f\uff0c\u6db5\u76d6RGB\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u50cf\u30013D\u70b9\u4e91\u3001LiDAR\u626b\u63cf\u3001\u533b\u5b66\u56fe\u50cf\u53ca\u89c6\u89c9-\u8bed\u8a00\u4ea4\u4e92\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7cfb\u7edf\u6027\u603b\u7ed3\u7684\u7a7a\u767d\u3002  \n\u25c6 \u5bf9\u6bd4\u5206\u6790\u4e86\u4f20\u7edf\u624b\u5de5\u65b9\u6cd5\uff08\u5982Harris\u89d2\u70b9\u3001SIFT\u548cORB\u63cf\u8ff0\u5b50\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982SuperPoint\u548cLoFTR\uff09\u7684\u4f18\u52a3\uff0c\u6307\u51fa\u540e\u8005\u5728\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002  \n\u25c6 \u91cd\u70b9\u4ecb\u7ecd\u4e86\u6a21\u6001\u611f\u77e5\u7684\u521b\u65b0\u6280\u672f\uff0c\u4f8b\u5982\u9488\u5bf9\u6df1\u5ea6\u56fe\u50cf\u7684\u51e0\u4f55\u4e0e\u6df1\u5ea6\u4e13\u7528\u63cf\u8ff0\u5b50\u3001\u9488\u5bf93D\u70b9\u4e91\u7684\u7a00\u758f\u4e0e\u7a20\u5bc6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u53caLiDAR\u626b\u63cf\u4e2d\u57fa\u4e8e\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u795e\u7ecf\u7f51\u7edc\u3002  \n\u25c6 \u5f3a\u8c03\u4e86\u8de8\u6a21\u6001\u5e94\u7528\u7684\u7a81\u7834\uff0c\u5982\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u4e2d\u7684MIND\u63cf\u8ff0\u5b50\u548c\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u4ea4\u4e92\u5339\u914d\u6280\u672f\uff0c\u5c55\u793a\u4e86\u7279\u5f81\u5339\u914d\u5728\u591a\u6837\u5316\u6570\u636e\u4ea4\u4e92\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\u3002  \n\u25c6 \u7cfb\u7edf\u603b\u7ed3\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u8de8\u6a21\u6001\u7279\u5f81\u5339\u914d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u5411\u66f4\u590d\u6742\u573a\u666f\u53d1\u5c55\u3002|\n",
    "2507.23371": "|2025-07-31|VMatcher: State-Space Semi-Dense Local Feature Matching|Ali Youssef|[2507.23371](http://arxiv.org/pdf/2507.23371)|\u65e0|\u25c6 \u63d0\u51faVMatcher\uff0c\u4e00\u79cd\u7ed3\u5408Mamba\u548cTransformer\u7684\u6df7\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u56fe\u50cf\u5bf9\u7684\u534a\u7a20\u5bc6\u7279\u5f81\u5339\u914d\u3002  \n\u25c6 \u9996\u6b21\u5c06\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5f15\u5165\u7279\u5f81\u5339\u914d\u4efb\u52a1\uff0c\u5229\u7528Mamba\u7684\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u4f20\u7edfTransformer\u7684\u4e8c\u6b21\u65b9\u8ba1\u7b97\u5f00\u9500\u3002  \n\u25c6 \u8bbe\u8ba1\u591a\u5c42\u7ea7\u6df7\u5408\u67b6\u6784\uff0c\u540c\u65f6\u4fdd\u7559Transformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u52bf\u548cMamba\u9ad8\u6548\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u6548\u7387\u3002  \n\u25c6 \u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u5408\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\u7684\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u5e76\u63d0\u4f9b\u591a\u79cd\u914d\u7f6e\u65b9\u6848\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6846\u67b6\u3002|\n",
    "2508.02278": "|2025-08-09|SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching|Xiangzeng Liu\u7b49|[2508.02278](http://arxiv.org/pdf/2508.02278)|\u65e0|\u25c6 \u63d0\u51faSGAD\u7f51\u7edc\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u533a\u5206\u5ea6\u7684\u533a\u57df\u63cf\u8ff0\u7b26\uff0c\u76f4\u63a5\u5b9e\u73b0\u533a\u57df\u5339\u914d\uff0c\u907f\u514d\u4f20\u7edf\u4f4e\u6548\u7684\u50cf\u7d20\u7ea7\u6bd4\u8f83\u548c\u590d\u6742\u56fe\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u5339\u914d\u7cbe\u5ea6\u548c\u6548\u7387\u3002  \n\u25c6 \u8bbe\u8ba1\u65b0\u9896\u7684\u76d1\u7763\u7b56\u7565\uff0c\u5c06\u533a\u57df\u5339\u914d\u4efb\u52a1\u5206\u89e3\u4e3a\u5206\u7c7b\u548c\u6392\u5e8f\u5b50\u4efb\u52a1\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5339\u914d\u6027\u80fd\u3002  \n\u25c6 \u5f15\u5165\u5c42\u6b21\u5305\u5bb9\u5197\u4f59\u8fc7\u6ee4\u5668\uff08HCRF\uff09\uff0c\u901a\u8fc7\u5206\u6790\u5305\u5bb9\u56fe\u6d88\u9664\u91cd\u53e0\u533a\u57df\uff0c\u4f18\u5316\u5339\u914d\u7ed3\u679c\u3002  \n\u25c6 \u5728\u6548\u7387\u4e0a\u5b9e\u73b0\u91cd\u5927\u7a81\u7834\uff0c\u76f8\u6bd4MESA\u65b9\u6cd5\u8fd0\u884c\u65f6\u51cf\u5c1160\u500d\uff080.82\u79d2 vs 60.23\u79d2\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7cbe\u5ea6\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff1aSGAD+LoFTR\u5728\u5ba4\u5916\u59ff\u6001\u4f30\u8ba1\u4e2d\u6bd4DKM\u66f4\u5feb\uff080.82\u79d2 vs 1.51\u79d2\uff09\u4e14\u66f4\u51c6\u786e\uff0865.98 vs 61.11\uff09\uff1bSGAD+ROMA\u5728\u5ba4\u5185\u59ff\u6001\u4f30\u8ba1\u4e2dAUC@5\u00b0\u63d0\u53477.39%\uff0c\u8fbe\u5230\u65b0SOTA\u3002|\n",
    "2508.05187": "|2025-08-07|Refining Gaussian Splatting: A Volumetric Densification Approach|Mohamed Abdul Gafoor\u7b49|[2508.05187](http://arxiv.org/pdf/2508.05187)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u60ef\u6027\u4f53\u79ef\u7684\u65b0\u578b\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u51fd\u6570\u7684\u60ef\u6027\u4f53\u79ef\u6307\u5bfc3D\u9ad8\u65af\u5206\u5e03\u7684\u7cbe\u7ec6\u5316\u8fc7\u7a0b\uff0c\u514b\u670d\u4e86\u539f\u59cb3DGS\u5bc6\u5ea6\u7b56\u7565\u7684\u7f3a\u9677\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u7814\u7a76\u4e86\u4f20\u7edfSfM\u4e0e\u6df1\u5ea6\u56fe\u50cf\u5339\u914d(DIM)\u4e24\u79cd\u70b9\u4e91\u521d\u59cb\u5316\u65b9\u6cd5\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4e3a\u521d\u59cb\u5316\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002  \n\u25c6 \u901a\u8fc7\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236(ADC)\u81ea\u52a8\u5316\u5b9e\u73b0\u4e86\u9ad8\u65af\u57fa\u5143\u7684\u52a8\u6001\u589e\u5220\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u57fa\u5143\u7ba1\u7406\u6548\u7387\u3002  \n\u25c6 \u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u5168\u9762\u8d85\u8d8a\u539f\u59cb3DGS\uff0c\u4e14\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002  \n\u25c6 \u5c06\u4f53\u79ef\u4fe1\u606f\u4e0e\u5bc6\u5ea6\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u4e3a3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u51e0\u4f55\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2508.07812": "|2025-08-11|Semi-supervised Multiscale Matching for SAR-Optical Image|Jingze Gai\u7b49|[2508.07812](http://arxiv.org/pdf/2508.07812)|\u65e0|\u25c6\u63d0\u51fa\u534a\u76d1\u7763\u591a\u5c3a\u5ea6\u5339\u914d\u6846\u67b6S2M2-SAR\uff0c\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u548c\u5927\u91cf\u65e0\u6807\u6ce8SAR-\u5149\u5b66\u56fe\u50cf\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0c\u89e3\u51b3\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002  \n\u25c6\u901a\u8fc7\u7ed3\u5408\u6df1\u5c42\u548c\u6d45\u5c42\u5339\u914d\u7ed3\u679c\u751f\u6210\u4f2a\u6807\u7b7e\u76f8\u4f3c\u6027\u70ed\u56fe\uff0c\u4e3a\u65e0\u6807\u6ce8\u6570\u636e\u63d0\u4f9b\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6\u8bbe\u8ba1\u8de8\u6a21\u6001\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u7684\u8de8\u6a21\u6001\u4e92\u72ec\u7acb\u6027\u635f\u5931\uff0c\u5206\u79bb\u6a21\u6001\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff0c\u589e\u5f3a\u7279\u5f81\u89e3\u8026\u80fd\u529b\u3002  \n\u25c6\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u4f18\u5316\u8de8\u6a21\u6001\u7279\u5f81\u8868\u793a\uff0c\u964d\u4f4e\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u6a21\u578b\u5b9e\u7528\u6027\u3002  \n\u25c6\u5b9e\u9a8c\u8868\u660e\uff0cS2M2-SAR\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5168\u76d1\u7763SOTA\u65b9\u6cd5\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2508.08521": "|2025-08-11|VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models|Mansi Phute\u7b49|[2508.08521](http://arxiv.org/pdf/2508.08521)|\u65e0|\u25c6\u63d0\u51faVISOR\u65b9\u6cd5\uff0c\u4ec5\u901a\u8fc7\u4f18\u5316\u89c6\u89c9\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u7cbe\u51c6\u7684\u884c\u4e3a\u63a7\u5236\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u5185\u90e8\u53c2\u6570\u6216\u6587\u672c\u6307\u4ee4\u3002  \n\u25c6\u9996\u521b\"\u901a\u7528\u5f15\u5bfc\u56fe\u50cf\"\u6982\u5ff5\uff0c\u901a\u8fc7\u89c6\u89c9\u523a\u6fc0\u8bf1\u5bfc\u76ee\u6807\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u9690\u853d\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u53cc\u5411\u884c\u4e3a\u8c03\u63a7\u3002  \n\u25c6\u5728LLaVA-1.5-7B\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u4e09\u5927\u5173\u952e\u5bf9\u9f50\u4efb\u52a1\uff08\u62d2\u7edd\u3001\u8c04\u5a9a\u3001\u751f\u5b58\u672c\u80fd\uff09\uff0c\u5355\u5f20150KB\u56fe\u50cf\u5373\u53ef\u8fbe\u5230\u4e0e\u6fc0\u6d3b\u5411\u91cf\u76f8\u5f53\u7684\u8c03\u63a7\u6548\u679c\u3002  \n\u25c6\u76f8\u6bd4\u7cfb\u7edf\u63d0\u793a\u8bcd\uff083-4%\u6539\u53d8\uff09\u548c\u6fc0\u6d3b\u5411\u91cf\uff08\u5fae\u5f31\u8d1f\u5411\u8c03\u63a7\uff09\uff0cVISOR\u5b9e\u73b0\u9ad8\u8fbe25%\u7684\u884c\u4e3a\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u630199.9%\u7684MMLU\u57fa\u51c6\u6027\u80fd\u3002  \n\u25c6\u63ed\u793a\u4e86\u89c6\u89c9\u901a\u9053\u7684\u65b0\u578b\u5b89\u5168\u5a01\u80c1\uff1a\u653b\u51fb\u8005\u4ec5\u901a\u8fc7\u56fe\u50cf\u5373\u53ef\u7ed5\u8fc7\u6587\u672c\u9632\u5fa1\u673a\u5236\uff0c\u5b9e\u73b0\u590d\u6742\u884c\u4e3a\u64cd\u63a7\u3002  \n\u25c6\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63a7\u5236\u63d0\u4f9b\u4e86\u65e0\u9700\u8fd0\u884c\u65f6\u5f00\u9500\u3001\u517c\u5bb9API\u670d\u52a1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u8b66\u793a\u4e86\u89c6\u89c9\u5f15\u5bfc\u653b\u51fb\u7684\u9632\u5fa1\u7d27\u8feb\u6027\u3002|\n",
    "2508.09486": "|2025-08-13|Episodic Memory Representation for Long-form Video Understanding|Yun Wang\u7b49|[2508.09486](http://arxiv.org/pdf/2508.09486)|\u65e0|\u25c6 \u63d0\u51faVideo-EM\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709Video-LLMs\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\u7684\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u89c6\u9891\u7406\u89e3\u3002  \n\u25c6 \u7a81\u7834\u4f20\u7edf\u5173\u952e\u5e27\u68c0\u7d22\u65b9\u6cd5\u7684\u9759\u6001\u56fe\u50cf\u5339\u914d\u5c40\u9650\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u673a\u5236\uff0c\u5c06\u5173\u952e\u5e27\u5efa\u6a21\u4e3a\u65f6\u5e8f\u5316\u60c5\u666f\u4e8b\u4ef6\uff0c\u4fdd\u7559\u65f6\u7a7a\u52a8\u6001\u5173\u7cfb\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u7ed3\u5408\u601d\u7ef4\u94fe\uff08CoT\uff09\u6280\u672f\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fed\u4ee3\u7b5b\u9009\u4fe1\u606f\u91cf\u6700\u5927\u5316\u7684\u6700\u5c0f\u60c5\u666f\u8bb0\u5fc6\u5b50\u96c6\uff0c\u907f\u514d\u5197\u4f59\u5e27\u5e72\u6270\u3002  \n\u25c6 \u9996\u6b21\u5728\u5173\u952e\u5e27\u8868\u793a\u4e2d\u540c\u65f6\u6355\u6349\u7a7a\u95f4\u5173\u8054\u4e0e\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u7cbe\u51c6\u8fd8\u539f\u89c6\u9891\u53d9\u4e8b\u903b\u8f91\uff0c\u63d0\u5347\u573a\u666f\u8f6c\u6362\u548c\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\u7684\u7406\u89e3\u80fd\u529b\u3002  \n\u25c6 \u5728\u56db\u5927\u4e3b\u6d41\u8bc4\u6d4b\u57fa\u51c6\uff08Video-MME\u7b49\uff09\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf4-9%\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u5e27\u6570\uff0c\u517c\u987e\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002|\n",
    "2508.10716": "|2025-08-14|Revisiting Cross-View Localization from Image Matching|Panwang Xia\u7b49|[2508.10716](http://arxiv.org/pdf/2508.10716)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u8de8\u89c6\u89d2\u56fe\u50cf\u5339\u914d\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u5b9a\u4f4d\u95ee\u9898\u8f6c\u5316\u4e3a\u5339\u914d\u95ee\u9898\uff0c\u7a81\u7834\u4f20\u7edf\u76f4\u63a5\u4f4d\u59ff\u56de\u5f52\u6216BEV\u7279\u5f81\u5bf9\u9f50\u7684\u5c40\u9650\u3002  \n\u25c6 \u5f15\u5165Surface Model\u7cbe\u786e\u5efa\u6a21\u5730\u9762\u89c6\u89d2\u53ef\u89c1\u533a\u57df\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u9e1f\u77b0\u56fe\u6295\u5f71\uff0c\u89e3\u51b3\u51e0\u4f55\u4e0d\u4e00\u81f4\u95ee\u9898\u3002  \n\u25c6 \u8bbe\u8ba1SimRefiner\u6a21\u5757\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u6b8b\u5dee\u6821\u6b63\u4f18\u5316\u76f8\u4f3c\u5ea6\u77e9\u9635\uff0c\u65e0\u9700RANSAC\u540e\u5904\u7406\u5373\u53ef\u83b7\u5f97\u7cbe\u7ec6\u5339\u914d\u3002  \n\u25c6 \u6784\u5efa\u9996\u4e2a\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u8de8\u89c6\u89d2\u5339\u914d\u57fa\u51c6CVFM\uff08\u542b32,509\u5bf9\u56fe\u50cf\uff09\uff0c\u586b\u8865\u9886\u57df\u6570\u636e\u7a7a\u767d\u3002  \n\u25c6 \u5728\u6781\u7aef\u89c6\u89d2\u5dee\u5f02\u4e0b\u5b9e\u73b0\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u5339\u914d\u8d28\u91cf\u53cc\u91cd\u63d0\u5347\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002  \n\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u5efa\u6a21\u3001\u5339\u914d\u3001\u6570\u636e\u4e09\u65b9\u9762\u7684\u521b\u65b0\uff0c\u9996\u6b21\u7cfb\u7edf\u89e3\u51b3\u4e86\u8de8\u89c6\u89d2\u56fe\u50cf\u4e25\u683c\u5bf9\u5e94\u96be\u9898\uff0c\u63a8\u52a8GNSS\u62d2\u6b62\u73af\u5883\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u53d1\u5c55\u3002|\n",
    "2508.10294": "|2025-08-14|A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method|Tao Huang\u7b49|[2508.10294](http://arxiv.org/pdf/2508.10294)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f4d\u4e00\u81f4\u6027\u52a0\u6743\u6700\u5c0f\u7edd\u5bf9\u504f\u5dee\uff08PCWLAD\uff09\u7684\u4e9a\u50cf\u7d20\u6a21\u677f\u5339\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u5149\u5b66\u56fe\u50cf\u7684\u5339\u914d\u7cbe\u5ea6\u3002  \n\u25c6 \u91c7\u7528\u4e24\u9636\u6bb5\u5339\u914d\u7b56\u7565\uff1a\u5148\u901a\u8fc7\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\uff08SSIM\uff09\u8fdb\u884c\u7c97\u5339\u914d\uff0c\u518d\u5229\u7528WLAD\u8fdb\u884c\u7cbe\u7ec6\u5339\u914d\uff0c\u517c\u987e\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002  \n\u25c6 \u5728\u7c97\u5339\u914d\u9636\u6bb5\u4fdd\u7559\u539f\u59cb\u7ed3\u6784\u7ec6\u8282\uff08\u65e0\u566a\u58f0\u6ee4\u6ce2\uff09\uff0c\u901a\u8fc7SSIM\u589e\u5f3a\u5bf9\u975e\u7ebf\u6027\u8f90\u5c04\u5dee\u5f02\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728\u7cbe\u7ec6\u5339\u914d\u9636\u6bb5\u5f15\u5165\u8f90\u5c04\u548c\u51e0\u4f55\u53d8\u6362\u6a21\u578b\uff0c\u7ed3\u5408\u4e92\u7ed3\u6784\u6ee4\u6ce2\u6291\u5236\u566a\u58f0\u5bf9\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u8de8\u6a21\u6001\u5339\u914d\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u5728\u53ef\u89c1\u5149-\u7ea2\u5916\uff08Landsat\u3001\u65e0\u4eba\u673a\uff09\u548c\u53ef\u89c1\u5149-\u8fd1\u7ea2\u5916\uff08\u8fd1\u666f\uff09\u4e09\u7c7b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5e73\u5747\u5339\u914d\u7cbe\u5ea6\u8fbe0.4\u50cf\u7d20\uff0c\u4f18\u4e8e\u73b0\u67098\u79cd\u5148\u8fdb\u65b9\u6cd5\u3002  \n\u25c6 \u516c\u5f00\u4e86\u8f6f\u4ef6\u548c\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u5339\u914d\u7814\u7a76\u7684\u53d1\u5c55\u3002|\n",
    "2508.19742": "|2025-09-09|POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection|Chenguang Liu\u7b49|[2508.19742](http://arxiv.org/pdf/2508.19742)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPOEv2\u7684\u901a\u7528\u4e14\u9c81\u68d2\u7684\u7ebf\u68c0\u6d4b\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u662f\u7edf\u4e00\u4e86\u901a\u7528\u7ebf\u6bb5\u68c0\u6d4b\u548c\u7ed3\u6784\u5316\u7ebf\u6bb5\u68c0\u6d4b\u4e24\u5927\u4efb\u52a1\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u80dc\u4efb\u901a\u7528\u7ebf\u68c0\u6d4b\u548c\u7ed3\u6784\u5316\u7ebf\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u4e24\u7c7b\u68c0\u6d4b\u5668\u56e0\u8bbe\u8ba1\u76ee\u6807\u4e0d\u540c\u800c\u65e0\u6cd5\u4e92\u76f8\u66ff\u4ee3\u7684\u95ee\u9898\u3002  \n\u25c6 \u4f5c\u4e3aPixel Orientation Estimation (POE)\u65b9\u6cd5\u7684\u6539\u8fdb\u7248\uff0c\u65b0\u6846\u67b6\u80fd\u4ece\u8fb9\u7f18\u5f3a\u5ea6\u56fe\u4e2d\u68c0\u6d4b\u7ebf\u6bb5\uff0c\u5e76\u53ef\u517c\u5bb9\u4efb\u4f55\u8fb9\u7f18\u68c0\u6d4b\u5668\u3002  \n\u25c6 \u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u7684\u8fb9\u7f18\u68c0\u6d4b\u5668\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002  \n\u25c6 \u8be5\u6846\u67b6\u517c\u5177\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u7ebf\u68c0\u6d4b\u9700\u6c42\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.04273": "|2025-09-04|Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation|Junying Meng\u7b49|[2509.04273](http://arxiv.org/pdf/2509.04273)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u65b0\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5c06\u53d8\u5206\u6a21\u578b\u4e2d\u7684\u5148\u9a8c\u77e5\u8bc6\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6709\u6548\u7ed3\u5408\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u4e86\u53cc\u5c3a\u5ea6\u4f53\u79ef\u5148\u9a8c\uff0c\u5373\u5728\u56fe\u50cf\u5c3a\u5ea6\u548c\u6570\u636e\u96c6\u5c3a\u5ea6\u4e0a\u5206\u522b\u5229\u7528\u5f3a\u663e\u5f0f\u5148\u9a8c\u548c\u5f31\u9690\u5f0f\u5148\u9a8c\u6765\u7ea6\u675f\u5206\u5272\u7f51\u7edc\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56de\u5f52\u7f51\u7edc\u6765\u4f30\u8ba1\u672a\u6807\u6ce8\u56fe\u50cf\u7684\u76ee\u6807\u533a\u57df\u4f53\u79ef\uff0c\u5e76\u901a\u8fc7\u56fe\u50cf\u5c3a\u5ea6\u7684Wasserstein\u8ddd\u79bb\u635f\u5931\uff0c\u5f3a\u5236\u5206\u5272\u7ed3\u679c\u4e0e\u56de\u5f52\u9884\u6d4b\u7684\u7c7b\u522b\u6bd4\u4f8b\u4e00\u81f4\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u5c3a\u5ea6\u7684Wasserstein\u8ddd\u79bb\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u5f97\u672a\u6807\u6ce8\u6570\u636e\u96c6\u9884\u6d4b\u7684\u4f53\u79ef\u5206\u5e03\u4e0e\u5df2\u6807\u6ce8\u6570\u636e\u96c6\u7684\u5206\u5e03\u76f8\u4f3c\uff0c\u4ece\u800c\u5229\u7528\u6570\u636e\u96c6\u5c42\u9762\u7684\u7edf\u8ba1\u4fe1\u606f\u3002  \n\u25c6 \u5c06Threshold Dynamics\u7a7a\u95f4\u6b63\u5219\u5316\u65b9\u6cd5\u878d\u5165\u5206\u5272\u7f51\u7edc\u4e3b\u5e72\uff0c\u589e\u5f3a\u4e86\u7279\u5f81\u63d0\u53d6\u7684\u51e0\u4f55\u7ea6\u675f\u80fd\u529b\u3002  \n\u5b9e\u9a8c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u5206\u5272\u6027\u80fd\u3002|\n",
    "2509.06566": "|2025-09-08|Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval|Emil Demi\u0107\u7b49|[2509.06566](http://arxiv.org/pdf/2509.06566)|\u65e0|\u672c\u6587\u9488\u5bf9\u573a\u666f\u7ea7\u8349\u56fe\u68c0\u7d22\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u8c03\u8349\u56fe\u56fa\u6709\u6a21\u7cca\u6027\u548c\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8bad\u7ec3\u65b9\u6cd5\u3002  \n\u25c6 \u6307\u51fa\u4ee5\u5f80\u7814\u7a76\u5ffd\u7565\u771f\u5b9e\u8349\u56fe\u7684\u6b67\u4e49\u4e0e\u566a\u58f0\u95ee\u9898\uff0c\u8f6c\u800c\u5173\u6ce8\u8bad\u7ec3\u76ee\u6807\u7684\u9c81\u68d2\u6027\u8bbe\u8ba1\u3002  \n\u25c6 \u63d0\u51fa\u7ed3\u5408\u9884\u8bad\u7ec3\u7b56\u7565\u3001\u7f16\u7801\u5668\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\u7684\u4f18\u5316\u65b9\u6848\uff0c\u65e0\u9700\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002  \n\u25c6 \u5728FS-COCO\u548cSketchyCOCO\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u8bbe\u8ba1\u5bf9\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u5173\u952e\u4f5c\u7528\u3002  \n\u25c6 \u5f3a\u8c03\u9700\u6539\u8fdb\u573a\u666f\u7ea7\u8349\u56fe\u68c0\u7d22\u7684\u8bc4\u4f30\u573a\u666f\uff0c\u63a8\u52a8\u4efb\u52a1\u5411\u66f4\u5b9e\u7528\u5316\u53d1\u5c55\u3002|\n",
    "2509.08805": "|2025-09-23|Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching|Matthieu Vilain\u7b49|[2509.08805](http://arxiv.org/pdf/2509.08805)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7a20\u5bc6\u56fe\u50cf\u5339\u914d\u4e2d\u5904\u7406\u591a\u91cd\u5047\u8bbe\u7684\u65b0\u65b9\u6cd5BEAMER\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u663e\u8457\u63d0\u5347\u4e86\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u5339\u914d\u9c81\u68d2\u6027\u3002  \n\u25c6 \u6452\u5f03\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6bcf\u4e2a\u5c3a\u5ea6\u4e0a\u4ec5\u4e3a\u6bcf\u4e2a\u6e90\u4f4d\u7f6e\u9884\u6d4b\u5355\u4e00\u5bf9\u5e94\u70b9\u7684\u505a\u6cd5\uff0c\u521b\u65b0\u6027\u5730\u63d0\u51fa\u5728\u6bcf\u4e2a\u5c3a\u5ea6\u4e0a\u9884\u6d4b\u5e76\u4fdd\u7559\u591a\u4e2a\u5bf9\u5e94\u5047\u8bbe\u3002  \n\u25c6 \u91c7\u7528\u675f\u641c\u7d22\uff08beam search\uff09\u7b56\u7565\uff0c\u5728\u7531\u7c97\u5230\u7ec6\u7684\u5339\u914d\u8fc7\u7a0b\u4e2d\u9010\u5c3a\u5ea6\u5730\u4f20\u64ad\u548c\u4fdd\u7559\u8fd9\u4e9b\u591a\u91cd\u5047\u8bbe\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u5c06\u591a\u91cd\u5047\u8bbe\u96c6\u6210\u5230\u4ea4\u53c9\u6ce8\u610f\u529b\uff08cross-attention\uff09\u5c42\u4e2d\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u5b66\u4e60\u5982\u4f55\u6709\u6548\u5730\u5728\u4e0d\u540c\u5c3a\u5ea6\u95f4\u7b5b\u9009\u548c\u4f20\u64ad\u6700\u4f18\u5047\u8bbe\u3002  \n\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u6df1\u5ea6\u4e0d\u8fde\u7eed\u6216\u76ee\u6807\u56fe\u50cf\u662f\u6e90\u56fe\u50cf\u7684\u6781\u5927\u7f29\u653e\u573a\u666f\u65f6\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6280\u672f\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u9519\u8bef\u5339\u914d\u3002|\n",
    "2509.09594": "|2025-09-11|ObjectReact: Learning Object-Relative Control for Visual Navigation|Sourav Garg\u7b49|[2509.09594](http://arxiv.org/pdf/2509.09594)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u4f53\u76f8\u5bf9\u63a7\u5236\u7684\u89c6\u89c9\u5bfc\u822a\u65b0\u8303\u5f0fObjectReact\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u56fe\u50cf\u76f8\u5bf9\u63a7\u5236\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u201c\u7269\u4f53\u76f8\u5bf9\u201d\u63a7\u5236\u66ff\u4ee3\u4e3b\u6d41\u7684\u201c\u56fe\u50cf\u76f8\u5bf9\u201d\u63a7\u5236\uff0c\u5229\u7528\u7269\u4f53\u4f5c\u4e3a\u5730\u56fe\u56fa\u6709\u5c5e\u6027\uff0c\u6446\u8131\u5bf9\u667a\u80fd\u4f53\u4f4d\u59ff\u548c\u5177\u4f53\u5f62\u6001\u7684\u4e25\u683c\u4f9d\u8d56\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u57fa\u4e8e\u201c\u76f8\u5bf93D\u573a\u666f\u56fe\u201d\u7684\u62d3\u6251-\u5ea6\u91cf\u6df7\u5408\u5730\u56fe\u8868\u793a\uff0c\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u6548\u7684\u5bf9\u8c61\u7ea7\u5168\u5c40\u8def\u5f84\u89c4\u5212\u4ee3\u4ef7\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u76f4\u63a5\u4ee5\u9ad8\u5c42\u201cWayObject Costmap\u201d\u4e3a\u8f93\u5165\u6761\u4ef6\u7684\u672c\u5730\u63a7\u5236\u5668\uff0c\u65e0\u9700\u663e\u5f0fRGB\u8f93\u5165\uff0c\u5b9e\u73b0\u4e86\u63a7\u5236\u9884\u6d4b\u4e0e\u56fe\u50cf\u5339\u914d\u95ee\u9898\u7684\u89e3\u8026\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u5728\u8de8\u5f62\u6001\u90e8\u7f72\uff08\u5982\u4f20\u611f\u5668\u9ad8\u5ea6\u53d8\u5316\uff09\u548c\u6311\u6218\u6027\u4efb\u52a1\uff08\u5982\u53cd\u5411\u8f68\u8ff9\u5bfc\u822a\uff09\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u4ec5\u4f7f\u7528\u4eff\u771f\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u3002|\n",
    "2509.09792": "|2025-09-29|Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local Feature Matching|Zimin Xia\u7b49|[2509.09792](http://arxiv.org/pdf/2509.09792)|\u65e0|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u7ec6\u7c92\u5ea6\u8de8\u89c6\u89d2\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u7279\u5f81\u5339\u914d\u4e0e\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\u5b9e\u73b0\u5730\u9762\u56fe\u50cf\u7684\u4e09\u81ea\u7531\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u3002  \n\u25c6 \u76f4\u63a5\u5efa\u7acb\u5730\u9762\u4e0e\u822a\u7a7a\u56fe\u50cf\u95f4\u7684\u5c40\u90e8\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56e0\u89c6\u89d2\u8f6c\u6362\u9020\u6210\u7684\u4fe1\u606f\u635f\u5931\u3002  \n\u25c6 \u5229\u7528\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\u4ec5\u5c06\u5339\u914d\u5173\u952e\u70b9\u63d0\u5347\u81f3\u9e1f\u77b0\u56fe\u7a7a\u95f4\uff0c\u652f\u6301\u5ea6\u91cf\u6df1\u5ea6\u4e0e\u76f8\u5bf9\u6df1\u5ea6\u4e24\u79cd\u6a21\u5f0f\u3002  \n\u25c6 \u63d0\u51fa\u5c3a\u5ea6\u611f\u77e5\u7684\u666e\u6c0f\u5bf9\u9f50\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u5bf9\u5e94\u5173\u7cfb\u4e2d\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\uff0c\u5e76\u5728\u4f7f\u7528\u76f8\u5bf9\u6df1\u5ea6\u65f6\u6062\u590d\u5c3a\u5ea6\u3002  \n\u25c6 \u4ec5\u9700\u5f31\u76d1\u7763\u4f4d\u59ff\u6807\u6ce8\u5373\u53ef\u5b66\u4e60\u7cbe\u786e\u7279\u5f81\u5bf9\u5e94\uff0c\u5728\u8de8\u533a\u57df\u6cdb\u5316\u4e0e\u672a\u77e5\u671d\u5411\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002  \n\u25c6 \u517c\u5bb9\u591a\u79cd\u76f8\u5bf9\u6df1\u5ea6\u6a21\u578b\u4e14\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u6a21\u578b\u5fae\u8c03\uff0c\u5177\u5907\u8f83\u5f3a\u7684\u5b9e\u7528\u6027\u4e0e\u90e8\u7f72\u7075\u6d3b\u6027\u3002|\n",
    "2509.11255": "|2025-09-14|A Geometrically Consistent Matching Framework for Side-Scan Sonar Mapping|Can Lei\u7b49|[2509.11255](http://arxiv.org/pdf/2509.11255)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u4fa7\u626b\u58f0\u7eb3\u56fe\u50cf\u56e0\u89c6\u89d2\u4f9d\u8d56\u3001\u9634\u5f71\u548c\u51e0\u4f55\u7578\u53d8\u5bfc\u81f4\u7684\u5339\u914d\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u89e3\u8026\u4e0e\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u521b\u65b0\u5339\u914d\u6846\u67b6\u3002  \n\u25c6 \u63d0\u51fa\u81ea\u76d1\u7763\u591a\u5206\u652f\u7f51\u7edc\uff0c\u57fa\u4e8e\u6717\u4f2f\u53cd\u5c04\u6a21\u578b\u5c06\u539f\u59cb\u58f0\u7eb3\u56fe\u50cf\u5206\u89e3\u4e3a\u6d77\u5e95\u53cd\u5c04\u7387\u3001\u5730\u5f62\u9ad8\u7a0b\u548c\u58f0\u5b66\u8def\u5f84\u635f\u8017\uff0c\u589e\u5f3a\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002  \n\u25c6 \u5229\u7528\u53cd\u5c04\u7387\u56fe\u8c31\u4f5c\u4e3a\u7a33\u5b9a\u5339\u914d\u57df\uff0c\u7ed3\u5408\u65e0\u8bad\u7ec3\u5339\u914d\u6d41\u7a0b\uff08SuperPoint\u4e0eMINIMA LightGlue\uff09\uff0c\u63d0\u5347\u8de8\u89c6\u89d2\u5bf9\u5e94\u5173\u7cfb\u51c6\u786e\u6027\u3002  \n\u25c6 \u5f15\u5165\u51e0\u4f55\u611f\u77e5\u5f02\u5e38\u70b9\u5254\u9664\u673a\u5236\uff0c\u8054\u5408\u5730\u5f62\u9ad8\u7a0b\u4e0e\u7269\u7406\u884d\u751f\u7684\u9634\u5f71\u56fe\u8c31\uff0c\u6709\u6548\u6291\u5236\u58f0\u5b66\u906e\u6321\u548c\u5730\u5f62\u4e0d\u4e00\u81f4\u533a\u57df\u7684\u8bef\u5339\u914d\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5339\u914d\u8bef\u5dee\u3001\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89d2\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u53ca\u57fa\u4e8eCNN\u4e0eTransformer\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u6d77\u5e95\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u3001\u6570\u636e\u9ad8\u6548\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u7684\u5339\u914d\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.14966": "|2025-09-18|RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching|Xingwu Zhang\u7b49|[2509.14966](http://arxiv.org/pdf/2509.14966)|\u65e0|\u25c6 The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult.\n\u25c6 As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features.\n\u25c6 Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps.|\n",
    "2509.16017": "|2025-09-19|DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching|Meng Yang\u7b49|[2509.16017](http://arxiv.org/pdf/2509.16017)|\u65e0|\u25c6 Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis.\n\u25c6 However, the significant appearance differences between modalities make this task challenging.\n\u25c6 Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios.|\n",
    "2509.17431": "|2025-09-23|Hierarchical Neural Semantic Representation for 3D Semantic Correspondence|Keyu Du\u7b49|[2509.17431](http://arxiv.org/pdf/2509.17431)|\u65e0|\u25c6 This paper presents a new approach to estimate accurate and robust 3D semantic correspondence with the hierarchical neural semantic representation.\n\u25c6 Our work has three key contributions.\n\u25c6 First, we design the hierarchical neural semantic representation (HNSR), which consists of a global semantic feature to capture high-level structure and multi-resolution local geometric features to preserve fine details, by carefully harnessing 3D priors from pre-trained 3D generative models.|\n",
    "2509.16519": "|2025-09-20|PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality|Yang Han|[2509.16519](http://arxiv.org/pdf/2509.16519)|\u65e0|\u25c6 We introduce PM25Vision (PM25V), the largest and most comprehensive dataset to date for estimating air quality - specifically PM2.5 concentrations - from street-level images.\n\u25c6 The dataset contains over 11,114 images matched with timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations and 11 years, significantly exceeding the scale of previous benchmarks.\n\u25c6 The spatial accuracy of this dataset has reached 5 kilometers, far exceeding the city-level accuracy of many datasets.|\n",
    "2510.05051": "|2025-10-24|SegMASt3R: Geometry Grounded Segment Matching|Rohit Jayanti\u7b49|[2510.05051](http://arxiv.org/pdf/2510.05051)|\u65e0|\u25c6 Segment matching is an important intermediate task in computer vision that establishes correspondences between semantically or geometrically coherent regions across images.\n\u25c6 Unlike keypoint matching, which focuses on localized features, segment matching captures structured regions, offering greater robustness to occlusions, lighting variations, and viewpoint changes.\n\u25c6 In this paper, we leverage the spatial understanding of 3D foundation models to tackle wide-baseline segment matching, a challenging setting involving extreme viewpoint shifts.|\n",
    "2510.06829": "|2025-10-08|Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera|Mikihiro Ikura\u7b49|[2510.06829](http://arxiv.org/pdf/2510.06829)|\u65e0|\u25c6 Line segment extraction is effective for capturing geometric features of human-made environments.\n\u25c6 Event-based cameras, which asynchronously respond to contrast changes along edges, enable efficient extraction by reducing redundant data.\n\u25c6 However, recent methods often rely on additional frame cameras or struggle with high event rates.|\n",
    "2510.06827": "|2025-10-08|StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance|Jaeseok Jeong\u7b49|[2510.06827](http://arxiv.org/pdf/2510.06827)|\u65e0|\u25c6 In the domain of text-to-image generation, diffusion models have emerged as powerful tools.\n\u25c6 Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content.\n\u25c6 However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style.|\n",
    "2510.06820": "|2025-10-08|Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking|Mitchell Keren Taraday\u7b49|[2510.06820](http://arxiv.org/pdf/2510.06820)|\u65e0|\u25c6 Multimodal retrieval still leans on embedding-based models like CLIP for fast vector search over pre-computed image embeddings.\n\u25c6 Yet, unlike text retrieval, where joint-encoder rerankers are standard, comparable vision--language rerankers are largely absent.\n\u25c6 We find that seminal joint encoders such as BLIP are severely bottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale.|\n",
    "2510.22900": "|2025-10-27|The MDW H\u03b1 Sky Survey: Data Release 1|Noor Aftab\u7b49|[2510.22900](http://arxiv.org/pdf/2510.22900)|\u65e0|\u25c6 The Mittelman-di Cicco-Walker (MDW) H$\\alpha$ Sky Survey is an autonomously-operated all-sky narrow-band (3nm) H$\\alpha$ imaging survey.\n\u25c6 The survey was founded by amateur astronomers and the northern sky (Decl.\n\u25c6 $\\geq$ 0$^\\circ$) is presented here in its second stage of refinement for academic use.|\n",
    "2510.22827": "|2025-10-26|FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment|Zahraa Al Sahili\u7b49|[2510.22827](http://arxiv.org/pdf/2510.22827)|\u65e0|\u25c6 Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how well images match prompts and how models treat social attributes.\n\u25c6 Common proxies -- face classifiers and contrastive similarity -- reward surface cues, lack calibrated abstention, and miss attributes only weakly visible (for example, religion, culture, disability).\n\u25c6 We present FairJudge, a lightweight protocol that treats instruction-following multimodal LLMs as fair judges.|\n",
    "2511.02489": "|2025-11-04|Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization|Tao Liu\u7b49|[2511.02489](http://arxiv.org/pdf/2511.02489)|\u65e0|\u25c6 With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems.\n\u25c6 However, in GNSS-denied areas, satellite-based localization methods are prone to failure.\n\u25c6 This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching.|\n",
    "2511.03416": "|2025-11-05|Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort|Nikolai Herrmann\u7b49|[2511.03416](http://arxiv.org/pdf/2511.03416)|\u65e0|\u25c6 Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans.\n\u25c6 In this work, we propose an automated method for standardizing this alignment.\n\u25c6 Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo's principal axes, from which four candidate orientations are derived.|\n",
    "2511.05949": "|2025-11-08|U(PM)$^2$:Unsupervised polygon matching with pre-trained models for challenging stereo images|Chang Li\u7b49|[2511.05949](http://arxiv.org/pdf/2511.05949)|\u65e0|\u25c6 Stereo image matching is a fundamental task in computer vision, photogrammetry and remote sensing, but there is an almost unexplored field, i.e., polygon matching, which faces the following challenges: disparity discontinuity, scale variation, training requirement, and generalization.\n\u25c6 To address the above-mentioned issues, this paper proposes a novel U(PM)$^2$: low-cost unsupervised polygon matching with pre-trained models by uniting automatically learned and handcrafted features, of which pipeline is as follows: firstly, the detector leverages the pre-trained segment anything model to obtain masks; then, the vectorizer converts the masks to polygons and graphic structure; secondly, the global matcher addresses challenges from global viewpoint changes and scale variation based on bidirectional-pyramid strategy with pre-trained LoFTR; finally, the local matcher further overcomes local disparity discontinuity and topology inconsistency of polygon matching by local-joint geometry and multi-feature matching strategy with Hungarian algorithm.\n\u25c6 We benchmark our U(PM)$^2$ on the ScanNet and SceneFlow datasets using our proposed new metric, which achieved state-of-the-art accuracy at a competitive speed and satisfactory generalization performance at low cost without any training requirement.|\n",
    "2511.10629": "|2025-11-13|One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models|Aleksandr Razin\u7b49|[2511.10629](http://arxiv.org/pdf/2511.10629)|\u65e0|\u25c6 Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding.\n\u25c6 We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step.\n\u25c6 LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space.|\n",
    "2511.10597": "|2025-11-13|From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis|Yen Nhi Truong Vu\u7b49|[2511.10597](http://arxiv.org/pdf/2511.10597)|\u65e0|\u25c6 Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT.\n\u25c6 To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information.\n\u25c6 Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data.|\n",
    "2511.10486": "|2025-11-13|Revealing the Connection Between the Filamentary Hierarchy and Star Cluster Formation in a Simulated NGC 628 Galaxy|Tamara Koletic\u7b49|[2511.10486](http://arxiv.org/pdf/2511.10486)|\u65e0|\u25c6 There is abundant observational evidence for the hierarchical, interconnected nature of filaments in the interstellar medium (ISM) extending from galactic down to sub-parsec scales.\n\u25c6 New JWST images of NGC 628 in particular, show clusters forming along the two spiral arms of this galaxy.\n\u25c6 In this paper we investigate filament and cluster properties in an NGC 628-like multi-scale high-resolution magnetohydrodynamic simulation.|\n",
    "2511.10424": "|2025-11-13|Domain Adaptation for Camera-Specific Image Characteristics using Shallow Discriminators|Maximiliane Gruber\u7b49|[2511.10424](http://arxiv.org/pdf/2511.10424)|\u65e0|\u25c6 Each image acquisition setup leads to its own camera-specific image characteristics degrading the image quality.\n\u25c6 In learning-based perception algorithms, characteristics occurring during the application phase, but absent in the training data, lead to a domain gap impeding the performance.\n\u25c6 Previously, pixel-level domain adaptation through unpaired learning of the pristine-to-distorted mapping function has been proposed.|\n",
    "2511.10316": "|2025-11-13|Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision|Yu Deng\u7b49|[2511.10316](http://arxiv.org/pdf/2511.10316)|\u65e0|\u25c6 Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions.\n\u25c6 Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions.\n\u25c6 This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting.|\n",
    "2511.10035": "|2025-11-13|DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection|Feiyang Jia\u7b49|[2511.10035](http://arxiv.org/pdf/2511.10035)|\u65e0|\u25c6 As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians.\n\u25c6 However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems.\n\u25c6 We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities.|\n",
    "2511.09954": "|2025-11-13|New ASKAP radio-continuum surveys of the Small Magellanic Cloud|O. K. Khattab\u7b49|[2511.09954](http://arxiv.org/pdf/2511.09954)|\u65e0|\u25c6 We present two new radio continuum images from the ASKAP POSSUM survey in the direction of the Small Magellanic Cloud.\n\u25c6 The two new source lists contain 36,571 radio continuum sources detected at 944 MHz and 15,227 sources at 1367 MHz, with beam sizes of approximately 14.5 by 12.2 arcsec and 8.7 by 8.2 arcsec, respectively.\n\u25c6 We used the Aegean software package to generate these point source catalogues, and together with the previously published MeerKAT catalogue, we estimated spectral indices for the full set of matched radio point sources.|\n",
    "2511.09948": "|2025-11-13|Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment|Zhicheng Liao\u7b49|[2511.09948](http://arxiv.org/pdf/2511.09948)|\u65e0|\u25c6 Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as \"a good photo\" or \"a bad photo.\" However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality.\n\u25c6 In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue.\n\u25c6 Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity.|\n",
    "2511.09820": "|2025-11-12|From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance|Jeongho Min\u7b49|[2511.09820](http://arxiv.org/pdf/2511.09820)|\u65e0|\u25c6 Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments.\n\u25c6 However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment.\n\u25c6 In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training.|\n",
    "2511.09771": "|2025-11-12|STORM: Segment, Track, and Object Re-Localization from a Single 3D Model|Yu Deng\u7b49|[2511.09771](http://arxiv.org/pdf/2511.09771)|\u65e0|\u25c6 Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots.\n\u25c6 However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement.\n\u25c6 To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation.|\n",
    "2511.11526": "|2025-11-14|Bridging Hidden States in Vision-Language Models|Benjamin Fein-Ashley\u7b49|[2511.11526](http://arxiv.org/pdf/2511.11526)|\u65e0|\u25c6 Vision-Language Models (VLMs) are a new family of models that align image content with natural language.\n\u25c6 Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings.\n\u25c6 Many methods also tie fusion to an autoregressive decoder.|\n",
    "2511.11522": "|2025-11-14|CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation|Luthira Abeykoon\u7b49|[2511.11522](http://arxiv.org/pdf/2511.11522)|\u65e0|\u25c6 Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms.\n\u25c6 However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences.\n\u25c6 This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move.|\n",
    "2511.11437": "|2025-11-14|Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping|Guowei Zhang\u7b49|[2511.11437](http://arxiv.org/pdf/2511.11437)|\u65e0|\u25c6 Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex.\n\u25c6 This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas.\n\u25c6 We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit.|\n",
    "2511.11435": "|2025-11-14|The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models|Maria-Teresa De Rosa Palmini\u7b49|[2511.11435](http://arxiv.org/pdf/2511.11435)|\u65e0|\u25c6 Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity.\n\u25c6 This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene.\n\u25c6 While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them.|\n",
    "2511.11422": "|2025-11-14|Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment|Lukun Wu\u7b49|[2511.11422](http://arxiv.org/pdf/2511.11422)|\u65e0|\u25c6 Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach.\n\u25c6 We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG's inherent noise and signal degradation, vs.\n\u25c6 vision's high-fidelity features) and a Semantic Gap (arising from EEG's shallow conceptual representation, vs.|\n",
    "2511.11388": "|2025-11-14|Robust inverse material design with physical guarantees using the Voigt-Reuss Net|Sanath Keshav\u7b49|[2511.11388](http://arxiv.org/pdf/2511.11388)|\u65e0|\u25c6 We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees.\n\u25c6 Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the L\u00f6wner sense.\n\u25c6 In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\\!7.5\\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \\ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs.|\n",
    "2511.11243": "|2025-11-14|Arcee: Differentiable Recurrent State Chain for Generative Vision Modeling with Mamba SSMs|Jitesh Chavan\u7b49|[2511.11243](http://arxiv.org/pdf/2511.11243)|\u65e0|\u25c6 State-space models (SSMs), Mamba in particular, are increasingly adopted for long-context sequence modeling, providing linear-time aggregation via an input-dependent, causal selective-scan operation.\n\u25c6 Along this line, recent \"Mamba-for-vision\" variants largely explore multiple scan orders to relax strict causality for non-sequential signals (e.g., images).\n\u25c6 Rather than preserving cross-block memory, the conventional formulation of the selective-scan operation in Mamba reinitializes each block's state-space dynamics from zero, discarding the terminal state-space representation (SSR) from the previous block.|\n",
    "2511.11236": "|2025-11-14|Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing|Cong Cao\u7b49|[2511.11236](http://arxiv.org/pdf/2511.11236)|\u65e0|\u25c6 In recent years, image editing has garnered growing attention.\n\u25c6 However, general image editing models often fail to produce satisfactory results when confronted with new styles.\n\u25c6 The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data.|\n",
    "2511.11074": "|2025-11-14|Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image|Matthias Humt\u7b49|[2511.11074](http://arxiv.org/pdf/2511.11074)|\u65e0|\u25c6 While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached.\n\u25c6 Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated.\n\u25c6 In this work, we compare two of the most promising generative models--Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers--which we adapt for the tasks of generative shape modeling and completion.|\n",
    "2511.11045": "|2025-11-14|Hyperbolic Hierarchical Alignment Reasoning Network for Text-3D Retrieval|Wenrui Li\u7b49|[2511.11045](http://arxiv.org/pdf/2511.11045)|\u65e0|\u25c6 With the daily influx of 3D data on the internet, text-3D retrieval has gained increasing attention.\n\u25c6 However, current methods face two major challenges: Hierarchy Representation Collapse (HRC) and Redundancy-Induced Saliency Dilution (RISD).\n\u25c6 HRC compresses abstract-to-specific and whole-to-part hierarchies in Euclidean embeddings, while RISD averages noisy fragments, obscuring critical semantic cues and diminishing the model's ability to distinguish hard negatives.|\n",
    "2511.13494": "|2025-11-17|Language-Guided Invariance Probing of Vision-Language Models|Jae Joong Lee|[2511.13494](http://arxiv.org/pdf/2511.13494)|\u65e0|\u25c6 Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations.\n\u25c6 We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching.\n\u25c6 Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.|\n",
    "2511.13377": "|2025-11-17|PDRs4All XX. Haute Couture: Spectral stitching of JWST MIRI-IFU cubes with matrix completion|Am\u00e9lie Canin\u7b49|[2511.13377](http://arxiv.org/pdf/2511.13377)|\u65e0|\u25c6 MIRI is the imager and spectrograph covering wavelengths from $4.9$ to $27.9$ $\u03bc$m onboard the James Webb Space Telescope (JWST).\n\u25c6 The Medium-Resolution Spectrometer (MRS) consists of four integral field units (IFU), each of which has three sub-channels.\n\u25c6 The twelve resulting spectral data cubes have different fields of view, spatial, and spectral resolutions.|\n",
    "2511.13353": "|2025-11-17|Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images|Lucas Gabriel Telesco\u7b49|[2511.13353](http://arxiv.org/pdf/2511.13353)|\u65e0|\u25c6 Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases.\n\u25c6 However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture.\n\u25c6 This gap is mainly due to the high cost of detailed annotations.|\n",
    "2511.13249": "|2025-11-17|Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention|Yu Wen\u7b49|[2511.13249](http://arxiv.org/pdf/2511.13249)|\u65e0|\u25c6 Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions.\n\u25c6 Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results.\n\u25c6 We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features.|\n",
    "2511.13183": "|2025-11-17|GenTract: Generative Global Tractography|Alec Sargood\u7b49|[2511.13183](http://arxiv.org/pdf/2511.13183)|\u65e0|\u25c6 Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI).\n\u25c6 Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data.\n\u25c6 In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive.|\n",
    "2511.13170": "|2025-11-17|THIR: Topological Histopathological Image Retrieval|Zahra Tabatabaei\u7b49|[2511.13170](http://arxiv.org/pdf/2511.13170)|\u65e0|\u25c6 According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020.\n\u25c6 Early diagnosis and accurate clinical decision making are critical in reducing this global burden.\n\u25c6 In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns.|\n",
    "2511.13168": "|2025-11-17|SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration|Haodong Wang\u7b49|[2511.13168](http://arxiv.org/pdf/2511.13168)|\u65e0|\u25c6 Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics.\n\u25c6 Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory.\n\u25c6 Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences.|\n",
    "2511.13102": "|2025-11-17|CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation|Yu Zhu\u7b49|[2511.13102](http://arxiv.org/pdf/2511.13102)|\u65e0|\u25c6 Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks.\n\u25c6 While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept \"leg\" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat).\n\u25c6 To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images.|\n",
    "2511.12972": "|2025-11-17|SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models|Siddarth Narasimhan\u7b49|[2511.12972](http://arxiv.org/pdf/2511.12972)|\u65e0|\u25c6 The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target.\n\u25c6 This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions.\n\u25c6 In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions.|\n",
    "2511.12898": "|2025-11-17|Functional Mean Flow in Hilbert Space|Zhiqi Li\u7b49|[2511.12898](http://arxiv.org/pdf/2511.12898)|\u65e0|\u25c6 We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space.\n\u25c6 FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling.\n\u25c6 We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form.|\n",
    "2511.14419": "|2025-11-18|FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis|Xiaowei Xu\u7b49|[2511.14419](http://arxiv.org/pdf/2511.14419)|\u65e0|\u25c6 Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases.\n\u25c6 Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells.\n\u25c6 This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation.|\n",
    "2511.14411": "|2025-11-18|Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays|Ravi Shankar Prasad\u7b49|[2511.14411](http://arxiv.org/pdf/2511.14411)|\u65e0|\u25c6 In forensic craniofacial identification and in many biomedical applications, craniometric landmarks are important.\n\u25c6 Traditional methods for locating landmarks are time-consuming and require specialized knowledge and expertise.\n\u25c6 Current methods utilize superimposition and deep learning-based methods that employ automatic annotation of landmarks.|\n",
    "2511.14315": "|2025-11-18|Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs|Yiyi Miao\u7b49|[2511.14315](http://arxiv.org/pdf/2511.14315)|\u65e0|\u25c6 Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery.\n\u25c6 While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging.\n\u25c6 The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation.|\n",
    "2511.14286": "|2025-11-18|NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration|Luohong Wu\u7b49|[2511.14286](http://arxiv.org/pdf/2511.14286)|\u65e0|\u25c6 In computer- and robot-assisted orthopedic surgery (CAOS), patient-specific surgical plans derived from preoperative imaging define target locations and implant trajectories.\n\u25c6 During surgery, these plans must be accurately transferred, relying on precise cross-registration between preoperative and intraoperative data.\n\u25c6 However, substantial modality heterogeneity across imaging modalities makes this registration challenging and error-prone.|\n",
    "2511.14229": "|2025-11-18|EBind: a practical approach to space binding|Jim Broadbent\u7b49|[2511.14229](http://arxiv.org/pdf/2511.14229)|\u65e0|\u25c6 We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days.\n\u25c6 We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models.\n\u25c6 We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size.|\n",
    "2511.14208": "|2025-11-18|InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior|Weimin Bai\u7b49|[2511.14208](http://arxiv.org/pdf/2511.14208)|\u65e0|\u25c6 Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints.\n\u25c6 Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use.\n\u25c6 We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior.|\n",
    "2511.14149": "|2025-11-18|iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion|Hao Wang\u7b49|[2511.14149](http://arxiv.org/pdf/2511.14149)|\u65e0|\u25c6 Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model.\n\u25c6 However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose.\n\u25c6 This multi-round process incurs significant computational overhead, hindering real-time performance in robotics.|\n",
    "2511.14109": "|2025-11-18|$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors|Zhenyu Li\u7b49|[2511.14109](http://arxiv.org/pdf/2511.14109)|\u65e0|\u25c6 Visual Place Recognition (VPR) aims to match query images against a database using visual cues.\n\u25c6 State-of-the-art methods aggregate features from deep backbones to form global descriptors.\n\u25c6 Optimal transport-based aggregation methods reformulate feature-to-cluster assignment as a transport problem, but the standard Sinkhorn algorithm symmetrically treats source and target marginals, limiting effectiveness when image features and cluster centers exhibit substantially different distributions.|\n",
    "2511.14093": "|2025-11-18|SMGeo: Cross-View Object Geo-Localization with Grid-Level Mixture-of-Experts|Fan Zhang\u7b49|[2511.14093](http://arxiv.org/pdf/2511.14093)|\u65e0|\u25c6 Cross-view object Geo-localization aims to precisely pinpoint the same object across large-scale satellite imagery based on drone images.\n\u25c6 Due to significant differences in viewpoint and scale, coupled with complex background interference, traditional multi-stage \"retrieval-matching\" pipelines are prone to cumulative errors.\n\u25c6 To address this, we present SMGeo, a promptable end-to-end transformer-based model for object Geo-localization.|\n",
    "2511.14044": "|2025-11-18|The CHASM-SWPC Dataset for Coronal Hole Detection & Analysis|Cutter Beck\u7b49|[2511.14044](http://arxiv.org/pdf/2511.14044)|\u65e0|\u25c6 Coronal holes (CHs) are low-activity, low-density solar coronal regions with open magnetic field lines (Cranmer 2009).\n\u25c6 In the extreme ultraviolet (EUV) spectrum, CHs appear as dark patches.\n\u25c6 Using daily hand-drawn maps from the Space Weather Prediction Center (SWPC), we developed a semi-automated pipeline to digitize the SWPC maps into binary segmentation masks.|\n",
    "2511.15706": "|2025-11-20|RoMa v2: Harder Better Faster Denser Feature Matching|Johan Edstedt\u7b49|[2511.15706](http://arxiv.org/pdf/2511.15706)|\u65e0|\u25c6 Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness.\n\u25c6 However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability.\n\u25c6 In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model.|\n",
    "2511.15316": "|2025-11-19|What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs|Zhihan Ren\u7b49|[2511.15316](http://arxiv.org/pdf/2511.15316)|\u65e0|\u25c6 Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA).\n\u25c6 Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage.\n\u25c6 To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features.|\n",
    "2511.15209": "|2025-11-19|Magnetic signal scan imaging system based on giant magnetoimpedance (GMI) differential sensor|Tao Yang\u7b49|[2511.15209](http://arxiv.org/pdf/2511.15209)|\u65e0|\u25c6 This paper presents the design and implementation of a magnetic signal scanning and imaging system based on the giant magnetoimpedance (GMI) effect.\n\u25c6 The system employs a pair of performance-matched GMI sensing elements configured as a differential probe structure.\n\u25c6 Through co-optimized low-noise electronic and probe design, the system effectively suppresses both intrinsic sensor common-mode drift and external environmental magnetic noise, enabling high signal-to-noise ratio detection of nono-tesla to micro-tesla-level magnetic signals without magnetic shielding.|\n",
    "2511.15066": "|2025-11-19|BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching|Yachuan Huang\u7b49|[2511.15066](http://arxiv.org/pdf/2511.15066)|\u65e0|\u25c6 Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest.\n\u25c6 Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge.\n\u25c6 Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency.|\n",
    "2511.15029": "|2025-11-19|Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans|Zekun Wang\u7b49|[2511.15029](http://arxiv.org/pdf/2511.15029)|\u65e0|\u25c6 Mathematical thinking is a fundamental aspect of human cognition.\n\u25c6 Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan.\n\u25c6 Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults.|\n",
    "2511.14963": "|2025-11-18|LFreeDA: Label-Free Drift Adaptation for Windows Malware Detection|Adrian Shuai Li\u7b49|[2511.14963](http://arxiv.org/pdf/2511.14963)|\u65e0|\u25c6 Machine learning (ML)-based malware detectors degrade over time as concept drift introduces new and evolving families unseen during training.\n\u25c6 Retraining is limited by the cost and time of manual labeling or sandbox analysis.\n\u25c6 Existing approaches mitigate this via drift detection and selective labeling, but fully label-free adaptation remains largely unexplored.|\n",
    "2511.16674": "|2025-11-20|Dataset Distillation for Pre-Trained Self-Supervised Vision Models|George Cazenavette\u7b49|[2511.16674](http://arxiv.org/pdf/2511.16674)|\u65e0|\u25c6 The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples.\n\u25c6 Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models.\n\u25c6 In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch.|\n",
    "2511.16566": "|2025-11-20|NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening|Misaal Khan\u7b49|[2511.16566](http://arxiv.org/pdf/2511.16566)|\u65e0|\u25c6 Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention.\n\u25c6 In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance.\n\u25c6 In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings.|\n",
    "2511.16520": "|2025-11-20|Saving Foundation Flow-Matching Priors for Inverse Problems|Yuxiang Wan\u7b49|[2511.16520](http://arxiv.org/pdf/2511.16520)|\u65e0|\u25c6 Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors.\n\u25c6 How can we unlock their potential?\n\u25c6 We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs.|\n",
    "2511.16435": "|2025-11-20|Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation|Jin Wang\u7b49|[2511.16435](http://arxiv.org/pdf/2511.16435)|\u65e0|\u25c6 Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm.\n\u25c6 Existing methods mainly mine references from support images as meta guidance.\n\u25c6 However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes.|\n",
    "2511.16364": "|2025-11-20|DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration|Meng-Cheng Shih\u7b49|[2511.16364](http://arxiv.org/pdf/2511.16364)|\u65e0|\u25c6 Offline signature verification (OSV) is a frequently utilized technology in forensics.\n\u25c6 This paper proposes a new model, DetailSemNet, for OSV.\n\u25c6 Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV.|\n",
    "2511.16349": "|2025-11-20|CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering|Joni Vanherck\u7b49|[2511.16349](http://arxiv.org/pdf/2511.16349)|\u65e0|\u25c6 Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content.\n\u25c6 Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure.\n\u25c6 This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud.|\n",
    "2511.16206": "|2025-11-20|A pilot VLBI study of the SQUAB quasar sample featuring multiple Gaia detections|Yingkang Zhang\u7b49|[2511.16206](http://arxiv.org/pdf/2511.16206)|\u65e0|\u25c6 Our previous work identified a class of SDSS quasars exhibiting multiple Gaia detections, classifying them as candidates for various astrophysical systems such as quasar-star pairs, dual quasars, and gravitationally lensed quasars.\n\u25c6 In this paper, we present a pilot VLBI study targeting a radio-bright subsample and report the first high-resolution imaging results.\n\u25c6 By leveraging the milliarcsecond-scale resolution of VLBI and its precise astrometric coordination incorporating with Gaia, we aim to refine the classification of these multiple matched sources, search for potential dual AGNs, and assess the efficacy of the combined Gaia-VLBI approach in resolving ambiguous quasar systems.|\n",
    "2511.15963": "|2025-11-20|PySERA: Open-Source Standardized Python Library for Automated, Scalable, and Reproducible Handcrafted and Deep Radiomics|Mohammad R. Salmanpour\u7b49|[2511.15963](http://arxiv.org/pdf/2511.15963)|\u65e0|\u25c6 Radiomics enables the extraction of quantitative biomarkers from medical images for precision modeling, but reproducibility and scalability remain limited due to heterogeneous software implementations and incomplete adherence to standards.\n\u25c6 Existing tools also lack unified support for deep learning based radiomics.\n\u25c6 To address these limitations, we introduce PySERA, an open source, Python native, standardized radiomics framework designed for automation, reproducibility, and seamless AI integration.|\n",
    "2511.15795": "|2025-11-19|The MeerKAT Fornax Survey VI. The collapse of the galaxy HI Mass Function in Fornax|D. Kleiner\u7b49|[2511.15795](http://arxiv.org/pdf/2511.15795)|\u65e0|\u25c6 We present the deepest HI mass Function (HIMF) ever measured, outside the Local Group.\n\u25c6 The observations are part of the MeerKAT Fornax Survey and cover a 4 x 4 deg^2 field, corresponding to ~ Rvir.\n\u25c6 The 3$\u03c3$ detection limit is log(MHI/Msun) = 5.7 for a 50 km/s-wide point source.|\n",
    "2511.17432": "|2025-11-21|SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation|Shrikant Kendre\u7b49|[2511.17432](http://arxiv.org/pdf/2511.17432)|\u65e0|\u25c6 Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment.\n\u25c6 While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important.\n\u25c6 Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations.|\n",
    "2511.17411": "|2025-11-21|SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding|Nikolay Nikolov\u7b49|[2511.17411](http://arxiv.org/pdf/2511.17411)|\u65e0|\u25c6 Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control.\n\u25c6 Yet their ability to generalize across new environments, tasks, and embodiments remains limited.\n\u25c6 We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs).|\n",
    "2511.17362": "|2025-11-21|ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP|Linxiang Su\u7b49|[2511.17362](http://arxiv.org/pdf/2511.17362)|\u65e0|\u25c6 Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images.\n\u25c6 As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness.\n\u25c6 In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC).|\n",
    "2511.17322": "|2025-11-21|NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior|Dongbo Shi\u7b49|[2511.17322](http://arxiv.org/pdf/2511.17322)|\u65e0|\u25c6 In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.\n\u25c6 Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios.\n\u25c6 To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation.|\n",
    "2511.17311": "|2025-11-21|Angular clustering and bias of photometric quasars in the Kilo-Degree Survey Data Release 4|Anjitha John William\u7b49|[2511.17311](http://arxiv.org/pdf/2511.17311)|\u65e0|\u25c6 We investigate the angular clustering and effective bias of photometrically selected quasars in the Kilo-Degree Survey Data Release 4 (KiDS DR4).\n\u25c6 We update the previous photometric redshifts (photo-$z$s) of the KiDS quasars using Hybrid-z, a deep learning framework combining four-band KiDS images and nine-band KiDS+VIKING magnitudes.\n\u25c6 Hybrid-z is trained on the latest Dark Energy Spectroscopic Instrument (DESI) DR1 and Sloan Digital Sky Survey (SDSS) DR17 quasars matching with KiDS, and achieves average bias $\\langle \u03b4z \\rangle < 0.01$ and scatter $\\sim 0.04(1 + z)$ on a test sample.|\n",
    "2511.17309": "|2025-11-21|MuM: Multi-View Masked Image Modeling for 3D Vision|David Nordstr\u00f6m\u7b49|[2511.17309](http://arxiv.org/pdf/2511.17309)|\u65e0|\u25c6 Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data.\n\u25c6 When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption.\n\u25c6 However, most prior efforts are optimized for semantic understanding rather than geometric reasoning.|\n",
    "2511.17155": "|2025-11-21|UI-Styler: Ultrasound Image Style Transfer with Class-Aware Prompts for Cross-Device Diagnosis Using a Frozen Black-Box Inference Network|Nhat-Tuong Do-Tran\u7b49|[2511.17155](http://arxiv.org/pdf/2511.17155)|\u65e0|\u25c6 The appearance of ultrasound images varies across acquisition devices, causing domain shifts that degrade the performance of fixed black-box downstream inference models when reused.\n\u25c6 To mitigate this issue, it is practical to develop unpaired image translation (UIT) methods that effectively align the statistical distributions between source and target domains, particularly under the constraint of a reused inference-blackbox setting.\n\u25c6 However, existing UIT approaches often overlook class-specific semantic alignment during domain adaptation, resulting in misaligned content-class mappings that can impair diagnostic accuracy.|\n",
    "2511.17111": "|2025-11-21|Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation|Sergio Torregrosa\u7b49|[2511.17111](http://arxiv.org/pdf/2511.17111)|\u65e0|\u25c6 Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs.\n\u25c6 Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped.\n\u25c6 To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes.|\n",
    "2511.16955": "|2025-11-21|Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models|Dailan He\u7b49|[2511.16955](http://arxiv.org/pdf/2511.16955)|\u65e0|\u25c6 Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences.\n\u25c6 However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm.\n\u25c6 Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity.|\n",
    "2511.16814": "|2025-11-20|Stable diffusion models reveal a persisting human and AI gap in visual creativity|Silvia Rondini\u7b49|[2511.16814](http://arxiv.org/pdf/2511.16814)|\u65e0|\u25c6 While recent research suggests Large Language Models match human creative performance in divergent thinking tasks, visual creativity remains underexplored.\n\u25c6 This study compared image generation in human participants (Visual Artists and Non Artists) and using an image generation AI model (two prompting conditions with varying human input: high for Human Inspired, low for Self Guided).\n\u25c6 Human raters (N=255) and GPT4o evaluated the creativity of the resulting images.|\n",
    "2511.19434": "|2025-11-24|Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts|Yasin Esfandiari\u7b49|[2511.19434](http://arxiv.org/pdf/2511.19434)|\u65e0|\u25c6 Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity.\n\u25c6 We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory.\n\u25c6 Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics.|\n",
    "2511.19379": "|2025-11-24|Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware|Srishti Gupta\u7b49|[2511.19379](http://arxiv.org/pdf/2511.19379)|\u65e0|\u25c6 Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps.\n\u25c6 This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware.\n\u25c6 By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency.|\n",
    "2511.19365": "|2025-11-24|DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation|Zehong Ma\u7b49|[2511.19365](http://arxiv.org/pdf/2511.19365)|\u65e0|\u25c6 Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion.\n\u25c6 This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity.\n\u25c6 Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT).|\n",
    "2511.19268": "|2025-11-24|BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment|Dewei Zhou\u7b49|[2511.19268](http://arxiv.org/pdf/2511.19268)|\u65e0|\u25c6 Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources.\n\u25c6 These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text.\n\u25c6 Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide.|\n",
    "2511.19071": "|2025-11-24|DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation|Fangda Chen\u7b49|[2511.19071](http://arxiv.org/pdf/2511.19071)|\u65e0|\u25c6 The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation.\n\u25c6 Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation.\n\u25c6 However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance.|\n",
    "2511.19040": "|2025-11-24|Machine Learning Based Identification of Solar Disk and Plages in Kodaikanal Solar Observatory Historical Suncharts|Dibya Kirti Mishra\u7b49|[2511.19040](http://arxiv.org/pdf/2511.19040)|\u65e0|\u25c6 Kodaikanal Solar Observatory (KoSO) is one of the oldest solar observatories, possessing an archive of multi-wavelength solar observations, including white light, Ca II K, and H-alpha images spanning over a century.\n\u25c6 In addition to these observations, KoSO has preserved hand-drawn suncharts (1904-2022), on which various solar features such as sunspots, plages, filaments, and prominences are marked on the Stonyhurst grid with distinct colour coding.\n\u25c6 In this study, we present the first comprehensive result that includes the entire data set from these suncharts using a supervised Machine Learning model called \"Convolutional Neural Networks (CNNs)\", firstly to identify the solar disks from the charts (1909-2007), secondly to identify the plages, spanning 9 solar cycles (1916-2007).|\n",
    "2511.18942": "|2025-11-24|VeCoR - Velocity Contrastive Regularization for Flow Matching|Zong-Wei Hong\u7b49|[2511.18942](http://arxiv.org/pdf/2511.18942)|\u65e0|\u25c6 Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models.\n\u25c6 Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.\n\u25c6 To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both \"where to go\" and \"where not to go.\" To be formal, we propose \\textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision.|\n",
    "2511.18834": "|2025-11-24|FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories|Lei Ke\u7b49|[2511.18834](http://arxiv.org/pdf/2511.18834)|\u65e0|\u25c6 With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application.\n\u25c6 Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching.\n\u25c6 This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation.|\n",
    "2511.18831": "|2025-11-24|VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction|Shaobo Wang\u7b49|[2511.18831](http://arxiv.org/pdf/2511.18831)|\u65e0|\u25c6 The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets.\n\u25c6 While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics.\n\u25c6 In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy.|\n",
    "2511.18765": "|2025-11-24|NI-Tex: Non-isometric Image-based Garment Texture Generation|Hui Shan\u7b49|[2511.18765](http://arxiv.org/pdf/2511.18765)|\u65e0|\u25c6 Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited.\n\u25c6 To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes.\n\u25c6 However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility.|\n",
    "2511.20648": "|2025-11-25|LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight|Yunze Man\u7b49|[2511.20648](http://arxiv.org/pdf/2511.20648)|\u65e0|\u25c6 To act in the world, a model must name what it sees and know where it is in 3D.\n\u25c6 Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox.\n\u25c6 We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem.|\n",
    "2511.20641": "|2025-11-25|Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition|Wei Tang\u7b49|[2511.20641](http://arxiv.org/pdf/2511.20641)|\u65e0|\u25c6 Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes.\n\u25c6 Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance.\n\u25c6 However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity.|\n",
    "2511.20549": "|2025-11-25|Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning|Guanjie Chen\u7b49|[2511.20549](http://arxiv.org/pdf/2511.20549)|\u65e0|\u25c6 Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive.\n\u25c6 Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation.\n\u25c6 Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking.|\n",
    "2511.20462": "|2025-11-25|STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow|Jiatao Gu\u7b49|[2511.20462](http://arxiv.org/pdf/2511.20462)|\u65e0|\u25c6 Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation.\n\u25c6 Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models.\n\u25c6 In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation.|\n",
    "2511.20424": "|2025-11-25|Planar Josephson junctions for sensors and electronics:Different geometry, new functionality|Vladimir M. Krasnov|[2511.20424](http://arxiv.org/pdf/2511.20424)|\u65e0|\u25c6 Josephson junctions are key elements in superconducting electronics.\n\u25c6 The most common type is the overlap (sandwich-type) junction, formed by vertically stacking two superconducting layers.\n\u25c6 In contrast, planar junctions are fabricated without overlap, at the edge of two superconducting films within a single plane.|\n",
    "2511.20152": "|2025-11-25|Restora-Flow: Mask-Guided Image Restoration with Flow Matching|Arnela Hadzic\u7b49|[2511.20152](http://arxiv.org/pdf/2511.20152)|\u65e0|\u25c6 Flow matching has emerged as a promising generative approach that addresses the lengthy sampling times associated with state-of-the-art diffusion models and enables a more flexible trajectory design, while maintaining high-quality image generation.\n\u25c6 This capability makes it suitable as a generative prior for image restoration tasks.\n\u25c6 Although current methods leveraging flow models have shown promising results in restoration, some still suffer from long processing times or produce over-smoothed results.|\n",
    "2511.20116": "|2025-11-25|LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening|Johannes Brandt\u7b49|[2511.20116](http://arxiv.org/pdf/2511.20116)|\u65e0|\u25c6 Lung cancer risk estimation is gaining increasing importance as more countries introduce population-wide screening programs using low-dose CT (LDCT).\n\u25c6 As imaging volumes grow, scalable methods that can process entire lung volumes efficiently are essential to tap into the full potential of these large screening datasets.\n\u25c6 Existing approaches either over-rely on pixel-level annotations, limiting scalability, or analyze the lung in fragments, weakening performance.|\n",
    "2511.19993": "|2025-11-25|Unusual Thermally Induced Blueshift and Emission Amplification of Mn2+ ions Enable Filter-Free Luminescent Thermal Imaging|Y. Abe\u7b49|[2511.19993](http://arxiv.org/pdf/2511.19993)|\u65e0|\u25c6 The shift from point-based thermal sensing to filter-free thermal imaging requires luminescent thermometers that exhibit pronounced and thermally driven spectral changes within spectral regions matching the sensitivity profiles of the R, G, and B channels of a digital camera.\n\u25c6 In this work, we introduce such a system, enabled by the synergistic interplay between (i) thermal redistribution among the vibronic components of the 4T1 excited state of Mn2+ ions and (ii) thermally assisted population of this state via optical trap sites.\n\u25c6 These combined processes result in a simultaneous thermal enhancement and blueshift of the Mn2+ emission band associated with the 4T1 -> 6A1 electronic transition.|\n",
    "2511.19985": "|2025-11-25|SONIC: Spectral Optimization of Noise for Inpainting with Consistency|Seungyeon Baek\u7b49|[2511.19985](http://arxiv.org/pdf/2511.19985)|\u65e0|\u25c6 We propose a novel training-free method for inpainting with off-the-shelf text-to-image models.\n\u25c6 While guidance-based methods in theory allow generic models to be used for inverse problems such as inpainting, in practice, their effectiveness is limited, leading to the necessity of specialized inpainting-specific models.\n\u25c6 In this work, we argue that the missing ingredient for training-free inpainting is the optimization (guidance) of the initial seed noise.|\n",
    "2511.19879": "|2025-11-25|Learning Degenerate Manifolds of Frustrated Magnets with Boltzmann Machines|Jackson C. Glass\u7b49|[2511.19879](http://arxiv.org/pdf/2511.19879)|\u65e0|\u25c6 We show that Restricted Boltzmann Machines (RBMs) provide a flexible generative framework for modeling spin configurations in disordered yet strongly correlated phases of frustrated magnets.\n\u25c6 As a benchmark, we first demonstrate that an RBM can learn the zero-temperature ground-state manifold of the one-dimensional ANNNI model at its multiphase point, accurately reproducing its characteristic oscillatory and exponentially decaying correlations.\n\u25c6 We then apply RBMs to kagome spin ice and show that they successfully learn the local ice rules and short-range correlations of the extensively degenerate ice-I manifold.|\n",
    "2511.21647": "|2025-11-26|Fast 3D Ultrasound Localization Microscopy via Projection-based Processing Framework|Jingke Zhang\u7b49|[2511.21647](http://arxiv.org/pdf/2511.21647)|\u65e0|\u25c6 Three-dimensional ultrasound localization microscopy (ULM) enables comprehensive visualization of the vasculature, thereby improving diagnostic reliability.\n\u25c6 Nevertheless, its clinical translation remains challenging, as the exponential growth in voxel count for full 3D reconstruction imposes heavy computational demands and extensive post-processing time.\n\u25c6 In this row-column array (RCA)-based 3D in vivo pig kidney ULM study, we reformulate each step of the full 3D ULM pipeline, including beamforming, clutter filtering, motion estimation, microbubble separation and localization into a series of computational-efficient 2D operations, substantially reducing the number of voxels to be processed while maintaining comparable accuracy.|\n",
    "2511.21592": "|2025-11-26|MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training|Haotian Xue\u7b49|[2511.21592](http://arxiv.org/pdf/2511.21592)|\u65e0|\u25c6 Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics.\n\u25c6 A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion.\n\u25c6 We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data.|\n",
    "2511.21452": "|2025-11-26|Semantic-Enhanced Feature Matching with Learnable Geometric Verification for Cross-Modal Neuron Registration|Wenwei Li\u7b49|[2511.21452](http://arxiv.org/pdf/2511.21452)|\u65e0|\u25c6 Accurately registering in-vivo two-photon and ex-vivo fluorescence micro-optical sectioning tomography images of individual neurons is critical for structure-function analysis in neuroscience.\n\u25c6 This task is profoundly challenging due to a significant cross-modality appearance gap, the scarcity of annotated data and severe tissue deformations.\n\u25c6 We propose a novel deep learning framework to address these issues.|\n",
    "2511.21335": "|2025-11-26|TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models|Haksoo Lim\u7b49|[2511.21335](http://arxiv.org/pdf/2511.21335)|\u65e0|\u25c6 Score-based generative models (SGMs) have demonstrated unparalleled sampling quality and diversity in numerous fields, such as image generation, voice synthesis, and tabular data synthesis, etc.\n\u25c6 Inspired by those outstanding results, we apply SGMs to synthesize time-series by learning its conditional score function.\n\u25c6 To this end, we present a conditional score network for time-series synthesis, deriving a denoising score matching loss tailored for our purposes.|\n",
    "2511.21265": "|2025-11-26|Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting|Juncheng Chen\u7b49|[2511.21265](http://arxiv.org/pdf/2511.21265)|\u65e0|\u25c6 Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data.\n\u25c6 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation.\n\u25c6 However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling.|\n",
    "2511.21215": "|2025-11-26|From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting|Umang Agarwal\u7b49|[2511.21215](http://arxiv.org/pdf/2511.21215)|\u65e0|\u25c6 We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow.\n\u25c6 While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals.\n\u25c6 We implement all three methods using a unified TinyUNet architecture (<1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98).|\n",
    "2511.21203": "|2025-11-26|Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching|Fuyuki Tokuda\u7b49|[2511.21203](http://arxiv.org/pdf/2511.21203)|\u65e0|\u25c6 In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched.\n\u25c6 We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control.\n\u25c6 This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat.|\n",
    "2511.21121": "|2025-11-26|Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval|Anup Roy\u7b49|[2511.21121](http://arxiv.org/pdf/2511.21121)|\u65e0|\u25c6 Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction.\n\u25c6 These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer.\n\u25c6 Vision first retrieval has emerged as a strong alternative.|\n",
    "2511.21097": "|2025-11-26|CLRecogEye : Curriculum Learning towards exploiting convolution features for Dynamic Iris Recognition|Geetanjali Sharma\u7b49|[2511.21097](http://arxiv.org/pdf/2511.21097)|\u65e0|\u25c6 Iris authentication algorithms have achieved impressive recognition performance, making them highly promising for real-world applications such as border control, citizen identification, and both criminal investigations and commercial systems.\n\u25c6 However, their robustness is still challenged by variations in rotation, scale, specular reflections, and defocus blur.\n\u25c6 In addition, most existing approaches rely on straightforward point-to-point comparisons, typically using cosine or L2 distance, without effectively leveraging the spatio-spatial-temporal structure of iris patterns.|\n",
    "2511.21028": "|2025-11-26|Deep Parameter Interpolation for Scalar Conditioning|Chicago Y. Park\u7b49|[2511.21028](http://arxiv.org/pdf/2511.21028)|\u65e0|\u25c6 We propose deep parameter interpolation (DPI), a general-purpose method for transforming an existing deep neural network architecture into one that accepts an additional scalar input.\n\u25c6 Recent deep generative models, including diffusion models and flow matching, employ a single neural network to learn a time- or noise level-dependent vector field.\n\u25c6 Designing a network architecture to accurately represent this vector field is challenging because the network must integrate information from two different sources: a high-dimensional vector (usually an image) and a scalar.|\n",
    "2511.23251": "|2025-11-28|Deep Learning for Restoring MPI System Matrices Using Simulated Training Data|Artyom Tsanda\u7b49|[2511.23251](http://arxiv.org/pdf/2511.23251)|\u65e0|\u25c6 Magnetic particle imaging reconstructs tracer distributions using a system matrix obtained through time-consuming, noise-prone calibration measurements.\n\u25c6 Methods for addressing imperfections in measured system matrices increasingly rely on deep neural networks, yet curated training data remain scarce.\n\u25c6 This study evaluates whether physics-based simulated system matrices can be used to train deep learning models for different system matrix restoration tasks, i.e., denoising, accelerated calibration, upsampling, and inpainting, that generalize to measured data.|\n",
    "2511.23199": "|2025-11-28|Vision Bridge Transformer at Scale|Zhenxiong Tan\u7b49|[2511.23199](http://arxiv.org/pdf/2511.23199)|\u65e0|\u25c6 We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation.\n\u25c6 Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm.\n\u25c6 By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks.|\n",
    "2511.23158": "|2025-11-28|REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection|Huangsen Cao\u7b49|[2511.23158](http://arxiv.org/pdf/2511.23158)|\u65e0|\u25c6 With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity.\n\u25c6 Consequently, there is an urgent need for efficient and truly explainable image forensic methods.\n\u25c6 Recent detection paradigms have shifted towards explainable forensics.|\n",
    "2511.23093": "|2025-11-28|On Computational Aspects of Ordered Matching Problems|Michal \u010cert\u00edk\u7b49|[2511.23093](http://arxiv.org/pdf/2511.23093)|\u65e0|\u25c6 Ordered matchings, defined as graphs with linearly ordered vertices, where each vertex is connected to exactly one edge, play a crucial role in the area of ordered graphs and their homomorphisms.\n\u25c6 Therefore, we consider related problems from the complexity point of view and determine their corresponding computational and parameterized complexities.\n\u25c6 We show that the subgraph of ordered matchings problem is NP-complete and we prove that the problem of finding ordered homomorphisms between ordered matchings is NP-complete as well, implying NP-completeness of more generic problems.|\n",
    "2511.23029": "|2025-11-28|Geodiffussr: Generative Terrain Texturing with Elevation Fidelity|Tai Inui\u7b49|[2511.23029](http://arxiv.org/pdf/2511.23029)|\u65e0|\u25c6 Large-scale terrain generation remains a labor-intensive task in computer graphics.\n\u25c6 We introduce Geodiffussr, a flow-matching pipeline that synthesizes text-guided texture maps while strictly adhering to a supplied Digital Elevation Map (DEM).\n\u25c6 The core mechanism is multi-scale content aggregation (MCA): DEM features from a pretrained encoder are injected into UNet blocks at multiple resolutions to enforce global-to-local elevation consistency.|\n",
    "2511.22959": "|2025-11-28|A Trainable Centrality Framework for Modern Data|Minh Duc Vu\u7b49|[2511.22959](http://arxiv.org/pdf/2511.22959)|\u65e0|\u25c6 Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data.\n\u25c6 We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations.\n\u25c6 FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential.|\n",
    "2511.22940": "|2025-11-28|One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfe|Shijun Shi\u7b49|[2511.22940](http://arxiv.org/pdf/2511.22940)|\u65e0|\u25c6 Recent advances in diffusion models have greatly improved pose-driven character animation.\n\u25c6 However, existing methods are limited to spatially aligned reference-pose pairs with matched skeletal structures.\n\u25c6 Handling reference-pose misalignment remains unsolved.|\n",
    "2511.22908": "|2025-11-28|ViGG: Robust RGB-D Point Cloud Registration using Visual-Geometric Mutual Guidance|Congjia Chen\u7b49|[2511.22908](http://arxiv.org/pdf/2511.22908)|\u65e0|\u25c6 Point cloud registration is a fundamental task in 3D vision.\n\u25c6 Most existing methods only use geometric information for registration.\n\u25c6 Recently proposed RGB-D registration methods primarily focus on feature fusion or improving feature learning, which limits their ability to exploit image information and hinders their practical applicability.|\n",
    "2511.22875": "|2025-11-28|Robust Indexing for Challenging Serial X-ray Diffraction Patterns|Marc M Nasser\u7b49|[2511.22875](http://arxiv.org/pdf/2511.22875)|\u65e0|\u25c6 Serial crystallography experiments routinely produce thousands of diffraction patterns from crystals in random orientations.\n\u25c6 To turn this stream of images into a usable dataset, each pattern must be indexed before integration and merging can proceed.\n\u25c6 In practice, diffraction patterns may contain only a small number of reliable peaks, be contaminated by background or spuriously detected reflections, or arise from crystals with highly skewed unit cells.|\n",
    "2511.22860": "|2025-11-28|MARVO: Marine-Adaptive Radiance-aware Visual Odometry|Sacchin Sundar\u7b49|[2511.22860](http://arxiv.org/pdf/2511.22860)|\u65e0|\u25c6 Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise.\n\u25c6 We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization.\n\u25c6 At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity.|\n",
    "2512.01908": "|2025-12-01|SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception|Gurmeher Khurana\u7b49|[2512.01908](http://arxiv.org/pdf/2512.01908)|\u65e0|\u25c6 Contact-rich robotic manipulation requires representations that encode local geometry.\n\u25c6 Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues.\n\u25c6 Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information.|\n",
    "2512.01850": "|2025-12-01|Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching|Yue Pan\u7b49|[2512.01850](http://arxiv.org/pdf/2512.01850)|\u65e0|\u25c6 Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\n\u25c6 In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\n\u25c6 Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.|\n",
    "2512.01816": "|2025-12-01|Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights|Juanxi Tian\u7b49|[2512.01816](http://arxiv.org/pdf/2512.01816)|\u65e0|\u25c6 Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency.\n\u25c6 However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time.\n\u25c6 To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation.|\n",
    "2512.01643": "|2025-12-01|ViT$^3$: Unlocking Test-Time Training in Vision|Dongchen Han\u7b49|[2512.01643](http://arxiv.org/pdf/2512.01643)|\u65e0|\u25c6 Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling.\n\u25c6 TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time.\n\u25c6 This reformulation opens a rich and flexible design space while achieving linear computational complexity.|\n",
    "2512.01611": "|2025-12-01|Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager|Fengfeng Li\u7b49|[2512.01611](http://arxiv.org/pdf/2512.01611)|\u65e0|\u25c6 In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction.\n\u25c6 This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm.\n\u25c6 The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment.|\n",
    "2512.01510": "|2025-12-01|Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation|Franz Thaler\u7b49|[2512.01510](http://arxiv.org/pdf/2512.01510)|\u65e0|\u25c6 We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation.\n\u25c6 To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training.\n\u25c6 We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM.|\n",
    "2512.01273": "|2025-12-01|nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis|Xin Li\u7b49|[2512.01273](http://arxiv.org/pdf/2512.01273)|\u65e0|\u25c6 Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases.\n\u25c6 Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation.\n\u25c6 As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient.|\n",
    "2512.01094": "|2025-11-30|Accelerating Inference of Masked Image Generators via Reinforcement Learning|Pranav Subbaraman\u7b49|[2512.01094](http://arxiv.org/pdf/2512.01094)|\u65e0|\u25c6 Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images.\n\u25c6 However, they need many sampling steps to create high-quality generations, resulting in slow inference speed.\n\u25c6 In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps.|\n",
    "2512.01030": "|2025-11-30|Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model|Jing He\u7b49|[2512.01030](http://arxiv.org/pdf/2512.01030)|\u65e0|\u25c6 Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures.\n\u25c6 While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning.\n\u25c6 Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions.|\n",
    "2512.00927": "|2025-11-30|LAHNet: Local Attentive Hashing Network for Point Cloud Registration|Wentao Qu\u7b49|[2512.00927](http://arxiv.org/pdf/2512.00927)|\u65e0|\u25c6 Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features.\n\u25c6 However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness.\n\u25c6 In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors.|\n",
    "2512.02944": "|2025-12-02|The Convex Matching Distance in Multiparameter Persistence|Patrizio Frosini\u7b49|[2512.02944](http://arxiv.org/pdf/2512.02944)|\u65e0|\u25c6 We introduce the convex matching distance, a novel metric for comparing functions with values in the real plane.\n\u25c6 This metric measures the maximal bottleneck distance between the persistence diagrams associated with the convex combinations of the two function components.\n\u25c6 Similarly to the traditional matching distance, the convex matching distance aggregates the information provided by two real-valued components.|\n",
    "2512.02920": "|2025-12-02|Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation|Ziniu Zhang\u7b49|[2512.02920](http://arxiv.org/pdf/2512.02920)|\u65e0|\u25c6 We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes.\n\u25c6 Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings.\n\u25c6 In this work, we construct a large multimodal dataset across six U.S.|\n",
    "2512.02889": "|2025-12-02|Terahertz Emission from Spintronic Stack Nanodecorated with Drop-Cast Core-Shell Plasmonic Nanoparticles|Vittorio Cecconi\u7b49|[2512.02889](http://arxiv.org/pdf/2512.02889)|\u65e0|\u25c6 Spintronic emitters promise to revolutionise terahertz (THz) sources by converting ultrafast optical pulses into broadband THz radiation without phase-matching constraints.\n\u25c6 Because the conversion relies on spin-current injection across a nanometre-thin magnetic layer, its efficiency is ordinarily limited by weak optical coupling.\n\u25c6 Here, we present a demonstration of a drop-casting based approach to introduce ultrafast plasmonic-mediated coupling: a sparse-layer of silica-gold core-shell nanoparticles is deposited directly onto a W/Fe/Pt spintronic trilayer.|\n",
    "2512.02833": "|2025-12-02|A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models|Ihab Ahmed\u7b49|[2512.02833](http://arxiv.org/pdf/2512.02833)|\u65e0|\u25c6 We investigate input normalization methods for Time-Series Foundation Models (TSFMs).\n\u25c6 While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical.\n\u25c6 Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity.|\n",
    "2512.02826": "|2025-12-02|From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity|Haoming Liu\u7b49|[2512.02826](http://arxiv.org/pdf/2512.02826)|\u65e0|\u25c6 Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos.\n\u25c6 However, their memorization-generalization behavior remains poorly understood.\n\u25c6 In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target.|\n",
    "2512.02805": "|2025-12-02|Direct observational evidence that higher-luminosity type 1 active galactic nuclei are most commonly triggered by galaxy mergers|Yongmin Yoon\u7b49|[2512.02805](http://arxiv.org/pdf/2512.02805)|\u65e0|\u25c6 We examine the connection between galaxy mergers and the triggering of active galactic nuclei (AGNs) using a sample of 614 type 1 AGNs at $z<0.07$, along with a control sample of inactive galaxies matched to the AGNs for comparison.\n\u25c6 We used tidal features, detected in deep images from the DESI Legacy Imaging Survey, as direct evidence of recent mergers.\n\u25c6 We find that the fraction of type 1 AGN hosts with tidal features ($f_T$) is higher for AGNs with higher luminosities and (to a lesser extent) more massive black holes.|\n",
    "2512.02768": "|2025-12-02|Diffusion-Prior Split Gibbs Sampling for Synthetic Aperture Radar Imaging under Incomplete Measurements|Hefei Gao\u7b49|[2512.02768](http://arxiv.org/pdf/2512.02768)|\u65e0|\u25c6 Synthetic aperture radar (SAR) imaging plays a critical role in all-weather, day-and-night remote sensing, yet reconstruction is often challenged by noise, undersampling, and complex scattering scenarios.\n\u25c6 Conventional methods, including matched filtering and sparsity-based compressed sensing, are limited in capturing intricate scene structures and frequently suffer from artifacts, elevated sidelobes, and loss of fine details.\n\u25c6 Recent diffusion models have demonstrated superior capability in representing high-order priors; however, existing diffusion-based SAR methods still yield degraded reconstructions due to oversimplified likelihood approximations in guided sampling.|\n",
    "2512.02737": "|2025-12-02|Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone|Tristan Amadei\u7b49|[2512.02737](http://arxiv.org/pdf/2512.02737)|\u65e0|\u25c6 Image-based localization in GNSS-denied environments is critical for UAV autonomy.\n\u25c6 Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training.\n\u25c6 Such data are costly to acquire and often unavailable, limiting their applicability.|\n",
    "2512.02697": "|2025-12-02|GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization|Zixuan Song\u7b49|[2512.02697](http://arxiv.org/pdf/2512.02697)|\u65e0|\u25c6 Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image.\n\u25c6 However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable.\n\u25c6 It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image).|\n",
    "2512.02359": "|2025-12-02|WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting|Bin Li\u7b49|[2512.02359](http://arxiv.org/pdf/2512.02359)|\u65e0|\u25c6 Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting.\n\u25c6 Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations.\n\u25c6 Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations.|\n",
    "2512.03715": "|2025-12-03|DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction|Kaichen Zhang\u7b49|[2512.03715](http://arxiv.org/pdf/2512.03715)|\u65e0|\u25c6 This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.\n\u25c6 The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.\n\u25c6 DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.|\n",
    "2512.03346": "|2025-12-03|Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus|Lynn Kandakji\u7b49|[2512.03346](http://arxiv.org/pdf/2512.03346)|\u65e0|\u25c6 The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge.\n\u25c6 The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention.\n\u25c6 This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved.|\n",
    "2512.05116": "|2025-12-04|Value Gradient Guidance for Flow Matching Alignment|Zhen Liu\u7b49|[2512.05116](http://arxiv.org/pdf/2512.05116)|\u65e0|\u25c6 While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation.\n\u25c6 In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models.\n\u25c6 The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function.|\n",
    "2512.05114": "|2025-12-04|Deep infant brain segmentation from multi-contrast MRI|Malte Hoffmann\u7b49|[2512.05114](http://arxiv.org/pdf/2512.05114)|\u65e0|\u25c6 Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures.\n\u25c6 However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints.\n\u25c6 Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts.|\n",
    "2512.05081": "|2025-12-04|Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression|Jung Yi\u7b49|[2512.05081](http://arxiv.org/pdf/2512.05081)|\u65e0|\u25c6 Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration.\n\u25c6 We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation.\n\u25c6 To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning.|\n",
    "2512.05078": "|2025-12-04|Improving Posterior Inference of Galaxy Properties with Image-Based Conditional Flow Matching|Mikaeel Yunus\u7b49|[2512.05078](http://arxiv.org/pdf/2512.05078)|\u65e0|\u25c6 Estimating physical properties of galaxies from wide-field surveys remains a central challenge in astrophysics.\n\u25c6 While spectroscopy provides precise measurements, it is observationally expensive, and photometry discards morphological information that correlates with mass, star formation history, metallicity, and dust.\n\u25c6 We present a conditional flow matching (CFM) framework that leverages pixel-level imaging alongside photometry to improve posterior inference of galaxy properties.|\n",
    "2512.05016": "|2025-12-04|Generative Neural Video Compression via Video Diffusion Prior|Qi Mao\u7b49|[2512.05016](http://arxiv.org/pdf/2512.05016)|\u65e0|\u25c6 We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec.\n\u25c6 Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering.\n\u25c6 To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details.|\n",
    "2512.04966": "|2025-12-04|Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels|Guangming Liang\u7b49|[2512.04966](http://arxiv.org/pdf/2512.04966)|\u65e0|\u25c6 Accurate channel state information (CSI) underpins reliable and efficient wireless communication.\n\u25c6 However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments.\n\u25c6 By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates.|\n",
    "2512.04821": "|2025-12-04|LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation|Huynh Trinh Ngoc\u7b49|[2512.04821](http://arxiv.org/pdf/2512.04821)|\u65e0|\u25c6 Generative models have achieved remarkable progress with the emergence of flow matching (FM).\n\u25c6 It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities.\n\u25c6 Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation.|\n",
    "2512.04804": "|2025-12-04|Unveiling gravitational waves from core-collapse supernovae with MUSE|Alessandro Veutro\u7b49|[2512.04804](http://arxiv.org/pdf/2512.04804)|\u65e0|\u25c6 The core collapse of a massive star at the end of its life can give rise to one of the most powerful phenomena in the Universe.\n\u25c6 Because of violent mass motions that take place during the explosion, core-collapse supernovae have been considered a potential source of detectable gravitational waveforms for decades.\n\u25c6 However, their intrinsic stochasticity makes ineffective the use of modelled techniques such as matched filtering, forcing us to develop model independent technique to unveil their nature.|\n",
    "2512.04677": "|2025-12-04|Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length|Yubo Huang\u7b49|[2512.04677](http://arxiv.org/pdf/2512.04677)|\u65e0|\u25c6 Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis.\n\u25c6 We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model.\n\u25c6 Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming.|\n",
    "2512.04662": "|2025-12-04|Spectral micro-CT for quantitative analysis of calcification in fibrocartilage|Vittoria Mazzini\u7b49|[2512.04662](http://arxiv.org/pdf/2512.04662)|\u65e0|\u25c6 This work introduces a quantitative method for assessing calcification in fibrocartilage using spectral micro-computed tomography ($\u03bc$CT).\n\u25c6 Tissue samples of hip acetabular labrum from patients with osteoarthritis and femoroacetabular impingement were imaged with a laboratory-based spectral $\u03bc$CT system equipped with a small-pixel photon-counting detector.\n\u25c6 The detector operated with two energy thresholds, allowing the simultaneous acquisition of two CT datasets at different X-ray energies.|\n",
    "2512.05663": "|2025-12-05|LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection|Johannes Meier\u7b49|[2512.05663](http://arxiv.org/pdf/2512.05663)|\u65e0|\u25c6 Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning.\n\u25c6 Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy.\n\u25c6 We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities.|\n",
    "2512.05613": "|2025-12-05|DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model|Pasquale De Marinis\u7b49|[2512.05613](http://arxiv.org/pdf/2512.05613)|\u65e0|\u25c6 Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples.\n\u25c6 This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time.\n\u25c6 To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process.|\n",
    "2512.05599": "|2025-12-05|An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation|Panagiotis Giannikos\u7b49|[2512.05599](http://arxiv.org/pdf/2512.05599)|\u65e0|\u25c6 Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources.\n\u25c6 Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities.\n\u25c6 Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets).|\n",
    "2512.05571": "|2025-12-05|MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging|Xingyu Zhang\u7b49|[2512.05571](http://arxiv.org/pdf/2512.05571)|\u65e0|\u25c6 Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions.\n\u25c6 Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions.\n\u25c6 Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information.|\n",
    "2512.05410": "|2025-12-05|Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images|Yida Lin\u7b49|[2512.05410](http://arxiv.org/pdf/2512.05410)|\u65e0|\u25c6 Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame.\n\u25c6 However, these algorithms require meticulous parameter tuning.\n\u25c6 We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency.|\n",
    "2512.05330": "|2025-12-05|On-Orbit Calibration of Danuri/PolCam. I. Geometric Calibration|Kilho Baek\u7b49|[2512.05330](http://arxiv.org/pdf/2512.05330)|\u65e0|\u25c6 The wide-angle Polarimetric Camera (PolCam) onboard South Korea's first lunar orbiter, Danuri, is a pioneering instrument designed to conduct the first global polarimetric and high-phase-angle survey of the Moon.\n\u25c6 Precise geometric calibration is critical for this mission, particularly due to PolCam's highly oblique viewing geometry, which introduces significant topographic distortion.\n\u25c6 We present a comprehensive on-orbit geometric calibration that relies on 160,256 tie points derived from matching features between PolCam images and the well-orthorectified global map of the Kaguya Multiband Imager (MI).|\n",
    "2512.05198": "|2025-12-04|Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models|Rowan Bradbury\u7b49|[2512.05198](http://arxiv.org/pdf/2512.05198)|\u65e0|\u25c6 Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask.\n\u25c6 We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC).\n\u25c6 An equivalent latent compositor should be the same as compositing in pixel space.|\n",
    "2511.17750": "|2025-11-21|SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration|Zhimin Shao\u7b49|[2511.17750](http://arxiv.org/pdf/2511.17750)|\u65e0|\u25c6 Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses.\n\u25c6 However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint.\n\u25c6 Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry.|\n",
    "2512.10379": "|2025-12-11|Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching|Alberto Rota\u7b49|[2512.10379](http://arxiv.org/pdf/2512.10379)|\u65e0|\u25c6 Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness.\n\u25c6 In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation.\n\u25c6 However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques.|\n",
    "2512.10284": "|2025-12-11|MotionEdit: Benchmarking and Learning Motion-Centric Image Editing|Yixin Wan\u7b49|[2512.10284](http://arxiv.org/pdf/2512.10284)|\u65e0|\u25c6 We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility.\n\u25c6 Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos.\n\u25c6 This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.|\n"
  },
  "NeRF": {
    "2504.20379": "|**2025-05-01**|**GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting**|Jongwon Lee et.al.|[2504.20379](http://arxiv.org/abs/2504.20379)|null|\n",
    "2505.00378": "|**2025-05-01**|**Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation**|Feng Xue et.al.|[2505.00378](http://arxiv.org/abs/2505.00378)|null|\n",
    "2505.05474": "|**2025-05-08**|**3D Scene Generation: A Survey**|Beichen Wen et.al.|[2505.05474](http://arxiv.org/abs/2505.05474)|**[link](https://github.com/hzxie/awesome-3d-scene-generation)**|\n",
    "2505.02079": "|**2025-05-04**|**HandOcc: NeRF-based Hand Rendering with Occupancy Networks**|Maksym Ivashechkin et.al.|[2505.02079](http://arxiv.org/abs/2505.02079)|null|\n",
    "2505.02005": "|**2025-05-04**|**Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields**|Zhenxing Mi et.al.|[2505.02005](http://arxiv.org/abs/2505.02005)|**[link](https://github.com/MiZhenxing/Switch-NeRF)**|\n",
    "2505.01799": "|**2025-05-03**|**AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting**|Junhao Shi et.al.|[2505.01799](http://arxiv.org/abs/2505.01799)|null|\n",
    "2505.01749": "|**2025-05-03**|**Unified Steganography via Implicit Neural Representation**|Qi Song et.al.|[2505.01749](http://arxiv.org/abs/2505.01749)|null|\n",
    "2505.09413": "|**2025-05-14**|**Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians**|Ma Changfeng et.al.|[2505.09413](http://arxiv.org/abs/2505.09413)|**[link](https://github.com/murcherful/gaupcrender)**|\n",
    "2505.09406": "|**2025-05-14**|**FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling**|Yue Wen et.al.|[2505.09406](http://arxiv.org/abs/2505.09406)|null|\n",
    "2505.08510": "|**2025-05-13**|**FOCI: Trajectory Optimization on Gaussian Splats**|Mario Gomez Andreu et.al.|[2505.08510](http://arxiv.org/abs/2505.08510)|null|\n",
    "2505.07396": "|**2025-05-13**|**TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset**|Olaf Wysocki et.al.|[2505.07396](http://arxiv.org/abs/2505.07396)|null|\n",
    "2505.07373": "|**2025-05-12**|**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**|Lintao Xiang et.al.|[2505.07373](http://arxiv.org/abs/2505.07373)|null|\n",
    "2505.08811": "|**2025-05-12**|**TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian**|Shijie Lian et.al.|[2505.08811](http://arxiv.org/abs/2505.08811)|null|\n",
    "2505.06894": "|**2025-05-11**|**NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization**|Ahmed Qazi et.al.|[2505.06894](http://arxiv.org/abs/2505.06894)|null|\n",
    "2505.06638": "|**2025-05-10**|**3D Characterization of Smoke Plume Dispersion Using Multi-View Drone Swarm**|Nikil Krishnakumar et.al.|[2505.06638](http://arxiv.org/abs/2505.06638)|null|\n",
    "2505.06504": "|**2025-05-10**|**FlexNeRFer: A Multi-Dataflow, Adaptive Sparsity-Aware Accelerator for On-Device NeRF Rendering**|Seock-Hwan Noh et.al.|[2505.06504](http://arxiv.org/abs/2505.06504)|null|\n",
    "2505.12875": "|**2025-05-19**|**3D Gaussian Adaptive Reconstruction for Fourier Light-Field Microscopy**|Chenyu Xu et.al.|[2505.12875](http://arxiv.org/abs/2505.12875)|null|\n",
    "2505.12384": "|**2025-05-18**|**Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey**|Calvin Galagain et.al.|[2505.12384](http://arxiv.org/abs/2505.12384)|null|\n",
    "2505.11386": "|**2025-05-16**|**MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory**|Zifan Wang et.al.|[2505.11386](http://arxiv.org/abs/2505.11386)|null|\n",
    "2505.10787": "|**2025-05-16**|**EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes**|Jianlin Guo et.al.|[2505.10787](http://arxiv.org/abs/2505.10787)|**[link](https://github.com/scut-bip-lab/ea-3dgs)**|\n",
    "2505.09915": "|**2025-05-15**|**Large-Scale Gaussian Splatting SLAM**|Zhe Xin et.al.|[2505.09915](http://arxiv.org/abs/2505.09915)|null|\n",
    "2505.16912": "|**2025-05-22**|**UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot Ground Vehicle Repeat**|Desiree Fisker et.al.|[2505.16912](http://arxiv.org/abs/2505.16912)|null|\n",
    "2505.13633": "|**2025-05-19**|**IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion**|Wentao Song et.al.|[2505.13633](http://arxiv.org/abs/2505.13633)|null|\n",
    "2505.23481": "|**2025-05-29**|**PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views**|Mohamed Rayan Barhdadi et.al.|[2505.23481](http://arxiv.org/abs/2505.23481)|**[link](https://github.com/anonymous-researcher-01/physicsnerf)**|\n",
    "2505.23158": "|**2025-05-29**|**LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering**|Jonas Kulhanek et.al.|[2505.23158](http://arxiv.org/abs/2505.23158)|null|\n",
    "2505.22441": "|**2025-05-28**|**Can NeRFs See without Cameras?**|Chaitanya Amballa et.al.|[2505.22441](http://arxiv.org/abs/2505.22441)|null|\n",
    "2505.22279": "|**2025-05-28**|**Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss**|Wenjun Lu et.al.|[2505.22279](http://arxiv.org/abs/2505.22279)|null|\n",
    "2505.21890": "|**2025-05-28**|**Hyperspectral Gaussian Splatting**|Sunil Kumar Narayanan et.al.|[2505.21890](http://arxiv.org/abs/2505.21890)|null|\n",
    "2505.21335": "|**2025-05-27**|**Structure from Collision**|Takuhiro Kaneko et.al.|[2505.21335](http://arxiv.org/abs/2505.21335)|null|\n",
    "2505.20126": "|**2025-05-26**|**OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender**|Shintaro Ito et.al.|[2505.20126](http://arxiv.org/abs/2505.20126)|**[link](https://github.com/gsisaoki/omnidirectional_blender_3d_dataset)**|\n",
    "2505.19883": "|**2025-05-30**|**ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization**|Shintaro Ito et.al.|[2505.19883](http://arxiv.org/abs/2505.19883)|null|\n",
    "2505.19813": "|**2025-05-26**|**GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis**|You Wang et.al.|[2505.19813](http://arxiv.org/abs/2505.19813)|**[link](https://github.com/klmav-cuc/golf-nrt)**|\n",
    "2505.19793": "|**2025-05-26**|**Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction**|Li Fang et.al.|[2505.19793](http://arxiv.org/abs/2505.19793)|**[link](https://github.com/klmav-cuc/gdb-nerf)**|\n",
    "2506.00083": "|**2025-05-30**|**Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments**|Jiawei Hou et.al.|[2506.00083](http://arxiv.org/abs/2506.00083)|null|\n",
    "2506.07917": "|2025-06-09|Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes|Allen Tu\u7b49|[2506.07917](http://arxiv.org/pdf/2506.07917)|[\u4ee3\u7801](https://github.com/tuallen/speede3dgs)|\u25c6 \u63d0\u51faSpeeDe3DGS\u6846\u67b6\uff0c\u663e\u8457\u52a0\u901f\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\uff083DGS/4DGS\uff09\u7684\u6e32\u67d3\u901f\u5ea6\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u9010\u5e27\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\u3002  \n\u25c6 \u8bbe\u8ba1\u65f6\u5e8f\u654f\u611f\u5ea6\u526a\u679d\u8bc4\u5206\u673a\u5236\uff0c\u81ea\u52a8\u8bc6\u522b\u5e76\u5254\u9664\u5bf9\u52a8\u6001\u573a\u666f\u91cd\u5efa\u8d21\u732e\u4f4e\u7684\u5197\u4f59\u9ad8\u65af\u5143\u7d20\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u5f15\u5165\u9000\u706b\u5e73\u6ed1\u526a\u679d\u7b56\u7565\uff0c\u589e\u5f3a\u5728\u76f8\u673a\u4f4d\u59ff\u4e0d\u7cbe\u786e\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u526a\u679d\u9c81\u68d2\u6027\uff0c\u907f\u514d\u8bef\u5220\u5173\u952e\u9ad8\u65af\u5143\u7d20\u3002  \n\u25c6 \u5f00\u53d1GroupFlow\u8fd0\u52a8\u5206\u6790\u6280\u672f\uff0c\u901a\u8fc7\u8f68\u8ff9\u76f8\u4f3c\u6027\u805a\u7c7b\u9ad8\u65af\u7fa4\u7ec4\uff0c\u4ee5\u5355\u7ec4\u521a\u6027\u53d8\u6362\u66ff\u4ee3\u9010\u9ad8\u65af\u5f62\u53d8\u9884\u6d4b\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u5728NeRF-DS\u6570\u636e\u96c6\u4e0a\u5b9e\u73b010.37\u500d\u6e32\u67d3\u52a0\u901f\u30017.71\u500d\u6a21\u578b\u538b\u7f29\u548c2.71\u500d\u8bad\u7ec3\u63d0\u901f\uff0c\u5728D-NeRF\u548cHyperNeRF\u6570\u636e\u96c6\u5206\u522b\u63d0\u53474.20\u500d\u548c58.23\u500d\u6027\u80fd\u3002  \n\u25c6 \u6a21\u5757\u5316\u8bbe\u8ba1\u517c\u5bb9\u73b0\u6709\u52a8\u60013DGS/4DGS\u6846\u67b6\uff0c\u517c\u5177\u9ad8\u6548\u6027\u4e0e\u901a\u7528\u6027\u3002|\n",
    "2506.07497": "|2025-06-20|Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency|Xiangyu Guo\u7b49|[2506.07497](http://arxiv.org/pdf/2506.07497)|\u65e0|\u25c6 \u63d0\u51faGenesis\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u591a\u89c6\u89d2\u9a7e\u9a76\u89c6\u9891\u4e0eLiDAR\u5e8f\u5217\u7684\u8054\u5408\u751f\u6210\uff0c\u4fdd\u8bc1\u65f6\u7a7a\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002  \n\u25c6 \u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u7ed3\u5408DiT\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0e3D-VAE\u7f16\u7801\uff0c\u4ee5\u53ca\u57fa\u4e8eBEV\u7684LiDAR\u751f\u6210\u5668\u4e0eNeRF\u6e32\u67d3\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8f93\u51fa\u3002  \n\u25c6 \u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u76f4\u63a5\u8026\u5408\u89c6\u89c9\u4e0e\u51e0\u4f55\u6a21\u6001\uff0c\u786e\u4fdd\u751f\u6210\u5185\u5bb9\u5728\u8de8\u6a21\u6001\u95f4\u7684\u8fde\u8d2f\u6f14\u5316\u3002  \n\u25c6 \u521b\u65b0\u5f15\u5165DataCrafter\u63cf\u8ff0\u6a21\u5757\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u573a\u666f\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u76d1\u7763\uff0c\u589e\u5f3a\u751f\u6210\u6570\u636e\u7684\u7ed3\u6784\u5316\u63a7\u5236\u3002  \n\u25c6 \u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u89c6\u9891\uff08FVD 16.95\uff09\u548cLiDAR\uff08Chamfer 0.611\uff09\u6307\u6807\u7684SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u751f\u6210\u6570\u636e\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002  \n\u25c6 \u751f\u6210\u6570\u636e\u53ef\u6709\u6548\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5206\u5272\u548c3D\u68c0\u6d4b\uff09\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002|\n",
    "2506.06890": "|2025-06-07|SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation|Sumit Sharma\u7b49|[2506.06890](http://arxiv.org/pdf/2506.06890)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u4ece\u4e8c\u8fdb\u5236\u5355\u5149\u5b50\u76f8\u673a(SPC)\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u5f69\u8272\u65b0\u89c6\u89d2\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u5408\u6210\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u4e25\u91cd\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002  \n\u25c6 \u7b2c\u4e00\u9636\u6bb5\u91c7\u7528Pix2PixHD\u7b49\u751f\u6210\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff0c\u5c06\u4e8c\u8fdb\u5236SPC\u8f93\u5165\u8f6c\u5316\u4e3a\u53ef\u4fe1\u7684RGB\u56fe\u50cf\uff0c\u6709\u6548\u6062\u590d\u4e22\u5931\u7684\u7eb9\u7406\u548c\u989c\u8272\u4fe1\u606f\u3002  \n\u25c6 \u7b2c\u4e8c\u9636\u6bb5\u7ed3\u5408\u795e\u7ecf\u8f90\u5c04\u573a(NeRF)\u6216\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7b49\u5148\u8fdb3D\u91cd\u5efa\u6280\u672f\uff0c\u4ece\u751f\u6210\u7684RGB\u56fe\u50cf\u4e2d\u5408\u6210\u65b0\u89c6\u89d2\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6(Pix2PixHD + Nerf/3DGS)\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u611f\u77e5\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002  \n\u25c6 \u8be5\u5de5\u4f5c\u4e3a\u5355\u5149\u5b50\u76f8\u673a\u8fd9\u7c7b\u65b0\u5174\u6210\u50cf\u6280\u672f\u76843D\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6781\u4f4e\u5149\u7167\u6216\u8d85\u9ad8\u901f\u6210\u50cf\u573a\u666f\u3002|\n",
    "2506.06462": "|2025-06-06|Splat and Replace: 3D Reconstruction with Repetitive Elements|Nicol\u00e1s Violante\u7b49|[2506.06462](http://arxiv.org/pdf/2506.06462)|\u65e0|\u25c6 \u5229\u7528\u573a\u666f\u4e2d\u7684\u91cd\u590d\u5143\u7d20\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfNeRF\u548c3DGS\u5728\u8bad\u7ec3\u89c6\u89d2\u4e0d\u8db3\u65f6\u6e32\u67d3\u6548\u679c\u5dee\u7684\u95ee\u9898\u3002  \n\u25c6 \u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u91cd\u590d\u5b9e\u4f8b\u5206\u5272\u4e0e\u914d\u51c6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e0d\u540c\u5b9e\u4f8b\u95f4\u7684\u4fe1\u606f\u5171\u4eab\u3002  \n\u25c6 \u901a\u8fc7\u51e0\u4f55\u4f18\u5316\u548c\u5916\u89c2\u53d8\u5316\u5efa\u6a21\uff0c\u540c\u65f6\u63d0\u5347\u573a\u666f\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002  \n\u25c6 \u5728\u5408\u6210\u4e0e\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u6539\u5584\u4e86\u906e\u6321\u548c\u4f4e\u8986\u76d6\u533a\u57df\u7684\u6e32\u67d3\u6548\u679c\u3002  \n\u25c6 \u9996\u6b21\u5c06\u91cd\u590d\u5143\u7d20\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u878d\u51653D\u91cd\u5efa\u6d41\u7a0b\uff0c\u4e3a\u590d\u6742\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2506.06412": "|2025-06-06|NeurNCD: Novel Class Discovery via Implicit Neural Representation|Junming Wang\u7b49|[2506.06412](http://arxiv.org/pdf/2506.06412)|\u65e0|\u25c6 NeurNCD\u9996\u6b21\u63d0\u51fa\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08Embedding-NeRF\u6a21\u578b\uff09\u66ff\u4ee3\u4f20\u7edf\u663e\u5f0f3D\u5206\u5272\u56fe\uff0c\u901a\u8fc7KL\u6563\u5ea6\u805a\u5408\u8bed\u4e49\u5d4c\u5165\u548c\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u7684\u71b5\uff0c\u89e3\u51b3\u79bb\u6563\u5316\u3001\u7a7a\u6d1e\u548c\u566a\u58f0\u95ee\u9898\u3002  \n\u25c6 \u7ed3\u5408\u7279\u5f81\u67e5\u8be2\u3001\u7279\u5f81\u8c03\u5236\u548c\u805a\u7c7b\u7b49\u5173\u952e\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u9884\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u4e0e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u4e4b\u95f4\u7684\u9ad8\u6548\u7279\u5f81\u589e\u5f3a\u548c\u4fe1\u606f\u4ea4\u4e92\u3002  \n\u25c6 \u8be5\u6846\u67b6\u5728\u5f00\u653e\u548c\u5c01\u95ed\u573a\u666f\u4e2d\u5747\u5b9e\u73b0\u4f18\u8d8a\u5206\u5272\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u5bc6\u96c6\u6807\u6ce8\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u6216\u4eba\u5de5\u751f\u6210\u7a00\u758f\u6807\u7b7e\u76d1\u7763\u3002  \n\u25c6 \u5728NYUv2\u548cReplica\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cNeurNCD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b0\u7c7b\u522b\u53d1\u73b0\u6846\u67b6\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2506.05965": "|2025-06-06|Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments|Mingrui Li\u7b49|[2506.05965](http://arxiv.org/pdf/2506.05965)|\u65e0|\u25c6 \u63d0\u51fa\u4e86Dy3DGS-SLAM\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u5355\u76eeRGB\u8f93\u5165\u7684\u52a8\u6001\u573a\u666f3D\u9ad8\u65af\u6cfc\u6e85SLAM\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7eaf\u89c6\u89c9SLAM\u7684\u7a7a\u767d\u3002  \n\u25c6 \u901a\u8fc7\u6982\u7387\u6a21\u578b\u878d\u5408\u5149\u6d41\u63a9\u7801\u548c\u6df1\u5ea6\u63a9\u7801\uff0c\u751f\u6210\u52a8\u6001\u878d\u5408\u63a9\u7801\uff0c\u4ec5\u9700\u5355\u6b21\u7f51\u7edc\u8fed\u4ee3\u5373\u53ef\u7ea6\u675f\u8ddf\u8e2a\u5c3a\u5ea6\u5e76\u4f18\u5316\u51e0\u4f55\u6e32\u67d3\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u8fd0\u52a8\u635f\u5931\u51fd\u6570\uff0c\u57fa\u4e8e\u52a8\u6001\u878d\u5408\u63a9\u7801\u7ea6\u675f\u4f4d\u59ff\u4f30\u8ba1\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7269\u4f53\u5e72\u6270\u4e0b\u7684\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728\u6620\u5c04\u9636\u6bb5\uff0c\u7ed3\u5408\u52a8\u6001\u50cf\u7d20\u7684\u6e32\u67d3\u635f\u5931\u3001\u989c\u8272\u548c\u6df1\u5ea6\u4fe1\u606f\uff0c\u6709\u6548\u6d88\u9664\u4e86\u52a8\u6001\u7269\u4f53\u5e26\u6765\u7684\u77ac\u6001\u5e72\u6270\u548c\u906e\u6321\u95ee\u9898\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u4e0e\u6e32\u67d3\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u90e8\u5206RGB-D\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5355\u76ee\u8f93\u5165\u7684\u6f5c\u529b\u3002|\n",
    "2506.05317": "|2025-06-06|ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation|Daniel Rho\u7b49|[2506.05317](http://arxiv.org/pdf/2506.05317)|\u65e0|\u25c6 \u63d0\u51faProJo4D\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8054\u5408\u4f18\u5316\u7b56\u7565\u89e3\u51b3\u7a00\u758f\u591a\u89c6\u89d2\u89c6\u9891\u4e0b\u7684\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u56e0\u5206\u9636\u6bb5\u4f18\u5316\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u53c2\u6570\u654f\u611f\u6027\u6307\u5bfc\u7684\u4f18\u5316\u987a\u5e8f\uff0c\u9010\u6b65\u8054\u5408\u4f18\u5316\u51e0\u4f55\u3001\u5916\u89c2\u3001\u7269\u7406\u72b6\u6001\u548c\u6750\u6599\u5c5e\u6027\uff0c\u907f\u514d\u76f4\u63a5\u5168\u53c2\u6570\u4f18\u5316\u5e26\u6765\u7684\u975e\u51f8\u548c\u975e\u53ef\u5fae\u96be\u9898\u3002  \n\u25c6 \u5728PAC-NeRF\u548cSpring-Gaus\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57284D\u672a\u6765\u72b6\u6001\u9884\u6d4b\u3001\u672a\u6765\u72b6\u6001\u7684\u65b0\u89c6\u89d2\u6e32\u67d3\u548c\u6750\u6599\u53c2\u6570\u4f30\u8ba1\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002  \n\u25c6 \u9996\u6b21\u5b9e\u73b0\u7a00\u758f\u591a\u89c6\u89d2\u8f93\u5165\u4e0b\u7684\u7269\u7406\u51c6\u786e\u6570\u5b57\u5b6a\u751f\u6784\u5efa\uff0c\u4e3a\u673a\u5668\u4eba\u548cXR\u5e94\u7528\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u901a\u8fc7\u6e10\u8fdb\u5f0f\u4f18\u5316\u7b56\u7565\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u4e3a\u590d\u6742\u7269\u7406\u573a\u666f\u7684\u795e\u7ecf\u6e32\u67d3\u4e0e\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\u3002|\n",
    "2506.05280": "|2025-06-06|Unifying Appearance Codes and Bilateral Grids for ...|Nan Wang\u7b49|[2506.05280](http://arxiv.org/pdf/2506.05280)|[\u4ee3\u7801](https://github.com/bigcileng/bilateral-driving)|\u25c6\u63d0\u51fa\u591a\u5c3a\u5ea6\u53cc\u8fb9\u7f51\u683c\u65b0\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u5916\u89c2\u7f16\u7801\u548c\u53cc\u8fb9\u7f51\u683c\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u4e2d\u5149\u5ea6\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002  \n\u25c6\u901a\u8fc7\u50cf\u7d20\u7ea7\u989c\u8272\u6620\u5c04\u548c\u5206\u5c42\u7ea6\u675f\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5149\u4e0d\u4e00\u81f4\u4ea7\u751f\u7684\u6f02\u6d6e\u4f2a\u5f71\uff0c\u5728\u56db\u5927\u81ea...|\n",
    "2506.04908": "|2025-06-05|Generating Synthetic Stereo Datasets using 3D Gaus...|Filip Slezak\u7b49|[2506.04908](http://arxiv.org/pdf/2506.04908)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u7acb\u4f53\u6570\u636e\u96c6\u751f\u6210\u6d41\u7a0b\uff0c\u76f8\u6bd4NeRF\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002  \n\u25c6 \u7ed3\u5408\u663e\u5f0f3D\u91cd\u5efa\u51e0\u4f55\u4e0eFoundationStereo\u6a21\u578b\u7684\u6df1\u5ea6\u4f30\u8ba1\u8fdb\u884c\u4e13\u5bb6\u77e5\u8bc6\u8fc1\u79fb\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u3002...|\n",
    "2506.08619": "|2025-06-10|A Probability-guided Sampler for Neural Implicit Surface Rendering|Gon\u00e7alo Dias Pais\u7b49|[2506.08619](http://arxiv.org/pdf/2506.08619)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u76843D\u56fe\u50cf\u6295\u5f71\u7a7a\u95f4\u6a21\u578b\uff0c\u5b9e\u73b0\u9488\u5bf9\u611f\u5174\u8da3\u533a\u57df\u7684\u5c04\u7ebf\u91c7\u6837\u4f18\u5316\uff0c\u63d0\u5347\u6e32\u67d3\u7cbe\u5ea6\u3002  \n\u25c6 \u8bbe\u8ba1\u65b0\u578b\u8868\u9762\u91cd\u5efa\u635f\u5931\u51fd\u6570\uff0c\u5145\u5206\u5229\u75283D\u6295\u5f71\u7a7a\u95f4\u6a21\u578b\uff0c\u6574\u5408\u8fd1\u8868\u9762\u548c\u7a7a\u767d\u7a7a\u95f4\u4fe1\u606f\u4ee5\u589e\u5f3a\u6027\u80fd\u3002  \n\u25c6 \u7ed3\u5408\u9690\u5f0f\u8868\u9762\u8868\u793a\uff0c\u901a\u8fc7\u6982\u7387\u5f15\u5bfc\u91c7\u6837\u7b56\u7565\u6709\u6548\u805a\u7126\u5173\u952e\u533a\u57df\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002  \n\u25c6 \u5c06\u63d0\u51fa\u7684\u91c7\u6837\u7b56\u7565\u4e0e\u635f\u5931\u51fd\u6570\u96c6\u6210\u5230\u73b0\u6709\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u6e32\u67d3\u5668\u4e2d\uff0c\u663e\u8457\u63d0\u53473D\u91cd\u5efa\u548c\u56fe\u50cf\u6e32\u67d3\u8d28\u91cf\u3002  \n\u25c6 \u7279\u522b\u9488\u5bf9\u573a\u666f\u4e2d\u611f\u5174\u8da3\u533a\u57df\uff08\u5982\u7269\u4f53\u8868\u9762\uff09\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u7ec6\u8282\u8fd8\u539f\uff0c\u514b\u670d\u4f20\u7edf\u5747\u5300\u91c7\u6837\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u901a\u8fc7\u8054\u5408\u4f18\u5316\u91c7\u6837\u4e0e\u91cd\u5efa\u8fc7\u7a0b\uff0c\u5728\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u83b7\u5f97\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u6e32\u67d3\u7ed3\u679c\u3002|\n",
    "2506.09885": "|2025-06-11|The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge|Haoru Wang\u7b49|[2506.09885](http://arxiv.org/pdf/2506.09885)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u97003D\u5148\u9a8c\u77e5\u8bc6\u548c\u76f8\u673a\u4f4d\u59ff\u6807\u6ce8\u7684\u901a\u7528\u5316\u65b0\u89c6\u89d2\u5408\u6210\u6846\u67b6\uff0c\u4ec5\u4f9d\u8d56\u7a00\u758f\u65e0\u4f4d\u59ff\u76842D\u56fe\u50cf\u5373\u53ef\u751f\u6210\u903c\u771f\u65b0\u89c6\u56fe\u3002  \n\u25c6 \u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u63ed\u793a\u4e86\u5173\u952e\u8d8b\u52bf\uff1a\u51cf\u5c11\u5bf93D\u77e5\u8bc6\u7684\u4f9d\u8d56\u80fd\u66f4\u9ad8\u6548\u5229\u7528\u6570\u636e\u89c4\u6a21\uff0c\u6700\u7ec8\u8fbe\u5230\u4e0e\u4f9d\u8d563D\u77e5\u8bc6\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u663e\u5f0f3D\u8868\u793a\uff08\u5982NeRF\u30013DGS\uff09\u548c\u8f93\u5165/\u76ee\u6807\u89c6\u89d2\u4f4d\u59ff\u6807\u6ce8\u7684\u53cc\u91cd\u4f9d\u8d56\uff0c\u5b9e\u73b0\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u9690\u5f0f3D\u7406\u89e3\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4ec5\u901a\u8fc7\u7a00\u758f2D\u56fe\u50cf\u5373\u53ef\u5b66\u4e60\u9690\u5f0f3D\u4e00\u81f4\u6027\uff0c\u751f\u6210\u8d28\u91cf\u5ab2\u7f8e\u4f9d\u8d56\u4f4d\u59ff\u8f93\u5165\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u4e3a\u4e2d\u5fc3\u8303\u5f0f\u7684\u53ef\u884c\u6027\u3002  \n\u25c6 \u4e3a\u5927\u89c4\u6a21\u6570\u636e\u65f6\u4ee3\u7684\u65b0\u89c6\u89d2\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u8868\u660e\u51cf\u5c113D\u5148\u9a8c\u4f9d\u8d56\u4e0e\u6570\u636e\u89c4\u6a21\u6269\u5c55\u4e4b\u95f4\u5b58\u5728\u6b63\u5411\u5173\u8054\u6027\u3002|\n",
    "2506.10335": "|2025-06-12|PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting|Lintao Xiang\u7b49|[2506.10335](http://arxiv.org/pdf/2506.10335)|\u65e0|\u25c6 \u63d0\u51faPointGS\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u6ce8\u610f\u529b\u611f\u77e5\u7684\u7a00\u758f\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u8f93\u5165\u89c6\u56fe\u4e0d\u8db3\u65f6\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002  \n\u25c6 \u5229\u7528\u6700\u65b0\u7684\u7acb\u4f53\u57fa\u7840\u6a21\u578b\u4f30\u8ba1\u7cbe\u786e\u76f8\u673a\u59ff\u6001\u5e76\u91cd\u5efa\u5bc6\u96c6\u70b9\u4e91\uff0c\u4e3a\u9ad8\u65af\u521d\u59cb\u5316\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8d77\u70b9\u3002  \n\u25c6 \u8bbe\u8ba1\u591a\u5c3a\u5ea62D\u5916\u89c2\u7279\u5f81\u91c7\u6837\u4e0e\u805a\u5408\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a3D\u9ad8\u65af\u70b9\u7f16\u7801\u989c\u8272\u5c5e\u6027\uff0c\u589e\u5f3a\u7a00\u758f\u8f93\u5165\u4e0b\u7684\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5f15\u5165\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u70b9\u4ea4\u4e92\u7f51\u7edc\uff0c\u4f7f\u9ad8\u65af\u70b9\u80fd\u4e0e\u90bb\u8fd1\u70b9\u4ea4\u4e92\uff0c\u63d0\u5347\u70b9\u7ea7\u5916\u89c2\u8868\u793a\u80fd\u529b\u3002  \n\u25c6 \u901a\u8fc7\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u5c06\u589e\u5f3a\u7279\u5f81\u89e3\u7801\u4e3a\u9ad8\u65af\u53c2\u6570\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb3DGS\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002|\n",
    "2506.12787": "|2025-06-18|Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting|Mufan Liu\u7b49|[2506.12787](http://arxiv.org/pdf/2506.12787)|\u65e0|\u25c6 \u63d0\u51faSwiftWRF\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splatting\uff09\u6280\u672f\u5f15\u5165\u65e0\u7ebf\u8f90\u5c04\u573a\uff08WRF\uff09\u5efa\u6a21\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u7684\u5c40\u9650\u3002  \n\u25c6 \u91c7\u7528\u53ef\u53d8\u5f622D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7MLP\u5efa\u6a21\u9ad8\u65af\u5f62\u53d8\uff0c\u6709\u6548\u6355\u6349\u6536\u53d1\u7aef\u5355\u4fa7\u79fb\u52a8\u5bfc\u81f4\u7684WRF\u52a8\u6001\u53d8\u5316\u3002  \n\u25c6 \u5b9e\u73b0CUDA\u52a0\u901f\u7684\u5149\u6805\u5316\u6e32\u67d3\uff0c\u9891\u8c31\u5408\u6210\u901f\u5ea6\u8d85\u8fc710\u4e07\u5e27/\u79d2\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5feb500\u500d\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u9700\u6c42\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u652f\u6301\u4efb\u610f\u4f4d\u7f6e\u7684WRF\u9891\u8c31\u5408\u6210\uff0c\u5e76\u5728\u5230\u8fbe\u89d2\uff08AoA\uff09\u548c\u4fe1\u53f7\u5f3a\u5ea6\uff08RSSI\uff09\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002  \n\u25c6 \u5728\u771f\u5b9e\u548c\u5408\u6210\u5ba4\u5185\u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4fe1\u53f7\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002|\n",
    "2506.12727": "|2025-06-17|Efficient multi-view training for 3D Gaussian Splatting|Minhyuk Choi\u7b49|[2506.12727](http://arxiv.org/pdf/2506.12727)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u591a\u89c6\u89d2\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4f20\u7edf\u5355\u89c6\u89d2\u8bad\u7ec3\u5bfc\u81f4\u7684\u968f\u673a\u68af\u5ea6\u65b9\u5dee\u8fc7\u5927\u95ee\u9898\uff0c\u4f18\u5316\u4e86\u8bad\u7ec3\u6548\u679c\u3002  \n\u25c6 \u6539\u8fdb\u4e86\u5149\u6805\u5316\u6d41\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u591a\u89c6\u89d2\u8bad\u7ec3\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4f7f\u5176\u66f4\u9ad8\u6548\u53ef\u884c\u3002  \n\u25c6 \u8bbe\u8ba1\u4e863D\u8ddd\u79bb\u611f\u77e5\u7684D-SSIM\u635f\u5931\u51fd\u6570\uff0c\u66f4\u597d\u5730\u9002\u5e94\u591a\u89c6\u89d2\u573a\u666f\uff0c\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u3002  \n\u25c6 \u63d0\u51fa\u591a\u89c6\u89d2\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5355\u89c6\u89d2\u5047\u8bbe\u4e0b\u9ad8\u65af\u5206\u5e03\u4f18\u5316\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863DGS\u53ca\u5176\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u7a81\u7834\u4e86\u5355\u89c6\u89d2\u8bad\u7ec3\u7684\u7ea6\u675f\u3002  \n\u25c6 \u4e3a3DGS\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u9006\u5411\u6e32\u67d3\u4e2d\u7684\u5e94\u7528\u3002|\n",
    "2506.15242": "|2025-06-24|RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories|Qingsong Yan\u7b49|[2506.15242](http://arxiv.org/pdf/2506.15242)|\u65e0|\u25c6 \u63d0\u51faRA-NeRF\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfNeRF\u548c3DGS\u4f9d\u8d56\u51c6\u786e\u4f4d\u59ff\u5148\u9a8c\u7684\u95ee\u9898\u3002  \n\u25c6 \u91c7\u7528\u589e\u91cf\u5f0f\u91cd\u5efa\u6d41\u7a0b\uff0c\u7ed3\u5408\u5149\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u5149\u6d41\u9a71\u52a8\u7684\u4f4d\u59ff\u8c03\u8282\u673a\u5236\uff0c\u63d0\u5347\u4e86\u521d\u59cb\u5316\u548c\u5b9a\u4f4d\u9636\u6bb5\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5f15\u5165\u9690\u5f0f\u4f4d\u59ff\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u6355\u6349\u76f8\u673a\u8fd0\u52a8\u6a21\u5f0f\u6709\u6548\u6d88\u9664\u4f4d\u59ff\u4f30\u8ba1\u4e2d\u7684\u566a\u58f0\uff0c\u589e\u5f3a\u590d\u6742\u8f68\u8ff9\u4e0b\u7684\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u5728Tanks&Temple\u548cNeRFBuster\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u4f4d\u59ff\u4f30\u8ba1\u548c\u89c6\u89c9\u8d28\u91cf\u5747\u8fbe\u5230SOTA\u6c34\u5e73\u3002  \n\u25c6 \u6574\u4f53\u6846\u67b6\u65e0\u9700\u5916\u90e8\u7ea6\u675f\uff0c\u4ec5\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5373\u53ef\u540c\u65f6\u4f18\u5316\u573a\u666f\u91cd\u5efa\u4e0e\u76f8\u673a\u4f4d\u59ff\uff0c\u9002\u7528\u4e8eSLAM\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002|\n",
    "2506.14856": "|2025-06-17|Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction|Zhengquan Zhang\u7b49|[2506.14856](http://arxiv.org/pdf/2506.14856)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u524d\u9988\u795e\u7ecf\u7f51\u7edcUPNet\u7684\u65b0\u9896\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\uff0c\u76f4\u63a5\u9884\u6d4b\u5019\u9009\u89c6\u89d2\u7684\u4e0d\u786e\u5b9a\u6027\u56fe\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97\u6bcf\u4e2a\u89c6\u89d2\u4e0d\u786e\u5b9a\u6027\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002  \n\u25c6UPNet\u4ec5\u9700\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u5373\u53ef\u9884\u6d4b\u6240\u6709\u5019\u9009\u89c6\u89d2\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5b66\u4e60\u81ea\u7136\u7269\u4f53\u89c6\u89d2\u4e0e\u4f53\u7d20\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4fe1\u606f\u63d0\u53d6\u3002  \n\u25c6\u901a\u8fc7\u805a\u5408\u5386\u53f2\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u56fe\u6765\u6291\u5236\u5197\u4f59\u89c6\u89d2\uff0c\u667a\u80fd\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u65b0\u89c6\u89d2\uff0c\u4ec5\u9700\u4e00\u534a\u89c6\u89d2\u5373\u53ef\u8fbe\u5230\u4e0e\u4e0a\u9650\u76f8\u5f53\u76843D\u91cd\u5efa\u7cbe\u5ea6\u3002  \n\u25c6\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u73b0\u9ad8\u8fbe400\u500d\u7684\u52a0\u901f\uff0c\u5e76\u51cf\u5c1150%\u4ee5\u4e0a\u7684CPU\u3001RAM\u548cGPU\u8d44\u6e90\u6d88\u8017\u3002  \n\u25c6\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9002\u7528\u4e8e\u65b0\u7269\u4f53\u7c7b\u522b\u7684\u89c6\u89d2\u9009\u62e9\u4efb\u52a1\uff0c\u5c55\u73b0\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002  \n\u25c6\u6574\u4f53\u65b9\u6848\u5c06\u795e\u7ecf\u6e32\u67d3\u4e0e\u9ad8\u6548\u89c6\u89d2\u9009\u62e9\u76f8\u7ed3\u5408\uff0c\u4e3a3D\u91cd\u5efa\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u4e0e\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u5b9e\u7528\u5316\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2506.16262": "|2025-06-23|R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision|Weeyoung Kwon\u7b49|[2506.16262](http://arxiv.org/pdf/2506.16262)|[\u4ee3\u7801](https://github.com/cmlab-korea/awesome-3d-low-level-vision)|\u25c6 \u63d0\u51fa\u201c3D\u4f4e\u5c42\u89c6\u89c9\uff083D LLV\uff09\u201d\u65b0\u9886\u57df\uff0c\u5c06\u4f20\u7edf2D\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u8d85\u5206\u3001\u53bb\u6a21\u7cca\u3001\u5929\u6c14\u9000\u5316\u4fee\u590d\u7b49\uff09\u6269\u5c55\u52303D\u7a7a\u95f4\uff0c\u89e3\u51b3\u795e\u7ecf\u6e32\u67d3\u5728\u771f\u5b9e\u9000\u5316\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u5316\u5b9a\u4e49\u201c\u9000\u5316\u611f\u77e5\u6e32\u67d3\u201d\u95ee\u9898\uff0c\u660e\u786e\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u75c5\u6001\u4f18\u5316\u7b49\u6838\u5fc3\u6311\u6218\uff0c\u4e3a3D LLV\u7814\u7a76\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u3002  \n\u25c6 \u7efc\u8ff0\u4e86\u5c06\u4f4e\u5c42\u89c6\u89c9\u6280\u672f\u4e0e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u30013D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7b49\u795e\u7ecf\u6e32\u67d3\u7ed3\u5408\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u5c55\u793a\u5176\u5728\u566a\u58f0\u3001\u6a21\u7cca\u3001\u4f4e\u5206\u8fa8\u7387\u7b49\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u80fd\u529b\u3002  \n\u25c6 \u68b3\u7406\u4e86\u81ea\u52a8\u9a7e\u9a76\u3001AR/VR\u3001\u673a\u5668\u4eba\u7b49\u5173\u952e\u5e94\u7528\u573a\u666f\uff0c\u5f3a\u8c03\u4ece\u9000\u5316\u8f93\u5165\u4e2d\u5b9e\u73b0\u53ef\u97603D\u611f\u77e5\u7684\u5b9e\u7528\u4ef7\u503c\u3002  \n\u25c6 \u6c47\u603b\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u672a\u67653D LLV\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u53c2\u8003\uff0c\u63a8\u52a8\u771f\u5b9e\u73af\u5883\u4e0b\u9c81\u68d23D\u5185\u5bb9\u751f\u6210\u4e0e\u573a\u666f\u91cd\u5efa\u7684\u53d1\u5c55\u3002|\n",
    "2506.18678": "|2025-06-23|MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation|Tianchen Deng\u7b49|[2506.18678](http://arxiv.org/pdf/2506.18678)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u795e\u7ecfSLAM\u6846\u67b6MCN-SLAM\uff0c\u7ed3\u5408\u6df7\u5408\u9690\u5f0f\u795e\u7ecf\u573a\u666f\u8868\u793a\uff0c\u89e3\u51b3\u4f20\u7edf\u5355\u667a\u80fd\u4f53\u9690\u5f0fSLAM\u5728\u5927\u573a\u666f\u548c\u957f\u5e8f\u5217\u4e2d\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u521b\u65b0\u8bbe\u8ba1\u4e09\u5e73\u9762-\u7f51\u683c\u8054\u5408\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u573a\u666f\u91cd\u5efa\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709NeRF-based\u65b9\u6cd5\u3002  \n\u25c6 \u5f00\u53d1\"\u5185\u90e8-\u8de8\u667a\u80fd\u4f53\"\u95ed\u73af\u68c0\u6d4b\u673a\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5c40\u90e8\u4e00\u81f4\u6027\u4e0e\u591a\u667a\u80fd\u4f53\u5168\u5c40\u4e00\u81f4\u6027\u7684\u534f\u540c\u4f18\u5316\u3002  \n\u25c6 \u63d0\u51fa\u5728\u7ebf\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u591a\u5b50\u5730\u56fe\u878d\u5408\uff0c\u7a81\u7834\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\uff0c\u786e\u4fdd\u5168\u5c40\u5730\u56fe\u4e00\u81f4\u6027\u3002  \n\u25c6 \u53d1\u5e03\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u5bc6\u96c6SLAM\u6570\u636e\u96c6DES\uff0c\u6db5\u76d6\u5355/\u591a\u667a\u80fd\u4f53\u573a\u666f\uff0c\u63d0\u4f9b\u8fde\u7eed\u8f68\u8ff9\u548c\u9ad8\u7cbe\u5ea63D\u7f51\u683c\u771f\u503c\uff0c\u586b\u8865\u9886\u57df\u7a7a\u767d\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5efa\u56fe\u3001\u5b9a\u4f4d\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u63a8\u52a8SLAM\u548c3D\u91cd\u5efa\u9886\u57df\u53d1\u5c55\u3002|\n",
    "2506.18575": "|2025-06-26|2D Triangle Splatting for Direct Differentiable Mesh Training|Kaifeng Sheng\u7b49|[2506.18575](http://arxiv.org/pdf/2506.18575)|[\u4ee3\u7801](https://github.com/GaodeRender/triangle-splatting)|\u25c6 \u63d0\u51fa2D\u4e09\u89d2\u5f62\u9762\u7247\uff082DTS\uff09\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf3D\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u76f4\u63a5\u53ef\u5fae\u5206\u7f51\u683c\u8bad\u7ec3\u3002  \n\u25c6 \u7ed3\u5408\u79bb\u6563\u7f51\u683c\u7ed3\u6784\u4e0e\u8fde\u7eed\u4f53\u79ef\u5efa\u6a21\u4f18\u52bf\uff0c\u5f62\u6210\u7c7b\u7f51\u683c\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002  \n\u25c6 \u5f15\u5165\u7d27\u51d1\u6027\u53c2\u6570\u5230\u4e09\u89d2\u5f62\u57fa\u5143\u4e2d\uff0c\u652f\u6301\u76f4\u63a5\u8bad\u7ec3\u9ad8\u771f\u5b9e\u611f\u7f51\u683c\uff0c\u7b80\u5316\u4f20\u7edf\u7f51\u683c\u91cd\u5efa\u6d41\u7a0b\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u5373\u4f7f\u672a\u4f18\u5316\u7d27\u51d1\u6027\u53c2\u6570\uff0c\u5176\u57fa\u7840\u7248\u672c\u4e5f\u80fd\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u9ad8\u65af\u57fa\u5143\u65b9\u6cd5\u7684\u6e32\u67d3\u4fdd\u771f\u5ea6\u3002  \n\u25c6 \u751f\u6210\u7684\u7f51\u683c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7f51\u683c\u91cd\u5efa\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u5149\u7167\u548c\u9634\u5f71\u6548\u679c\u4e2d\u8868\u73b0\u7a81\u51fa\u3002  \n\u25c6 \u4e3a\u53ef\u5fae\u5206\u6e32\u67d3\u9886\u57df\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u6e32\u67d3\u901f\u5ea6\u4e0e\u9ad8\u7ea7\u6e32\u67d3\u6548\u679c\uff08\u5982\u91cd\u5149\u7167\uff09\u7684\u517c\u5bb9\u6027\u3002|\n",
    "2506.18208": "|2025-06-22|Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction|Ankit Sanjyal|[2506.18208](http://arxiv.org/pdf/2506.18208)|\u65e0|\u25c6 \u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86DINO\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u5728NeRF\u5c11\u6837\u672c3D\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6240\u6709\u53d8\u4f53\u6027\u80fd\u5747\u4f4e\u4e8e\u539f\u59cbNeRF\u57fa\u7ebf\uff08PSNR 12.9-13.0 vs 14.71\uff09\u3002  \n\u25c6 \u63ed\u793a\u4e86\u53cd\u76f4\u89c9\u7ed3\u8bba\uff1a\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u4e0d\u4ec5\u65e0\u52a9\u4e8e\u5c11\u6837\u672c\u91cd\u5efa\uff0c\u53cd\u800c\u53ef\u80fd\u5f15\u5165\u6709\u5bb3\u504f\u5dee\uff0c\u6311\u6218\u4e86\u8be5\u9886\u57df\u666e\u904d\u5047\u8bbe\u3002  \n\u25c6 \u63d0\u51fa\u4e09\u79cd\u6f5c\u5728\u5931\u6548\u539f\u56e0\u5206\u6790\u6846\u67b6\uff1a\u7279\u5f81-\u4efb\u52a1\u4e0d\u5339\u914d\u3001\u6709\u9650\u6570\u636e\u8fc7\u62df\u5408\u95ee\u9898\u4ee5\u53ca\u7279\u5f81\u878d\u5408\u6280\u672f\u74f6\u9888\u3002  \n\u25c6 \u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u51bb\u7ed3\u7279\u5f81\u3001LoRA\u5fae\u8c03\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u7b49\u4e3b\u6d41\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u6392\u9664\u65e0\u6548\u8def\u5f84\u3002  \n\u25c6 \u6307\u51fa\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5e94\u4f18\u5148\u5173\u6ce8\u51e0\u4f55\u4e00\u81f4\u6027\u800c\u975e\u590d\u6742\u7279\u5f81\u5de5\u7a0b\uff0c\u4e3a\u7b80\u5316\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u5411\u3002  \n\u25c6 \u7814\u7a76\u6210\u679c\u5bf9\u57fa\u4e8e\u9884\u8bad\u7ec3\u7279\u5f81\u76843D\u91cd\u5efa\u65b9\u6cd5\u63d0\u51fa\u91cd\u8981\u8b66\u793a\uff0c\u53ef\u80fd\u6539\u53d8\u8be5\u9886\u57df\u6280\u672f\u8def\u7ebf\u9009\u62e9\u3002|\n",
    "2506.17636": "|2025-06-21|3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene|Shihan Chen\u7b49|[2506.17636](http://arxiv.org/pdf/2506.17636)|\u65e0|\u25c6 \u63d0\u51fa\u4ece\u7c97\u5230\u7cbe\u7684\u6e10\u8fdb\u5f0f\u91cd\u5efa\u7b56\u7565\uff0c\u5148\u5feb\u901f\u6784\u5efa\u7c97\u7cd9\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u81ea\u9002\u5e94\u573a\u666f\u5206\u5272\u548c\u5b50\u573a\u666f\u7ec6\u5316\u5b9e\u73b0\u5927\u89c4\u6a21\u573a\u666f\u7684\u9ad8\u6548\u91cd\u5efa\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u7ed3\u5408\u89e3\u8026\u5916\u89c2\u6a21\u578b\uff0c\u6709\u6548\u6355\u6349\u6237\u5916\u73af\u5883\u4e2d\u590d\u6742\u7684\u5168\u5c40\u5149\u7167\u53d8\u5316\uff0c\u63d0\u5347\u52a8\u6001\u5916\u89c2\u7684\u5efa\u6a21\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1\u77ac\u6001\u63a9\u6a21\u6a21\u578b\uff0c\u81ea\u52a8\u8fc7\u6ee4\u79fb\u52a8\u7269\u4f53\uff08\u5982\u8f66\u8f86\u3001\u884c\u4eba\uff09\u7684\u5e72\u6270\uff0c\u663e\u8457\u63d0\u9ad8\u91cd\u5efa\u7eaf\u51c0\u5ea6\u3002  \n\u25c6 \u6269\u5c55\u591a\u89c6\u89d2\u7ea6\u675f\u5e76\u5f15\u5165\u5355\u89c6\u89d2\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u9488\u5bf9\u6027\u89e3\u51b3\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u7684\u51e0\u4f55\u4f18\u5316\u96be\u9898\u3002  \n\u25c6 \u5728\u65e0\u4eba\u673a\u822a\u62cd\u6570\u636e\u96c6GauU-Scene V2\u4e0a\u9a8c\u8bc1\uff0c\u9996\u6b21\u5b9e\u73b0\u5168\u5c3a\u5bf8\u56fe\u50cf\u4f18\u5316\u7684\u5927\u89c4\u6a21\u573a\u666f\u7cbe\u7ec6\u91cd\u5efa\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709NeRF\u548cGaussian\u7c7b\u65b9\u6cd5\u3002  \n\uff08\u6ce8\uff1a\u5168\u6587\u4e25\u683c\u9075\u5faa5\u70b9\u521b\u65b0\u6027\u603b\u7ed3\uff0c\u672a\u4f7f\u7528Markdown\u7b26\u53f7\uff0c\u5b57\u6570\u63a7\u5236\u5728400\u5b57\u5185\uff09|\n",
    "2506.19742": "|2025-06-24|NeRF-based CBCT Reconstruction needs Normalization and Initialization|Zhuowei Xu\u7b49|[2506.19742](http://arxiv.org/pdf/2506.19742)|\u65e0|\u25c6 \u63d0\u51fa\u5f52\u4e00\u5316\u54c8\u5e0c\u7f16\u7801\u5668\uff08Normalized Hash Encoder\uff09\uff0c\u89e3\u51b3NeRF-based CBCT\u91cd\u5efa\u4e2d\u54c8\u5e0c\u7f16\u7801\u5668\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u5c40\u90e8-\u5168\u5c40\u8bad\u7ec3\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u589e\u5f3a\u7279\u5f81\u4e00\u81f4\u6027\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u8bbe\u8ba1\u6620\u5c04\u4e00\u81f4\u6027\u521d\u59cb\u5316\u7b56\u7565\uff08MCI\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5168\u5c40\u6620\u5c04\u7279\u6027\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u51cf\u5c11\u65e9\u671f\u8bad\u7ec3\u6ce2\u52a8\uff0c\u52a0\u901f\u6536\u655b\u5e76\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u54c8\u5e0c\u7f16\u7801\u5668\u53c2\u6570\u5c40\u90e8\u7a00\u758f\u6027\u4e0e\u795e\u7ecf\u7f51\u7edc\u5168\u5c40\u5bc6\u96c6\u66f4\u65b0\u7684\u77db\u76fe\uff0c\u6307\u51fa\u7279\u5f81\u9519\u4f4d\u662f\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6838\u5fc3\u539f\u56e0\u3002  \n\u25c6 \u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u4ee3\u7801\u6539\u52a8\uff0c\u5373\u53ef\u57284\u4e2a\u6570\u636e\u96c6\u3001128\u4f8bCT\u6570\u636e\uff08\u6db5\u76d67\u4e2a\u89e3\u5256\u533a\u57df\uff09\u4e0a\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u91cd\u5efa\u6027\u80fd\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5f52\u4e00\u5316\u4e0e\u521d\u59cb\u5316\u7b56\u7565\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4e3aNeRF-based\u533b\u5b66\u5f71\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u4f18\u5316\u8303\u5f0f\u3002|\n",
    "2506.19615": "|2025-06-25|Self-Supervised Multimodal NeRF for Autonomous Driving|Gaurav Sharma\u7b49|[2506.19615](http://arxiv.org/pdf/2506.19615)|\u65e0|\u25c6 \u63d0\u51fa\u81ea\u76d1\u7763\u591a\u6a21\u6001NeRF\u6846\u67b6NVSF\uff0c\u65e0\u97003D\u6807\u6ce8\u5373\u53ef\u8054\u5408\u5b66\u4e60LiDAR\u548c\u76f8\u673a\u7684\u65f6\u7a7a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u3002  \n\u25c6 \u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u8bbe\u8ba1\uff0c\u540c\u65f6\u5904\u7406\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u7684\u9002\u5e94\u6027\u3002  \n\u25c6 \u5f15\u5165\u542f\u53d1\u5f0f\u56fe\u50cf\u50cf\u7d20\u91c7\u6837\u7b56\u7565\uff0c\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u50cf\u7d20\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u91c7\u7528\u53cc\u68af\u5ea6\u63a9\u7801\u6280\u672f\uff0c\u6709\u6548\u4fdd\u7559LiDAR\u70b9\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u589e\u5f3a\u70b9\u4e91\u6570\u636e\u91cd\u5efa\u7cbe\u5ea6\u3002  \n\u25c6 \u5728KITTI-360\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cLiDAR\u548c\u76f8\u673a\u57df\u6027\u80fd\u5747\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u73b0\u591a\u6a21\u6001\u4f18\u52bf\u3002  \n\u25c6 \u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u76f8\u5173\u7814\u7a76\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u63d0\u4f9b\u53ef\u590d\u7528\u7684\u65b0\u578b\u795e\u7ecf\u6e32\u67d3\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2506.19291": "|2025-06-24|HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis|Xiaoyuan Wang\u7b49|[2506.19291](http://arxiv.org/pdf/2506.19291)|\u65e0|\u25c6 \u63d0\u51faHoliGS\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u53ef\u53d8\u5f62\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5e94\u7528\u4e8e\u957f\u65f6\u5e8f\u5355\u76eeRGB\u89c6\u9891\u7684\u6c89\u6d78\u5f0f\u89c6\u89d2\u5408\u6210\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf4D\u9ad8\u65af\u6cfc\u6e85\u548c\u52a8\u6001NeRF\u5728\u5206\u949f\u7ea7\u89c6\u9891\u4e2d\u8bad\u7ec3\u5f00\u9500\u8fc7\u5927\u7684\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u5206\u5c42\u53d8\u5f62\u7b56\u7565\uff0c\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u9759\u6001\u80cc\u666f\u548c\u52a8\u6001\u7269\u4f53\uff0c\u5176\u4e2d\u52a8\u6001\u90e8\u5206\u901a\u8fc7\u53ef\u9006\u795e\u7ecf\u6d41\u5b9e\u73b0\u5168\u5c40\u521a\u6027\u53d8\u6362\u3001\u9aa8\u9abc\u9a71\u52a8\u5f62\u53d8\u548c\u7ec6\u5fae\u975e\u521a\u6027\u5f62\u53d8\u7684\u7edf\u4e00\u5efa\u6a21\u3002  \n\u25c6 \u901a\u8fc7\u5c06\u9ad8\u65af\u57fa\u5143\u7ed1\u5b9a\u5230\u5b8c\u6574\u7684\u524d\u666f\u89c4\u8303\u5f62\u72b6\uff08\u5982\u7b2c\u4e00\u4eba\u79f0\u6216\u8ddf\u968f\u89c6\u89d2\uff09\uff0c\u652f\u6301\u591a\u6f14\u5458\u4ea4\u4e92\u548c\u5927\u89c6\u89d2\u53d8\u5316\u7684\u81ea\u7531\u89c6\u70b9\u6e32\u67d3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u52a8\u6001\u573a\u666f\u7684\u91cd\u5efa\u9c81\u68d2\u6027\u3002  \n\u25c6 \u63d0\u51fa\u53ef\u9006\u9ad8\u65af\u53d8\u5f62\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u73b0\u6709\u5355\u76ee\u53ef\u53d8\u5f62NeRF\u65b9\u6cd5\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u548c\u6e32\u67d3\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u3002  \n\u25c6 \u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u6280\u672f\uff0c\u4e3a\u6c89\u6d78\u5f0f\u89c6\u89d2\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2506.20638": "|2025-06-25|Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects|Cl\u00e9ment Forray\u7b49|[2506.20638](http://arxiv.org/pdf/2506.20638)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u4f30\u8ba1\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u7684\u59ff\u6001\uff08\u76f8\u673a\u4f4d\u59ff\uff09\u5e76\u5229\u7528\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u8fdb\u884c3D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u672a\u77e5\u7269\u4f53\u59ff\u6001\u4e0b\u7684\u91cd\u5efa\u96be\u9898\u3002  \n\u25c6 \u9488\u5bf9\u7a7a\u95f4\u573a\u666f\u7684\u7279\u6b8a\u6311\u6218\uff08\u5982\u5355\u8272\u56fe\u50cf\u3001\u672a\u77e5\u7269\u4f53\u65b9\u5411\u3001\u6709\u9650\u89c6\u89d2\u3001\u65e0\u6f2b\u53cd\u5c04\u5149\u7167\u7b49\uff09\uff0c\u6539\u8fdb\u4e86NeRF\u7684\u9002\u5e94\u6027\uff0c\u4f7f\u5176\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\uff0c\u91c7\u7528\u9010\u5e27\u987a\u5e8f\u8bad\u7ec3\u56fe\u50cf\u7684\u65b9\u5f0f\uff08\u800c\u975e\u6279\u91cf\u8bad\u7ec3\uff09\u80fd\u663e\u8457\u63d0\u53473D\u91cd\u5efa\u7684\u7cbe\u5ea6\uff0c\u4e3a\u52a8\u6001\u7a7a\u95f4\u7269\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002  \n\u25c6 \u901a\u8fc7\u4f18\u5316\u5747\u5300\u65cb\u8f6c\u53c2\u6570\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u7ea6\u675f\u76f8\u90bb\u59ff\u6001\u7684\u8fde\u7eed\u6027\uff0c\u907f\u514d\u4e86\u4f4d\u59ff\u4f30\u8ba1\u7684\u7a81\u53d8\u95ee\u9898\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u4e3a\u7a7a\u95f4\u6001\u52bf\u611f\u77e5\uff08SSA\uff09\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u76843D\u6a21\u578b\uff0c\u53ef\u5e94\u7528\u4e8e\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u3001\u5728\u8f68\u7ef4\u62a4\u7b49\u5b9e\u9645\u573a\u666f\u3002|\n",
    "2506.21348": "|2025-06-26|PanSt3R: Multi-view Consistent Panoptic Segmentation|Lojze Zust\u7b49|[2506.21348](http://arxiv.org/pdf/2506.21348)|\u65e0|\u25c6 \u63d0\u51faPanSt3R\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u5355\u6b21\u524d\u5411\u9884\u6d4b\uff0c\u76f4\u63a5\u8054\u5408\u8f93\u51fa3D\u51e0\u4f55\u548c\u591a\u89c6\u89d2\u5168\u666f\u5206\u5272\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002  \n\u25c6 \u57fa\u4e8eMUSt3R\u6846\u67b6\u6539\u8fdb\uff0c\u5f15\u5165\u8bed\u4e49\u611f\u77e5\u80fd\u529b\uff0c\u5c063D\u91cd\u5efa\u4e0e\u591a\u89c6\u89d2\u5168\u666f\u5206\u5272\u4efb\u52a1\u7edf\u4e00\u6574\u5408\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d562D\u9884\u5206\u5272\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u91cd\u65b0\u8bbe\u8ba1\u63a9\u7801\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u63d0\u51fa\u66f4\u7406\u8bba\u5316\u7684\u591a\u89c6\u89d2\u5206\u5272\u878d\u5408\u7b56\u7565\uff0c\u4f18\u5316\u8de8\u89c6\u89d2\u7a7a\u95f4\u5173\u7cfb\u5229\u7528\u3002  \n\u25c6 \u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\uff0c\u63d0\u51fa\u7b80\u5355\u6709\u6548\u7684\u65b0\u89c6\u89d2\u751f\u6210\u65b9\u6cd5\uff0c\u6269\u5c55\u6a21\u578b\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u517c\u5177\u6982\u5ff5\u7b80\u6d01\u6027\u4e0e\u8ba1\u7b97\u9ad8\u6548\u6027\u3002|\n",
    "2506.21884": "|2025-06-27|UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields|Fabian Perez\u7b49|[2506.21884](http://arxiv.org/pdf/2506.21884)|\u65e0|\u25c6 \u9996\u6b21\u5c06\u5149\u8c31\u89e3\u6df7\u6280\u672f\u878d\u5165\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\uff0c\u5b9e\u73b0\u8054\u5408\u9ad8\u5149\u8c31\u65b0\u89c6\u89d2\u5408\u6210\u4e0e\u65e0\u76d1\u7763\u6750\u8d28\u5206\u5272\uff0c\u7a81\u7834\u4f20\u7edfNeRF\u4ec5\u4f9d\u8d56RGB\u6570\u636e\u7684\u5c40\u9650\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u53cd\u5c04\u5206\u91cf\u7684\u5149\u8c31\u53cd\u5c04\u7387\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u7aef\u5143\u5b57\u5178\u5b66\u4e60\u7eaf\u6750\u8d28\u7279\u5f81\uff0c\u7ed3\u5408\u9010\u70b9\u4e30\u5ea6\u5206\u5e03\u5b9e\u73b0\u6750\u8d28\u7cbe\u51c6\u8868\u8fbe\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u5b66\u4e60\u5230\u7684\u7aef\u5143\u5149\u8c31\u7279\u5f81\u8fdb\u884c\u65e0\u76d1\u7763\u6750\u8d28\u805a\u7c7b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b8c\u6210\u573a\u666f\u6750\u8d28\u5206\u5272\u3002  \n\u25c6 \u652f\u6301\u901a\u8fc7\u4fee\u6539\u7aef\u5143\u5b57\u5178\u5b9e\u73b0\u573a\u666f\u6750\u8d28\u7f16\u8f91\uff0c\u4e3a\u57fa\u4e8e\u6750\u8d28\u7684\u7075\u6d3b\u5916\u89c2\u64cd\u63a7\uff08\u5982\u865a\u62df\u4eff\u771f\u3001AR\u5e94\u7528\uff09\u63d0\u4f9b\u65b0\u5de5\u5177\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u9ad8\u5149\u8c31\u91cd\u5efa\u548c\u6750\u8d28\u5206\u5272\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u3001\u865a\u62df\u73b0\u5b9e\u7b49\u9700\u7cbe\u786e\u6750\u8d28\u5efa\u6a21\u7684\u9886\u57df\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2506.21629": "|2025-06-24|ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes|Chenhao Zhang\u7b49|[2506.21629](http://arxiv.org/pdf/2506.21629)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700SfM\u9884\u5904\u7406\u7684\u65b9\u6cd5ICP-3DGS\uff0c\u901a\u8fc7\u7ed3\u5408\u8fed\u4ee3\u6700\u8fd1\u70b9\uff08ICP\uff09\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u4f4d\u59ff\u7ec6\u5316\uff0c\u5b9e\u73b0\u4e86\u5927\u8303\u56f4\u76f8\u673a\u8fd0\u52a8\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u3002  \n\u25c6 \u5f15\u5165\u57fa\u4e8e\u4f53\u7d20\u7684\u573a\u666f\u81f4\u5bc6\u5316\u7b56\u7565\uff0c\u6709\u6548\u6307\u5bfc\u5927\u89c4\u6a21\u65e0\u8fb9\u754c\u573a\u666f\u76843D\u9ad8\u65af\u5206\u5e03\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6237\u5916\u573a\u666f\u4e2d\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002  \n\u25c6 \u9996\u6b21\u5c06ICP\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\u7ed3\u5408\uff0c\u5728\u795e\u7ecf\u6e32\u67d3\u6846\u67b6\u4e2d\u76f4\u63a5\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\uff0c\u6446\u8131\u4e86\u5bf9SfM\u5148\u9a8c\u6570\u636e\u7684\u4f9d\u8d56\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u5ba4\u5185\u5916\u4e0d\u540c\u5c3a\u5ea6\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u5728\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002  \n\u25c6 \u5f00\u6e90\u4e86\u5b8c\u6574\u4ee3\u7801\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u65e0\u7ea6\u675f\u573a\u666f\u795e\u7ecf\u6e32\u67d3\u7684\u5b9e\u7528\u5316\u8fdb\u7a0b\u3002|\n",
    "2506.23611": "|2025-06-30|AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention|Ziao Liu\u7b49|[2506.23611](http://arxiv.org/pdf/2506.23611)|\u65e0|\u25c6 \u63d0\u51faAttentionGS\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u9ad8\u8d28\u91cf\u521d\u59cb\u70b9\u4e91\u76843D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\uff0c\u7a81\u7834\u4f20\u7edf3DGS\u5bf9SfM\u70b9\u4e91\u7684\u5f3a\u4f9d\u8d56\u3002  \n\u25c6 \u521b\u65b0\u6027\u5f15\u5165\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u673a\u5236\uff1a\u51e0\u4f55\u6ce8\u610f\u529b\u5feb\u901f\u6062\u590d\u573a\u666f\u5168\u5c40\u7ed3\u6784\uff0c\u7eb9\u7406\u6ce8\u610f\u529b\u540e\u671f\u4f18\u5316\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5b9e\u73b0\u4ece\u968f\u673a\u521d\u59cb\u5316\u76f4\u63a5\u91cd\u5efa\u3002  \n\u25c6 \u8bbe\u8ba1\u4e0d\u900f\u660e\u5ea6\u52a0\u6743\u68af\u5ea6\u7b56\u7565\uff0c\u6539\u8fdb\u9ad8\u65af\u5206\u5e03\u81f4\u5bc6\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002  \n\u25c6 \u5728\u7eb9\u7406\u7f3a\u5931\u548c\u53d7\u9650\u89c6\u89d2\u7b49\u6781\u7aef\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u91cd\u5efa\u8d28\u91cf\u63d0\u5347\u663e\u8457\u3002  \n\u25c6 \u901a\u8fc7\u591a\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u9c81\u68d2\u76843D\u91cd\u5efa\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u6269\u5c55\u4e863DGS\u7684\u5e94\u7528\u8fb9\u754c\u3002|\n",
    "2506.23153": "|2025-06-29|Dynamic View Synthesis from Small Camera Motion Videos|Huiqiang Sun\u7b49|[2506.23153](http://arxiv.org/pdf/2506.23153)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u9488\u5bf9\u52a8\u60013D\u573a\u666f\u5728\u5c0f\u8303\u56f4\u76f8\u673a\u8fd0\u52a8\u4e0b\u7684\u65b0\u89c6\u89d2\u5408\u6210\u95ee\u9898\u63d0\u51fa\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u6838\u5fc3\u8d21\u732e\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u5206\u5e03\u7684\u6df1\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5(DDR)\uff0c\u901a\u8fc7Gumbel-softmax\u4ece\u79bb\u6563\u6e32\u67d3\u6743\u91cd\u5206\u5e03\u4e2d\u53ef\u5fae\u5206\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u635f\u5931\u4ec5\u8ba1\u7b97\u671f\u671b\u8bef\u5dee\u7684\u5c40\u9650\u6027\u3002\n\n\u25c6 \u5f15\u5165\u7269\u4f53\u8fb9\u754c\u524d\u7a7a\u95f4\u70b9\u4f53\u79ef\u5bc6\u5ea6\u8d8b\u8fd1\u96f6\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u786e\u4fdd\u573a\u666f\u51e0\u4f55\u7ed3\u6784\u7684\u6b63\u786e\u5b66\u4e60\uff0c\u6709\u6548\u6539\u5584\u4e86\u5c0f\u76f8\u673a\u8fd0\u52a8\u4e0b\u7684\u51e0\u4f55\u8868\u793a\u95ee\u9898\u3002\n\n\u25c6 \u5f00\u53d1\u4e86\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u53ef\u76f4\u63a5\u5728\u6e32\u67d3\u6743\u91cd\u5c42\u9762\u89c2\u5bdf\u573a\u666f\u51e0\u4f55\u8868\u793a\uff0c\u4e3a\u65b9\u6cd5\u539f\u7406\u63d0\u4f9b\u4e86\u76f4\u89c2\u89e3\u91ca\u3002\n\n\u25c6 \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a0\u5165\u76f8\u673a\u53c2\u6570\u5b66\u4e60\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u76f8\u673a\u53c2\u6570\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u5c0f\u8fd0\u52a8\u4e0b\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u4e0d\u51c6\u7684\u95ee\u9898\u3002\n\n\u8bba\u6587\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u8303\u56f4\u76f8\u673a\u8fd0\u52a8\u8f93\u5165\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.01631": "|2025-07-02|Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation|Camille Billouard\u7b49|[2507.01631](http://arxiv.org/pdf/2507.01631)|\u65e0|\u25c6 \u63d0\u51faSnake-NeRF\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u8bbe\u5907\u4e0a\u5927\u89c4\u6a21\u536b\u661f\u5f71\u50cf\u7684NeRF\u4e09\u7ef4\u91cd\u5efa\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u5185\u5b58\u7684\u5c0f\u573a\u666f\u7ea6\u675f\u3002  \n\u25c6 \u8bbe\u8ba1\u5916\u5b58\uff08out-of-core\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u540c\u65f6\u52a0\u8f7d\u6240\u6709\u56fe\u50cf\u548c\u7f51\u7edc\uff0c\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u9700\u6c42\u3002  \n\u25c6 \u521b\u65b0\u6027\u91c7\u7528\u65e0\u91cd\u53e0\u4e09\u7ef4\u5206\u5757\uff083D tile\uff09\u7b56\u7565\uff0c\u5c06\u76ee\u6807\u533a\u57df\u5212\u5206\u4e3a\u72ec\u7acb\u8bad\u7ec3\u7684NeRF\u5b50\u6a21\u5757\u3002  \n\u25c6 \u63d0\u51fa\u91cd\u53e0\u88c1\u526a\u56fe\u50cf\u6280\u672f\uff0c\u786e\u4fdd\u6bcf\u4e2a\u5b50\u6a21\u5757\u8bad\u7ec3\u65f6\u83b7\u53d6\u5b8c\u6574\u5fc5\u8981\u50cf\u7d20\uff0c\u907f\u514d\u8fb9\u754c\u4fe1\u606f\u7f3a\u5931\u3002  \n\u25c6 \u5f00\u53d12\u00d72\u4e09\u7ef4\u5206\u5757\u9012\u8fdb\u7b56\u7565\u4e0e\u5206\u6bb5\u91c7\u6837\u5668\uff0c\u6709\u6548\u6d88\u9664\u5206\u5757\u8fb9\u7f18\u7684\u4e09\u7ef4\u91cd\u5efa\u8bef\u5dee\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5355GPU\u4e0a\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u4e14\u4e0d\u635f\u5931\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u5168\u7403\u5c3a\u5ea6\u5730\u7403\u89c2\u6d4b\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2507.00969": "|2025-07-01|Surgical Neural Radiance Fields from One Image|Alberto Neri\u7b49|[2507.00969](http://arxiv.org/pdf/2507.00969)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5f20\u672f\u4e2d\u56fe\u50cf\u548c\u672f\u524dMRI\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u624b\u672f\u573a\u666f\u4e2d\u591a\u89c6\u89d2\u6570\u636e\u4e0d\u8db3\u7684\u9650\u5236\u3002  \n\u25c6 \u5229\u7528\u672f\u524dMRI\u6570\u636e\u9884\u5148\u5b9a\u4e49\u76f8\u673a\u89c6\u89d2\u548c\u56fe\u50cf\u96c6\uff0c\u7ed3\u5408\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u6280\u672f\uff08WTC2\u548cSTROTSS\uff09\u5c06\u672f\u4e2d\u56fe\u50cf\u5916\u89c2\u8fc1\u79fb\u81f3\u9884\u6784\u5efa\u6570\u636e\u96c6\uff0c\u907f\u514d\u8fc7\u5ea6\u98ce\u683c\u5316\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u5feb\u901f\u5355\u56fe\u50cfNeRF\u8bad\u7ec3\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u672f\u4e2d\u6570\u636e\u91c7\u96c6\u7684\u65f6\u95f4\u6210\u672c\uff0c\u63d0\u5347\u4e86\u4e34\u5e8a\u5b9e\u7528\u6027\u3002  \n\u25c6 \u5728\u56db\u4f8b\u795e\u7ecf\u5916\u79d1\u624b\u672f\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9a\u91cf\u5bf9\u6bd4\u663e\u793a\u5176\u5408\u6210\u7ed3\u679c\u4e0e\u771f\u5b9e\u624b\u672f\u663e\u5fae\u955c\u56fe\u50cf\u9ad8\u5ea6\u4e00\u81f4\u3002  \n\u25c6 \u91cd\u5efa\u7ed3\u679c\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u6bd4\u5177\u6709\u9ad8\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u8bc1\u660e\u4e86\u826f\u597d\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u7eb9\u7406\u4fdd\u7559\u80fd\u529b\u3002  \n\u25c6 \u4e3a\u624b\u672f\u573a\u666f\u4e2d\u7684\u5b9e\u65f63D\u91cd\u5efa\u548c\u89c6\u89d2\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u591a\u89c6\u89d2\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002|\n",
    "2507.00371": "|2025-07-01|PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching|Xin Yang\u7b49|[2507.00371](http://arxiv.org/pdf/2507.00371)|\u65e0|\u25c6\u63d0\u51faPlantSegNeRF\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4ece\u591a\u89c6\u89d2RGB\u56fe\u50cf\u5e8f\u5217\u76f4\u63a5\u751f\u6210\u9ad8\u7cbe\u5ea6\u690d\u7269\u5668\u5b98\u5b9e\u4f8b\u70b9\u4e91\uff0c\u7a81\u7834\u4f20\u7edf\u70b9\u4e91\u5206\u5272\u6280\u672f\u7684\u5c40\u9650\u6027\u3002  \n\u25c6\u5f00\u53d1\u8054\u5408\u901a\u9053NeRF\u6a21\u578b\uff0c\u540c\u65f6\u6e32\u67d3\u989c\u8272\u3001\u5bc6\u5ea6\u3001\u8bed\u4e49\u548c\u5b9e\u4f8b\u4fe1\u606f\uff0c\u6784\u5efa\u5305\u542b\u591a\u7ef4\u5ea6\u7279\u5f81\u7684\u9690\u5f0f\u573a\u666f\u8868\u793a\u3002  \n\u25c6\u8bbe\u8ba1\u521b\u65b0\u7684\u591a\u89c6\u89d2\u5b9e\u4f8b\u5339\u914d\u6a21\u5757\uff0c\u901a\u8fc72D\u5b9e\u4f8b\u5206\u5272\u7ed3\u679c\u8de8\u89c6\u56fe\u5173\u8054\u540c\u4e00\u5668\u5b98\u7684\u5b9e\u4f8bID\uff0c\u89e3\u51b3\u590d\u6742\u690d\u7269\u7ed3\u6784\u7684\u5bf9\u5e94\u96be\u9898\u3002  \n\u25c6\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u5173\u952e\u6307\u6807\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u7b49\uff09\u5e73\u5747\u63d0\u534716.1%-24.2%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002  \n\u25c6\u5728\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u56db\u9879\u6838\u5fc3\u6307\u6807\uff08mPrec\u7b49\uff09\u6700\u9ad8\u63d0\u5347\u8fbe38.2%\uff0c\u5b9e\u73b0\u8de8\u7269\u79cd\u7684\u9ad8\u6cdb\u5316\u6027\u8868\u73b0\u3002  \n\u25c6\u4e3a\u690d\u7269\u8868\u578b\u7814\u7a76\u63d0\u4f9b\u9ad8\u901a\u91cf\u4e09\u7ef4\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u652f\u6301\u5927\u89c4\u6a21\u690d\u7269\u6a21\u578b\u5f00\u53d1\u3002|\n",
    "2507.04408": "|2025-07-06|A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields|Aoxiang Fan\u7b49|[2507.04408](http://arxiv.org/pdf/2507.04408)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u89c6\u89d2\u4e00\u81f4\u5206\u5e03\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u56fa\u5b9a\u6df1\u5ea6\u503c\u4f30\u8ba1\uff0c\u7528\u4e8eNeRF\u7684\u6b63\u5219\u5316\u8bad\u7ec3\u3002  \n\u25c6 \u5229\u7528\u4f4e\u5c42\u989c\u8272\u7279\u5f81\u548c\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684\u9ad8\u5c42\u7279\u5f81\uff0c\u6784\u5efa3D\u91c7\u6837\u70b9\u57282D\u6295\u5f71\u4f4d\u7f6e\u7684\u89c6\u89d2\u4e00\u81f4\u6027\u5206\u5e03\u3002  \n\u25c6 \u901a\u8fc7\u4ece\u89c6\u89d2\u4e00\u81f4\u6027\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u5b9e\u73b0\u5bf9NeRF\u8bad\u7ec3\u7684\u9690\u5f0f\u6b63\u5219\u5316\uff0c\u907f\u514d\u4f9d\u8d56\u8bef\u5dee\u8f83\u5927\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002  \n\u25c6 \u7ed3\u5408\u6df1\u5ea6\u63a8\u8fdb\u635f\u5931\uff08depth-pushing loss\uff09\u4e0e\u91c7\u6837\u6280\u672f\uff0c\u5171\u540c\u6d88\u9664\u8bad\u7ec3\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u3002  \n\u25c6 \u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709NeRF\u53d8\u4f53\u548c\u6df1\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6237\u5916\u65e0\u754c\u573a\u666f\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u9700\u8981\u6602\u8d353D\u76d1\u7763\u548c\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u771f\u5b9e\u573a\u666f\u4e0b\u76843D\u91cd\u5efa\u8d28\u91cf\u3002|\n",
    "2507.06103": "|2025-07-08|Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering|Jiayi Song\u7b49|[2507.06103](http://arxiv.org/pdf/2507.06103)|\u65e0|\u25c6 \u63d0\u51faRef-Unlock\u6846\u67b6\uff0c\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5b9e\u73b0\u51e0\u4f55\u611f\u77e5\u7684\u53cd\u5c04\u5206\u79bb\uff0c\u9996\u6b21\u57283DGS\u4e2d\u663e\u5f0f\u89e3\u8026\u900f\u5c04\u4e0e\u53cd\u5c04\u6210\u5206\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c06\u53cd\u5c04\u8bef\u5224\u4e3a\u51e0\u4f55\u7ed3\u6784\u7684\u95ee\u9898\u3002  \n\u25c6 \u91c7\u7528\u53cc\u5206\u652f\u8868\u793a\u7ed3\u5408\u9ad8\u9636\u7403\u8c10\u51fd\u6570\uff0c\u6709\u6548\u6355\u6349\u9ad8\u9891\u53cd\u5c04\u7ec6\u8282\uff0c\u540c\u65f6\u901a\u8fc7\u53cd\u5c04\u79fb\u9664\u6a21\u5757\u63d0\u4f9b\u4f2a\u65e0\u53cd\u5c04\u76d1\u7763\u4fe1\u53f7\uff0c\u5b9e\u73b0\u66f4\u5e72\u51c0\u7684\u53cd\u5c04\u5206\u89e3\u3002  \n\u25c6 \u5f15\u5165\u4f2a\u6df1\u5ea6\u56fe\u4e0e\u51e0\u4f55\u611f\u77e5\u7684\u53cc\u8fb9\u5e73\u6ed1\u7ea6\u675f\uff0c\u589e\u5f3a3D\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u5206\u89e3\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u51cf\u5c11\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u9762\u4f2a\u5f71\u4e0e\u6a21\u7cca\u91cd\u5efa\u3002  \n\u25c6 \u652f\u6301\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7684\u7075\u6d3b\u53cd\u5c04\u7f16\u8f91\u529f\u80fd\uff0c\u6269\u5c55\u4e86\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u64cd\u4f5c\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5927\u5e45\u8d85\u8d8a\u4f20\u7edf\u57fa\u4e8eGS\u7684\u53cd\u5c04\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u4e0eNeRF\u7c7b\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u4e3a\u542b\u53cd\u5c04\u573a\u666f\u7684\u5149\u7167\u771f\u5b9e\u6e32\u67d3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002|\n",
    "2507.05763": "|2025-07-08|DreamArt: Generating Interactable Articulated Objects from a Single Image|Ruijie Lu\u7b49|[2507.05763](http://arxiv.org/pdf/2507.05763)|\u65e0|\u25c6 DreamArt\u9996\u6b21\u63d0\u51fa\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u53ef\u4ea4\u4e92\u7684\u5173\u8282\u53163D\u7269\u4f53\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u90e8\u4ef6\u5206\u89e3\u548c\u5173\u8282\u5efa\u6a21\u65b9\u9762\u7684\u7a7a\u767d\u3002  \n\u25c6 \u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u521b\u65b0\uff1a\u7ed3\u5408\u56fe\u50cf\u751f\u62103D\u3001\u63a9\u7801\u63d0\u793a\u7684\u90e8\u4ef6\u5206\u5272\u4e0e\u4fee\u590d\uff0c\u89e3\u51b3\u4e86\u5355\u89c6\u89d2\u4e0b\u90e8\u4ef6\u5f62\u72b6\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5173\u8282\u8fd0\u52a8\u5148\u9a8c\u5b66\u4e60\uff0c\u5229\u7528\u90e8\u4ef6\u906e\u7f69\u548c\u4fee\u590d\u56fe\u50cf\u6d88\u9664\u906e\u6321\u6b67\u4e49\uff0c\u5b9e\u73b0\u903c\u771f\u5173\u8282\u8fd0\u52a8\u751f\u6210\u3002  \n\u25c6 \u91c7\u7528\u53cc\u56db\u5143\u6570\u8868\u793a\u5173\u8282\u8fd0\u52a8\u53c2\u6570\uff0c\u914d\u5408\u5168\u5c40\u7eb9\u7406\u4f18\u5316\uff0c\u786e\u4fdd\u591a\u90e8\u4ef6\u7eb9\u7406\u4e00\u81f4\u6027\u4e0e\u9ad8\u8d28\u91cf\u6e32\u67d3\u6548\u679c\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u90e8\u4ef6\u5f62\u72b6\u51c6\u786e\u3001\u5916\u89c2\u903c\u771f\u4e14\u5173\u8282\u8fd0\u52a8\u5408\u7406\u76843D\u8d44\u4ea7\uff0c\u4e3aAR/VR\u548c\u5177\u8eabAI\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2507.06269": "|2025-07-14|BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields|Rushil Desai|[2507.06269](http://arxiv.org/pdf/2507.06269)|\u65e0|\u25c6 \u63d0\u51faBayesSDF\uff0c\u9996\u4e2a\u9488\u5bf9\u795e\u7ecf\u9690\u5f0fSDF\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6\uff0c\u89e3\u51b33D\u51e0\u4f55\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u79d1\u5b66\u6a21\u62df\uff08\u5982\u68ee\u6797\u6d41\u4f53\u5efa\u6a21\uff09\u3002  \n\u25c6 \u901a\u8fc7\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u548c\u57fa\u4e8eHessian\u7684\u5c40\u90e8\u8868\u9762\u7a33\u5b9a\u6027\u5ea6\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u4e14\u51e0\u4f55\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002  \n\u25c6 \u9996\u6b21\u5c06\u51e0\u4f55\u4e00\u81f4\u6027\u76f4\u63a5\u878d\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u751f\u6210\u4e0e\u91cd\u5efa\u8bef\u5dee\u9ad8\u5ea6\u76f8\u5173\u7684\u6821\u51c6\u5316\u7f6e\u4fe1\u5ea6\u5730\u56fe\uff0c\u4f18\u4e8e\u5ffd\u7565\u51e0\u4f55\u7684\u73b0\u6709\u65b9\u6cd5\u3002  \n\u25c6 \u8bc1\u660eSDF\u7684\u8fde\u7eed\u53ef\u5fae\u51e0\u4f55\u7279\u6027\u6bd4\u8f90\u5c04\u573a\u6a21\u578b\uff08\u5982NeRF\uff09\u66f4\u9002\u5408\u7269\u7406\u6a21\u62df\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u673a\u5668\u4eba\u51b3\u7b56\uff09\u63d0\u4f9b\u53ef\u9760\u51e0\u4f55\u57fa\u7840\u3002  \n\u25c6 \u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5176\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u4e0e\u91cd\u5efa\u7f3a\u9677\u9ad8\u5ea6\u543b\u5408\uff0c\u6821\u51c6\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u5747\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002|\n",
    "2507.07519": "|2025-07-10|MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation|Bangning Wei\u7b49|[2507.07519](http://arxiv.org/pdf/2507.07519)|\u65e0|\u25c6 \u63d0\u51fa\u4e86MUVOD\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u52a8\u6001\u573a\u666f4D\u76ee\u6807\u5206\u5272\u7684\u5927\u89c4\u6a21\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002  \n\u25c6 \u6570\u636e\u96c6\u5305\u542b17\u4e2a\u771f\u5b9e\u573a\u666f\uff0c\u6db5\u76d6\u5ba4\u5185\u5916\u591a\u79cd\u6d3b\u52a8\uff0c\u63d0\u4f9b7830\u5f20RGB\u56fe\u50cf\u53ca\u5bf9\u5e94\u76844D\u8fd0\u52a8\u5206\u5272\u63a9\u7801\uff0c\u652f\u6301\u8de8\u89c6\u89d2\u548c\u8de8\u5e27\u7684\u76ee\u6807\u8ddf\u8e2a\u3002  \n\u25c6 \u6570\u636e\u96c6\u4e2d\u5305\u542b459\u4e2a\u5b9e\u4f8b\uff0c\u8986\u76d673\u4e2a\u7c7b\u522b\uff0c\u4e3a\u591a\u89c6\u89d2\u89c6\u9891\u5206\u5272\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\u5206\u5272\u65b9\u6cd5\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5206\u5272\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002  \n\u25c6 \u57fa\u4e8eMUVOD\u6570\u636e\u96c6\u6784\u5efa\u4e863D\u76ee\u6807\u5206\u5272\u5b50\u96c6\uff0c\u5305\u542b50\u4e2a\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6807\u6ce8\u5bf9\u8c61\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u73b0\u67093D\u5206\u5272\u65b9\u6cd5\u7684\u6027\u80fd\u3002  \n\u25c6 \u6570\u636e\u96c6\u6765\u6e90\u591a\u6837\uff0c\u5305\u542b\u4e0d\u540c\u76f8\u673a\u8bbe\u5907\u91c7\u96c6\u7684\u89c6\u89d2\uff089-46\u4e2a\u89c6\u89d2\uff09\uff0c\u589e\u5f3a\u4e86\u6570\u636e\u96c6\u7684\u6cdb\u5316\u6027\u548c\u5b9e\u7528\u6027\u3002|\n",
    "2507.09987": "|2025-07-14|VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling|Zihang Zeng\u7b49|[2507.09987](http://arxiv.org/pdf/2507.09987)|\u65e0|\u25c6 \u63d0\u51faVoxelRF\u65b0\u578b\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f53\u7d20\u5316\u8f90\u5c04\u573a\u5b9e\u73b0\u590d\u6742\u73af\u5883\u4e2d\u65e0\u7ebf\u4fe1\u9053\u7684\u5feb\u901f\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u5e73\u8861\u96be\u9898\u3002  \n\u25c6 \u7528\u57fa\u4e8e\u4f53\u7d20\u7f51\u683c\u7684\u4e09\u7ebf\u6027\u63d2\u503c\u66ff\u4ee3NeRF\u4e2d\u6602\u8d35\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u7ed3\u5408\u4e24\u4e2a\u6d45\u5c42MLP\u5206\u522b\u5efa\u6a21\u4f20\u64ad\u548c\u53d1\u5c04\u7aef\u76f8\u5173\u6548\u5e94\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002  \n\u25c6 \u5f15\u5165\u6e10\u8fdb\u5f0f\u5b66\u4e60\u7b56\u7565\uff0c\u9010\u6b65\u4f18\u5316\u4f53\u7d20\u7f51\u683c\u5206\u8fa8\u7387\uff0c\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u91c7\u7528\u7a7a\u533a\u57df\u8df3\u8fc7\u6280\u672f\uff0c\u907f\u514d\u5bf9\u65e0\u4fe1\u53f7\u533a\u57df\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002  \n\u25c6 \u8bbe\u8ba1\u80cc\u666f\u71b5\u635f\u5931\u51fd\u6570\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u7a00\u758f\u4fe1\u53f7\u533a\u57df\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u63d0\u5347\u6574\u4f53\u7cbe\u5ea6\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0cVoxelRF\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u80fd\u4ee5\u66f4\u4f4e\u8ba1\u7b97\u91cf\u8fbe\u5230\u7ade\u4e89\u6027\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u7ebf\u901a\u4fe1\u573a\u666f\u3002|\n",
    "2507.09168": "|2025-07-12|Stable Score Distillation|Haiming Zhu\u7b49|[2507.09168](http://arxiv.org/pdf/2507.09168)|\u65e0|\u25c6 \u63d0\u51fa\u7a33\u5b9a\u5206\u6570\u84b8\u998f\uff08SSD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5206\u7c7b\u5668\u951a\u5b9a\u5230\u6e90\u63d0\u793a\u8bcd\uff0c\u663e\u8457\u63d0\u5347\u7f16\u8f91\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u5bf9\u9f50\u6027\u3002  \n\u25c6 \u5229\u7528\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u65b9\u7a0b\u5b9e\u73b0\u8de8\u63d0\u793a\u8bcd\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u6052\u5b9a\u7a7a\u6587\u672c\u5206\u652f\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u907f\u514d\u51b2\u7a81\u4fe1\u53f7\u3002  \n\u25c6 \u8bbe\u8ba1\u63d0\u793a\u8bcd\u589e\u5f3a\u5206\u652f\uff0c\u4e13\u95e8\u5f3a\u5316\u98ce\u683c\u8f6c\u6362\u7b49\u7f16\u8f91\u4efb\u52a1\u7684\u4fee\u6539\u5f3a\u5ea6\uff0c\u63d0\u5347\u7f16\u8f91\u6548\u679c\u3002  \n\u25c6 \u5728\u4fdd\u6301\u539f\u59cb\u5185\u5bb9\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u7f16\u8f91\u8f68\u8ff9\u4e0e\u6e90\u63d0\u793a\u8bcd\u7d27\u5bc6\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5c40\u90e8\u7cbe\u51c6\u7f16\u8f91\u4e14\u4e0d\u5f71\u54cd\u5468\u56f4\u533a\u57df\u3002  \n\u25c6 \u57282D\u548c3D\u7f16\u8f91\u4efb\u52a1\uff08\u5982NeRF\u548c\u6587\u672c\u9a71\u52a8\u98ce\u683c\u7f16\u8f91\uff09\u4e2d\u8fbe\u5230\u6700\u4f18\u6548\u679c\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002|\n",
    "2507.09005": "|2025-07-11|From images to properties: a NeRF-driven framework for granular material parameter inversion|Cheng-Hsi Hsiao\u7b49|[2507.09005](http://arxiv.org/pdf/2507.09005)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684NeRF\u4e0eMPM\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u89c2\u6d4b\u53cd\u6f14\u9897\u7c92\u6750\u6599\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u4ece\u56fe\u50cf\u5230\u7269\u6027\u53c2\u6570\u7684\u8de8\u6a21\u6001\u63a8\u7406\u3002  \n\u25c6 \u5229\u7528NeRF\u4ece\u591a\u89c6\u89d2\u521d\u59cb\u56fe\u50cf\u91cd\u5efa\u9ad8\u7cbe\u5ea63D\u51e0\u4f55\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u8868\u9762\u7ec6\u8282\u6355\u6349\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3aMPM\u4eff\u771f\u63d0\u4f9b\u51c6\u786e\u521d\u59cb\u6761\u4ef6\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u91c7\u7528\u65f6\u5e8f\u53cc\u56fa\u5b9a\u76f8\u673a\u56fe\u50cf\u4f5c\u4e3a\u89c2\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u4eff\u771f\u6e32\u67d3\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684\u6bd4\u5bf9\u6784\u5efa\u76ee\u6807\u51fd\u6570\uff0c\u5b9e\u73b0\u7eaf\u89c6\u89c9\u9a71\u52a8\u7684\u53c2\u6570\u53cd\u6f14\u3002  \n\u25c6 \u5f15\u5165\u8d1d\u53f6\u65af\u4f18\u5316\u9ad8\u6548\u641c\u7d22\u6469\u64e6\u89d2\u53c2\u6570\uff0c\u5c06\u53cd\u6f14\u8bef\u5dee\u63a7\u5236\u57282\u5ea6\u4ee5\u5185\uff0c\u9a8c\u8bc1\u4e86\u7eaf\u89c6\u89c9\u53cd\u5206\u6790\u7684\u53ef\u884c\u6027\u3002  \n\u25c6 \u8be5\u6846\u67b6\u4e3a\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u7269\u6027\u7684\u5b9e\u9645\u573a\u666f\uff08\u5982\u9065\u611f\u3001\u707e\u5bb3\u8bc4\u4f30\uff09\u63d0\u4f9b\u4e86\u975e\u63a5\u89e6\u5f0f\u6750\u6599\u8868\u5f81\u65b0\u601d\u8def\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002|\n",
    "2507.12132": "|2025-07-16|DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi|Navid Hasanzadeh\u7b49|[2507.12132](http://arxiv.org/pdf/2507.12132)|\u65e0|\u25c6 \u63d0\u51faDoRF\uff08\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff09\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u601d\u60f3\u5f15\u5165Wi-Fi\u4f20\u611f\u9886\u57df\uff0c\u901a\u8fc7\u4e00\u7ef4\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u91cd\u5efa3D\u6f5c\u5728\u8fd0\u52a8\u8868\u5f81\u3002  \n\u25c6 \u6784\u5efa\u4e86\u7edf\u4e00\u7684\u8fd0\u52a8\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff0c\u63d0\u4f9b\u6d3b\u52a8\u5168\u666f\u89c6\u89d2\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u73af\u5883\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5229\u7528Wi-Fi CSI\u63d0\u53d6\u7684\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u73af\u5883\u7279\u5b9a\u7279\u5f81\u7684\u9650\u5236\u3002  \n\u25c6 \u6240\u63d03D\u6f5c\u5728\u8868\u5f81\u80fd\u6709\u6548\u6355\u6349\u4eba\u4f53\u6d3b\u52a8\u65f6\u7a7a\u7279\u6027\uff0c\u6bd4\u73b0\u67092D\u65b9\u6cd5\u66f4\u5177\u5224\u522b\u529b\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u73af\u5883\u3001\u8de8\u7528\u6237\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u63a8\u52a8Wi-Fi\u4f20\u611f\u8d70\u5411\u5b9e\u7528\u5316\u3002  \n\u25c6 \u4e3a\u65e0\u7ebf\u611f\u77e5\u5f00\u8f9f\u65b0\u601d\u8def\uff0c\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4f53\u79ef\u6e32\u67d3\u6280\u672f\u6210\u529f\u8fc1\u79fb\u81f3\u5c04\u9891\u4fe1\u53f7\u5904\u7406\u9886\u57df\u3002|\n",
    "2507.11971": "|2025-07-16|HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing|Tielong Wang\u7b49|[2507.11971](http://arxiv.org/pdf/2507.11971)|\u65e0|\u25c6\u63d0\u51fa\u65b0\u578b3D\u5206\u5c42\u4ee3\u7406\u8282\u70b9\u8868\u793a\uff08HPR3D\uff09\uff0c\u901a\u8fc7\u7269\u4f53\u8868\u9762\u53ca\u5185\u90e8\u7684\u7a00\u758f\u5c42\u7ea7\u6811\u72b6\u4ee3\u7406\u8282\u70b9\u7f51\u7edc\u7edf\u4e00\u8868\u5f81\u5f62\u72b6\u4e0e\u7eb9\u7406\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u4efb\u52a1\u5c40\u9650\u6027\u7684\u6846\u67b6\u521b\u65b0\u3002  \n\u25c6\u6bcf\u4e2a\u4ee3\u7406\u8282\u70b9\u91c7\u7528\u8f7b\u91cfMLP\u9690\u5f0f\u7f16\u7801\u5c40\u90e8\u51e0\u4f55\u4e0e\u7eb9\u7406\u4fe1\u606f\uff0c\u7ed3\u5408\u90bb\u8fd1\u53ca\u7236\u8282\u70b9\u7684\u9ad8\u6548\u795e\u7ecf\u63d2\u503c\u89e3\u7801\u673a\u5236\uff0c\u5b9e\u73b0\u590d\u6742\u6027\u4e0e\u4fdd\u771f\u5ea6\u7684\u52a8\u6001\u5e73\u8861\u3002  \n\u25c6\u5c42\u7ea7\u7ed3\u6784\u5929\u7136\u652f\u6301\u8bed\u4e49\u5bf9\u9f50\uff0c\u7528\u6237\u53ef\u76f4\u63a5\u901a\u8fc7\u62d6\u62fd\u4ee3\u7406\u8282\u70b9\u5b9e\u73b0\u76f4\u89c2\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86NeRF\u7ed3\u6784\u6a21\u7cca\u5bfc\u81f4\u7684\u64cd\u63a7\u96be\u9898\u3002  \n\u25c6\u7a00\u758f\u8282\u70b9\u5206\u5e03\u4e0e\u5206\u5c42\u67e5\u8be2\u673a\u5236\u663e\u8457\u964d\u4f4e\u6570\u636e\u590d\u6742\u5ea6\uff08\u76f8\u6bd4\u7f51\u683c\u9876\u70b9\u5bc6\u5ea6\u548cNeRF\u4f53\u7d20\u91c7\u6837\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e9a\u6beb\u7c73\u7ea7\u91cd\u5efa\u7cbe\u5ea6\u3002  \n\u25c6\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u8868\u793a\u5728\u91cd\u5efa\u8d28\u91cf\uff08PSNR\u63d0\u53472.1dB\uff09\u3001\u7f16\u8f91\u6548\u7387\uff08\u4ea4\u4e92\u5ef6\u8fdf<10ms\uff09\u548c\u8de8\u4efb\u52a1\u901a\u7528\u6027\uff08\u91cd\u5efa/\u751f\u6210/\u9a71\u52a8\uff09\u4e0a\u7684\u7efc\u5408\u4f18\u52bf\u3002|\n",
    "2507.13929": "|2025-07-18|TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views|Hsiang-Hui Hung\u7b49|[2507.13929](http://arxiv.org/pdf/2507.13929)|\u65e0|\u25c6 TimeNeRF\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5c11\u91cf\u8f93\u5165\u89c6\u56fe\u4e0b\u6e32\u67d3\u4efb\u610f\u89c6\u89d2\u548c\u4efb\u610f\u65f6\u95f4\u70b9\u7684\u65b0\u89c6\u56fe\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u91c7\u96c6\u6210\u672c\u9ad8\u548c\u573a\u666f\u91cd\u590d\u4f18\u5316\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u9996\u6b21\u63a2\u7d22\u4e86NeRF\u5728\u65f6\u5e8f3D\u573a\u666f\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u4e86\u5f53\u524d\u6280\u672f\u5728\u8be5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5143\u5b87\u5b99\u4e2d\u663c\u591c\u81ea\u7136\u8fc7\u6e21\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c\u9700\u6c42\u3002  \n\u25c6 \u7ed3\u5408\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u8de8\u6570\u636e\u96c6\u89e3\u8026\u7b56\u7565\uff0c\u6784\u5efa\u4e86\u9690\u5f0f\u5185\u5bb9\u8f90\u5c04\u573a\uff0c\u5b9e\u73b0\u4e86\u573a\u666f\u8868\u793a\u548c\u65f6\u95f4\u7ef4\u5ea6\u5efa\u6a21\u7684\u7edf\u4e00\u6846\u67b6\u3002  \n\u25c6 \u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u5373\u53ef\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u751f\u6210\u65b0\u89c6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u6355\u6349\u4ece\u9ece\u660e\u5230\u9ec4\u660f\u7684\u590d\u6742\u81ea\u7136\u573a\u666f\u53d8\u5316\u3002  \n\u25c6 \u901a\u8fc7\u4f53\u6e32\u67d3\u6280\u672f\u5408\u6210\u4efb\u610f\u65f6\u95f4\u70b9\u7684\u903c\u771f\u65b0\u89c6\u56fe\uff0c\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u5e73\u6ed1\u8fc7\u6e21\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002|\n",
    "2507.13648": "|2025-07-18|EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation|Seungjun Moon\u7b49|[2507.13648](http://arxiv.org/pdf/2507.13648)|\u65e0|\u25c6 \u63d0\u51faEPSilon\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u70b9\u91c7\u6837\u7b56\u7565\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u6df7\u5408\u8868\u793a\uff08SMPL\u7f51\u683c+NeRF\uff09\u76843D\u865a\u62df\u4eba\u751f\u6210\u6548\u7387\uff0c\u517c\u987e\u751f\u6210\u8d28\u91cf\u4e0e\u901f\u5ea6\u3002  \n\u25c6 \u521b\u65b0\u6027\u8bbe\u8ba1\u7a7a\u5c04\u7ebf\u5254\u9664\uff08ERO\uff09\u65b9\u6cd5\uff0c\u76f4\u63a5\u8df3\u8fc7\u7a7a\u573a\u666f\u4e2d\u7684\u5149\u7ebf\u8ba1\u7b97\uff0c\u51cf\u5c11\u65e0\u6548\u91c7\u6837\u70b9\u3002  \n\u25c6 \u63d0\u51fa\u7a7a\u533a\u95f4\u5254\u9664\uff08EIO\uff09\u6280\u672f\uff0c\u8fdb\u4e00\u6b65\u538b\u7f29\u5149\u7ebf\u91c7\u6837\u533a\u95f4\uff0c\u4ec5\u4fdd\u7559\u8863\u7269\u6216\u7f51\u683c\u8986\u76d6\u7684\u6709\u6548\u533a\u57df\u3002  \n\u25c6 \u901a\u8fc7\u7cbe\u7ec6\u5316\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u5355\u9636\u6bb5NeRF\u7ed3\u6784\uff0c\u65e0\u9700\u4f20\u7edf\u5206\u5c42\u91c7\u6837\uff0c\u7b80\u5316\u6a21\u578b\u67b6\u6784\u3002  \n\u25c6 \u5b9e\u9a8c\u8868\u660e\uff0cEPSilon\u4ec5\u97003.9%\u7684\u91c7\u6837\u70b9\u5373\u53ef\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea620\u500d\uff0c\u8bad\u7ec3\u6536\u655b\u52a0\u5feb4\u500d\u3002|\n",
    "2507.14596": "|2025-07-19|DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF|Doriand Petit\u7b49|[2507.14596](http://arxiv.org/pdf/2507.14596)|\u65e0|\u25c6 DiSCO-3D\u9996\u6b21\u63d0\u51fa3D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u4efb\u52a1\uff0c\u7ed3\u5408\u573a\u666f\u5185\u5bb9\u548c\u7528\u6237\u67e5\u8be2\u9700\u6c42\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u76843D\u8bed\u4e49\u5206\u5272\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u57fa\u4e8e\u795e\u7ecf\u573a\u8868\u793a\uff0c\u5c06\u65e0\u76d1\u7763\u5206\u5272\u4e0e\u5f31\u5f00\u653e\u8bcd\u6c47\u6307\u5bfc\u76f8\u7ed3\u5408\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u4ec5\u9002\u5e94\u5355\u4e00\u4efb\u52a1\u6216\u573a\u666f\u7684\u9650\u5236\u3002  \n\u25c6 \u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\uff0cDiSCO-3D\u80fd\u591f\u52a8\u6001\u53d1\u73b0\u5e76\u5206\u5272\u5b50\u6982\u5ff5\uff0c\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u9700\u6c42\u3002  \n\u25c6 \u5728\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u76d1\u7763\u5206\u5272\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\uff0cDiSCO-3D\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u5c42\u6b21\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2507.14501": "|2025-07-19|Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey|Jiahui Zhang\u7b49|[2507.14501](http://arxiv.org/pdf/2507.14501)|\u65e0|\u25c6 \u7cfb\u7edf\u68b3\u7406\u4e86\u57fa\u4e8e\u524d\u9988\u5f0f\u6df1\u5ea6\u5b66\u4e60\u76843D\u91cd\u5efa\u4e0e\u89c6\u56fe\u5408\u6210\u6280\u672f\uff0c\u63d0\u51fa\u6309\u8868\u793a\u67b6\u6784\uff08\u5982\u70b9\u4e91\u30013D\u9ad8\u65af\u6cfc\u6e85\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u7b49\uff09\u7684\u5206\u7c7b\u4f53\u7cfb\u3002  \n\u25c6 \u91cd\u70b9\u5206\u6790\u4e86\u65e0\u59ff\u6001\u91cd\u5efa\u3001\u52a8\u60013D\u91cd\u5efa\u30013D\u611f\u77e5\u56fe\u50cf/\u89c6\u9891\u5408\u6210\u7b49\u5173\u952e\u4efb\u52a1\uff0c\u62d3\u5c55\u4e86\u5728\u6570\u5b57\u4eba\u3001SLAM\u7b49\u9886\u57df\u7684\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u5bf9\u6bd4\u4f20\u7edf\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u7a81\u663e\u524d\u9988\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u7a81\u7834\uff0c\u63a8\u52a8AR/VR\u7b49\u5b9e\u65f6\u5e94\u7528\u843d\u5730\u3002  \n\u25c6 \u9996\u6b21\u6574\u5408\u8be5\u9886\u57df\u5e38\u7528\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u6d4b\u57fa\u51c6\u3002  \n\u25c6 \u6307\u51fa\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3001\u8de8\u6a21\u6001\u751f\u6210\u7b49\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002|\n",
    "2507.16406": "|2025-07-22|Sparse-View 3D Reconstruction: Recent Advances and Open Challenges|Tanveer Younis\u7b49|[2507.16406](http://arxiv.org/pdf/2507.16406)|\u65e0|\u25c6 \u8be5\u8bba\u6587\u9996\u6b21\u5c06\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u9886\u57df\u7684\u51e0\u4f55\u65b9\u6cd5\u3001\u795e\u7ecf\u9690\u5f0f\u6a21\u578b\uff08\u5982NeRF\uff09\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u7eb3\u5165\u7edf\u4e00\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\u3002  \n\u25c6 \u6df1\u5165\u5206\u6790\u4e86\u7a00\u758f\u573a\u666f\u4e0b\u51e0\u4f55\u6b63\u5219\u5316\u3001\u663e\u5f0f\u5f62\u72b6\u5efa\u6a21\u548c\u751f\u6210\u63a8\u7406\u5982\u4f55\u89e3\u51b3\u6d6e\u6e38\u4f2a\u5f71\u548c\u4f4d\u59ff\u6a21\u7cca\u7b49\u5173\u952e\u95ee\u9898\u3002  \n\u25c6 \u5bf9\u6bd4\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7b49\u663e\u5f0f\u70b9\u4e91\u65b9\u6cd5\u4e0e\u795e\u7ecf\u9690\u5f0f\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u6743\u8861\u5173\u7cfb\u3002  \n\u25c6 \u63d0\u51fa\u5f53\u524d\u9886\u57df\u5c1a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u5305\u62ec\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u65e0\u4f4d\u59ff\u7ea6\u675f\u7684\u91cd\u5efa\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002  \n\u25c6 \u7279\u522b\u5f3a\u8c03\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u548c3D\u539f\u751f\u751f\u6210\u5148\u9a8c\u5728\u7a00\u758f\u91cd\u5efa\u4e2d\u7684\u521b\u65b0\u5e94\u7528\u6f5c\u529b\u3002  \n\u25c6 \u533a\u522b\u4e8e\u4ee5\u5f80\u7efc\u8ff0\uff0c\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u68b3\u7406\u4e86\u6269\u6563\u6a21\u578b\u4e0e\u795e\u7ecf\u9690\u5f0f\u65b9\u6cd5\u7684\u878d\u5408\u6846\u67b6\u53ca\u5176\u5728\u7a00\u758f\u6570\u636e\u4e0b\u7684\u4f18\u52bf\u3002|\n",
    "2507.17351": "|2025-07-23|Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field|Yuzhe Zhu\u7b49|[2507.17351](http://arxiv.org/pdf/2507.17351)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u5c06\u4e3b\u52a8\u5b66\u4e60\u5e94\u7528\u4e8e\u8bed\u4e49\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u8bad\u7ec3\uff0c\u4ee5\u964d\u4f4e\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u3002  \n\u25c6 \u7814\u7a76\u4e86\u8bed\u4e49NeRF\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u5305\u62ec\u9009\u62e9\u7c92\u5ea6\u548c\u9009\u62e9\u7b56\u7565\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u86513D\u51e0\u4f55\u7ea6\u675f\u7684\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4e3b\u52a8\u5b66\u4e60\u7684\u6548\u679c\u3002  \n\u25c6 \u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u8bed\u4e49NeRF\u8bad\u7ec3\u7684\u6807\u6ce8\u6210\u672c\uff0c\u76f8\u6bd4\u968f\u673a\u91c7\u6837\u53ef\u964d\u4f4e2\u500d\u4ee5\u4e0a\u3002  \n\u25c6 \u4e3a\u8bed\u4e49\u573a\u666f\u7406\u89e3\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u3002  \n\u25c6 \u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u4e3b\u52a8\u5b66\u4e60\u57283D\u8bed\u4e49\u795e\u7ecf\u8868\u793a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002|\n",
    "2507.18023": "|2025-07-24|High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details|Jun Zhou\u7b49|[2507.18023](http://arxiv.org/pdf/2507.18023)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684\u9ad8\u4fdd\u771f\u4e09\u7ef4\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u4fee\u590d\u89c6\u56fe\u91cd\u5efa\u5b8c\u6574\u4e09\u7ef4\u573a\u666f  \n\u25c6 \u8bbe\u8ba1\u81ea\u52a8\u63a9\u819c\u4f18\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u9ad8\u65af\u573a\u666f\u8fc7\u6ee4\u4e0e\u53cd\u5411\u6295\u5f71\u6280\u672f\uff0c\u7cbe\u51c6\u5b9a\u4f4d\u906e\u6321\u533a\u57df\u5e76\u5b9e\u73b0\u903c\u771f\u8fb9\u754c\u4fee\u590d  \n\u25c6 \u521b\u65b0\u6027\u5f00\u53d1\u533a\u57df\u7ea7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u91cd\u8981\u6027\u8bc4\u4f30\u7f13\u89e3\u89c6\u89d2\u4e0d\u4e00\u81f4\u95ee\u9898  \n\u25c6 \u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4fee\u590d\u7ed3\u679c\u4e2d\u9ad8\u9891\u7ec6\u8282\u7684\u4fdd\u771f\u5ea6\u4e0e\u771f\u5b9e\u611f  \n\u25c6 \u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6280\u672f  \n\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e86\u4e09\u7ef4\u573a\u666f\u4fee\u590d\u4e2d\u89c6\u89d2\u4e0d\u4e00\u81f4\u548c\u7ec6\u8282\u5931\u771f\u7684\u6838\u5fc3\u96be\u9898\uff0c\u4e3a\u4e09\u7ef4\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002|\n",
    "2507.19474": "|2025-07-25|DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations|Ziren Gong\u7b49|[2507.19474](http://arxiv.org/pdf/2507.19474)|\u65e0|\u25c6 \u63d0\u51faDINO-SLAM\u6846\u67b6\uff0c\u901a\u8fc7DINO\u7279\u5f81\u589e\u5f3a\u795e\u7ecf\u9690\u5f0f\uff08NeRF\uff09\u548c\u663e\u5f0f\uff083DGS\uff09SLAM\u7cfb\u7edf\u7684\u573a\u666f\u8868\u793a\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1\u573a\u666f\u7ed3\u6784\u7f16\u7801\u5668\uff08SSE\uff09\uff0c\u5c06\u539f\u59cbDINO\u7279\u5f81\u5347\u7ea7\u4e3a\u589e\u5f3a\u7248EDINO\uff0c\u6709\u6548\u6355\u6349\u573a\u666f\u5c42\u6b21\u7ed3\u6784\u548c\u5143\u7d20\u95f4\u5173\u7cfb\u3002  \n\u25c6 \u5f00\u53d1\u4e24\u79cd\u57fa\u4e8eEDINO\u7684SLAM\u8303\u5f0f\uff0c\u5206\u522b\u9488\u5bf9NeRF\u548c3DGS\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002  \n\u25c6 \u5728Replica\u3001ScanNet\u548cTUM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u8bc1\u660e\u901a\u7528\u573a\u666f\u9002\u5e94\u80fd\u529b\u3002  \n\u25c6 \u9996\u6b21\u5c06DINO\u8bed\u4e49\u5148\u9a8c\u4e0e\u51e0\u4f55\u91cd\u5efa\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001/\u5f31\u7eb9\u7406\u573a\u666f\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8868\u793a\u65b9\u6848\u3002|\n",
    "2507.19459": "|2025-07-25|Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization|Pol Francesch Huc\u7b49|[2507.19459](http://arxiv.org/pdf/2507.19459)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8eCNN\u76843D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4ec5\u9700\u5355\u76ee\u56fe\u50cf\u5373\u53ef\u751f\u6210\u7c97\u7cd93D\u6a21\u578b\u548c\u76ee\u6807\u59ff\u6001\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u89d2\u7cbe\u786e\u59ff\u6001\u7684\u95ee\u9898\u3002  \n\u25c6 \u5f00\u53d1\u652f\u6301\u566a\u58f0\u6216\u9690\u5f0f\u59ff\u6001\u4f30\u8ba1\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7a81\u7834NeRF/3DGS\u5728\u592a\u7a7a\u573a\u666f\u4e2d\u5fc5\u987b\u4f9d\u8d56\u7cbe\u786e\u59ff\u6001\u7684\u9650\u5236\u3002  \n\u25c6 \u901a\u8fc7\u5206\u6790\u4e0d\u540c\u521d\u59cb\u5316\u53d8\u4f53\uff0c\u663e\u8457\u964d\u4f4e\u9ad8\u7cbe\u5ea63D\u6a21\u578b\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u6240\u9700\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u548c\u8f93\u5165\u56fe\u50cf\u6570\u91cf\u51cf\u5c11\u81f3\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002  \n\u25c6 CNN\u6a21\u5757\u96c6\u6210\u591a\u79cd\u59ff\u6001\u4f30\u8ba1\u6280\u672f\u53d8\u4f53\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u7075\u6d3b\u6027\uff0c\u5e76\u5728\u566a\u58f0\u59ff\u6001\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u5373\u4f7f\u4f7f\u7528\u4e0d\u5b8c\u7f8e\u7684\u59ff\u6001\u76d1\u7763\uff0c\u8be5\u6846\u67b6\u4ecd\u80fd\u5b66\u4e60\u9ad8\u4fdd\u771f3D\u8868\u793a\uff0c\u4e3a\u592a\u7a7a\u5e94\u7528\u4e2d\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6280\u672f\u94fa\u5e73\u9053\u8def\u3002|\n",
    "2507.19328": "|2025-07-25|NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography|Kirsten W. H. Maas\u7b49|[2507.19328](http://arxiv.org/pdf/2507.19328)|\u65e0|\u25c6 NerT-CA\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u5f20\u91cf\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u795e\u7ecf\u573a\u4e0e\u5f20\u91cf\u573a\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u89c6\u89d2X\u5c04\u7ebf\u51a0\u72b6\u52a8\u8109\u9020\u5f71\uff08CA\uff09\u7684\u52a8\u60014D\u91cd\u5efa\u6548\u7387\u3002  \n\u25c6 \u901a\u8fc7\u5c06CA\u573a\u666f\u5206\u89e3\u4e3a\u4f4e\u79e9\u9759\u6001\u6210\u5206\uff08\u5f20\u91cf\u573a\uff09\u4e0e\u52a8\u6001\u7a00\u758f\u6210\u5206\uff08\u795e\u7ecf\u573a\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56MLP\u5bfc\u81f4\u8bad\u7ec3\u8017\u65f6\u8fc7\u957f\u7684\u95ee\u9898\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u5728\u4ec5\u97003\u4e2a\u9020\u5f71\u89c6\u89d2\u4e0b\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u7a81\u7834\u4e86\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u74f6\u9888\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5229\u7528\u4f4e\u79e9\u5148\u9a8c\u52a0\u901f\u9759\u6001\u80cc\u666f\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u7559\u795e\u7ecf\u573a\u5bf9\u8840\u7ba1\u52a8\u6001\u7ec6\u8282\u7684\u6355\u6349\u80fd\u529b\uff0c\u517c\u987e\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u3002  \n\u25c6 \u57284D\u4eff\u771f\u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u4e0e\u5b9a\u6027\u9a8c\u8bc1\u663e\u793a\uff0c\u5176\u8bad\u7ec3\u901f\u5ea6\u4e0e\u91cd\u5efa\u7cbe\u5ea6\u5747\u8d85\u8d8a\u73b0\u6709NeRF-based\u65b9\u6cd5\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u53ef\u80fd\u3002|\n",
    "2507.18713": "|2025-07-24|SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time|Yun Chen\u7b49|[2507.18713](http://arxiv.org/pdf/2507.18713)|\u65e0|\u25c6 \u63d0\u51faSaLF\uff08\u7a00\u758f\u5c40\u90e8\u573a\uff09\u65b0\u578b\u4f53\u7d20\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u573a\u666f\u8868\u793a\u4e3a\u7a00\u758f3D\u4f53\u7d20\u96c6\u5408\uff0c\u6bcf\u4e2a\u4f53\u7d20\u5305\u542b\u5c40\u90e8\u9690\u5f0f\u573a\uff0c\u517c\u5177\u6805\u683c\u5316\u548c\u5149\u7ebf\u8ffd\u8e2a\u80fd\u529b\u3002  \n\u25c6 \u7a81\u7834\u73b0\u6709\u6280\u672f\u9650\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u540c\u65f6\u652f\u6301\u975e\u9488\u5b54\u76f8\u673a\u548c\u65cb\u8f6c\u6fc0\u5149\u96f7\u8fbe\u7684\u9ad8\u6548\u6e32\u67d3\uff08\u76f8\u673a50+ FPS\uff0cLiDAR 600+ FPS\uff09\u3002  \n\u25c6 \u91c7\u7528\u81ea\u9002\u5e94\u526a\u679d\u4e0e\u81f4\u5bc6\u5316\u7b56\u7565\uff0c\u65e0\u9700\u9884\u5904\u7406\u5373\u53ef\u52a8\u6001\u4f18\u5316\u5927\u573a\u666f\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002  \n\u25c6 \u89e3\u8026\u573a\u666f\u8868\u793a\u4e0e\u6e32\u67d3\u6d41\u7a0b\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u7edf\u4e00\u6846\u67b6\uff0c\u514b\u670d\u4e86NeRF\u548c3D\u9ad8\u65af\u6cfc\u6e85\u7684\u4e92\u64cd\u4f5c\u6027\u7f3a\u9677\u3002  \n\u25c6 \u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3\uff08<30\u5206\u949f\uff09\u4e0e\u5b9e\u65f6\u6e32\u67d3\uff0c\u5728\u4fdd\u6301\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\u4eff\u771f\u771f\u5b9e\u6027\u7684\u540c\u65f6\uff0c\u6548\u7387\u8fdc\u8d85\u4f20\u7edfNeRF\u65b9\u6cd5\u3002  \n\u25c6 \u4e3a\u591a\u4f20\u611f\u5668\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u63d0\u4f9b\u9996\u4e2a\u517c\u987e\u9ad8\u4fdd\u771f\u5ea6\u4e0e\u9ad8\u6548\u7387\u7684\u4eff\u771f\u65b9\u6848\uff0c\u63a8\u52a8\u89c4\u6a21\u5316\u865a\u62df\u6d4b\u8bd5\u53d1\u5c55\u3002|\n",
    "2507.20110": "|2025-07-27|NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding|Shiyu Liu\u7b49|[2507.20110](http://arxiv.org/pdf/2507.20110)|\u65e0|\u25c6 NeuroVoxel-LM\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u52a8\u6001\u4f53\u7d20\u5316\u4e0e\u8f7b\u91cf\u7ea7\u5143\u5d4c\u5165\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u8bed\u8a00\u6a21\u578b\u5904\u7406\u7a00\u758f\u5927\u89c4\u6a21\u70b9\u4e91\u65f6\u6548\u7387\u4f4e\u548c\u8868\u793a\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86\u52a8\u6001\u5206\u8fa8\u7387\u591a\u5c3a\u5ea6\u4f53\u7d20\u5316\uff08DR-MSV\uff09\u6280\u672f\uff0c\u6839\u636e\u51e0\u4f55\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u4f53\u7d20\u7c92\u5ea6\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002  \n\u25c6 \u63d0\u51fa\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u52a0\u6743\u548c\u6b8b\u5dee\u878d\u5408\u7684\u8f7b\u91cf\u7ea7\u5143\u5d4c\u5165\u673a\u5236\uff08TAP-LME\uff09\uff0c\u901a\u8fc7\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u6c60\u5316\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u6700\u5927\u6c60\u5316\u65b9\u6cd5\u3002  \n\u25c6 DR-MSV\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u7279\u5f81\u63d0\u53d6\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u8303\u56f4\u590d\u6742\u573a\u666f\u7684\u5feb\u901f\u5904\u7406\u3002  \n\u25c6 TAP-LME\u673a\u5236\u80fd\u591f\u4eceNeRF\u6743\u91cd\u4e2d\u6355\u83b7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u8bed\u8a00\u9a71\u52a8\u76843D\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\u3002  \n\u25c6 \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u57283D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u7a81\u7834\uff0c\u4e3a\u8bed\u8a00\u5bf9\u9f50\u76843D\u611f\u77e5\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002|\n",
    "2507.21350": "|2025-07-28|DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation|Wenkai Tan\u7b49|[2507.21350](http://arxiv.org/pdf/2507.21350)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b26\u53f7\u6846\u67b6DEM-NeRF\uff0c\u76f4\u63a5\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u5e8f\u5217\u91cd\u5efa\u548c\u6a21\u62df\u5f39\u6027\u7269\u4f53\uff0c\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u4fe1\u606f\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c06\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u4e0e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7ed3\u5408\uff0c\u540c\u65f6\u5229\u7528\u56fe\u50cf\u76d1\u7763\u548c\u5f39\u6027\u529b\u5b66\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u7269\u7406\u7ea6\u675f\u3002  \n\u25c6 \u901a\u8fc7\u80fd\u91cf\u7ea6\u675f\u7684PINN\u67b6\u6784\u5904\u7406\u590d\u6742\u8fb9\u754c\u548c\u521d\u59cb\u6761\u4ef6\uff0c\u66ff\u4ee3\u4f20\u7edf\u6709\u9650\u5143\u6216\u8fb9\u754c\u5143\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u62df\u7cbe\u5ea6\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u65f6\u7a7a\u53d8\u5f62\u7269\u4f53\u7684\u8054\u5408\u8868\u5f81\u5b66\u4e60\uff0c\u5f25\u5408\u4e86\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e0e\u4f20\u7edf\u6570\u503c\u6a21\u62df\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002  \n\u25c6 \u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u517c\u5177\u6570\u636e\u9002\u5e94\u6027\u4e0e\u7269\u7406\u4e00\u81f4\u6027\u7684\u65b0\u5de5\u5177\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u9ad8\u4fdd\u771f\u4eff\u771f\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4ec5\u9700\u7a00\u758f\u89c2\u6d4b\u6570\u636e\u5373\u53ef\u5b8c\u6210\u7269\u7406\u89c4\u5f8b\u5efa\u6a21\u3002|\n",
    "2507.23374": "|2025-07-31|NeRF Is a Valuable Assistant for 3D Gaussian Splatting|Shuangkang Fang\u7b49|[2507.23374](http://arxiv.org/pdf/2507.23374)|\u65e0|\u25c6 \u63d0\u51faNeRF-GS\u6846\u67b6\uff0c\u9996\u6b21\u8054\u5408\u4f18\u5316\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\uff0c\u5b9e\u73b0\u4e24\u79cd\u6280\u672f\u7684\u4f18\u52bf\u4e92\u8865\u3002  \n\u25c6 \u5229\u7528NeRF\u7684\u8fde\u7eed\u7a7a\u95f4\u8868\u5f81\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b33DGS\u5bf9\u9ad8\u65af\u521d\u59cb\u5316\u654f\u611f\u3001\u7a7a\u95f4\u611f\u77e5\u5f31\u3001\u9ad8\u65af\u95f4\u5173\u8054\u6027\u4e0d\u8db3\u7b49\u56fa\u6709\u7f3a\u9677\u3002  \n\u25c6 \u901a\u8fc7\u6e10\u8fdb\u5f0f\u5bf9\u9f503DGS\u4e0eNeRF\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u4f7f\u4e24\u8005\u80fd\u57fa\u4e8e\u5171\u4eab\u76843D\u7a7a\u95f4\u4fe1\u606f\u5728\u540c\u4e00\u573a\u666f\u4e2d\u534f\u540c\u4f18\u5316\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u4f18\u5316\u9690\u5f0f\u7279\u5f81\u4e0e\u9ad8\u65af\u4f4d\u7f6e\u7684\u6b8b\u5dee\u5411\u91cf\uff0c\u5f25\u5408\u4e24\u79cd\u65b9\u6cd5\u7684\u7406\u8bba\u5dee\u5f02\uff0c\u589e\u5f3a3DGS\u7684\u4e2a\u6027\u5316\u5efa\u6a21\u80fd\u529b\u3002  \n\u25c6 \u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86NeRF\u4e0e3DGS\u7684\u4e92\u8865\u6027\u800c\u975e\u7ade\u4e89\u5173\u7cfb\uff0c\u4e3a\u6df7\u5408\u8868\u5f81\u65b9\u6cd5\u63d0\u4f9b\u65b0\u601d\u8def\u3002  \n\u25c6 \u4e3a\u7ed3\u5408\u663e\u5f0f\uff083DGS\uff09\u4e0e\u9690\u5f0f\uff08NeRF\uff09\u8868\u793a\u76843D\u573a\u666f\u9ad8\u6548\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002|\n",
    "2507.23033": "|2025-07-30|Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields|Ranxi Lin\u7b49|[2507.23033](http://arxiv.org/pdf/2507.23033)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u6001\u65f6\u95f4\u6b65\u8bad\u7ec3\u7b56\u7565\uff08PATA\uff09\uff0c\u9996\u6b21\u5c06SNN\u7684\u8282\u80fd\u7279\u6027\u4e0eNeRF\u7684\u9ad8\u8d28\u91cf\u6e32\u67d3\u80fd\u529b\u76f8\u7ed3\u5408\u3002  \n\u25c6 \u901a\u8fc7\u9884\u8bad\u7ec3-\u81ea\u9002\u5e94\u65f6\u95f4\u6b65\u8c03\u6574\u673a\u5236\uff0c\u81ea\u52a8\u4f18\u5316\u6e32\u67d3\u8d28\u91cf\u4e0e\u65f6\u95f4\u6b65\u957f\u7684\u5e73\u8861\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfNeRF\u4f9d\u8d56\u5bc6\u96c6\u91c7\u6837\u5bfc\u81f4\u7684\u8d44\u6e90\u6d88\u8017\u95ee\u9898\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u573a\u666f\u81ea\u9002\u5e94\u7684\u52a8\u6001\u63a8\u7406\uff0c\u53ef\u6839\u636e\u4e0d\u540c\u573a\u666f\u7684\u5c3a\u5ea6\u548c\u7eb9\u7406\u590d\u6742\u5ea6\u7075\u6d3b\u8c03\u6574\u65f6\u95f4\u6b65\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002  \n\u25c6 \u5728\u4fdd\u6301Instant-NGP\u67b6\u6784\u4f18\u52bf\u7684\u57fa\u7840\u4e0a\uff0c\u5b9e\u9a8c\u8bc1\u660ePATA\u80fd\u51cf\u5c1164%\u7684\u63a8\u7406\u65f6\u95f4\u6b65\u548c61.55%\u7684\u8fd0\u884c\u529f\u8017\uff0c\u540c\u65f6\u7ef4\u6301\u6e32\u67d3\u7cbe\u5ea6\u3002  \n\u25c6 \u4e3a\u8fb9\u7f18\u8ba1\u7b97\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u795e\u7ecf\u6e32\u67d3\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86NeRF\u6280\u672f\u7684\u5e94\u7528\u8fb9\u754c\u3002|\n",
    "2508.02304": "|2025-08-04|ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering|Fangxin Liu\u7b49|[2508.02304](http://arxiv.org/pdf/2508.02304)|\u65e0|\u25c6 \u63d0\u51faASDR\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u5b58\u5185\u8ba1\u7b97(CIM)\u6280\u672f\u5e94\u7528\u4e8e\u5373\u65f6\u795e\u7ecf\u6e32\u67d3\u9886\u57df\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6848\u5ef6\u8fdf\u9ad8\u3001\u80fd\u6548\u5dee\u7684\u95ee\u9898\u3002  \n\u25c6 \u7b97\u6cd5\u5c42\u9762\u521b\u65b0\u6027\u5730\u5f15\u5165\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5b9e\u65f6\u611f\u77e5\u50cf\u7d20\u6e32\u67d3\u96be\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u91c7\u6837\u70b9\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u548c\u8ba1\u7b97\u5f00\u9500\u3002  \n\u25c6 \u63d0\u51fa\u989c\u8272\u4e0e\u5bc6\u5ea6\u4f53\u6e32\u67d3\u89e3\u8026\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bbMLP\u8ba1\u7b97\u6d41\u7a0b\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u8d1f\u8377\uff0c\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u3002  \n\u25c6 \u67b6\u6784\u5c42\u9762\u8bbe\u8ba1\u65b0\u578bReRAM\u5b58\u7b97\u67b6\u6784\uff0c\u521b\u65b0\u6027\u5730\u5f00\u53d1\u6570\u636e\u6620\u5c04\u4e0e\u91cd\u7528\u5fae\u67b6\u6784\uff0c\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4ee5\u5339\u914d\u6e32\u67d3\u7279\u6027\u3002  \n\u25c6 \u5b9e\u9a8c\u9a8c\u8bc1\u53d6\u5f97\u7a81\u7834\u6027\u6027\u80fd\uff1a\u76f8\u6bd4\u5148\u8fdbNeRF\u52a0\u901f\u5668\u548cXavier NX GPU\u5206\u522b\u5b9e\u73b09.55\u500d\u548c69.75\u500d\u52a0\u901f\uff0c\u4ec5\u635f\u59310.1 PSNR\u753b\u8d28\u3002  \n\u25c6 \u6574\u4f53\u65b9\u6848\u9996\u6b21\u5728CIM\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u795e\u7ecf\u6e32\u67d3\uff0c\u4e3a\u4f4e\u529f\u8017\u5373\u65f6\u56fe\u5f62\u751f\u6210\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2508.00967": "|2025-08-01|Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF|Massoud Pourmandi|[2508.00967](http://arxiv.org/pdf/2508.00967)|\u65e0|\u25c6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u7684\u591a\u65e0\u4eba\u673a\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u548c\u4f4e\u5e26\u5bbd\u901a\u4fe1\u4e0b\u7684\u5b9e\u65f63D\u573a\u666f\u91cd\u5efa\u96be\u9898\u3002  \n\u25c6\u521b\u65b0\u6027\u5730\u5c06\u6269\u6563\u6a21\u578b\u4e0eNeRF\u7ed3\u5408\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u8054\u5408\u573a\u666f\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u5e76\u4fdd\u6301\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u3002  \n\u25c6\u91c7\u7528\u8f7b\u91cf\u7ea7YOLOv12\u8fdb\u884c\u8bed\u4e49\u63d0\u53d6\uff0c\u914d\u5408\u5c40\u90e8NeRF\u66f4\u65b0\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002  \n\u25c6\u8bbe\u8ba1\u4e86\u8bed\u4e49\u611f\u77e5\u538b\u7f29\u534f\u8bae\uff0c\u4f18\u5316\u4e86\u65e0\u4eba\u673a\u95f4\u7684\u6570\u636e\u4f20\u8f93\u6548\u7387\uff0c\u63d0\u5347\u4e86\u534f\u540c\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002  \n\u25c6\u91cd\u65b0\u8bbe\u8ba1\u4e86\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u591a\u89c6\u89d2\u8054\u5408\u573a\u666f\u91cd\u5efa\u4efb\u52a1\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002  \n\u25c6\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u591a\u667a\u80fd\u4f53AI\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002|\n",
    "2508.02831": "|2025-08-04|GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing|Miko\u0142aj Zieli\u0144ski\u7b49|[2508.02831](http://arxiv.org/pdf/2508.02831)|\u65e0|\u25c6 \u63d0\u51faGENIE\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408NeRF\u7684\u9ad8\u8d28\u91cf\u6e32\u67d3\u4e0e\u9ad8\u65af\u6cfc\u6e85(GS)\u7684\u53ef\u7f16\u8f91\u6027\uff0c\u5b9e\u73b0\u65e2\u903c\u771f\u53c8\u53ef\u4ea4\u4e92\u76843D\u573a\u666f\u8868\u793a\u3002  \n\u25c6 \u91c7\u7528\u53ef\u8bad\u7ec3\u7279\u5f81\u5d4c\u5165\u66ff\u4ee3\u4f20\u7edf\u7403\u8c10\u51fd\u6570\uff0c\u901a\u8fc7\u6bcf\u4e2a\u9ad8\u65af\u70b9\u9644\u8fd1\u7684k\u8fd1\u90bb\u6761\u4ef6\u5316NeRF\u7f51\u7edc\uff0c\u589e\u5f3a\u5c40\u90e8\u7f16\u8f91\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1RT-GPS\uff08\u5149\u7ebf\u8ffd\u8e2a\u9ad8\u65af\u90bb\u8fd1\u641c\u7d22\uff09\u7b97\u6cd5\uff0c\u57fa\u4e8e\u6539\u8fdb\u7684\u5149\u7ebf\u8ffd\u8e2a\u7ba1\u7ebf\u5feb\u901f\u5b9a\u4f4d\u6700\u8fd1\u9ad8\u65af\u70b9\uff0c\u63d0\u5347\u67e5\u8be2\u6548\u7387\u3002  \n\u25c6 \u5f15\u5165\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u521d\u59cb\u5316\u4e0e\u66f4\u65b0\u9ad8\u65af\u7279\u5f81\uff0c\u652f\u6301\u52a8\u6001\u573a\u666f\u7684\u5b9e\u65f6\u7279\u5f81\u8c03\u6574\u3002  \n\u25c6 \u5b9e\u73b0\u5b9e\u65f6\u5c40\u90e8\u611f\u77e5\u7f16\u8f91\uff1a\u9ad8\u65af\u57fa\u5143\u7684\u4f4d\u7f6e\u6216\u5c5e\u6027\u4fee\u6539\u80fd\u5373\u65f6\u5f71\u54cd\u6e32\u67d3\u7ed3\u679c\uff0c\u4fdd\u7559NeRF\u7684\u8fde\u7eed\u6027\u4f18\u52bf\u3002  \n\u25c6 \u5f25\u5408\u9690\u5f0f\u795e\u7ecf\u6e32\u67d3\u4e0e\u663e\u5f0f\u51e0\u4f55\u7f16\u8f91\u7684\u9e3f\u6c9f\uff0c\u517c\u5bb9\u7269\u7406\u6a21\u62df\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u521b\u4f5c\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002|\n",
    "2508.04326": "|2025-08-07|Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research|Ke Li\u7b49|[2508.04326](http://arxiv.org/pdf/2508.04326)|\u65e0|\u25c6 \u7cfb\u7edf\u68b3\u7406\u4e86365\u7bc7\u8f90\u5c04\u573a\uff08RF\uff09\u76f8\u5173\u6587\u732e\uff0c\u9996\u6b21\u5168\u9762\u5206\u6790RF\u6280\u672f\u5728XR\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u7814\u7a76\u73b0\u72b6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u8c03\u7814\u7a7a\u767d\u3002  \n\u25c6 \u63d0\u51fa\u4e09\u7ef4\u5206\u6790\u6846\u67b6\uff1a\u4eceXR\u5e94\u7528\u613f\u666f\uff08i\uff09\u3001\u73b0\u6709\u6280\u672f\u5b9e\u73b0\uff08ii\uff09\u548c\u7814\u7a76\u7f3a\u53e3\uff08iii\uff09\u4e09\u4e2a\u7ef4\u5ea6\u89e3\u6784RF\u4e0eXR\u7684\u4ea4\u53c9\u7814\u7a76\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u7ed3\u6784\u5316\u89c6\u89d2\u3002  \n\u25c6 \u7b5b\u900966\u7bc7\u6838\u5fc3\u8bba\u6587\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\uff0c\u63ed\u793aRF\u5728XR\u4e2d\u7684\u5177\u4f53\u6280\u672f\u8def\u5f84\uff08\u59823DGS/NeRF\u7684\u4ea4\u4e92\u6027\u4f18\u5316\uff09\uff0c\u6bd4\u4f20\u7edf\u7efc\u8ff0\u66f4\u5177\u6280\u672f\u9897\u7c92\u5ea6\u3002  \n\u25c6 \u5c06XR\u7279\u5f02\u6027\u7814\u7a76\u95ee\u9898\uff08\u5982\u5b9e\u65f6\u6e32\u67d3\u3001\u7528\u6237\u4ea4\u4e92\uff09\u5d4c\u5165\u5e7f\u4e49RF\u7814\u7a76\u7248\u56fe\uff0c\u660e\u786eXR\u793e\u533a\u7684\u72ec\u7279\u6280\u672f\u6311\u6218\u4e0e\u673a\u9047\u3002  \n\u25c6 \u6784\u5efa\u8de8\u5b66\u79d1\u6587\u732e\u8d44\u6e90\u5e93\uff0c\u8986\u76d6\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u5f62\u5b66\u3001\u4eba\u673a\u4ea4\u4e92\u7b496\u5927\u9886\u57df\uff0c\u52a9\u529b\u7814\u7a76\u8005\u5feb\u901f\u5b9a\u4f4dXR\u76f8\u5173RF\u6280\u672f\u8fdb\u5c55\u3002  \n\u25c6 \u901a\u8fc7\u91cf\u5316\u5206\u6790\u6307\u51faRF\u5728XR\u9886\u57df\u7684\u7814\u7a76\u7a00\u758f\u6027\uff0c\u63a8\u52a8\u5b66\u754c\u5173\u6ce8\u8fd9\u4e00\u6f5c\u529b\u5de8\u5927\u7684\u4ea4\u53c9\u65b9\u5411\u3002|\n",
    "2508.04297": "|2025-08-06|MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction|Yaopeng Lou\u7b49|[2508.04297](http://arxiv.org/pdf/2508.04297)|\u65e0|\u25c6 \u63d0\u51faMuGS\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u591a\u57fa\u7ebf\u8bbe\u7f6e\uff08\u5305\u62ec\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u5c0f/\u5927\u57fa\u7ebf\uff09\u7edf\u4e00\u6574\u5408\u5230\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u6cdb\u5316\u6027\u65b0\u89c6\u89d2\u5408\u6210\u6846\u67b6\u4e2d\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u878d\u5408\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\uff08MVS\uff09\u4e0e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u7279\u5f81\uff0c\u589e\u5f3a\u8de8\u573a\u666f\u7684\u6cdb\u5316\u91cd\u5efa\u80fd\u529b\u3002  \n\u25c6 \u8bbe\u8ba1\u6295\u5f71-\u91c7\u6837\u673a\u5236\u7684\u6df1\u5ea6\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u7cbe\u7ec6\u6982\u7387\u4f53\u79ef\u6784\u5efa\u6307\u5bfc\u7279\u5f81\u56fe\u56de\u5f52\uff0c\u63d0\u5347\u51e0\u4f55\u7cbe\u5ea6\u3002  \n\u25c6 \u5f15\u5165\u53c2\u8003\u89c6\u56fe\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u4f18\u5316\u51e0\u4f55\u4e00\u81f4\u6027\u5e76\u52a0\u901f\u8bad\u7ec3\u6536\u655b\u6548\u7387\u3002  \n\u25c6 \u91c7\u75283D\u9ad8\u65af\u8868\u5f81\u5b9e\u73b0\u8bad\u7ec3/\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u6e32\u67d3\u8d28\u91cf\u3002  \n\u25c6 \u5728DTU\u7b80\u5355\u7269\u4f53\u5230RealEstate10K\u590d\u6742\u573a\u666f\u7684\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5e76\u5728LLFF\u7b49\u6570\u636e\u96c6\u5c55\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u6f5c\u529b\u3002|\n",
    "2508.05187": "|2025-08-07|Refining Gaussian Splatting: A Volumetric Densification Approach|Mohamed Abdul Gafoor\u7b49|[2508.05187](http://arxiv.org/pdf/2508.05187)|\u65e0|\u25c6 \u63d0\u51fa\u57fa\u4e8e\u60ef\u6027\u4f53\u79ef\u7684\u65b0\u578b\u5bc6\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u51fd\u6570\u7684\u60ef\u6027\u4f53\u79ef\u6307\u5bfc3D\u9ad8\u65af\u5206\u5e03\u7684\u7cbe\u7ec6\u5316\u8fc7\u7a0b\uff0c\u6539\u8fdb\u539f\u59cb3DGS\u7684\u5bc6\u5ea6\u63a7\u5236\u7b56\u7565\u3002  \n\u25c6 \u7cfb\u7edf\u7814\u7a76\u4e86\u4f20\u7edf\u8fd0\u52a8\u6062\u590d\u7ed3\u6784(SfM)\u4e0e\u6df1\u5ea6\u56fe\u50cf\u5339\u914d(DIM)\u4e24\u79cd\u70b9\u4e91\u521d\u59cb\u5316\u65b9\u6cd5\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4e3a\u521d\u59cb\u5316\u9009\u62e9\u63d0\u4f9b\u4f9d\u636e\u3002  \n\u25c6 \u5728Mip-NeRF 360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u539f\u59cb3DGS\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002  \n\u25c6 \u89e3\u51b3\u4e86\u539f\u59cb3DGS\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236(ADC)\u5728\u70b9\u57fa\u5143\u7ba1\u7406\u4e0a\u7684\u5173\u952e\u7f3a\u9677\uff0c\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6548\u679c\u3002  \n\u25c6 \u901a\u8fc7\u66f4\u7cbe\u7ec6\u7684\u5bc6\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u65af\u5206\u5e03\u66f4\u5408\u7406\u7684\u5206\u88c2\u4e0e\u526a\u679d\u64cd\u4f5c\uff0c\u4f18\u5316\u4e86\u573a\u666f\u8868\u793a\u3002|\n",
    "2508.05064": "|2025-08-07|A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding|Mahmoud Chick Zaouali\u7b49|[2508.05064](http://arxiv.org/pdf/2508.05064)|\u65e0|\u25c6 \u9996\u6b21\u7cfb\u7edf\u7efc\u8ff0\u4e86\u8bed\u8a00\u5d4c\u5165\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splatting\uff09\u7ed3\u5408\u7684\u8de8\u9886\u57df\u7814\u7a76\uff0c\u586b\u8865\u4e86\u8be5\u65b0\u5174\u4ea4\u53c9\u9886\u57df\u7684\u7a7a\u767d\u3002  \n\u25c6 \u63d0\u51fa\u8bed\u8a00\u5f15\u5bfc\u76843D\u573a\u666f\u7406\u89e3\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u6587\u672c\u6761\u4ef6\u751f\u6210\u3001\u7f16\u8f91\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u6269\u5c55\u4e86\u9ad8\u65af\u6cfc\u6e85\u7684\u5e94\u7528\u573a\u666f\u3002  \n\u25c6 \u8be6\u7ec6\u5206\u6790\u4e86\u8bed\u8a00\u4e0e3D\u9ad8\u65af\u8868\u5f81\u878d\u5408\u7684\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u8def\u5f84\uff0c\u5305\u62ec\u5d4c\u5165\u7b56\u7565\u3001\u8bed\u4e49\u5bf9\u9f50\u65b9\u6cd5\u53ca\u5b9e\u65f6\u6e32\u67d3\u4f18\u5316\u65b9\u6848\u3002  \n\u25c6 \u603b\u7ed3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u8ba1\u7b97\u6548\u7387\u74f6\u9888\u3001\u6cdb\u5316\u6027\u4e0d\u8db3\u53ca\u8bed\u4e49\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002  \n\u25c6 \u68b3\u7406\u4e86\u673a\u5668\u4eba\u3001\u4ea4\u4e92\u5185\u5bb9\u521b\u4f5c\u7b49\u9886\u57df\u7684\u843d\u5730\u6848\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u589e\u5f3a\u578b3D\u5efa\u6a21\u7684\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2508.06169": "|2025-08-08|UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting|Wenpeng Xing\u7b49|[2508.06169](http://arxiv.org/pdf/2508.06169)|\u65e0|\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u901a\u8fc7\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff083DGS\uff09\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6c34\u4e0b\u4e09\u7ef4\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u6c34\u4e0b\u5149\u7ebf\u5438\u6536\u3001\u6563\u5c04\u548c\u6d51\u6d4a\u5ea6\u5bfc\u81f4\u7684\u51e0\u4f55\u4e0e\u8272\u5f69\u5931\u771f\u95ee\u9898\u3002\u4e3b\u8981\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\n\u25c6 \u63d0\u51fa\u53ef\u63d2\u62d4\u7684\u5b66\u4e60\u578b\u6c34\u4e0b\u6210\u50cf\u6a21\u5757\uff0c\u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u56de\u5f52\u65b9\u6cd5\u6a21\u62df\u7a7a\u95f4\u53d8\u5316\u7684\u8870\u51cf\u548c\u80cc\u6563\u5c04\u6548\u5e94\uff0c\u63d0\u5347\u989c\u8272\u4fdd\u771f\u5ea6\u3002\n\n\u25c6 \u8bbe\u8ba1\u4e86\u7269\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u526a\u679d\uff08PAUP\uff09\u5206\u652f\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u81ea\u9002\u5e94\u5254\u9664\u566a\u58f0\u9ad8\u65af\u70b9\uff0c\u663e\u8457\u51cf\u5c11\u6d6e\u6e38\u4f2a\u5f71\uff08\u964d\u4f4e\u7ea665%\uff09\u3002\n\n\u25c6 \u6784\u5efa\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u540c\u6b65\u4f18\u5316\u9ad8\u65af\u70b9\u53c2\u6570\u4e0e\u6c34\u4e0b\u7269\u7406\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e0e\u5149\u4f20\u8f93\u7684\u8054\u5408\u5efa\u6a21\u3002\n\n\u25c6 \u5728\u6e32\u67d3\u9636\u6bb5\u751f\u6210\u4e24\u79cd\u8f93\u51fa\uff1a\u53bb\u9664\u4ecb\u8d28\u5f71\u54cd\u7684\u7eaf\u51c0\u672a\u8870\u51cf\u8f90\u5c04\u56fe\u50cf\uff08URI\uff09\u548c\u5305\u542b\u771f\u5b9e\u5149\u4f20\u8f93\u6548\u679c\u7684\u6c34\u4e0b\u56fe\u50cf\uff08UWI\uff09\u3002\n\n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SeaThru-NeRF\u548cUWBundle\u6570\u636e\u96c6\u4e0a\u8fbe\u5230PSNR 27.604\u3001SSIM 0.868\u3001LPIPS 0.104\u7684\u6307\u6807\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002|\n",
    "2508.06136": "|2025-08-08|Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation|YoungChan Choi\u7b49|[2508.06136](http://arxiv.org/pdf/2508.06136)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u663e\u5f0f3D\u773c\u7403\u7ed3\u6784\u7684\u89c6\u7ebf\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u7a81\u7834\u4f20\u7edf\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u9690\u5f0f\u8868\u793a\u7684\u5c40\u9650\u3002  \n\u25c6 \u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\u7cbe\u786e\u5efa\u6a21\u773c\u7403\uff0c\u901a\u8fc7\u663e\u5f0f\u65cb\u8f6c\u548c\u5e73\u79fb\u63a7\u5236\u89c6\u7ebf\u65b9\u5411\uff0c\u63d0\u5347\u7269\u7406\u5408\u7406\u6027\u3002  \n\u25c6 \u521b\u65b0\u6027\u8bbe\u8ba1\u81ea\u9002\u5e94\u5f62\u53d8\u6a21\u5757\uff0c\u6a21\u62df\u773c\u90e8\u5468\u56f4\u808c\u8089\u7684\u7ec6\u5fae\u8fd0\u52a8\uff0c\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u7684\u52a8\u6001\u771f\u5b9e\u611f\u3002  \n\u25c6 \u5728ETH-XGaze\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u56fe\u50cf\u7684\u5149\u5f71\u3001\u7eb9\u7406\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u89c6\u7ebf\u4f30\u8ba1\u7cbe\u5ea6\u66f4\u9ad8\u3002  \n\u25c6 \u6846\u67b6\u652f\u6301\u591a\u6837\u5316\u65b0\u89c6\u7ebf\u751f\u6210\uff0c\u4e3a\u865a\u62df\u73b0\u5b9e\u3001\u4eba\u673a\u4ea4\u4e92\u7b49\u573a\u666f\u63d0\u4f9b\u9ad8\u4fdd\u771f\u773c\u90e8\u8fd0\u52a8\u5408\u6210\u65b9\u6848\u3002|\n",
    "2508.05819": "|2025-08-07|MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses|Jong-Ik Park\u7b49|[2508.05819](http://arxiv.org/pdf/2508.05819)|\u65e0|\u25c6 \u63d0\u51fa\u9996\u4e2a\u539f\u751f\u652f\u6301\u591a\u7f29\u653e\u56fe\u50cf\u96c6\u7684NeRF\u6846\u67b6MZEN\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u9ad8\u7cbe\u5ea6\u7ec6\u8282\u91cd\u5efa\u7684\u96be\u9898\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5728\u9488\u5b54\u76f8\u673a\u6a21\u578b\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u6807\u91cf\uff0c\u52a8\u6001\u8c03\u6574\u7126\u8ddd\u4ee5\u9002\u5e94\u4e0d\u540c\u7f29\u653e\u7ea7\u522b\u7684\u56fe\u50cf\u3002  \n\u25c6 \u8bbe\u8ba1\u5206\u5c42\u4f4d\u59ff\u4f18\u5316\u7b56\u7565\uff1a\u5148\u901a\u8fc7\u5e7f\u89d2\u56fe\u50cf\u5efa\u7acb\u5168\u5c40\u5750\u6807\u7cfb\uff0c\u518d\u901a\u8fc7\u7f29\u653e\u4e00\u81f4\u88c1\u526a\u5339\u914d\u65b9\u6cd5\u5c06\u653e\u5927\u56fe\u50cf\u4e0e\u6700\u8fd1\u5e7f\u89d2\u56fe\u50cf\u5bf9\u9f50\uff0c\u6700\u540e\u8054\u5408\u4f18\u5316\u3002  \n\u25c6 \u5728\u5408\u6210TCAD\u6a21\u578b\u3001\u771f\u5b9eSEM\u5fae\u7ed3\u6784\u7b498\u4e2a\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0cPSNR\u63d0\u5347\u6700\u9ad8\u8fbe28%\uff0cSSIM\u63d0\u534710%\uff0cLPIPS\u964d\u4f4e222%\u3002  \n\u25c6 \u7a81\u7834\u4f20\u7edfNeRF\u5728\u591a\u7f29\u653e\u573a\u666f\u4e0b\u7684\u9650\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u5168\u5c40\u7cbe\u5ea6\u4e0e\u5fae\u7c73\u7ea7\u7ec6\u8282\u7684\u534f\u540c\u6355\u6349\uff0c\u63a8\u52a8NeRF\u5728\u5de5\u4e1a\u68c0\u6d4b\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002|\n",
    "2508.08219": "|2025-08-11|SAGOnline: Segment Any Gaussians Online|Wentao Sun\u7b49|[2508.08219](http://arxiv.org/pdf/2508.08219)|\u65e0|SAGOnline\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u548c\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u9996\u4e2a\u8f7b\u91cf\u7ea7\u96f6\u6837\u672c\u6846\u67b6SAGOnline\uff0c\u5b9e\u73b0\u9ad8\u65af\u573a\u666f\u7684\u5b9e\u65f63D\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002\n\n\u25c6 \u91c7\u7528\u89e3\u8026\u7b56\u7565\u6574\u5408\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff08\u5982SAM2\uff09\uff0c\u901a\u8fc7\u5408\u6210\u89c6\u56fe\u95f4\u76842D\u63a9\u7801\u4f20\u64ad\u5b9e\u73b0\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u5206\u5272\u3002\n\n\u25c6 \u5f00\u53d1GPU\u52a0\u901f\u76843D\u63a9\u7801\u751f\u6210\u7b97\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u7ea7\u5b9e\u4f8b\u6807\u6ce8\u4e3a3D\u56fe\u5143\u5206\u914d\u552f\u4e00ID\uff0c\u652f\u6301\u65e0\u635f\u591a\u76ee\u6807\u8ddf\u8e2a\u4e0e\u8de8\u89c6\u89d2\u5206\u5272\u3002\n\n\u25c6 \u5728NVOS\uff0892.7% mIoU\uff09\u548cSpin-NeRF\uff0895.2% mIoU\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb15-1500\u500d\uff0827\u6beb\u79d2/\u5e27\uff09\u3002\n\n\u25c6 \u521b\u65b0\u6027\u5c062D\u89c6\u9891\u57fa\u7840\u6a21\u578b\u9002\u914d\u52303D\u9886\u57df\uff0c\u9996\u6b21\u5b9e\u73b0\u590d\u6742\u573a\u666f\u4e2d\u7a33\u5065\u7684\u591a\u76ee\u6807\u5206\u5272\u4e0e\u8ddf\u8e2a\u3002\n\n\u25c6 \u901a\u8fc7\u663e\u5f0f\u6807\u6ce8\u9ad8\u65af\u56fe\u5143\uff0c\u540c\u65f6\u652f\u6301\u5206\u5272\u4e0e\u8ddf\u8e2a\u529f\u80fd\uff0c\u4e3aAR/VR\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u5b9e\u65f63D\u573a\u666f\u7406\u89e3\u65b0\u65b9\u6848\u3002|\n",
    "2508.07182": "|2025-08-10|3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction|Xuesong Li\u7b49|[2508.07182](http://arxiv.org/pdf/2508.07182)|\u65e0|\u25c6 \u63d0\u51fa\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u8fd0\u52a8\u8f68\u8ff9\u573a\u7684\u65b0\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u52a8\u6001\u573a\u666f\u4e2d\u590d\u6742\u8fd0\u52a8\u7684\u7cbe\u786e\u5efa\u6a21\u4e0e\u7269\u7406\u5408\u7406\u8f68\u8ff9\u751f\u6210\u3002  \n\u25c6 \u901a\u8fc7\u52a8\u6001\u7269\u4f53\u4e0e\u9759\u6001\u80cc\u666f\u89e3\u8026\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u8f68\u8ff9\u573a\u7684\u4f18\u5316\u6548\u7387\u4e0e\u573a\u666f\u8868\u793a\u7d27\u51d1\u6027\u3002  \n\u25c6 \u521b\u65b0\u91c7\u7528\u65f6\u95f4\u4e0d\u53d8\u8fd0\u52a8\u7cfb\u6570\u548c\u5171\u4eab\u8fd0\u52a8\u8f68\u8ff9\u57fa\uff0c\u5728\u6355\u6349\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4f18\u5316\u590d\u6742\u5ea6\u3002  \n\u25c6 \u5b9e\u73b0\u5355\u76ee\u89c6\u9891\u4e2d\u52a8\u6001\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u65b0\u89c6\u89d2\u5408\u6210\u4e0e\u8fd0\u52a8\u8f68\u8ff9\u91cd\u5efa\u53cc\u7a81\u7834\uff0c\u6027\u80fd\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002  \n\u25c6 \u6240\u63d0\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7b49\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u9996\u4e2a\u80fd\u540c\u65f6\u5904\u7406\u52a8\u6001\u6e32\u67d3\u4e0e\u8fd0\u52a8\u63a8\u7406\u7684\u7edf\u4e00\u6846\u67b6\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6848\u5728\u8fd0\u52a8\u7ec6\u8282\u8fd8\u539f\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u52a8\u6001NeRF\u4e0e3DGS\u65b9\u6cd5\u7684\u4f18\u52bf\u3002|\n",
    "2508.06632": "|2025-08-08|CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition|Wenpeng Xing\u7b49|[2508.06632](http://arxiv.org/pdf/2508.06632)|\u65e0|\u25c6 \u63d0\u51fa\u52a8\u6001\u7cfb\u6570\u5206\u89e3\u6846\u67b6CoDe-NeRF\uff0c\u5c06\u590d\u6742\u5916\u89c2\u89e3\u8026\u4e3a\u9759\u6001\u795e\u7ecf\u57fa\u5e95\uff08\u7f16\u7801\u6750\u8d28\u5c5e\u6027\uff09\u548c\u52a8\u6001\u7cfb\u6570\uff08\u7531\u89c6\u89d2/\u5149\u7167\u6761\u4ef6\u751f\u6210\uff09\uff0c\u7a81\u7834\u4f20\u7edfNeRF\u5bf9\u955c\u9762\u53cd\u5c04\u5efa\u6a21\u7684\u5c40\u9650\u3002  \n\u25c6 \u8bbe\u8ba1\u7cfb\u6570\u7f51\u7edc\uff08Coefficient Network\uff09\u52a8\u6001\u751f\u6210\u4e0e\u89c6\u89d2/\u5149\u7167\u76f8\u5173\u7684\u7cfb\u6570\uff0c\u914d\u5408\u9759\u6001\u57fa\u5e95\u5b9e\u73b0\u9ad8\u6548\u7684\u5149-\u7269\u7406\u89e3\u8026\uff0c\u907f\u514d\u9006\u5411\u6e32\u67d3\u7684\u4e0d\u7a33\u5b9a\u6027\u3002  \n\u25c6 \u5f15\u5165\u52a8\u6001\u8f90\u5c04\u79ef\u5206\u5668\uff08Dynamic Radiance Integrator\uff09\u81ea\u9002\u5e94\u878d\u5408\u9759\u6001\u57fa\u5e95\u4e0e\u52a8\u6001\u7cfb\u6570\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u5149\u4e0e\u955c\u9762\u53cd\u5c04\u7684\u9510\u5229\u5ea6\u548c\u771f\u5b9e\u611f\u3002  \n\u25c6 \u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u53cd\u5c04\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u751f\u6210\u66f4\u6e05\u6670\u7684\u955c\u9762\u6548\u679c\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u7269\u7406\u9006\u5411\u6e32\u67d3\u7684\u5f3a\u5047\u8bbe\u3002  \n\u25c6 \u4e3a\u795e\u7ecf\u573a\u666f\u8868\u793a\u4e2d\u7684\u590d\u6742\u5916\u89c2\u5efa\u6a21\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u89e3\u8026\u5f0f\u8bbe\u8ba1\u589e\u5f3a\u4e86\u5bf9\u52a8\u6001\u5149\u7167\u6761\u4ef6\u7684\u9002\u5e94\u6027\u3002|\n",
    "2508.08798": "|2025-08-12|MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields|Yao Lu\u7b49|[2508.08798](http://arxiv.org/pdf/2508.08798)|\u65e0|\u25c6 \u63d0\u51faMonoPartNeRF\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u57fa\u4e8e\u5206\u533a\u7684\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u5e94\u7528\u4e8e\u5355\u76ee\u89c6\u9891\u4eba\u4f53\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u59ff\u6001\u4e0b\u8fb9\u754c\u8fc7\u6e21\u4e0d\u81ea\u7136\u548c\u906e\u6321\u533a\u57df\u91cd\u5efa\u4e0d\u51c6\u7684\u96be\u9898\u3002  \n\u25c6 \u8bbe\u8ba1\u53cc\u5411\u53d8\u5f62\u6a21\u578b\uff0c\u7ed3\u5408\u521a\u6027\u4e0e\u975e\u521a\u6027\u53d8\u6362\uff0c\u5efa\u7acb\u89c2\u5bdf\u7a7a\u95f4\u4e0e\u89c4\u8303\u7a7a\u95f4\u7684\u53ef\u9006\u8fde\u7eed\u6620\u5c04\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u8868\u9762-\u65f6\u95f4\u7a7a\u95f4\uff08u, v, t\uff09\u66f4\u7cbe\u51c6\u6355\u6349\u975e\u521a\u6027\u8fd0\u52a8\u3002  \n\u25c6 \u5f15\u5165\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u6291\u5236\u53d8\u5f62\u5bfc\u81f4\u7684\u4f2a\u5f71\u548c\u65ad\u88c2\u95ee\u9898\uff0c\u63d0\u5347\u91cd\u5efa\u7684\u51e0\u4f55\u8fde\u8d2f\u6027\u3002  \n\u25c6 \u521b\u65b0\u63d0\u51fa\u5206\u533a\u59ff\u6001\u5d4c\u5165\u673a\u5236\uff0c\u5c06\u5168\u5c40\u59ff\u6001\u5411\u91cf\u5206\u89e3\u4e3a\u5c40\u90e8\u5173\u8282\u5d4c\u5165\uff0c\u7ed3\u5408\u4e09\u8f74\u65b9\u5411\u7684\u5173\u952e\u5e27\u59ff\u6001\u68c0\u7d22\u4e0e\u63d2\u503c\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7684\u59ff\u52bf\u611f\u77e5\u7279\u5f81\u91c7\u6837\u3002  \n\u25c6 \u96c6\u6210\u53ef\u5b66\u4e60\u7684\u5916\u89c2\u7f16\u7801\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u52a8\u6001\u5efa\u6a21\u7eb9\u7406\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u906e\u6321\u4e0b\u7684\u7eb9\u7406\u4fdd\u771f\u5ea6\u3002\u5b9e\u9a8c\u5728ZJU-MoCap\u548cMonoCap\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u59ff\u6001\u9002\u5e94\u6027\u4e0e\u906e\u6321\u6062\u590d\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002|\n",
    "2508.09977": "|2025-08-22|A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation|Shuting He\u7b49|[2508.09977](http://arxiv.org/pdf/2508.09977)|\u65e0|\u25c6 \u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7efc\u8ff0\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\u5728\u5206\u5272\u3001\u7f16\u8f91\u548c\u751f\u6210\u7b49\u5e94\u7528\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u8c03\u7814\u7a7a\u767d\u3002  \n\u25c6 \u901a\u8fc7\u5f15\u51652D\u57fa\u7840\u6a21\u578b\u4e0eNeRF\u65b9\u6cd5\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u63ed\u793a\u4e863DGS\u5728\u8bed\u4e49\u7406\u89e3\u548c\u51e0\u4f55\u63a7\u5236\u65b9\u9762\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u7a81\u51fa\u4e86\u5176\u663e\u5f0f\u7d27\u51d1\u8868\u793a\u7684\u6f5c\u529b\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u5c063DGS\u5e94\u7528\u5212\u5206\u4e3a\u5206\u5272\u3001\u7f16\u8f91\u3001\u751f\u6210\u7b49\u529f\u80fd\u4efb\u52a1\uff0c\u5e76\u603b\u7ed3\u4e86\u5404\u7c7b\u4efb\u52a1\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u76d1\u7763\u7b56\u7565\u548c\u5b66\u4e60\u8303\u5f0f\uff0c\u63d0\u70bc\u51fa\u901a\u7528\u8bbe\u8ba1\u539f\u5219\u3002  \n\u25c6 \u63d0\u4f9b\u4e86\u516c\u5f00\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u7684\u8be6\u7ec6\u603b\u7ed3\uff0c\u5e76\u5bf9\u73b0\u6709\u65b9\u6cd5\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u6a2a\u5411\u5bf9\u6bd4\u5206\u6790\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u6846\u67b6\u3002  \n\u25c6 \u5efa\u7acb\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u5f00\u6e90\u8d44\u6e90\u5e93\uff08GitHub\uff09\uff0c\u6574\u5408\u4e86\u76f8\u5173\u8bba\u6587\u3001\u4ee3\u7801\u548c\u5de5\u5177\uff0c\u63a8\u52a83DGS\u5e94\u7528\u751f\u6001\u7684\u534f\u540c\u53d1\u5c55\u3002  \n\u25c6 \u901a\u8fc7\u8de8\u9886\u57df\u8d8b\u52bf\u5206\u6790\uff0c\u6307\u51fa3DGS\u5728\u5b9e\u65f6\u6e32\u67d3\u4e0e\u8bed\u4e49\u64cd\u4f5c\u7ed3\u5408\u65b9\u5411\u7684\u53d1\u5c55\u524d\u666f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u6f5c\u5728\u7a81\u7834\u70b9\u3002|\n",
    "2508.09681": "|2025-08-13|Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision|Gerardo Loza\u7b49|[2508.09681](http://arxiv.org/pdf/2508.09681)|\u65e0|\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9006\u795e\u7ecf\u8f90\u5c04\u573a\uff08InvNeRF\uff09\u7684\u65b0\u578b\u6d4b\u8bd5\u65f6\u4f18\u5316\uff08TTO\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u624b\u672f\u573a\u666f\u4e2d\u7684\u957f\u671f3D\u70b9\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e00\u81f4\u6027\u8fd0\u52a8\u62163D\u8ddf\u8e2a\u4e0a\u7684\u5c40\u9650\u6027\u3002  \n\u25c6 \u901a\u8fc7\u6e32\u67d3\u76d1\u7763\u50cf\u7d20\u5bf9\u5e94\u5173\u7cfb\uff0c\u5229\u7528\u53cc\u5411\u53ef\u53d8\u5f62-\u89c4\u8303\u6620\u5c04\u7b56\u7565\uff0c\u6709\u6548\u5904\u7406\u5b9a\u4e49\u7684\u5de5\u4f5c\u7a7a\u95f4\u5e76\u4f18\u5316\u5149\u7ebf\u5bc6\u5ea6\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3002  \n\u25c6 \u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6HexPlanes\u7ed3\u6784\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u63d0\u51fa\u9ad8\u6548\u50cf\u7d20\u91c7\u6837\u548c\u6536\u655b\u51c6\u5219\u7b97\u6cd5\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002  \n\u25c6 \u57282D\u70b9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u7cbe\u5ea6\u6bd4\u73b0\u6709TTO\u65b9\u6cd5\u63d0\u5347\u8fd150%\uff0c\u5e76\u4e0e\u975eTTO\u65b9\u6cd5\u7ade\u4e89\uff1b\u57283D\u70b9\u8ddf\u8e2a\u4e2d\u9996\u6b21\u5b9e\u73b0TTO\u6846\u67b6\uff0c\u6027\u80fd\u8d85\u8d8a\u524d\u9988\u65b9\u6cd5\u3002  \n\u25c6 \u7ed3\u5408\u53ef\u53d8\u5f62NeRF\u91cd\u5efa\u4f18\u52bf\uff0c\u652f\u63012D\u548c3D\u8ddf\u8e2a\u4e00\u4f53\u5316\uff0c\u5e76\u5728STIR\u548cSCARE\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u4e0e\u8fd0\u52a8\u5b66\u6570\u636e\u6574\u5408\u80fd\u529b\u3002|\n",
    "2508.12163": "|2025-08-16|RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis|Wenqing Wang\u7b49|[2508.12163](http://arxiv.org/pdf/2508.12163)|\u65e0|\u25c6 \u63d0\u51faRealTalk\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u9ad8\u60c5\u611f\u51c6\u786e\u5ea6\u3001\u5f3a\u60c5\u611f\u53ef\u63a7\u6027\u548c\u7a33\u5b9a\u8eab\u4efd\u4fdd\u6301\u7684\u62df\u771f\u8bf4\u8bdd\u5934\u90e8\u5408\u6210\u3002  \n\u25c6 \u521b\u65b0\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4ece\u97f3\u9891\u751f\u62103D\u9762\u90e8\u6807\u5fd7\u70b9\uff0c\u7ed3\u5408ResNet\u6807\u5fd7\u70b9\u53d8\u5f62\u6a21\u578b\uff08LDM\uff09\u878d\u5408\u60c5\u611f\u6807\u7b7e\u5d4c\u5165\uff0c\u7cbe\u51c6\u63a7\u5236\u8868\u60c5\u3002  \n\u25c6 \u8bbe\u8ba1\u65b0\u578b\u4e09\u5e73\u9762\u6ce8\u610f\u529b\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\uff0c\u901a\u8fc7\u6807\u5fd7\u70b9\u548c\u9762\u90e8\u6df7\u5408\u5f62\u72b6\u7cfb\u6570\u8054\u5408\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u5408\u6210\u56fe\u50cf\u7684\u903c\u771f\u5ea6\u3002  \n\u25c6 \u5f15\u5165\u60c5\u611f\u6807\u7b7e\u5d4c\u5165\u673a\u5236\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u5bf9\u60c5\u611f\u8868\u8fbe\u63a7\u5236\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u60c5\u611f\u8c03\u8282\u3002  \n\u25c6 \u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u60c5\u611f\u51c6\u786e\u6027\u3001\u53ef\u63a7\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u7b49\u6838\u5fc3\u6307\u6807\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u63a8\u52a8\u793e\u4ea4\u667a\u80fdAI\u53d1\u5c55\u3002|\n",
    "2508.13808": "|2025-08-19|Is-NeRF: In-scattering Neural Radiance Field for Blurred Images|Nan Luo\u7b49|[2508.13808](http://arxiv.org/pdf/2508.13808)|\u65e0|Is-NeRF\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6563\u5c04\u611f\u77e5\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u7528\u4e8e\u4ece\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u6062\u590d\u6e05\u6670\u7684\u4e09\u7ef4\u573a\u666f\u3002\u5176\u521b\u65b0\u70b9\u4e3b\u8981\u4f53\u73b0\u5728\u4ee5\u4e0b\u56db\u4e2a\u65b9\u9762\uff1a\n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u5185\u6563\u5c04\u8868\u793a\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u516d\u79cd\u5e38\u89c1\u7684\u5149\u7ebf\u4f20\u64ad\u73b0\u8c61\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u4f20\u7edfNeRF\u7684\u76f4\u7ebf\u4f53\u6e32\u67d3\u65b9\u5f0f\u3002\n\u25c6 \u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u3001\u53ef\u9002\u5e94\u590d\u6742\u5149\u8def\u7684\u6563\u5c04\u611f\u77e5\u4f53\u6e32\u67d3\u7ba1\u7ebf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u51e0\u4f55\u6a21\u7cca\u5bfc\u81f4\u7684\u8bad\u7ec3\u6b67\u4e49\u95ee\u9898\u3002\n\u25c6 \u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u80fd\u81ea\u4e3b\u786e\u5b9a\u6563\u5c04\u65b9\u5411\u548c\u91c7\u6837\u95f4\u9694\uff0c\u4ece\u800c\u6355\u6349\u5230\u66f4\u7cbe\u7ec6\u7684\u7269\u4f53\u51e0\u4f55\u7ec6\u8282\u3002\n\u25c6 \u5b9e\u73b0\u4e86\u5bf9NeRF\u53c2\u6570\u3001\u6563\u5c04\u53c2\u6570\u548c\u76f8\u673a\u8fd0\u52a8\u7684\u8054\u5408\u4f18\u5316\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u4ec5\u4ece\u6a21\u7cca\u56fe\u50cf\u4e2d\u5c31\u80fd\u6062\u590d\u51fa\u9ad8\u4fdd\u771f\u573a\u666f\u8868\u793a\u548c\u7cbe\u786e\u51e0\u4f55\u7ec6\u8282\u3002|\n",
    "2508.13228": "|2025-08-17|PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism|Yuyan Ye\u7b49|[2508.13228](http://arxiv.org/pdf/2508.13228)|\u65e0|PreSem-Surf\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u4eceRGB-D\u5e8f\u5217\u5feb\u901f\u91cd\u5efa\u9ad8\u8d28\u91cf\u573a\u666f\u8868\u9762\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u878d\u5408\u4e86\u989c\u8272\u3001\u6df1\u5ea6\u548c\u8bed\u4e49\u4fe1\u606f\u4ee5\u5168\u9762\u63d0\u5347\u91cd\u5efa\u6027\u80fd\u3002\n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684SG-MLP\u91c7\u6837\u7ed3\u6784\uff0c\u7ed3\u5408PR-MLP\uff08\u9884\u6761\u4ef6\u591a\u5c42\u611f\u77e5\u673a\uff09\u8fdb\u884c\u4f53\u7d20\u9884\u6e32\u67d3\uff0c\u4f7f\u6a21\u578b\u80fd\u66f4\u65e9\u6355\u83b7\u573a\u666f\u4fe1\u606f\u5e76\u66f4\u597d\u5730\u533a\u5206\u566a\u58f0\u4e0e\u5c40\u90e8\u7ec6\u8282\u3002\n\u25c6 \u91c7\u7528\u4e86\u6e10\u8fdb\u5f0f\u8bed\u4e49\u5efa\u6a21\u7b56\u7565\uff0c\u901a\u8fc7\u9010\u6b65\u63d0\u53d6\u66f4\u7cbe\u7ec6\u7684\u8bed\u4e49\u4fe1\u606f\u6765\u589e\u5f3a\u573a\u666f\u7406\u89e3\uff0c\u540c\u65f6\u6709\u6548\u51cf\u5c11\u4e86\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u65f6\u95f4\u3002\n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728C-L1\u3001F-score\u548cIoU\u591a\u9879\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u5e76\u5728\u5176\u4ed6\u6307\u6807\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002|\n",
    "2508.14563": "|2025-08-20|GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels|Xingyuan Yang\u7b49|[2508.14563](http://arxiv.org/pdf/2508.14563)|\u65e0|GOGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u7ef4\u9ad8\u65af\u9762\u5143\u7684\u65b0\u578b\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u5149\u7269\u4f53\u9006\u5411\u6e32\u67d3\u4e2d\u7684\u51e0\u4f55\u566a\u58f0\u548c\u91cd\u5149\u7167\u4e0d\u771f\u5b9e\u95ee\u9898\u3002\u5176\u6838\u5fc3\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u91c7\u7528\u57fa\u4e8e\u7269\u7406\u6e32\u67d3\u7684\u5206\u88c2\u548c\u8fd1\u4f3c\u6cd5\uff0c\u5e76\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u8868\u9762\u91cd\u5efa\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u7ed3\u6784\u7455\u75b5\u3002\n\u25c6 \u5229\u7528\u8499\u7279\u5361\u6d1b\u91cd\u8981\u6027\u91c7\u6837\u5b8c\u6574\u6e32\u67d3\u65b9\u7a0b\u8fdb\u884c\u6750\u8d28\u5206\u89e3\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u4e8c\u7ef4\u9ad8\u65af\u5149\u7ebf\u8ffd\u8e2a\u6a21\u62df\u95f4\u63a5\u5149\u7167\uff0c\u63d0\u5347\u4e86\u5149\u7167\u8ba1\u7b97\u7684\u51c6\u786e\u6027\u3002\n\u25c6 \u5f15\u5165\u57fa\u4e8e\u7403\u5f62mipmap\u7684\u65b9\u5411\u7f16\u7801\u6765\u7ec6\u5316\u9ad8\u9891\u955c\u9762\u7ec6\u8282\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u5404\u5411\u5f02\u6027\u9ad8\u5149\uff0c\u4ece\u800c\u5728\u590d\u6742\u5149\u7167\u4e0b\u751f\u6210\u903c\u771f\u7684\u91cd\u5149\u7167\u6548\u679c\u3002\n\u8be5\u6846\u67b6\u5728\u51e0\u4f55\u91cd\u5efa\u3001\u6750\u8d28\u5206\u79bb\u548c\u65b0\u5149\u7167\u91cd\u5149\u7167\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u9006\u5411\u6e32\u67d3\u65b9\u6cd5\u3002|\n",
    "2508.17645": "|2025-08-28|Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph|Xiaoyang Huang\u7b49|[2508.17645](http://arxiv.org/pdf/2508.17645)|\u65e0|\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53ef\u5fae\u5206\u64cd\u4f5c\u56fe\u751f\u6210\u4eba\u673a\u534f\u4f5c3D\u8d44\u4ea7\u8bbe\u8ba1\u5e8f\u5217\u7684\u65b9\u6cd5\uff0c\u4ee5\u5f25\u5408AI\u751f\u6210\u5185\u5bb9\u4e0e\u4eba\u7c7b\u53c2\u6570\u5316\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002  \n\u25c6 \u5c06\u4f20\u7edf\u5efa\u6a21\u64cd\u4f5c\uff08\u5982\u62c9\u4f38\u3001\u5e03\u5c14\u8fd0\u7b97\uff09\u91cd\u65b0\u6784\u5efa\u4e3a\u53ef\u5fae\u5206\u5355\u5143\uff0c\u652f\u6301\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u8054\u5408\u4f18\u5316\u8fde\u7eed\u548c\u79bb\u6563\u53c2\u6570\u3002  \n\u25c6 \u6784\u5efa\u4e86\u5e26\u6709\u95e8\u63a7\u673a\u5236\u7684\u5206\u5c42\u64cd\u4f5c\u56fe\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5012\u89d2\u8ddd\u79bb\u5b9e\u73b0\u4e0e\u76ee\u6807\u51e0\u4f55\u7684\u9ad8\u4fdd\u771f\u5bf9\u9f50\u3002  \n\u25c6 \u63d0\u51fa\u591a\u9636\u6bb5\u5e8f\u5217\u957f\u5ea6\u7ea6\u675f\u4e0e\u9886\u57df\u89c4\u5219\u60e9\u7f5a\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u771f\u5b9e\u5e8f\u5217\u6807\u6ce8\u7684\u65e0\u76d1\u7763\u7d27\u51d1\u5e8f\u5217\u5b66\u4e60\u3002  \n\u25c6 \u751f\u6210\u7684\u5e8f\u5217\u5177\u5907\u9ad8\u51e0\u4f55\u51c6\u786e\u6027\u3001\u5e73\u6ed1\u7f51\u683c\u7ed3\u6784\u3001\u5408\u7406\u6b65\u9aa4\u7ec4\u6210\u548c\u7075\u6d3b\u7f16\u8f91\u80fd\u529b\uff0c\u5b8c\u5168\u517c\u5bb9\u4e3b\u6d41\u8bbe\u8ba1\u8f6f\u4ef6\u6d41\u7a0b\u3002  \n\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86AI\u751f\u6210\u5185\u5bb9\u5728\u8bbe\u8ba1\u5b9e\u8df5\u4e2d\u7684\u53ef\u7528\u6027\u548c\u534f\u4f5c\u6548\u7387\u3002|\n",
    "2508.16932": "|2025-08-23|Align 3D Representation and Text Embedding for 3D Content Personalization|Qi Song\u7b49|[2508.16932](http://arxiv.org/pdf/2508.16932)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInvert3D\u7684\u65b0\u578b\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b33D\u5185\u5bb9\u9ad8\u6548\u4e2a\u6027\u5316\u8fd9\u4e00\u5173\u952e\u6311\u6218\u3002\u5176\u6838\u5fc3\u8d21\u732e\u4e0e\u521b\u65b0\u70b9\u5982\u4e0b\uff1a\n\n\u25c6 \u63d0\u51fa\u4e86\u4e00\u4e2a\u5c063D\u8868\u793a\u4e0e\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u7684\u521b\u65b0\u6846\u67b6\uff0c\u5f25\u5408\u4e862D\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0e3D\u5185\u5bb9\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u4e00\u79cd\u4ee5\u76f8\u673a\u4e3a\u6761\u4ef6\u76843D\u5230\u6587\u672c\u9006\u5411\u6620\u5c04\u673a\u5236\uff0c\u80fd\u591f\u5c063D\u5185\u5bb9\u6295\u5f71\u5230\u4e0e\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\u76843D\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\n\u25c6 \u5b9e\u73b0\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u76f4\u63a5\u5bf93D\u5185\u5bb9\u8fdb\u884c\u64cd\u4f5c\u548c\u4e2a\u6027\u5316\u5b9a\u5236\uff0c\u65e0\u9700\u4f9d\u8d56\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u7e41\u7410\u91cd\u8bad\u7ec3\u8fc7\u7a0b\u3002\n\u25c6 \u663e\u8457\u63d0\u5347\u4e863D\u5185\u5bb9\u4e2a\u6027\u5316\u7684\u6548\u7387\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u518d\u8bad\u7ec3\u9700\u6c42\u3002\n\u25c6 \u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a3D\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2508.18971": "|2025-08-26|Can we make NeRF-based visual localization privacy-preserving?|Maxime Pietrantoni\u7b49|[2508.18971](http://arxiv.org/pdf/2508.18971)|\u65e0|\u8be5\u8bba\u6587\u9488\u5bf9\u57fa\u4e8eNeRF\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5b58\u5728\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002  \n\u25c6 \u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u7cfb\u7edf\u68c0\u9a8cNeRF\u8868\u793a\u4e2d\u7684\u9690\u79c1\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u79fb\u9664\u989c\u8272\u9884\u6d4b\u5934\uff0c\u5176\u51e0\u4f55\u8868\u793a\u4ecd\u4f1a\u6cc4\u9732\u654f\u611f\u7ec6\u8282\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u63d0\u51fappNeRF\uff08\u9690\u79c1\u4fdd\u62a4\u795e\u7ecf\u5206\u5272\u573a\uff09\uff0c\u5c06NeRF\u7684\u4f20\u7edf\u5149\u5ea6\u76d1\u7763\u66ff\u6362\u4e3a\u5206\u5272\u6807\u7b7e\u76d1\u7763\uff0c\u907f\u514d\u76f4\u63a5\u5b66\u4e60\u539f\u59cb\u56fe\u50cf\u7eb9\u7406\u3002  \n\u25c6 \u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u5b66\u4e60\u5206\u5272\u6807\u7b7e\uff0c\u786e\u4fdd\u6807\u7b7e\u65e2\u4fdd\u7559\u8db3\u591f\u7684\u51e0\u4f55\u5224\u522b\u6027\u4ee5\u652f\u6301\u7cbe\u51c6\u5b9a\u4f4d\uff0c\u53c8\u5145\u5206\u6a21\u7cca\u5316\u7ec6\u8282\u4ee5\u4fdd\u62a4\u9690\u79c1\u3002  \n\u25c6 \u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9690\u79c1\u4fdd\u62a4\u6c34\u5e73\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002  \n\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5b9a\u4f4d\u7ed3\u679c\u3002|\n",
    "2508.18540": "|2025-08-25|Real-time 3D Visualization of Radiance Fields on Light Field Displays|Jonghyun Kim\u7b49|[2508.18540](http://arxiv.org/pdf/2508.18540)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5149\u573a\u663e\u793a\u5668\u7684\u5b9e\u65f6\u8f90\u5c04\u573a\u4e09\u7ef4\u53ef\u89c6\u5316\u7edf\u4e00\u6846\u67b6\u3002  \n\u25c6 \u5f00\u53d1\u4e86\u57fa\u4e8e\u5355\u904d\u5e73\u9762\u626b\u63cf\u7b56\u7565\u7684\u5171\u4eab\u67b6\u6784\uff0c\u9ad8\u6548\u652f\u6301\u591a\u79cd\u8f90\u5c04\u573a\u8868\u793a\uff08\u5982NeRF\u30013D\u9ad8\u65af\u6cfc\u6e85\u548c\u7a00\u758f\u4f53\u7d20\uff09\uff0c\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u91cd\u65b0\u8bad\u7ec3\u3002  \n\u25c6 \u901a\u8fc7\u7f13\u5b58\u975e\u65b9\u5411\u6027\u5171\u4eab\u7ec4\u4ef6\uff0c\u663e\u8457\u51cf\u5c11\u8de8\u89c6\u89d2\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe22\u500d\u7684\u6e32\u67d3\u52a0\u901f\u3002  \n\u25c6 \u572845\u4e2a\u89c6\u89d2\u4e0b\u4ee5512p\u5206\u8fa8\u7387\u8fbe\u5230200+ FPS\u7684\u5b9e\u65f6\u4ea4\u4e92\u6027\u80fd\uff0c\u5e76\u5728Looking Glass\u663e\u793a\u5668\u4e0a\u9a8c\u8bc1\u4e86\u6c89\u6d78\u5f0f\u4e09\u7ef4\u4ea4\u4e92\u5e94\u7528\u3002  \n\u25c6 \u5728\u63d0\u5347\u6e32\u67d3\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u5149\u573a\u663e\u793a\u591a\u89c6\u89d2\u6e32\u67d3\u4e0e\u8f90\u5c04\u573a\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f53\u79ef\u6e32\u67d3\u4e4b\u95f4\u7684\u96c6\u6210\u96be\u9898\u3002|\n",
    "2508.20526": "|2025-08-28|Adam SLAM - the last mile of camera calibration with 3DGS|Matthieu Gendrin\u7b49|[2508.20526](http://arxiv.org/pdf/2508.20526)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6a21\u578b\u4f18\u5316\u76f8\u673a\u6807\u5b9a\u7684\u65b0\u65b9\u6cd5\u3002  \n\u25c6 \u521b\u65b0\u6027\u5730\u901a\u8fc7\u65b0\u89c6\u56fe\u989c\u8272\u635f\u5931\u7684\u53cd\u5411\u4f20\u64ad\u6765\u7cbe\u7ec6\u8c03\u6574\u76f8\u673a\u53c2\u6570\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6807\u5b9a\u65b9\u6cd5\u7684\u7cbe\u5ea6\u9650\u5236\u3002  \n\u25c6 \u5c06\u6807\u5b9a\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5fae\u5206\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u6807\u5b9a\u4f18\u5316\u6d41\u7a0b\u3002  \n\u25c6 \u57283DGS\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53470.4 dB PSNR\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u89c6\u56fe\u5408\u6210\u8d28\u91cf\u3002  \n\u25c6 \u4e3a\u9ad8\u7cbe\u5ea6\u53c2\u8003\u573a\u666f\uff08\u5982Mip-NeRF 360\uff09\u7684\u6807\u5b9a\u63d0\u4f9b\u4e86\u4ee5\u6e32\u67d3\u8d28\u91cf\u4e3a\u6838\u5fc3\u7684\u65b0\u8303\u5f0f\u3002  \n\u8be5\u65b9\u6cd5\u867d\u9700\u8f83\u957f\u8c03\u4f18\u65f6\u95f4\uff0c\u4f46\u5bf9\u6807\u5b9a\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\u7684\u573a\u666f\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002|\n",
    "2509.00911": "|2025-09-03|GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency|Joongho Jo\u7b49|[2509.00911](http://arxiv.org/pdf/2509.00911)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GS-TG\uff0c\u4e00\u79cd\u7528\u4e8e\u52a0\u901f3D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u901a\u8fc7\u4f18\u5316\u6392\u5e8f\u4e0e\u5149\u6805\u5316\u8fc7\u7a0b\u6765\u63d0\u5347\u6e32\u67d3\u901f\u5ea6\u3002  \n\u25c6 \u63d0\u51fa\u57fa\u4e8e\u74e6\u7247\u5206\u7ec4\uff08Tile Grouping\uff09\u7684\u6392\u5e8f\u7b56\u7565\uff0c\u5c06\u591a\u4e2a\u5c0f\u74e6\u7247\u7ec4\u5408\u6210\u5927\u7ec4\u4ee5\u5171\u4eab\u6392\u5e8f\u64cd\u4f5c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5197\u4f59\u8ba1\u7b97\u3002  \n\u25c6 \u5f15\u5165\u4f4d\u63a9\u7801\uff08Bitmask\uff09\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u56fe\u5143\u6807\u8bb0\u5176\u6240\u5c5e\u7684\u6709\u6548\u5c0f\u74e6\u7247\uff0c\u4f7f\u5f97\u5149\u6805\u5316\u9636\u6bb5\u4ecd\u53ef\u6309\u539f\u59cb\u5c0f\u74e6\u7247\u9ad8\u6548\u6267\u884c\uff0c\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684\u50cf\u7d20\u8ba1\u7b97\u3002  \n\u25c6 \u5b9e\u73b0\u4e86\u6392\u5e8f\u9636\u6bb5\u4f7f\u7528\u201c\u5927\u74e6\u7247\u201d\u903b\u8f91\u4ee5\u964d\u4f4e\u6392\u5e8f\u5f00\u9500\uff0c\u800c\u5149\u6805\u5316\u9636\u6bb5\u4fdd\u6301\u201c\u5c0f\u74e6\u7247\u201d\u7c92\u5ea6\u4ee5\u7ef4\u6301\u6e32\u67d3\u6548\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u74e6\u7247\u5c3a\u5bf8\u589e\u5927\u5bfc\u81f4\u5149\u6805\u5316\u8ba1\u7b97\u91cf\u4e0a\u5347\u7684\u77db\u76fe\u3002  \n\u25c6 \u8be5\u65b9\u6cd5\u662f\u65e0\u635f\u7684\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u5e76\u53ef\u4e0e\u5176\u4ed6\u73b0\u6709\u4f18\u5316\u6280\u672f\u65e0\u7f1d\u96c6\u6210\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGS-TG\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u52a0\u901f\u5668\u5e73\u5747\u5b9e\u73b0\u4e861.54\u500d\u7684\u6e32\u67d3\u901f\u5ea6\u63d0\u5347\u3002|\n",
    "2509.00800": "|2025-08-31|SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting|Zhuodong Jiang\u7b49|[2509.00800](http://arxiv.org/pdf/2509.00800)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5f15\u5bfc\u4e0e3D\u9ad8\u65af\u6e85\u5c04\u7684\u6c34\u4e0b\u573a\u666f\u589e\u5f3a\u91cd\u5efa\u65b9\u6cd5SWAGSplatting\uff0c\u6838\u5fc3\u89e3\u51b3\u4e86\u6c34\u4e0b\u73af\u5883\u56e0\u5149\u7ebf\u626d\u66f2\u3001\u6d51\u6d4a\u548c\u4f4e\u53ef\u89c1\u5ea6\u5bfc\u81f4\u76843D\u91cd\u5efa\u96be\u9898\u3002\u5176\u521b\u65b0\u70b9\u5305\u62ec\uff1a\n\u25c6 \u5f15\u5165\u591a\u6a21\u6001\u8de8\u77e5\u8bc6\u878d\u5408\u673a\u5236\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\uff08CLIP\uff09\u63d0\u53d6\u7684\u8bed\u4e49\u7279\u5f81\u5d4c\u5165\u6bcf\u4e2a\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u8bed\u4e49\u4e0e\u7ed3\u6784\u611f\u77e5\u7684\u8054\u5408\u4f18\u5316\u3002\n\u25c6 \u8bbe\u8ba1\u4e13\u7528\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u91cd\u5efa\u7ed3\u679c\u4e0e\u9ad8\u5c42\u573a\u666f\u7406\u89e3\u4fdd\u6301\u4e00\u81f4\uff0c\u63d0\u5347\u91cd\u5efa\u7684\u8bed\u4e49\u51c6\u786e\u6027\u3002\n\u25c6 \u63d0\u51fa\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u7531\u7c97\u5230\u7ec6\u7684\u5b66\u4e60\u4e0e\u540e\u671f\u53c2\u6570\u7ec6\u5316\uff0c\u663e\u8457\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002\n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SeaThru-NeRF\u548cSubmerged3D\u6570\u636e\u96c6\u4e0a\u5168\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cPSNR\u6307\u6807\u5e73\u5747\u63d0\u5347\u6700\u9ad8\u8fbe3.09 dB\uff0c\u4e3a\u6c34\u4e0b\u52d8\u63a2\u548c\u6d77\u6d0b\u611f\u77e5\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002|\n",
    "2509.07809": "|2025-09-09|SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting|Mahtab Dahaghin\u7b49|[2509.07809](http://arxiv.org/pdf/2509.07809)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSplatFill\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u76843D\u573a\u666f\u4fee\u590d\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u5931\u533a\u57df\u4fee\u590d\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002  \n\u25c6 \u5f15\u5165\u6df1\u5ea6\u5f15\u5bfc\u4e0e\u7269\u4f53\u611f\u77e5\u7684\u8054\u5408\u76d1\u7763\u673a\u5236\uff0c\u786e\u4fdd\u4fee\u590d\u7684\u9ad8\u65af\u70b9\u4e91\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u4f4d\u7f6e\u51c6\u786e\u4e14\u4e0e\u5468\u56f4\u51e0\u4f55\u5bf9\u9f50\u3002  \n\u25c6 \u63d0\u51fa\u4e00\u81f4\u6027\u611f\u77e5\u7ec6\u5316\u65b9\u6848\uff0c\u80fd\u667a\u80fd\u8bc6\u522b\u5e76\u4fee\u6b63\u4e0d\u4e00\u81f4\u533a\u57df\uff0c\u907f\u514d\u5bf9\u573a\u666f\u5176\u4ed6\u90e8\u5206\u9020\u6210\u5e72\u6270\u3002  \n\u25c6 \u5728SPIn-NeRF\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u4e8eNeRF\u548c3DGS\u7684\u4fee\u590d\u65b9\u6cd5\u3002  \n\u25c6 \u8bad\u7ec3\u6548\u7387\u63d0\u534724.5%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u6e05\u6670\u7684\u7ec6\u8282\u3001\u66f4\u5c11\u7684\u4f2a\u5f71\u548c\u66f4\u597d\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002|\n",
    "2509.07493": "|2025-09-09|DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning|Wenzhi Guo\u7b49|[2509.07493](http://arxiv.org/pdf/2509.07493)|\u65e0|\u672c\u6587\u63d0\u51faDiGS\u6846\u67b6\uff0c\u5c063D\u9ad8\u65af\u8868\u793a\u4e0e\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece3D\u9ad8\u65af\u6a21\u578b\u4e2d\u91cd\u5efa\u7cbe\u786e\u5b8c\u6574\u8868\u9762\u7684\u80fd\u529b\u3002  \n\u25c6 \u9996\u6b21\u5c06\u53ef\u5b66\u4e60SDF\u5d4c\u51653D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u6d41\u7a0b\uff0c\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u57fa\u5143\u8d4b\u4e88\u51e0\u4f55\u610f\u4e49\uff0c\u589e\u5f3a\u4e86\u89e3\u91ca\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002  \n\u25c6 \u63d0\u51fa\u51e0\u4f55\u5f15\u5bfc\u7684\u7f51\u683c\u589e\u957f\u7b56\u7565\uff0c\u5728\u591a\u5c3a\u5ea6\u5c42\u6b21\u4e0b\u81ea\u9002\u5e94\u6cbf\u51e0\u4f55\u4e00\u81f4\u533a\u57df\u5206\u5e03\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u66f4\u5bc6\u96c6\u548c\u8fde\u8d2f\u7684\u8868\u9762\u8986\u76d6\u3002  \n\u25c6 \u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u9ad8\u65af\u57fa\u5143\u4e0e\u5e95\u5c42\u51e0\u4f55\uff0c\u6709\u6548\u6539\u5584\u4e86\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u91cd\u5efa\u5b8c\u6574\u6027\u3002  \n\u5b9e\u9a8c\u8bc1\u660e\uff0cDiGS\u5728DTU\u3001Mip-NeRF 360\u7b49\u591a\u4e2a\u57fa\u51c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e32\u67d3\u8d28\u91cf\u3002|\n",
    "2509.11275": "|2025-09-14|ROSGS: Relightable Outdoor Scenes With Gaussian Splatting|Lianjun Liao\u7b49|[2509.11275](http://arxiv.org/pdf/2509.11275)|\u65e0|ROSGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u7528\u4e8e\u6237\u5916\u53ef\u91cd\u5149\u7167\u573a\u666f\u7684\u9ad8\u6548\u91cd\u5efa\u4e0e\u6e32\u67d3\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5149\u7167\u5efa\u6a21\u7cbe\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002  \n\u25c6 \u91c7\u7528\u7d27\u51d1\u7684\u4e8c\u7ef4\u9ad8\u65af\u6e85\u5c04\uff082DGS\uff09\u8868\u793a\u7ed3\u5408\u5355\u76ee\u6cd5\u5411\u5148\u9a8c\uff0c\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u91cd\u5efa\u573a\u666f\u51e0\u4f55\u7ed3\u6784\u3002  \n\u25c6 \u63d0\u51fa\u6df7\u5408\u5149\u7167\u6a21\u578b\uff0c\u5206\u522b\u7528\u7403\u9762\u9ad8\u65af\u51fd\u6570\u523b\u753b\u65b9\u5411\u6027\u7684\u9ad8\u9891\u9633\u5149\u6210\u5206\uff0c\u5e76\u901a\u8fc7\u7403\u8c10\u7cfb\u6570\u5b66\u4e60\u8f90\u5c04\u4f20\u8f93\u51fd\u6570\u4ee5\u5efa\u6a21\u4f4e\u9891\u5929\u5149\u3002  \n\u25c6 \u5728\u4fdd\u6301\u9ad8\u6e32\u67d3\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65e0\u7ea6\u675f\u6237\u5916\u7167\u660e\u6761\u4ef6\u7684\u9ad8\u7cbe\u5ea6\u5206\u89e3\u4e0e\u91cd\u5149\u7167\u3002  \n\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u5bf9\u6bd4\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6237\u5916\u91cd\u5149\u7167\u6027\u80fd\uff0c\u517c\u987e\u4e86\u6e32\u67d3\u901f\u5ea6\u4e0e\u771f\u5b9e\u611f\u3002|\n",
    "2509.11171": "|2025-09-14|SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion|Zhiwen Yang\u7b49|[2509.11171](http://arxiv.org/pdf/2509.11171)|\u65e0|SPHERE\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u76f8\u673a3D\u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u7684\u65b0\u8868\u5f81\u65b9\u6cd5\uff0c\u65e8\u5728\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u51c6\u786e\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u7684\u771f\u5b9e\u6027\u3002\u5176\u6838\u5fc3\u8d21\u732e\u662f\u521b\u65b0\u6027\u5730\u878d\u5408\u4e86\u4f53\u7d20\u4e0e\u9ad8\u65af\u8868\u5f81\uff0c\u4ee5\u8054\u5408\u5229\u7528\u8bed\u4e49\u548c\u7269\u7406\u4fe1\u606f\u3002\n\u25c6 \u63d0\u51fa\u8bed\u4e49-\u7269\u7406\u878d\u5408\u8868\u5f81(SPHERE)\uff0c\u5c06\u663e\u5f0f\u4f53\u7d20\u4e0e\u9690\u5f0f\u9ad8\u65af\u8868\u5f81\u76f8\u7ed3\u5408\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u89c4\u5f8b\u6355\u83b7\u6216\u8bed\u4e49\u7cbe\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002\n\u25c6 \u8bbe\u8ba1\u4e86\u8bed\u4e49\u5f15\u5bfc\u7684\u9ad8\u65af\u521d\u59cb\u5316(SGI)\u6a21\u5757\uff0c\u5229\u7528\u53cc\u5206\u652f\u8868\u5f81\u5b9a\u4f4d\u7126\u70b9\u4f53\u7d20\u4f5c\u4e3a\u951a\u70b9\uff0c\u6307\u5bfc\u9ad8\u6548\u7684\u9ad8\u65af\u521d\u59cb\u5316\uff0c\u63d0\u5347\u4e86\u5904\u7406\u6548\u7387\u3002\n\u25c6 \u5f00\u53d1\u4e86\u7269\u7406\u611f\u77e5\u7684\u8c10\u6ce2\u589e\u5f3a(PHE)\u6a21\u5757\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u7403\u8c10\u51fd\u6570\u6765\u5efa\u6a21\u7269\u7406\u4e0a\u4e0b\u6587\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u7126\u70b9\u5206\u5e03\u5bf9\u9f50\u4fc3\u8fdb\u8bed\u4e49-\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u751f\u6210\u5177\u6709\u903c\u771f\u7ec6\u8282\u7684\u7ed3\u679c\u3002\n\u8be5\u65b9\u6cd5\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u6709\u6548\u5e73\u8861\u4e86\u573a\u666f\u8865\u5168\u7684\u8bed\u4e49\u51c6\u786e\u6027\u4e0e\u51e0\u4f55\u771f\u5b9e\u6027\u3002|\n",
    "2509.11169": "|2025-09-14|Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields|Hong Zhang\u7b49|[2509.11169](http://arxiv.org/pdf/2509.11169)|\u65e0|\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Multispectral-NeRF\uff0c\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u591a\u5149\u8c31\u4e09\u7ef4\u91cd\u5efa\u65b0\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u89e3\u51b3\u4e86\u73b0\u6709NeRF\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u591a\u6ce2\u6bb5\u5149\u8c31\u6570\u636e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5b9e\u73b0\u4e86\u5bf96\u6ce2\u6bb5\u5149\u8c31\u6570\u636e\u7684\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u3002  \n\u25c6 \u6269\u5c55\u4e86\u795e\u7ecf\u7f51\u7edc\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c\u4f7f\u5176\u80fd\u591f\u76f4\u63a5\u5904\u74066\u6ce2\u6bb5\u5149\u8c31\u8f93\u5165\u6570\u636e\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u4e09\u6ce2\u6bb5\uff08RGB\uff09\u7684\u9650\u5236\u3002  \n\u25c6 \u91cd\u65b0\u8bbe\u8ba1\u4e86\u6b8b\u5dee\u51fd\u6570\uff0c\u4f18\u5316\u4e86\u91cd\u5efa\u56fe\u50cf\u4e0e\u53c2\u8003\u56fe\u50cf\u4e4b\u95f4\u7684\u5149\u8c31\u5dee\u5f02\u8ba1\u7b97\uff0c\u63d0\u5347\u4e86\u591a\u5149\u8c31\u7279\u5f81\u7684\u8fd8\u539f\u7cbe\u5ea6\u3002  \n\u25c6 \u8c03\u6574\u4e86\u6570\u636e\u538b\u7f29\u6a21\u5757\uff0c\u4ee5\u9002\u5e94\u591a\u5149\u8c31\u56fe\u50cf\u66f4\u9ad8\u6bd4\u7279\u6df1\u5ea6\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u786e\u4fdd\u6570\u636e\u5904\u7406\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002  \n\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u539f\u59cb\u573a\u666f\u51e0\u4f55\u7279\u5f81\u7684\u540c\u65f6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u6ce2\u6bb5\u5149\u8c31\u7279\u5f81\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5149\u8c31\u4e09\u7ef4\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002|\n",
    "2509.12836": "|2025-09-16|Exploring Metric Fusion for Evaluation of NeRFs|Shreyas Shivakumara\u7b49|[2509.12836](http://arxiv.org/pdf/2509.12836)|\u65e0|\u25c6 Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints.\n\u25c6 Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets.\n\u25c6 We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores.|\n",
    "2509.12458": "|2025-09-15|Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles|\u00c0lmos Veres-Vit\u00e0lyos\u7b49|[2509.12458](http://arxiv.org/pdf/2509.12458)|\u65e0|\u25c6 Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction.\n\u25c6 To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams.\n\u25c6 Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control.|\n",
    "2509.13571": "|2025-09-16|SuNeRF-CME: Physics-Informed Neural Radiance Fields for Tomographic Reconstruction of Coronal Mass Ejections|Robert Jarolim\u7b49|[2509.13571](http://arxiv.org/pdf/2509.13571)|\u65e0|\u25c6 Coronagraphic observations enable direct monitoring of coronal mass ejections (CMEs) through scattered light from free electrons, but determining the 3D plasma distribution from 2D imaging data is challenging due to the optically-thin plasma and the complex image formation processes.\n\u25c6 We introduce SuNeRF-CME, a framework for 3D tomographic reconstructions of the heliosphere using multi-viewpoint coronagraphic observations.\n\u25c6 The method leverages Neural Radiance Fields (NeRFs) to estimate the electron density in the heliosphere through a ray-tracing approach, while accounting for the underlying Thomson scattering of image formation.|\n",
    "2509.15123": "|2025-09-19|RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes|Fang Li\u7b49|[2509.15123](http://arxiv.org/pdf/2509.15123)|\u65e0|\u25c6 Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes.\n\u25c6 Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos.\n\u25c6 In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video.|\n",
    "2509.14890": "|2025-09-18|NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation|Antoine Legrand\u7b49|[2509.14890](http://arxiv.org/pdf/2509.14890)|\u65e0|\u25c6 On-orbit operations require the estimation of the relative 6D pose, i.e., position and orientation, between a chaser spacecraft and its target.\n\u25c6 While data-driven spacecraft pose estimation methods have been developed, their adoption in real missions is hampered by the lack of understanding of their decision process.\n\u25c6 This paper presents a method to visualize the 3D visual cues on which a given pose estimator relies.|\n",
    "2509.15548": "|2025-09-26|MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild|Deming Li\u7b49|[2509.15548](http://arxiv.org/pdf/2509.15548)|\u65e0|\u25c6 In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis.\n\u25c6 Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting.\n\u25c6 In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS.|\n",
    "2509.15242": "|2025-09-17|ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images|Jaydeep Rade\u7b49|[2509.15242](http://arxiv.org/pdf/2509.15242)|\u65e0|\u25c6 AI-based in silico methods have improved protein structure prediction but often struggle with large protein complexes (PCs) involving multiple interacting proteins due to missing 3D spatial cues.\n\u25c6 Experimental techniques like Cryo-EM are accurate but costly and time-consuming.\n\u25c6 We present ProFusion, a hybrid framework that integrates a deep learning model with Atomic Force Microscopy (AFM), which provides high-resolution height maps from random orientations, naturally yielding multi-view data for 3D reconstruction.|\n",
    "2509.17789": "|2025-09-22|From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes|Guoxi Huang\u7b49|[2509.17789](http://arxiv.org/pdf/2509.17789)|\u65e0|\u25c6 Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes.\n\u25c6 We propose \\textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity.\n\u25c6 Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline.|\n",
    "2509.17232": "|2025-09-21|DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction|Bo Liu\u7b49|[2509.17232](http://arxiv.org/pdf/2509.17232)|\u65e0|\u25c6 This paper proposes a Diffusion Model-Optimized Neural Radiance Field (DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency in 3D scene reconstruction.\n\u25c6 By combining diffusion models with Transformers, DT-NeRF effectively restores details under sparse viewpoints and maintains high accuracy in complex geometric scenes.\n\u25c6 Experimental results demonstrate that DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art methods on the Matterport3D and ShapeNet datasets, particularly in metrics such as PSNR, SSIM, Chamfer Distance, and Fidelity.|\n",
    "2509.17083": "|2025-09-23|HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis|Zipeng Wang\u7b49|[2509.17083](http://arxiv.org/pdf/2509.17083)|\u65e0|\u25c6 Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians.\n\u25c6 However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes.\n\u25c6 While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details.|\n",
    "2509.16922": "|2025-09-21|PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control|Tianheng Zhu\u7b49|[2509.16922](http://arxiv.org/pdf/2509.16922)|\u65e0|\u25c6 Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production.\n\u25c6 While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization.\n\u25c6 This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS).|\n",
    "2509.19073": "|2025-09-23|WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction|Hung Nguyen\u7b49|[2509.19073](http://arxiv.org/pdf/2509.19073)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings.\n\u25c6 Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization.\n\u25c6 While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps.|\n",
    "2509.18956": "|2025-09-23|Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting|Zijing Guo\u7b49|[2509.18956](http://arxiv.org/pdf/2509.18956)|\u65e0|\u25c6 Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies.\n\u25c6 While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors.\n\u25c6 Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections.|\n",
    "2509.25075": "|2025-10-02|GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction|Huaizhi Qu\u7b49|[2509.25075](http://arxiv.org/pdf/2509.25075)|\u65e0|\u25c6 Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive.\n\u25c6 Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead.\n\u25c6 Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency.|\n",
    "2509.23555": "|2025-09-28|From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations|Javed Ahmad\u7b49|[2509.23555](http://arxiv.org/pdf/2509.23555)|\u65e0|\u25c6 Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted.\n\u25c6 NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding.\n\u25c6 This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation.|\n",
    "2509.23438": "|2025-09-30|FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation|Mohammed Alsakabi\u7b49|[2509.23438](http://arxiv.org/pdf/2509.23438)|\u65e0|\u25c6 Existing periodic activation-based implicit neural representation (INR) networks, such as SIREN and FINER, suffer from hidden feature redundancy, where neurons within a layer capture overlapping frequency components due to the use of a fixed frequency multiplier.\n\u25c6 This redundancy limits the expressive capacity of multilayer perceptrons (MLPs).\n\u25c6 Drawing inspiration from classical signal processing methods such as the Discrete Sine Transform (DST), we propose FM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency multipliers to periodic activations.|\n",
    "2509.23258": "|2025-10-04|OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting|Atakan Topaloglu\u7b49|[2509.23258](http://arxiv.org/pdf/2509.23258)|\u65e0|\u25c6 Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity.\n\u25c6 Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies.\n\u25c6 We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting.|\n",
    "2509.25191": "|2025-10-08|VGGT-X: When VGGT Meets Dense Novel View Synthesis|Yang Liu\u7b49|[2509.25191](http://arxiv.org/pdf/2509.25191)|\u65e0|\u25c6 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS).\n\u25c6 Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures.\n\u25c6 Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS.|\n",
    "2510.02314": "|2025-10-02|StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions|Bo-Hsu Ke\u7b49|[2510.02314](http://arxiv.org/pdf/2510.02314)|\u65e0|\u25c6 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis.\n\u25c6 As these methods become prevalent, addressing their vulnerabilities becomes critical.\n\u25c6 We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method.|\n",
    "2510.00592": "|2025-10-01|Multi-level Dynamic Style Transfer for NeRFs|Zesheng Li\u7b49|[2510.00592](http://arxiv.org/pdf/2510.00592)|\u65e0|\u25c6 As the application of neural radiance fields (NeRFs) in various 3D vision tasks continues to expand, numerous NeRF-based style transfer techniques have been developed.\n\u25c6 However, existing methods typically integrate style statistics into the original NeRF pipeline, often leading to suboptimal results in both content preservation and artistic stylization.\n\u25c6 In this paper, we present multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that reengineers the NeRF pipeline specifically for stylization and incorporates an innovative dynamic style injection module.|\n",
    "2510.03163": "|2025-10-03|ROGR: Relightable 3D Objects using Generative Relighting|Jiapeng Tang\u7b49|[2510.03163](http://arxiv.org/pdf/2510.03163)|\u65e0|\u25c6 We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations.\n\u25c6 Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting.\n\u25c6 The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately.|\n",
    "2510.07667": "|2025-10-09|An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit Data Reuse Strategies|Binzhe Yuan\u7b49|[2510.07667](http://arxiv.org/pdf/2510.07667)|\u65e0|\u25c6 Neural radiance fields (NeRF) have transformed 3D reconstruction and rendering, facilitating photorealistic image synthesis from sparse viewpoints.\n\u25c6 This work introduces an explicit data reuse neural rendering (EDR-NR) architecture, which reduces frequent external memory accesses (EMAs) and cache misses by exploiting the spatial locality from three phases, including rays, ray packets (RPs), and samples.\n\u25c6 The EDR-NR architecture features a four-stage scheduler that clusters rays on the basis of Z-order, prioritize lagging rays when ray divergence happens, reorders RPs based on spatial proximity, and issues samples out-of-orderly (OoO) according to the availability of on-chip feature data.|\n",
    "2510.10993": "|2025-10-13|Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency|Yuxin Cheng\u7b49|[2510.10993](http://arxiv.org/pdf/2510.10993)|\u65e0|\u25c6 3D Gaussian inpainting, a critical technique for numerous applications in virtual reality and multimedia, has made significant progress with pretrained diffusion models.\n\u25c6 However, ensuring multi-view consistency, an essential requirement for high-quality inpainting, remains a key challenge.\n\u25c6 In this work, we present PAInpainter, a novel approach designed to advance 3D Gaussian inpainting by leveraging perspective-aware content propagation and consistency verification across multi-view inpainted images.|\n",
    "2510.10257": "|2025-10-11|Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting|Abdelrhman Elrawy\u7b49|[2510.10257](http://arxiv.org/pdf/2510.10257)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its standard adaptive density control (ADC) can lead to overfitting and bloated reconstructions.\n\u25c6 While state-of-the-art methods like FSGS improve quality, they often do so by significantly increasing the primitive count.\n\u25c6 This paper presents a framework that revises the core 3DGS optimization to prioritize efficiency.|\n",
    "2510.10097": "|2025-10-27|Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting|Jiahui Lu\u7b49|[2510.10097](http://arxiv.org/pdf/2510.10097)|\u65e0|\u25c6 Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced 3D reconstruction and novel view synthesis, but remain heavily dependent on accurate camera poses and dense viewpoint coverage.\n\u25c6 These requirements limit their applicability in sparse-view settings, where pose estimation becomes unreliable and supervision is insufficient.\n\u25c6 To overcome these challenges, we introduce Gesplat, a 3DGS-based framework that enables robust novel view synthesis and geometrically consistent reconstruction from unposed sparse images.|\n",
    "2510.09880": "|2025-10-10|Geometry-Aware Scene Configurations for Novel View Synthesis|Minkwan Kim\u7b49|[2510.09880](http://arxiv.org/pdf/2510.09880)|\u65e0|\u25c6 We propose scene-adaptive strategies to efficiently allocate representation capacity for generating immersive experiences of indoor environments from incomplete observations.\n\u25c6 Indoor scenes with multiple rooms often exhibit irregular layouts with varying complexity, containing clutter, occlusion, and flat walls.\n\u25c6 We maximize the utilization of limited resources with guidance from geometric priors, which are often readily available after pre-processing stages.|\n",
    "2510.09586": "|2025-10-10|Vision Language Models: A Survey of 26K Papers|Fengming Lin|[2510.09586](http://arxiv.org/pdf/2510.09586)|\u65e0|\u25c6 We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025.\n\u25c6 Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities.\n\u25c6 The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding.|\n",
    "2510.09010": "|2025-10-10|HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization|Yipu Zhang\u7b49|[2510.09010](http://arxiv.org/pdf/2510.09010)|\u65e0|\u25c6 Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction method, delivering high-quality results for AR/VR applications.\n\u25c6 While quantization methods and hardware accelerators have been proposed to enhance NeRF's computational efficiency, existing approaches face crucial limitations.\n\u25c6 Current quantization methods operate without considering hardware architecture, resulting in sub-optimal solutions within the vast design space encompassing accuracy, latency, and model size.|\n",
    "2510.12901": "|2025-10-16|SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms|Haithem Turki\u7b49|[2510.12901](http://arxiv.org/pdf/2510.12901)|\u65e0|\u25c6 Rigorous testing of autonomous robots, such as self-driving vehicles, is essential to ensure their safety in real-world deployments.\n\u25c6 This requires building high-fidelity simulators to test scenarios beyond those that can be safely or exhaustively collected in the real-world.\n\u25c6 Existing neural rendering methods based on NeRF and 3DGS hold promise but suffer from low rendering speeds or can only render pinhole camera models, hindering their suitability to applications that commonly require high-distortion lenses and LiDAR data.|\n",
    "2510.14270": "|2025-11-03|GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering|Alexander Valverde\u7b49|[2510.14270](http://arxiv.org/pdf/2510.14270)|\u65e0|\u25c6 Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress.\n\u25c6 While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.\n\u25c6 In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction.|\n",
    "2510.19371": "|2025-10-22|AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields|Woo Jae Kim\u7b49|[2510.19371](http://arxiv.org/pdf/2510.19371)|\u65e0|\u25c6 As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D scene representation and novel view synthesis, protecting their intellectual property (IP) from unauthorized use is becoming increasingly crucial.\n\u25c6 In this work, we aim to protect the IP of NeRFs by injecting adversarial perturbations that disrupt their unauthorized applications.\n\u25c6 However, perturbing the 3D geometry of NeRFs can easily deform the underlying scene structure and thus substantially degrade the rendering quality, which has led existing attempts to avoid geometric perturbations or restrict them to explicit spaces like meshes.|\n",
    "2510.19255": "|2025-10-22|Advances in 4D Representation: Geometry, Motion, and Interaction|Mingrui Zhao\u7b49|[2510.19255](http://arxiv.org/pdf/2510.19255)|\u65e0|\u25c6 We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI).\n\u25c6 While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\\/}, to model 3D geometry evolving over time while exhibiting motion and interaction.\n\u25c6 Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios.|\n",
    "2510.20558": "|2025-10-23|From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail|Xiaohan Sun\u7b49|[2510.20558](http://arxiv.org/pdf/2510.20558)|\u65e0|\u25c6 In this paper, we investigate how users perceive the visual quality of crowd character representations at different levels of detail (LoD) and viewing distances.\n\u25c6 Each representation: geometric meshes, image-based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between visual fidelity and computational performance.\n\u25c6 Our qualitative and quantitative results provide insights to guide the design of perceptually optimized LoD strategies for crowd rendering.|\n",
    "2510.20027": "|2025-10-22|Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses|Damian Bowness\u7b49|[2510.20027](http://arxiv.org/pdf/2510.20027)|\u65e0|\u25c6 When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs.\n\u25c6 These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model.\n\u25c6 To address this issue, we propose a novel real-time render-aware filtering method.|\n",
    "2510.22161": "|2025-10-25|I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions|Shuhong Liu\u7b49|[2510.22161](http://arxiv.org/pdf/2510.22161)|\u65e0|\u25c6 Participating in efforts to endow generative AI with the 3D physical world perception, we propose I2-NeRF, a novel neural radiance field framework that enhances isometric and isotropic metric perception under media degradation.\n\u25c6 While existing NeRF models predominantly rely on object-centric sampling, I2-NeRF introduces a reverse-stratified upsampling strategy to achieve near-uniform sampling across 3D space, thereby preserving isometry.\n\u25c6 We further present a general radiative formulation for media degradation that unifies emission, absorption, and scattering into a particle model governed by the Beer-Lambert attenuation law.|\n",
    "2510.25319": "|2025-10-29|4-Doodle: Text to 3D Sketches that Move!|Hao Chen\u7b49|[2510.25319](http://arxiv.org/pdf/2510.25319)|\u65e0|\u25c6 We present a novel task: text-to-3D sketch animation, which aims to bring freeform sketches to life in dynamic 3D space.\n\u25c6 Unlike prior works focused on photorealistic content generation, we target sparse, stylized, and view-consistent 3D vector sketches, a lightweight and interpretable medium well-suited for visual communication and prototyping.\n\u25c6 However, this task is very challenging: (i) no paired dataset exists for text and 3D (or 4D) sketches; (ii) sketches require structural abstraction that is difficult to model with conventional 3D representations like NeRFs or point clouds; and (iii) animating such sketches demands temporal coherence and multi-view consistency, which current pipelines do not address.|\n",
    "2510.27318": "|2025-10-31|SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction|Wenfeng Huang\u7b49|[2510.27318](http://arxiv.org/pdf/2510.27318)|\u65e0|\u25c6 Surgical reconstruction of dynamic tissues from endoscopic videos is a crucial technology in robot-assisted surgery.\n\u25c6 The development of Neural Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction, achieving high-quality results from video and image sequences.\n\u25c6 However, reconstructing deformable endoscopic scenes remains challenging due to aliasing and artifacts caused by tissue movement, which can significantly degrade visualization quality.|\n",
    "2511.02510": "|2025-11-04|LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization|Jee Won Lee\u7b49|[2511.02510](http://arxiv.org/pdf/2511.02510)|\u65e0|\u25c6 Sparse-voxel rasterization is a fast, differentiable alternative for optimization-based scene reconstruction, but it tends to underfit low-frequency content, depends on brittle pruning heuristics, and can overgrow in ways that inflate VRAM.\n\u25c6 We introduce LiteVoxel, a self-tuning training pipeline that makes SV rasterization both steadier and lighter.\n\u25c6 Our loss is made low-frequency aware via an inverse-Sobel reweighting with a mid-training gamma-ramp, shifting gradient budget to flat regions only after geometry stabilize.|\n",
    "2511.02207": "|2025-11-04|Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping|Jiajia Li\u7b49|[2511.02207](http://arxiv.org/pdf/2511.02207)|\u65e0|\u25c6 Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value.\n\u25c6 Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics.\n\u25c6 However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive.|\n",
    "2511.04283": "|2025-11-06|FastGS: Training 3D Gaussian Splatting in 100 Seconds|Shiwei Ren\u7b49|[2511.04283](http://arxiv.org/pdf/2511.04283)|\u65e0|\u25c6 The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead.\n\u25c6 In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality.\n\u25c6 We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism.|\n",
    "2511.05229": "|2025-11-07|4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos|Mengqi Guo\u7b49|[2511.05229](http://arxiv.org/pdf/2511.05229)|\u65e0|\u25c6 Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics.\n\u25c6 While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses.\n\u25c6 We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach.|\n",
    "2511.05109": "|2025-11-07|Efficient representation of 3D spatial data for defense-related applications|Benjamin Kahl\u7b49|[2511.05109](http://arxiv.org/pdf/2511.05109)|\u65e0|\u25c6 Geospatial sensor data is essential for modern defense and security, offering indispensable 3D information for situational awareness.\n\u25c6 This data, gathered from sources like lidar sensors and optical cameras, allows for the creation of detailed models of operational environments.\n\u25c6 In this paper, we provide a comparative analysis of traditional representation methods, such as point clouds, voxel grids, and triangle meshes, alongside modern neural and implicit techniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS).|\n",
    "2511.04797": "|2025-11-06|3D Gaussian Point Encoders|Jim James\u7b49|[2511.04797](http://arxiv.org/pdf/2511.04797)|\u65e0|\u25c6 In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians.\n\u25c6 This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet.\n\u25c6 However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers.|\n",
    "2511.07122": "|2025-11-10|Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction|Changyue Shi\u7b49|[2511.07122](http://arxiv.org/pdf/2511.07122)|\u65e0|\u25c6 Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction.\n\u25c6 However, these approaches rely on dense-frame video sequences for photorealistic reconstruction.\n\u25c6 In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible.|\n",
    "2511.06457": "|2025-11-09|Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360\u00b0 Scenes|Shaoxiang Wang\u7b49|[2511.06457](http://arxiv.org/pdf/2511.06457)|\u65e0|\u25c6 Despite recent advances in single-object front-facing inpainting using NeRF and 3D Gaussian Splatting (3DGS), inpainting in complex 360{\\deg} scenes remains largely underexplored.\n\u25c6 This is primarily due to three key challenges: (i) identifying target objects in the 3D field of 360{\\deg} environments, (ii) dealing with severe occlusions in multi-object scenes, which makes it hard to define regions to inpaint, and (iii) maintaining consistent and high-quality appearance across views effectively.\n\u25c6 To tackle these challenges, we propose Inpaint360GS, a flexible 360{\\deg} editing framework based on 3DGS that supports multi-object removal and high-fidelity inpainting in 3D space.|\n",
    "2511.06408": "|2025-11-09|VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes|Zhengyu Zou\u7b49|[2511.06408](http://arxiv.org/pdf/2511.06408)|\u65e0|\u25c6 Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional scenes using a set of images with known camera poses, enabling the rendering of photorealistic novel views.\n\u25c6 However, existing NeRF-based methods encounter challenges in applications such as autonomous driving and robotic perception, primarily due to the difficulty of capturing accurate camera poses and limitations in handling large-scale dynamic environments.\n\u25c6 To address these issues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately recovers camera trajectories and learns spatiotemporal representations for dynamic urban scenes without requiring additional camera pose information or expensive sensor data.|\n",
    "2502.09623": "|2025-05-30|Weight Space Representation Learning on Diverse NeRF Architectures|Francesco Ballerini\u7b49|[2502.09623](http://arxiv.org/pdf/2502.09623)|\u65e0|\u25c6 Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network.\n\u25c6 Recent studies have demonstrated that these weights can be used as input for frameworks designed to address deep learning tasks; however, such frameworks require NeRFs to adhere to a specific, predefined architecture.\n\u25c6 In this paper, we introduce the first framework capable of processing NeRFs with diverse architectures and performing inference on architectures unseen at training time.|\n",
    "2511.08545": "|2025-11-11|RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses|Sriram Srinivasan\u7b49|[2511.08545](http://arxiv.org/pdf/2511.08545)|\u65e0|\u25c6 Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding.\n\u25c6 However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known.\n\u25c6 This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates.|\n",
    "2511.07940": "|2025-11-11|Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?|Rui-Qing Sun\u7b49|[2511.07940](http://arxiv.org/pdf/2511.07940)|\u65e0|\u25c6 Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas.\n\u25c6 Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention.\n\u25c6 They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos.|\n",
    "2511.12935": "|2025-11-18|PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos|Dianbing Xi\u7b49|[2511.12935](http://arxiv.org/pdf/2511.12935)|\u65e0|\u25c6 We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds.\n\u25c6 Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF).\n\u25c6 In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance.|\n",
    "2511.12614": "|2025-11-16|OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding|Artem Moroz\u7b49|[2511.12614](http://arxiv.org/pdf/2511.12614)|\u65e0|\u25c6 We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process.\n\u25c6 Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images.\n\u25c6 Given a test image, our system first employs the CNOS detector to localize target objects.|\n",
    "2511.12304": "|2025-11-15|LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors|Qifeng Chen\u7b49|[2511.12304](http://arxiv.org/pdf/2511.12304)|\u65e0|\u25c6 Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed.\n\u25c6 However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans.\n\u25c6 To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads.|\n",
    "2511.14149": "|2025-11-18|iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion|Hao Wang\u7b49|[2511.14149](http://arxiv.org/pdf/2511.14149)|\u65e0|\u25c6 Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model.\n\u25c6 However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose.\n\u25c6 This multi-round process incurs significant computational overhead, hindering real-time performance in robotics.|\n",
    "2511.16542": "|2025-11-20|EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering|Pierrick Bournez\u7b49|[2511.16542](http://arxiv.org/pdf/2511.16542)|\u65e0|\u25c6 Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times.\n\u25c6 In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing.\n\u25c6 Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation.|\n",
    "2511.17322": "|2025-11-21|NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior|Dongbo Shi\u7b49|[2511.17322](http://arxiv.org/pdf/2511.17322)|\u65e0|\u25c6 In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.\n\u25c6 Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios.\n\u25c6 To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation.|\n",
    "2511.18806": "|2025-11-24|TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging|Qinglei Cao\u7b49|[2511.18806](http://arxiv.org/pdf/2511.18806)|\u65e0|\u25c6 X-ray imaging, based on penetration, enables detailed visualization of internal structures.\n\u25c6 Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction.\n\u25c6 However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios.|\n",
    "2511.18441": "|2025-11-23|ReCoGS: Real-time ReColoring for Gaussian Splatting scenes|Lorenzo Rutayisire\u7b49|[2511.18441](http://arxiv.org/pdf/2511.18441)|\u65e0|\u25c6 Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions.\n\u25c6 Beyond view synthesis, this 3D representation has also been explored for editing tasks.\n\u25c6 Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand.|\n",
    "2511.18293": "|2025-11-23|AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization|Shuai Zhang\u7b49|[2511.18293](http://arxiv.org/pdf/2511.18293)|\u65e0|\u25c6 Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis.\n\u25c6 However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging.\n\u25c6 Localization methods face challenges related to local minima due to the selection of initial poses.|\n",
    "2511.19542": "|2025-11-24|Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation|Jaeyeong Kim\u7b49|[2511.19542](http://arxiv.org/pdf/2511.19542)|\u65e0|\u25c6 We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph.\n\u25c6 Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead.\n\u25c6 An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds.|\n",
    "2511.19527": "|2025-11-24|MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training|Hongyu Lyu\u7b49|[2511.19527](http://arxiv.org/pdf/2511.19527)|\u65e0|\u25c6 Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure.\n\u25c6 The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors.\n\u25c6 However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments.|\n",
    "2511.20804": "|2025-11-25|$\u0394$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer|Kriti Ghosh\u7b49|[2511.20804](http://arxiv.org/pdf/2511.20804)|\u65e0|\u25c6 Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis.\n\u25c6 However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially.\n\u25c6 This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time.|\n",
    "2511.23052": "|2025-11-28|Image Valuation in NeRF-based 3D reconstruction|Grigorios Aris Cheimariotis\u7b49|[2511.23052](http://arxiv.org/pdf/2511.23052)|\u65e0|\u25c6 Data valuation and monetization are becoming increasingly important across domains such as eXtended Reality (XR) and digital media.\n\u25c6 In the context of 3D scene reconstruction from a set of images -- whether casually or professionally captured -- not all inputs contribute equally to the final output.\n\u25c6 Neural Radiance Fields (NeRFs) enable photorealistic 3D reconstruction of scenes by optimizing a volumetric radiance field given a set of images.|\n",
    "2511.22997": "|2025-11-28|MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis|Minseong Kweon\u7b49|[2511.22997](http://arxiv.org/pdf/2511.22997)|\u65e0|\u25c6 Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction.\n\u25c6 However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored.\n\u25c6 Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property.|\n",
    "2511.22939": "|2025-12-01|DenoiseGS: Gaussian Reconstruction Model for Burst Denoising|Yongsen Cheng\u7b49|[2511.22939](http://arxiv.org/pdf/2511.22939)|\u65e0|\u25c6 Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs.\n\u25c6 In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising.\n\u25c6 Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details.|\n",
    "2511.22228": "|2025-11-27|3D-Consistent Multi-View Editing by Diffusion Guidance|Josef Bengtson\u7b49|[2511.22228](http://arxiv.org/pdf/2511.22228)|\u65e0|\u25c6 Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene.\n\u25c6 Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models.\n\u25c6 We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process.|\n",
    "2512.01296": "|2025-12-01|EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly|Xiaokun Pan\u7b49|[2512.01296](http://arxiv.org/pdf/2512.01296)|\u65e0|\u25c6 Real-time 3D reconstruction is a fundamental task in computer graphics.\n\u25c6 Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).\n\u25c6 Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.|\n",
    "2512.00677": "|2025-11-30|Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer|Dong In Lee\u7b49|[2512.00677](http://arxiv.org/pdf/2512.00677)|\u65e0|\u25c6 Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction.\n\u25c6 However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing.\n\u25c6 Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing.|\n",
    "2512.00413": "|2025-11-29|SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control|Ji Gan\u7b49|[2512.00413](http://arxiv.org/pdf/2512.00413)|\u65e0|\u25c6 Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts.\n\u25c6 However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored.\n\u25c6 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views.|\n",
    "2512.02664": "|2025-12-02|PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes|Derui Shan\u7b49|[2512.02664](http://arxiv.org/pdf/2512.02664)|\u65e0|\u25c6 Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions.\n\u25c6 However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence.\n\u25c6 We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation.|\n",
    "2512.02172": "|2025-12-01|SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting|Pranav Asthana\u7b49|[2512.02172](http://arxiv.org/pdf/2512.02172)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training.\n\u25c6 A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders.\n\u25c6 Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image.|\n",
    "2512.04076": "|2025-12-03|Radiance Meshes for Volumetric Reconstruction|Alexander Mai\u7b49|[2512.04076](http://arxiv.org/pdf/2512.04076)|\u65e0|\u25c6 We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization.\n\u25c6 Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware.\n\u25c6 As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing.|\n",
    "2512.03422": "|2025-12-03|What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models|Tianchen Deng\u7b49|[2512.03422](http://arxiv.org/pdf/2512.03422)|\u65e0|\u25c6 In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.\n\u25c6 While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.\n\u25c6 Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.|\n",
    "2512.03210": "|2025-12-02|Flux4D: Flow-based Unsupervised 4D Reconstruction|Jingkang Wang\u7b49|[2512.03210](http://arxiv.org/pdf/2512.03210)|\u65e0|\u25c6 Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems.\n\u25c6 While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion.\n\u25c6 Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning.|\n",
    "2512.04542": "|2025-12-04|Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization|Hong Kuang\u7b49|[2512.04542](http://arxiv.org/pdf/2512.04542)|\u65e0|\u25c6 3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency.\n\u25c6 \\replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment.\n\u25c6 Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\\&T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360.|\n",
    "2512.07527": "|2025-12-09|From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images|Fei Yu\u7b49|[2512.07527](http://arxiv.org/pdf/2512.07527)|\u65e0|\u25c6 City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax.\n\u25c6 This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.\n\u25c6 To address this problem, we propose two design choices tailored for city structures and satellite inputs.|\n",
    "2512.07309": "|2025-12-08|Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals|Guosheng Wang\u7b49|[2512.07309](http://arxiv.org/pdf/2512.07309)|\u65e0|\u25c6 Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing.\n\u25c6 While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data.\n\u25c6 To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP).|\n",
    "2512.06438": "|2025-12-10|AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars|Ramazan Fazylov\u7b49|[2512.06438](http://arxiv.org/pdf/2512.06438)|\u65e0|\u25c6 The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment.\n\u25c6 Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control.\n\u25c6 We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars.|\n",
    "2512.08334": "|2025-12-09|HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting|Chang Liu\u7b49|[2512.08334](http://arxiv.org/pdf/2512.08334)|\u65e0|\u25c6 Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage.\n\u25c6 This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives.\n\u25c6 Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting.|\n",
    "2512.08215": "|2025-12-09|Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement|Chia-Hern Lai\u7b49|[2512.08215](http://arxiv.org/pdf/2512.08215)|\u65e0|\u25c6 The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics.\n\u25c6 Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions.\n\u25c6 To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view.|\n",
    "2512.09375": "|2025-12-10|Log NeRF: Comparing Spaces for Learning Radiance Fields|Sihe Chen\u7b49|[2512.09375](http://arxiv.org/pdf/2512.09375)|\u65e0|\u25c6 Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision.\n\u25c6 However, little attention has been paid to the color space in which the network is learning the radiance field representation.\n\u25c6 Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance.|\n",
    "2512.09335": "|2025-12-11|Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video|Seonghwa Choi\u7b49|[2512.09335](http://arxiv.org/pdf/2512.09335)|\u65e0|\u25c6 Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task.\n\u25c6 Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars.\n\u25c6 However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles.|\n",
    "2512.10293": "|2025-12-11|Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings|Karthikeya KV\u7b49|[2512.10293](http://arxiv.org/pdf/2512.10293)|\u65e0|\u25c6 We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360\u00b0 unique view synthesis for applications in medical imaging and natural scene reconstruction.\n\u25c6 In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone.\n\u25c6 We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings.|\n"
  }
}