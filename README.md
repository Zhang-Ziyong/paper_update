# 计算机视觉领域最新论文 (2026.02.12)

> 每日自动更新计算机视觉领域的最新arXiv论文

> 使用说明: [点击查看](./docs/README.md#usage)

<details>
<summary>分类目录</summary>
<ol>
<li><a href='#slam'>SLAM</a></li>
<li><a href='#sfm'>SFM</a></li>
<li><a href='#visual-localization'>Visual Localization</a></li>
<li><a href='#keypoint-detection'>Keypoint Detection</a></li>
<li><a href='#image-matching'>Image Matching</a></li>
<li><a href='#nerf'>NeRF</a></li>
</ol>
</details>

<h2 id='slam'>SLAM</h2>

<div class="table-container">
<table>
<thead><tr><th>日期</th><th>标题</th><th>论文与代码</th><th>摘要</th></tr></thead>
<tbody>
<tr><td>2026-02-09</td><td>Thegra: Graph-based SLAM for Thermal Imagery</td><td>[2602.08531](http://arxiv.org/pdf/2602.08531)</td><td>该论文提出了一种适用于热成像的、基于图优化的稀疏单目SLAM系统，旨在解决热图像纹理少、对比度低和噪声高导致的特征提取与匹配难题。

◆ 创新性地将在大规模可见光数据上训练的通用学习特征（SuperPoint检测器与LightGlue匹配器）引入热成像SLAM，利用其强大的跨域泛化能力，避免了针对稀缺热数据专门训练的需求。
◆ 设计了一个针对热图像的预处理流程，以增强输入图像对特征提取网络的适应性，并修改了SLAM核心模块以处理热图像中更稀疏且易含异常值的特征匹配。
◆ 提出了一种置信度加权的因子图优化方法，将SuperPoint提取的关键点置信度分数融入图优化过程，从而提高了系统在恶劣条件下的状态估计鲁棒性。</td></tr>
<tr><td>2026-02-11</td><td>Thermal odometry and dense mapping using learned odometry and Gaussian splatting</td><td>[2602.07493](http://arxiv.org/pdf/2602.07493)</td><td>本文提出了一种名为TOM-GS的新型热成像里程计与稠密建图方法，其核心贡献在于首次将基于学习的方法与高斯泼溅技术相结合，以解决热成像在恶劣环境下的鲁棒感知与稠密重建问题。

◆ 首次为热成像相机量身定制了基于高斯泼溅的SLAM系统，实现了高效的里程计估计与高质量稠密地图重建。
◆ 创新性地整合了基于学习的里程计与基于高斯泼溅的稠密建图模块，克服了传统几何方法在多样化数据集上易失效且无法生成稠密地图的局限。
◆ 设计了专门的热图像增强模块与单目深度集成方法，显著提升了热成像数据的处理质量与深度估计的准确性。
◆ 通过大量实验验证，该方法在运动估计和新视角渲染任务上均优于现有的基于学习方法，证明了所提学习框架对于鲁棒热成像里程计与稠密重建的有效性。</td></tr>
<tr><td>2026-02-06</td><td>A Consistency-Improved LiDAR-Inertial Bundle Adjustment</td><td>[2602.06380](http://arxiv.org/pdf/2602.06380)</td><td>◆ Simultaneous Localization and Mapping (SLAM) using 3D LiDAR has emerged as a cornerstone for autonomous navigation in robotics.
◆ While feature-based SLAM systems have achieved impressive results by leveraging edge and planar structures, they often suffer from the inconsistent estimator associated with feature parameterization and estimated covariance.
◆ In this work, we present a consistency-improved LiDAR-inertial bundle adjustment (BA) with tailored parameterization and estimator.</td></tr>
<tr><td>2026-02-05</td><td>Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation</td><td>[2602.05582](http://arxiv.org/pdf/2602.05582)</td><td>◆ We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3).
◆ Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems.
◆ To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).</td></tr>
<tr><td>2026-02-05</td><td>VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency</td><td>[2602.05508](http://arxiv.org/pdf/2602.05508)</td><td>◆ Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences.
◆ Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive.
◆ To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories.</td></tr>
<tr><td>2026-02-05</td><td>Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014</td><td>[2602.05487](http://arxiv.org/pdf/2602.05487)</td><td>◆ What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for &#x27;&#x27;Photorealistic Fisheye Sequence&#x27;&#x27; and make available at https://doi.org/10.
◆ 57745/DYIVVU, and comprehensive experiments.
◆ This work should be considered as a draft, and has been done during my PhD thesis &#x27;&#x27;Construction of 3D models from fisheye video data-Application to the localisation in urban area&#x27;&#x27; in 2014 [Mor16].</td></tr>
<tr><td>2026-02-04</td><td>Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions</td><td>[2602.04251](http://arxiv.org/pdf/2602.04251)</td><td>◆ Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments.
◆ 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM.
◆ This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM.</td></tr>
<tr><td>2026-02-03</td><td>Beyond the Vehicle: Cooperative Localization by Fusing Point Clouds for GPS-Challenged Urban Scenarios</td><td>[2602.03908](http://arxiv.org/pdf/2602.03908)</td><td>◆ Accurate vehicle localization is a critical challenge in urban environments where GPS signals are often unreliable.
◆ This paper presents a cooperative multi-sensor and multi-modal localization approach to address this issue by fusing data from vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems.
◆ Our approach integrates cooperative data with a point cloud registration-based simultaneous localization and mapping (SLAM) algorithm.</td></tr>
<tr><td>2026-02-02</td><td>Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning</td><td>[2602.02456](http://arxiv.org/pdf/2602.02456)</td><td>◆ Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings.
◆ While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning.
◆ To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships.</td></tr>
<tr><td>2026-02-02</td><td>3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM</td><td>[2602.02430](http://arxiv.org/pdf/2602.02430)</td><td>◆ Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots.
◆ Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements.
◆ In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping.</td></tr>
<tr><td>2026-02-02</td><td>Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures</td><td>[2602.02389](http://arxiv.org/pdf/2602.02389)</td><td>◆ Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data.
◆ By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning.
◆ In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters.</td></tr>
<tr><td>2026-02-02</td><td>Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss</td><td>[2602.01673](http://arxiv.org/pdf/2602.01673)</td><td>◆ Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift.
◆ Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing.
◆ In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM.</td></tr>
<tr><td>2026-01-29</td><td>IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation</td><td>[2601.21506](http://arxiv.org/pdf/2601.21506)</td><td>◆ Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both.
◆ Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning.
◆ Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues.</td></tr>
<tr><td>2026-01-28</td><td>Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned</td><td>[2601.21063](http://arxiv.org/pdf/2601.21063)</td><td>◆ Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure.
◆ This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets.
◆ In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network.</td></tr>
<tr><td>2026-01-30</td><td>VGGT-SLAM 2.0: Real-time Dense Feed-forward Scene Reconstruction</td><td>[2601.19887](http://arxiv.org/pdf/2601.19887)</td><td>◆ We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT.
◆ Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics.
◆ Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures.</td></tr>
<tr><td>2026-01-27</td><td>The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments</td><td>[2601.19557](http://arxiv.org/pdf/2601.19557)</td><td>◆ We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities.
◆ Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy.
◆ The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water.</td></tr>
<tr><td>2026-01-28</td><td>Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction</td><td>[2601.19489](http://arxiv.org/pdf/2601.19489)</td><td>◆ We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge.
◆ The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate).
◆ To robustly handle these heterogeneous settings, we develop a two-stage solution.</td></tr>
<tr><td>2026-01-26</td><td>Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing</td><td>[2601.18252](http://arxiv.org/pdf/2601.18252)</td><td>◆ Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM).
◆ Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness.
◆ We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps.</td></tr>
<tr><td>2026-01-22</td><td>Keyframe-Based Feed-Forward Visual Odometry</td><td>[2601.16020](http://arxiv.org/pdf/2601.16020)</td><td>◆ The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network.
◆ However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately.
◆ This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information.</td></tr>
<tr><td>2026-01-22</td><td>Parallelizable Riemannian Alternating Direction Method of Multipliers for Non-convex Pose Graph Optimization</td><td>[2601.15684](http://arxiv.org/pdf/2601.15684)</td><td>◆ Pose graph optimization (PGO) is fundamental to robot perception and navigation systems, serving as the mathematical backbone for solving simultaneous localization and mapping (SLAM).
◆ Existing solvers suffer from polynomial growth in computational complexity with graph size, hindering real-time deployment in large-scale scenarios.
◆ In this paper, by duplicating variables and introducing equality constraints, we reformulate the problem and propose a Parallelizable Riemannian Alternating Direction Method of Multipliers (PRADMM) to solve it efficiently.</td></tr>
<tr><td>2026-01-19</td><td>Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints</td><td>[2601.13252](http://arxiv.org/pdf/2601.13252)</td><td>◆ Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight &lt; 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms.
◆ This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes.
◆ We critically analyse the transition from classical geometry-based methods to emerging &quot;Edge AI&quot; paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control.</td></tr>
<tr><td>2026-01-18</td><td>R-VoxelMap: Accurate Voxel Mapping with Recursive Plane Fitting for Online LiDAR Odometry</td><td>[2601.12377](http://arxiv.org/pdf/2601.12377)</td><td>◆ This paper proposes R-VoxelMap, a novel voxel mapping method that constructs accurate voxel maps using a geometry-driven recursive plane fitting strategy to enhance the localization accuracy of online LiDAR odometry.
◆ VoxelMap and its variants typically fit and check planes using all points in a voxel, which may lead to plane parameter deviation caused by outliers, over segmentation of large planes, and incorrect merging across different physical planes.
◆ To address these issues, R-VoxelMap utilizes a geometry-driven recursive construction strategy based on an outlier detect-and-reuse pipeline.</td></tr>
<tr><td>2026-01-10</td><td>PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM</td><td>[2601.11617](http://arxiv.org/pdf/2601.11617)</td><td>◆ Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise.
◆ This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping.
◆ It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy.</td></tr>
<tr><td>2026-01-16</td><td>ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</td><td>[2601.11514](http://arxiv.org/pdf/2601.11514)</td><td>◆ Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs.
◆ Such conditions are rarely met in real-world scenarios.
◆ We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences.</td></tr>
<tr><td>2026-01-20</td><td>SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM</td><td>[2601.10814](http://arxiv.org/pdf/2601.10814)</td><td>◆ Localization and mapping are core perceptual capabilities for underwater robots.
◆ Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks.
◆ However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging.</td></tr>
<tr><td>2026-01-14</td><td>SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings</td><td>[2601.09665](http://arxiv.org/pdf/2601.09665)</td><td>◆ Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences.
◆ Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows.
◆ To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference.</td></tr>
<tr><td>2026-01-14</td><td>Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping</td><td>[2601.09578](http://arxiv.org/pdf/2601.09578)</td><td>◆ In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology.
◆ This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information.
◆ By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream.</td></tr>
<tr><td>2026-01-14</td><td>SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing</td><td>[2601.09385](http://arxiv.org/pdf/2601.09385)</td><td>◆ The recent surge in open-source Multimodal Large Language Models (MLLM) frameworks, such as LLaVA, provides a convenient kickoff for artificial intelligence developers and researchers.
◆ However, most of the MLLM frameworks take vision as the main input modality, and provide limited in-depth support for the modality of speech, audio, and music.
◆ This situation hinders the development of audio-language models, and forces researchers to spend a lot of effort on code writing and hyperparameter tuning.</td></tr>
<tr><td>2026-01-13</td><td>Efficient Incremental SLAM via Information-Guided and Selective Optimization</td><td>[2601.08110](http://arxiv.org/pdf/2601.08110)</td><td>◆ We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost.
◆ The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO).
◆ IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed.</td></tr>
<tr><td>2026-01-09</td><td>InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection</td><td>[2601.05805](http://arxiv.org/pdf/2601.05805)</td><td>◆ This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS).
◆ Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity.
◆ We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation.</td></tr>
<tr><td>2026-01-09</td><td>FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time</td><td>[2601.05738](http://arxiv.org/pdf/2601.05738)</td><td>◆ We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS).
◆ Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model.
◆ This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy.</td></tr>
<tr><td>2026-01-08</td><td>Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain</td><td>[2601.04551](http://arxiv.org/pdf/2601.04551)</td><td>◆ Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions.
◆ While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods.
◆ To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT).</td></tr>
<tr><td>2026-01-08</td><td>Fast Continuum Robot Shape and External Load State Estimation on SE(3)</td><td>[2601.04493](http://arxiv.org/pdf/2601.04493)</td><td>◆ Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads.
◆ We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements.
◆ By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \textit{spacetime} state estimation.</td></tr>
<tr><td>2026-01-06</td><td>Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM</td><td>[2601.02723](http://arxiv.org/pdf/2601.02723)</td><td>◆ Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM.
◆ We propose a method to improve loop closure performance in DPV-SLAM.
◆ Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method.</td></tr>
<tr><td>2026-01-09</td><td>360DVO: Deep Visual Odometry for Monocular 360-Degree Camera</td><td>[2601.02309](http://arxiv.org/pdf/2601.02309)</td><td>◆ Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems.
◆ However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination.
◆ To address this, we present 360DVO, the first deep learning-based OVO framework.</td></tr>
<tr><td>2026-01-05</td><td>Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors</td><td>[2601.02184](http://arxiv.org/pdf/2601.02184)</td><td>◆ Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments.
◆ In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package.
◆ Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization.</td></tr>
<tr><td>2026-01-03</td><td>VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction</td><td>[2601.01144](http://arxiv.org/pdf/2601.01144)</td><td>◆ Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction.
◆ In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity.
◆ We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera.</td></tr>
<tr><td>2025-12-28</td><td>RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization</td><td>[2601.00705](http://arxiv.org/pdf/2601.00705)</td><td>◆ We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization.
◆ Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization.
◆ This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines.</td></tr>
<tr><td>2026-01-13</td><td>Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference &amp; Estimation</td><td>[2601.00545](http://arxiv.org/pdf/2601.00545)</td><td>◆ Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem.
◆ Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations.
◆ In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables.</td></tr>
<tr><td>2026-01-01</td><td>FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</td><td>[2512.25008](http://arxiv.org/pdf/2512.25008)</td><td>◆ We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping.
◆ Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models.
◆ To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes.</td></tr>
<tr><td>2025-12-27</td><td>Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM</td><td>[2512.22690](http://arxiv.org/pdf/2512.22690)</td><td>◆ Motion capture remains costly and complex to deploy, limiting use outside specialized laboratories.
◆ We present Mesquite, an open-source, low-cost inertial motion-capture system that combines a body-worn network of 15 IMU sensor nodes with a hip-worn Android smartphone for position tracking.
◆ A low-power wireless link streams quaternion orientations to a central USB dongle and a browser-based application for real-time visualization and recording.</td></tr>
<tr><td>2025-12-30</td><td>Simultaneous Source Separation, Synchronization, Localization and Mapping for 6G Systems</td><td>[2512.22393](http://arxiv.org/pdf/2512.22393)</td><td>◆ Multipath-based simultaneous localization and mapping (MP-SLAM) is a promising approach for future 6G networks to jointly estimate the positions of transmitters and receivers together with the propagation environment.
◆ In cooperative MP-SLAM, information collected by multiple mobile terminals (MTs) is fused to enhance accuracy and robustness.
◆ Existing methods, however, typically assume perfectly synchronized base stations (BSs) and orthogonal transmission sequences, rendering inter-BS interference at the MTs negligible.</td></tr>
<tr><td>2025-12-25</td><td>World-Coordinate Human Motion Retargeting via SAM 3D Body</td><td>[2512.21573](http://arxiv.org/pdf/2512.21573)</td><td>◆ Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics.
◆ To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate.
◆ Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization.</td></tr>
<tr><td>2025-12-25</td><td>FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration</td><td>[2512.20355](http://arxiv.org/pdf/2512.20355)</td><td>◆ Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation.
◆ While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms.
◆ Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots.</td></tr>
<tr><td>2025-12-22</td><td>Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction</td><td>[2512.19110](http://arxiv.org/pdf/2512.19110)</td><td>◆ This work presents two novel solvers for estimating the relative poses among views with known vertical directions.
◆ The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs).
◆ Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors.</td></tr>
<tr><td>2025-12-18</td><td>SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</td><td>[2512.16461](http://arxiv.org/pdf/2512.16461)</td><td>◆ Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction.
◆ While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics.
◆ Conversely, geometric perception captures structure and motion but remains semantically sparse.</td></tr>
<tr><td>2025-12-17</td><td>Spatia: Video Generation with Updatable Spatial Memory</td><td>[2512.15716](http://arxiv.org/pdf/2512.15716)</td><td>◆ Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals.
◆ To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory.
◆ Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM.</td></tr>
<tr><td>2025-12-17</td><td>NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles</td><td>[2512.15080](http://arxiv.org/pdf/2512.15080)</td><td>◆ Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments.
◆ A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations.
◆ These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.</td></tr>
<tr><td>2025-12-17</td><td>A Parameter-Free Stochastic LineseArch Method (SLAM) for Minimizing Expectation Residuals</td><td>[2512.14979](http://arxiv.org/pdf/2512.14979)</td><td>◆ Most existing rate and complexity guarantees for stochastic gradient methods in $L$-smooth settings mandates that such sequences be non-adaptive, non-increasing, and upper bounded by $\tfrac{a}{L}$ for $a &gt; 0$.
◆ This requires knowledge of $L$ and may preclude larger steps.
◆ Motivated by these shortcomings, we present an Armijo-enabled stochastic linesearch framework with standard stochastic zeroth- and first-order oracles.</td></tr>
<tr><td>2025-12-16</td><td>Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations</td><td>[2512.14428](http://arxiv.org/pdf/2512.14428)</td><td>◆ The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth.
◆ The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal.
◆ While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments.</td></tr>
<tr><td>2025-12-16</td><td>Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments</td><td>[2512.14340](http://arxiv.org/pdf/2512.14340)</td><td>◆ The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years.
◆ While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge.
◆ The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight.</td></tr>
<tr><td>2025-12-16</td><td>SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry</td><td>[2512.14189](http://arxiv.org/pdf/2512.14189)</td><td>◆ While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime.
◆ This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO.
◆ The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties.</td></tr>
<tr><td>2025-12-16</td><td>ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM</td><td>[2512.14032](http://arxiv.org/pdf/2512.14032)</td><td>◆ We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time.
◆ For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates.
◆ SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.</td></tr>
<tr><td>2025-12-16</td><td>Deep Learning Perspective of Scene Understanding in Autonomous Robots</td><td>[2512.14020](http://arxiv.org/pdf/2512.14020)</td><td>◆ This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM.
◆ It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better.
◆ When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction.</td></tr>
<tr><td>2025-12-16</td><td>Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline</td><td>[2512.13974](http://arxiv.org/pdf/2512.13974)</td><td>◆ Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining.
◆ Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive.
◆ This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report.</td></tr>
<tr><td>2025-12-13</td><td>INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset</td><td>[2512.12377](http://arxiv.org/pdf/2512.12377)</td><td>◆ We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception.
◆ Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection.
◆ INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations.</td></tr>
<tr><td>2025-12-13</td><td>Semantic Zone based 3D Map Management for Mobile Robot</td><td>[2512.12228](http://arxiv.org/pdf/2512.12228)</td><td>◆ Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations.
◆ However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources.
◆ Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments.</td></tr>
<tr><td>2025-12-13</td><td>Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion</td><td>[2512.12203](http://arxiv.org/pdf/2512.12203)</td><td>◆ As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids.
◆ The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector&#x27;s relative pose using a lidar or conventional camera.
◆ However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive.</td></tr>
<tr><td>2025-12-11</td><td>Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks</td><td>[2512.10481](http://arxiv.org/pdf/2512.10481)</td><td>◆ Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment.
◆ In some scenarios, vision is occluded.
◆ The robot can then no longer obtain real-time scene state information through visual feedback.</td></tr>
<tr><td>2025-12-11</td><td>CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation</td><td>[2512.10360](http://arxiv.org/pdf/2512.10360)</td><td>◆ Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps.
◆ While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks.
◆ To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR).</td></tr>
<tr><td>2025-12-10</td><td>Inertial Magnetic SLAM Systems Using Low-Cost Sensors</td><td>[2512.10128](http://arxiv.org/pdf/2512.10128)</td><td>◆ Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning.
◆ Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly.
◆ Moreover, they have bounded error within mapped regions.</td></tr>
<tr><td>2025-12-10</td><td>Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization</td><td>[2512.09608](http://arxiv.org/pdf/2512.09608)</td><td>◆ Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather.
◆ Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures.
◆ Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization.</td></tr>
<tr><td>2025-12-10</td><td>D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM</td><td>[2512.09411](http://arxiv.org/pdf/2512.09411)</td><td>◆ Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments.
◆ However, dense SLAM in dynamic environments remains challenging.
◆ Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects.</td></tr>
<tr><td>2025-12-09</td><td>A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation</td><td>[2512.08653](http://arxiv.org/pdf/2512.08653)</td><td>◆ Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior.
◆ This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing.
◆ Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion.</td></tr>
<tr><td>2025-12-09</td><td>OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics</td><td>[2512.08625](http://arxiv.org/pdf/2512.08625)</td><td>◆ Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems.
◆ With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction.
◆ Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments.</td></tr>
<tr><td>2025-12-08</td><td>Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization</td><td>[2512.07969](http://arxiv.org/pdf/2512.07969)</td><td>◆ Robotic perception often requires solving large nonlinear least-squares (NLS) problems.
◆ While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution.
◆ Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties.</td></tr>
<tr><td>2025-12-08</td><td>OptMap: Geometric Map Distillation via Submodular Maximization</td><td>[2512.07775](http://arxiv.org/pdf/2512.07775)</td><td>◆ Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms.
◆ As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance.
◆ Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization.</td></tr>
<tr><td>2025-12-08</td><td>Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality</td><td>[2512.07221](http://arxiv.org/pdf/2512.07221)</td><td>◆ Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications.
◆ As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent.
◆ Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements.</td></tr>
<tr><td>2025-12-07</td><td>Dynamic Visual SLAM using a General 3D Prior</td><td>[2512.06868](http://arxiv.org/pdf/2512.06868)</td><td>◆ Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality.
◆ However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy.
◆ In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes.</td></tr>
<tr><td>2025-12-04</td><td>ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety</td><td>[2512.05299](http://arxiv.org/pdf/2512.05299)</td><td>◆ Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support.
◆ This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets.
◆ By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user&#x27;s passthrough view.</td></tr>
<tr><td>2025-12-04</td><td>TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards</td><td>[2512.04772](http://arxiv.org/pdf/2512.04772)</td><td>◆ In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation.
◆ However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials.
◆ The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions.</td></tr>
<tr><td>2025-12-03</td><td>What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</td><td>[2512.03422](http://arxiv.org/pdf/2512.03422)</td><td>◆ In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.
◆ While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.
◆ Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.</td></tr>
<tr><td>2025-12-02</td><td>VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM</td><td>[2512.02293](http://arxiv.org/pdf/2512.02293)</td><td>◆ We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction.
◆ Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations.
◆ Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states.</td></tr>
<tr><td>2025-12-01</td><td>KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</td><td>[2512.01889](http://arxiv.org/pdf/2512.01889)</td><td>◆ We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.
◆ Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.
◆ KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).</td></tr>
<tr><td>2025-12-01</td><td>Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching</td><td>[2512.01850](http://arxiv.org/pdf/2512.01850)</td><td>◆ Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.
◆ In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.
◆ Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.</td></tr>
<tr><td>2025-12-01</td><td>AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields</td><td>[2512.01753](http://arxiv.org/pdf/2512.01753)</td><td>◆ Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection.
◆ However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce.
◆ To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments.</td></tr>
<tr><td>2025-12-01</td><td>EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly</td><td>[2512.01296](http://arxiv.org/pdf/2512.01296)</td><td>◆ Real-time 3D reconstruction is a fundamental task in computer graphics.
◆ Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).
◆ Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.</td></tr>
<tr><td>2025-11-30</td><td>Integration of UWB Radar on Mobile Robots for Continuous Obstacle and Environment Mapping</td><td>[2512.01018](http://arxiv.org/pdf/2512.01018)</td><td>◆ This paper presents an infrastructure-free approach for obstacle detection and environmental mapping using ultra-wideband (UWB) radar mounted on a mobile robotic platform.
◆ Traditional sensing modalities such as visual cameras and Light Detection and Ranging (LiDAR) fail in environments with poor visibility due to darkness, smoke, or reflective surfaces.
◆ In these visioned-impaired conditions, UWB radar offers a promising alternative.</td></tr>
<tr><td>2025-11-30</td><td>EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes</td><td>[2512.00771](http://arxiv.org/pdf/2512.00771)</td><td>◆ Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction.
◆ Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction.
◆ However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras.</td></tr>
<tr><td>2025-11-29</td><td>Odometry Without Correspondence from Inertially Constrained Ruled Surfaces</td><td>[2512.00327](http://arxiv.org/pdf/2512.00327)</td><td>◆ Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow.
◆ This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate&#x27;s quality.
◆ Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence.</td></tr>
<tr><td>2025-11-28</td><td>Robust 3DGS-based SLAM via Adaptive Kernel Smoothing</td><td>[2511.23221](http://arxiv.org/pdf/2511.23221)</td><td>◆ In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy.
◆ We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking.
◆ To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM.</td></tr>
<tr><td>2025-12-01</td><td>Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures</td><td>[2511.23156](http://arxiv.org/pdf/2511.23156)</td><td>◆ To ensure the safety of marine and coastal structures, extreme (design) values should be known at the design stage.
◆ But for such complex systems, estimating the magnitude of events which are both non-linear and rare is extremely challenging, and involves considerable computational cost to capture the high-fidelity physics.
◆ To address this challenge, we offer a new multi-fidelity screening method, Probabilistic Adaptive Screening (PAS), which accurately predicts extreme values of strongly non-linear wave-induced loads while minimising the required high-fidelity simulation duration.</td></tr>
<tr><td>2025-11-28</td><td>DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management</td><td>[2511.23030](http://arxiv.org/pdf/2511.23030)</td><td>◆ Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities.
◆ However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments.
◆ We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk.</td></tr>
<tr><td>2025-11-28</td><td>Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM</td><td>[2511.22968](http://arxiv.org/pdf/2511.22968)</td><td>◆ Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems.
◆ To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs.
◆ First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene&#x27;s intrinsic properties, such as albedo, from transient lighting.</td></tr>
<tr><td>2025-11-28</td><td>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</td><td>[2511.22860](http://arxiv.org/pdf/2511.22860)</td><td>◆ Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise.
◆ We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization.
◆ At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity.</td></tr>
<tr><td>2025-11-26</td><td>Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry</td><td>[2511.21083](http://arxiv.org/pdf/2511.21083)</td><td>◆ Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality.
◆ Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms.
◆ Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked.</td></tr>
<tr><td>2025-11-25</td><td>Estimating Fog Parameters from a Sequence of Stereo Images</td><td>[2511.20865](http://arxiv.org/pdf/2511.20865)</td><td>◆ We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically.
◆ In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem.
◆ By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous.</td></tr>
<tr><td>2025-11-25</td><td>The origin of B-type runaway stars based on kinematics</td><td>[2511.20566](http://arxiv.org/pdf/2511.20566)</td><td>◆ Runaway stars depart their birthplaces with high peculiar velocities.
◆ Two mechanisms are commonly invoked to explain their origin, the binary supernova scenario (BSS) and the dynamical ejection scenario (DES).
◆ Investigating the kinematic properties of runaway stars is key to understanding their origins.We intend to investigate the origins of 39 B-type runaway stars from LAMOST using orbital traceback analysis.</td></tr>
<tr><td>2025-11-25</td><td>Metric, inertially aligned monocular state estimation via kinetodynamic priors</td><td>[2511.20496](http://arxiv.org/pdf/2511.20496)</td><td>◆ Accurate state estimation for flexible robotic systems poses significant challenges, particular for platforms with dynamically deforming structures that invalidate rigid-body assumptions.
◆ This paper tackles this problem and allows to extend existing rigid-body pose estimation methods to non-rigid systems.
◆ Our approach hinges on two core assumptions: first, the elastic properties are captured by an injective deformation-force model, efficiently learned via a Multi-Layer Perceptron; second, we solve the platform&#x27;s inherently smooth motion using continuous-time B-spline kinematic models.</td></tr>
<tr><td>2025-11-25</td><td>AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend</td><td>[2511.20343](http://arxiv.org/pdf/2511.20343)</td><td>◆ We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks.
◆ The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness.
◆ Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization.</td></tr>
<tr><td>2025-11-25</td><td>Stellar Parameters of BOSS M dwarfs in SDSS-V DR19</td><td>[2511.20005](http://arxiv.org/pdf/2511.20005)</td><td>◆ We utilized the Stellar LAbel Machine (SLAM), a data-driven model based on Support Vector Regression, to derive stellar parameters ([Fe/H], $T_{\rm eff}$, and $\log{g}$) for SDSS-V M dwarfs using low-resolution optical spectra (R$\sim$2000) obtained with the BOSS spectrographs.
◆ These parameters are calibrated using LAMOST F, G or K dwarf companions ([Fe/H]), and APOGEE Net ($T_{\rm eff}$ and $\log{g}$), respectively.
◆ Comparisons of SLAM predicted [Fe/H] values between two components of M+M dwarfs wide binaries show no bias but with a scatter of 0.11 dex.</td></tr>
<tr><td>2025-11-26</td><td>Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors</td><td>[2511.19031](http://arxiv.org/pdf/2511.19031)</td><td>◆ Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot&#x27;s pose while simultaneously reconstructing an unknown 3D scene using a single camera.
◆ While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization.
◆ To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses.</td></tr>
<tr><td>2025-11-24</td><td>AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion</td><td>[2511.18857](http://arxiv.org/pdf/2511.18857)</td><td>◆ Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail.
◆ Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection.
◆ This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm.</td></tr>
<tr><td>2025-11-24</td><td>SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map</td><td>[2511.18756](http://arxiv.org/pdf/2511.18756)</td><td>◆ Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation.
◆ To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints.
◆ Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates.</td></tr>
<tr><td>2025-11-24</td><td>Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing</td><td>[2511.18755](http://arxiv.org/pdf/2511.18755)</td><td>◆ 3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence.
◆ However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.
◆ This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices.</td></tr>
<tr><td>2025-11-24</td><td>Stable Multi-Drone GNSS Tracking System for Marine Robots</td><td>[2511.18694](http://arxiv.org/pdf/2511.18694)</td><td>◆ Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface.
◆ Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence.
◆ In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots.</td></tr>
<tr><td>2025-11-23</td><td>Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span</td><td>[2511.18470](http://arxiv.org/pdf/2511.18470)</td><td>◆ People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors.
◆ While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies.
◆ We address the challenge of egocentric 3D visual span forecasting, predicting where a person&#x27;s visual perception will focus next within their three-dimensional environment.</td></tr>
<tr><td>2025-11-22</td><td>Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation</td><td>[2511.17992](http://arxiv.org/pdf/2511.17992)</td><td>◆ The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge.
◆ While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction.
◆ Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators.</td></tr>
<tr><td>2025-11-21</td><td>Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</td><td>[2511.17792](http://arxiv.org/pdf/2511.17792)</td><td>◆ While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified.
◆ We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments.
◆ Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories.</td></tr>
<tr><td>2025-11-21</td><td>IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation</td><td>[2511.17384](http://arxiv.org/pdf/2511.17384)</td><td>◆ While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning.
◆ Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity.
◆ To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning.</td></tr>
<tr><td>2025-11-21</td><td>MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning</td><td>[2511.17299](http://arxiv.org/pdf/2511.17299)</td><td>◆ Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors.
◆ In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning.
◆ The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty.</td></tr>
<tr><td>2025-11-21</td><td>SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors</td><td>[2511.17207](http://arxiv.org/pdf/2511.17207)</td><td>◆ Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis.
◆ To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework.
◆ The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications.</td></tr>
<tr><td>2025-11-20</td><td>CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering</td><td>[2511.16349](http://arxiv.org/pdf/2511.16349)</td><td>◆ Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content.
◆ Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure.
◆ This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud.</td></tr>
<tr><td>2025-11-20</td><td>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</td><td>[2511.16282](http://arxiv.org/pdf/2511.16282)</td><td>◆ We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT).
◆ The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation.
◆ To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT&#x27;s high memory demands.</td></tr>
<tr><td>2025-11-20</td><td>LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM</td><td>[2511.16144](http://arxiv.org/pdf/2511.16144)</td><td>◆ Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps.
◆ However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction.
◆ Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments.</td></tr>
<tr><td>2025-11-20</td><td>Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments</td><td>[2511.16091](http://arxiv.org/pdf/2511.16091)</td><td>◆ We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation.
◆ Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy.
◆ Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity.</td></tr>
<tr><td>2025-11-20</td><td>Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud</td><td>[2511.16048](http://arxiv.org/pdf/2511.16048)</td><td>◆ While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately &quot;lo-fi&quot; approach.
◆ We present the &quot;Semantic Glitch,&quot; a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a &quot;physical glitch&quot; derived from digital archaeology.
◆ We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate.</td></tr>
<tr><td>2025-11-19</td><td>Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition</td><td>[2511.15597](http://arxiv.org/pdf/2511.15597)</td><td>◆ LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving.
◆ However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting.
◆ To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism.</td></tr>
<tr><td>2025-11-18</td><td>A visual study of ICP variants for Lidar Odometry</td><td>[2511.14919](http://arxiv.org/pdf/2511.14919)</td><td>◆ Odometry with lidar sensors is a state-of-the-art method to estimate the ego pose of a moving vehicle.
◆ Many implementations of lidar odometry use variants of the Iterative Closest Point (ICP) algorithm.
◆ Real-world effects such as dynamic objects, non-overlapping areas, and sensor noise diminish the accuracy of ICP.</td></tr>
<tr><td>2025-11-18</td><td>SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology</td><td>[2511.14639](http://arxiv.org/pdf/2511.14639)</td><td>◆ Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low.
◆ We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks.
◆ To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse.</td></tr>
<tr><td>2025-11-18</td><td>Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors</td><td>[2511.14335](http://arxiv.org/pdf/2511.14335)</td><td>◆ Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera.
◆ Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive.
◆ Monocular SLAM also faces scale ambiguities, which affect its accuracy.</td></tr>
<tr><td>2025-11-18</td><td>MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning</td><td>[2511.14330](http://arxiv.org/pdf/2511.14330)</td><td>◆ Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system&#x27;s movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community.
◆ While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery.
◆ In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments.</td></tr>
<tr><td>2025-11-18</td><td>iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion</td><td>[2511.14149](http://arxiv.org/pdf/2511.14149)</td><td>◆ Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model.
◆ However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose.
◆ This multi-round process incurs significant computational overhead, hindering real-time performance in robotics.</td></tr>
<tr><td>2025-11-17</td><td>GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry</td><td>[2511.13216](http://arxiv.org/pdf/2511.13216)</td><td>◆ Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms.
◆ In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping.
◆ Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation.</td></tr>
<tr><td>2025-11-16</td><td>DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry</td><td>[2511.12653](http://arxiv.org/pdf/2511.12653)</td><td>◆ Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms.
◆ This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry).
◆ Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model.</td></tr>
<tr><td>2025-11-14</td><td>Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture</td><td>[2511.11845](http://arxiv.org/pdf/2511.11845)</td><td>◆ Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments.
◆ This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions.
◆ The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning.</td></tr>
<tr><td>2025-11-12</td><td>DualVision ArthroNav: Investigating Opportunities to Enhance Localization and Reconstruction in Image-based Arthroscopy Navigation via External Cameras</td><td>[2511.10699](http://arxiv.org/pdf/2511.10699)</td><td>◆ Arthroscopic procedures can greatly benefit from navigation systems that enhance spatial awareness, depth perception, and field of view.
◆ However, existing optical tracking solutions impose strict workspace constraints and disrupt surgical workflow.
◆ Vision-based alternatives, though less invasive, often rely solely on the monocular arthroscope camera, making them prone to drift, scale ambiguity, and sensitivity to rapid motion or occlusion.</td></tr>
<tr><td>2025-11-12</td><td>Generation-Agnostic Zero-Energy Devices for Sustainable Connectivity, Sensing, and Localization</td><td>[2511.09372](http://arxiv.org/pdf/2511.09372)</td><td>◆ The massive scale of Internet of Things (IoT) connectivity expected in 6G networks raises unprecedented challenges in energy use, battery waste, and lifecycle sustainability.
◆ Current cellular IoT solutions remain bound to the lifetime of underlying network generations and rely on billions of disposable batteries, creating unsustainable economic and environmental costs.
◆ This article proposes generation-agnostic zero-energy devices (XG-ZEDs), a new class of backscatter based IoT devices that are battery-less, spectrum-agnostic, and future-proof across successive network generations.</td></tr>
<tr><td>2025-11-12</td><td>UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning</td><td>[2511.09302](http://arxiv.org/pdf/2511.09302)</td><td>◆ Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods.
◆ The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely.
◆ Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera&#x27;s field of view.</td></tr>
<tr><td>2025-11-12</td><td>SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields</td><td>[2511.09072](http://arxiv.org/pdf/2511.09072)</td><td>◆ Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a &#x27;pose-centric&#x27; paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization.
◆ This approach is computationally expensive, limiting their real-time performance on resource-constrained devices.
◆ To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, &#x27;motion-centric&#x27; framework.</td></tr>
<tr><td>2025-11-10</td><td>Integration of Visual SLAM into Consumer-Grade Automotive Localization</td><td>[2511.06919](http://arxiv.org/pdf/2511.06919)</td><td>◆ Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e.
◆ wheel odometry and IMUs, whose performance is limited by systematic errors and calibration.
◆ While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored.</td></tr>
<tr><td>2025-11-10</td><td>Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes</td><td>[2511.06765](http://arxiv.org/pdf/2511.06765)</td><td>◆ 3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality.
◆ To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation.
◆ For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments.</td></tr>
<tr><td>2025-11-10</td><td>Semi-distributed Cross-modal Air-Ground Relative Localization</td><td>[2511.06749](http://arxiv.org/pdf/2511.06749)</td><td>◆ Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks.
◆ However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy.
◆ To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework.</td></tr>
<tr><td>2025-11-08</td><td>ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface</td><td>[2511.05858](http://arxiv.org/pdf/2511.05858)</td><td>◆ Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently.
◆ However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks.
◆ In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks.</td></tr>
<tr><td>2025-11-08</td><td>3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots</td><td>[2511.05816](http://arxiv.org/pdf/2511.05816)</td><td>◆ Limbed climbing robots are designed to explore challenging vertical walls, such as the skylights of the Moon and Mars.
◆ In such robots, the primary role of a hand-eye camera is to accurately estimate 3D positions of graspable points (i.e., convex terrain surfaces) thanks to its close-up views.
◆ While conventional climbing robots often employ RGB-D cameras as hand-eye cameras to facilitate straightforward 3D terrain mapping and graspable point detection, RGB-D cameras are large and consume considerable power.</td></tr>
<tr><td>2025-11-07</td><td>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</td><td>[2511.05404](http://arxiv.org/pdf/2511.05404)</td><td>◆ Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration.
◆ In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity.
◆ This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments.</td></tr>
<tr><td>2025-11-06</td><td>Synchronous Observer Design for Landmark-Inertial SLAM with Almost-Global Convergence</td><td>[2511.04531](http://arxiv.org/pdf/2511.04531)</td><td>◆ Landmark Inertial Simultaneous Localisation and Mapping (LI-SLAM) is the problem of estimating the locations of landmarks in the environment and the robot&#x27;s pose relative to those landmarks using landmark position measurements and measurements from Inertial Measurement Unit (IMU).
◆ This paper proposes a nonlinear observer for LI-SLAM posed in continuous time and analyses the observer in a base space that encodes all the observable states of LI-SLAM.
◆ The local exponential stability and almost-global asymptotic stability of the error dynamics in base space is established in the proof section and validated using simulations.</td></tr>
<tr><td>2025-11-06</td><td>PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration</td><td>[2511.04180](http://arxiv.org/pdf/2511.04180)</td><td>◆ Existing Active SLAM methodologies face issues such as slow exploration speed and suboptimal paths.
◆ To address these limitations, we propose a hybrid framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement Learning framework and a Lightweight Stagnation Detection mechanism.
◆ The Path-Uncertainty Co-Optimization framework jointly optimizes travel distance and map uncertainty through a dual-objective reward function, balancing exploration and exploitation.</td></tr>
<tr><td>2025-11-04</td><td>Analytical modelling of a stop-less modular bus service with an application to charging strategies comparison</td><td>[2511.03754](http://arxiv.org/pdf/2511.03754)</td><td>◆ Buses are a vital component of metropolitan public transport, yet conventional bus services often struggle with inefficiencies including extended dwelling time, which increases in-vehicle travel time for non-alighting passengers.
◆ A stop-less autonomous modular (SLAM) bus service has emerged as a solution, enabling dynamic capacity to reduce dwelling time.
◆ Meanwhile, the electrification of buses is advancing as a strategy to mitigate greenhouse gas emissions and reduces operators&#x27; costs, but introduces new operational constraints due to charging requirements.</td></tr>
<tr><td>2025-11-04</td><td>Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds</td><td>[2511.02395](http://arxiv.org/pdf/2511.02395)</td><td>◆ Moving object segmentation is a crucial task for safe and reliable autonomous mobile systems like self-driving cars, improving the reliability and robustness of subsequent tasks like SLAM or path planning.
◆ While the segmentation of camera or LiDAR data is widely researched and achieves great results, it often introduces an increased latency by requiring the accumulation of temporal sequences to gain the necessary temporal context.
◆ Radar sensors overcome this problem with their ability to provide a direct measurement of a point&#x27;s Doppler velocity, which can be exploited for single-scan moving object segmentation.</td></tr>
<tr><td>2025-11-03</td><td>TurboMap: GPU-Accelerated Local Mapping for Visual SLAM</td><td>[2511.02036](http://arxiv.org/pdf/2511.02036)</td><td>◆ This paper presents TurboMap, a GPU-accelerated and CPU-optimized local mapping module for visual SLAM systems.
◆ We identify key performance bottlenecks in the local mapping process for visual SLAM and address them through targeted GPU and CPU optimizations.
◆ Specifically, we offload map point triangulation and fusion to the GPU, accelerate redundant keyframe culling on the CPU, and integrate a GPU-accelerated solver to speed up local bundle adjustment.</td></tr>
<tr><td>2025-11-03</td><td>CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels</td><td>[2511.01379](http://arxiv.org/pdf/2511.01379)</td><td>◆ Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and GPS-denied underground coal mine environments presents significant challenges.
◆ Sensors must contend with abnormal operating conditions: GPS unavailability impedes scene reconstruction and absolute geographic referencing, uneven or slippery terrain degrades wheel odometer accuracy, and long, feature-poor tunnels reduce LiDAR effectiveness.
◆ To address these issues, we propose CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM framework based on the Iterated Error-State Kalman Filter (IESKF).</td></tr>
<tr><td>2025-11-03</td><td>Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference</td><td>[2511.01219](http://arxiv.org/pdf/2511.01219)</td><td>◆ This paper addresses the Kidnapped Robot Problem (KRP), a core localization challenge of relocalizing a robot in a known map without prior pose estimate when localization loss or at SLAM initialization.
◆ For this purpose, a passive 2-D global relocalization framework is proposed.
◆ It estimates the global pose efficiently and reliably from a single LiDAR scan and an occupancy grid map while the robot remains stationary, thereby enhancing the long-term autonomy of mobile robots.</td></tr>
<tr><td>2025-11-03</td><td>LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</td><td>[2511.01186](http://arxiv.org/pdf/2511.01186)</td><td>◆ Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding.
◆ Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration.
◆ Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale.</td></tr>
<tr><td>2025-11-01</td><td>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</td><td>[2511.00635](http://arxiv.org/pdf/2511.00635)</td><td>◆ As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted.
◆ Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions.
◆ In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration.</td></tr>
<tr><td>2025-10-31</td><td>WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond</td><td>[2510.27133](http://arxiv.org/pdf/2510.27133)</td><td>◆ 3D Gaussian splatting (3DGS) and its subsequent variants have led to remarkable progress in simultaneous localization and mapping (SLAM).
◆ While most recent 3DGS-based SLAM works focus on small-scale indoor scenes, developing 3DGS-based SLAM methods for large-scale forest scenes holds great potential for many real-world applications, especially for wildfire emergency response and forest management.
◆ However, this line of research is impeded by the absence of a comprehensive and high-quality dataset, and collecting such a dataset over real-world scenes is costly and technically infeasible.</td></tr>
<tr><td>2025-10-30</td><td>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</td><td>[2510.26358](http://arxiv.org/pdf/2510.26358)</td><td>◆ Autonomous robots in orchards require real-time 3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion.
◆ We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering.
◆ Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradient-driven map lifecycle executed between keyframes preserves fine details and bounds memory.</td></tr>
<tr><td>2025-10-30</td><td>Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM</td><td>[2510.26131](http://arxiv.org/pdf/2510.26131)</td><td>◆ Attention models have recently emerged as a powerful approach, demonstrating significant progress in various fields.
◆ Visualization techniques, such as class activation mapping, provide visual insights into the reasoning of convolutional neural networks (CNNs).
◆ Using network gradients, it is possible to identify regions where the network pays attention during image recognition tasks.</td></tr>
<tr><td>2025-10-29</td><td>EA3D: Online Open-World 3D Object Extraction from Streaming Videos</td><td>[2510.25146](http://arxiv.org/pdf/2510.25146)</td><td>◆ Current 3D scene understanding methods are limited by offline-collected multi-view data or pre-constructed 3D geometry.
◆ In this paper, we present ExtractAnything3D (EA3D), a unified online framework for open-world 3D object extraction that enables simultaneous geometric reconstruction and holistic scene understanding.
◆ Given a streaming video, EA3D dynamically interprets each frame using vision-language and 2D vision foundation encoders to extract object-level knowledge.</td></tr>
<tr><td>2025-10-28</td><td>Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots</td><td>[2510.24571](http://arxiv.org/pdf/2510.24571)</td><td>◆ The calibration of extrinsic parameters and clock offsets between sensors for high-accuracy performance in underwater SLAM systems remains insufficiently explored.
◆ Existing methods for Doppler Velocity Log (DVL) calibration are either constrained to specific sensor configurations or rely on oversimplified assumptions, and none jointly estimate translational extrinsics and time offsets.
◆ We propose a Unified Iterative Calibration (UIC) framework for general DVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a Gaussian Process (GP) motion prior for high-fidelity motion interpolation.</td></tr>
<tr><td>2025-10-28</td><td>GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots</td><td>[2510.24533](http://arxiv.org/pdf/2510.24533)</td><td>◆ Accurate visual inertial simultaneous localization and mapping (VI SLAM) for underwater robots remains a significant challenge due to frequent visual degeneracy and insufficient inertial measurement unit (IMU) motion excitation.
◆ In this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system designed to address these issues.
◆ By leveraging the stereo camera&#x27;s direct depth estimation ability, we eliminate the need to estimate scale during IMU initialization, enabling stable operation even under low acceleration dynamics.</td></tr>
<tr><td>2025-10-28</td><td>A Survey on Collaborative SLAM with 3D Gaussian Splatting</td><td>[2510.23988](http://arxiv.org/pdf/2510.23988)</td><td>◆ This survey comprehensively reviews the evolving field of multi-robot collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian Splatting (3DGS).
◆ As an explicit scene representation, 3DGS has enabled unprecedented real-time, high-fidelity rendering, ideal for robotics.
◆ However, its use in multi-robot systems introduces significant challenges in maintaining global consistency, managing communication, and fusing data from heterogeneous sources.</td></tr>
<tr><td>2025-10-26</td><td>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</td><td>[2510.22754](http://arxiv.org/pdf/2510.22754)</td><td>◆ Multi-agent cooperative SLAM often encounters challenges in similar indoor environments characterized by repetitive structures, such as corridors and rooms.
◆ These challenges can lead to significant inaccuracies in shared location identification when employing point cloud-based techniques.
◆ To mitigate these issues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that integrates text semantics and WiFi signal features to enhance location identification and loop closure detection.</td></tr>
<tr><td>2025-10-26</td><td>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</td><td>[2510.22740](http://arxiv.org/pdf/2510.22740)</td><td>◆ We consider the distributed pose-graph optimization (PGO) problem, which is fundamental in accurate trajectory estimation in multi-robot simultaneous localization and mapping (SLAM).
◆ Conventional iterative approaches linearize a highly non-convex optimization objective, requiring repeated solving of normal equations, which often converge to local minima and thus produce suboptimal estimates.
◆ We propose a scalable, outlier-robust distributed planar PGO framework using Multi-Agent Reinforcement Learning (MARL).</td></tr>
<tr><td>2025-10-26</td><td>LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering</td><td>[2510.22669](http://arxiv.org/pdf/2510.22669)</td><td>◆ 3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence.
◆ However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity.
◆ To address these challenges, we propose \textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system.</td></tr>
<tr><td>2025-10-26</td><td>RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience</td><td>[2510.22600](http://arxiv.org/pdf/2510.22600)</td><td>◆ The reliability of Simultaneous Localization and Mapping (SLAM) is severely constrained in environments where visual inputs suffer from noise and low illumination.
◆ Although recent 3D Gaussian Splatting (3DGS) based SLAM frameworks achieve high-fidelity mapping under clean conditions, they remain vulnerable to compounded degradations that degrade mapping and tracking performance.
◆ A key observation underlying our work is that the original 3DGS rendering pipeline inherently behaves as an implicit low-pass filter, attenuating high-frequency noise but also risking over-smoothing.</td></tr>
<tr><td>2025-10-26</td><td>UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models</td><td>[2510.22588](http://arxiv.org/pdf/2510.22588)</td><td>◆ Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering.
◆ To address this limitation, we introduce UltraVoice, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control.
◆ Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles.</td></tr>
<tr><td>2025-10-26</td><td>Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</td><td>[2510.22529](http://arxiv.org/pdf/2510.22529)</td><td>◆ Loop closure is critical in Simultaneous Localization and Mapping (SLAM) systems to reduce accumulative drift and ensure global mapping consistency.
◆ However, conventional methods struggle in perceptually aliased environments, such as narrow pipes, due to vector quantization, feature sparsity, and repetitive textures, while existing solutions often incur high computational costs.
◆ This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure detection method that achieves superior precision-recall, robustness, and computational efficiency.</td></tr>
<tr><td>2025-10-24</td><td>Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments</td><td>[2510.21215](http://arxiv.org/pdf/2510.21215)</td><td>◆ Visual degradation caused by limited visibility, insufficient lighting, and feature scarcity in underwater environments presents significant challenges to visual-inertial simultaneous localization and mapping (SLAM) systems.
◆ To address these challenges, this paper proposes a graph-based visual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an inertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure sensor.
◆ The key innovation lies in the tight integration of four distinct sensor modalities to ensure reliable operation, even under degraded visual conditions.</td></tr>
<tr><td>2025-10-23</td><td>Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</td><td>[2510.20549](http://arxiv.org/pdf/2510.20549)</td><td>◆ Despite advancements in SLAM technologies, robust operation under challenging conditions such as low-texture, motion-blur, or challenging lighting remains an open challenge.
◆ Such conditions are common in applications such as assistive navigation for the visually impaired.
◆ These challenges undermine localization accuracy and tracking stability, reducing navigation reliability and safety.</td></tr>
<tr><td>2025-10-21</td><td>Underwater Dense Mapping with the First Compact 3D Sonar</td><td>[2510.18991](http://arxiv.org/pdf/2510.18991)</td><td>◆ In the past decade, the adoption of compact 3D range sensors, such as LiDARs, has driven the developments of robust state-estimation pipelines, making them a standard sensor for aerial, ground, and space autonomy.
◆ Unfortunately, poor propagation of electromagnetic waves underwater, has limited the visibility-independent sensing options of underwater state-estimation to acoustic range sensors, which provide 2D information including, at-best, spatially ambiguous information.
◆ This paper, to the best of our knowledge, is the first study examining the performance, capacity, and opportunities arising from the recent introduction of the first compact 3D sonar.</td></tr>
<tr><td>2025-10-21</td><td>DeepDetect: Learning All-in-One Dense Keypoints</td><td>[2510.17422](http://arxiv.org/pdf/2510.17422)</td><td>◆ Keypoint detection is the foundation of many computer vision tasks, including image registration, structure-from motion, 3D reconstruction, visual odometry, and SLAM.
◆ Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong performance yet suffer from key limitations: sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding, often failing to prioritize visually important regions.
◆ We present DeepDetect, an intelligent, all-in-one, dense keypoint detector that unifies the strengths of classical detectors using deep learning.</td></tr>
<tr><td>2025-10-18</td><td>LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching</td><td>[2510.16438](http://arxiv.org/pdf/2510.16438)</td><td>◆ Lines and points are complementary local features, whose combination has proven effective for applications such as SLAM and Structure-from-Motion.
◆ The backbone of these pipelines are the local feature matchers, establishing correspondences across images.
◆ Traditionally, point and line matching have been treated as independent tasks.</td></tr>
<tr><td>2025-10-17</td><td>VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments</td><td>[2510.16205](http://arxiv.org/pdf/2510.16205)</td><td>◆ Visual SLAM in dynamic environments remains challenging, as several existing methods rely on semantic filtering that only handles known object classes, or use fixed robust kernels that cannot adapt to unknown moving objects, leading to degraded accuracy when they appear in the scene.
◆ We present VAR-SLAM (Visual Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a lightweight semantic keypoint filter to deal with known moving objects, with Barron&#x27;s adaptive robust loss to handle unknown ones.
◆ The shape parameter of the robust kernel is estimated online from residuals, allowing the system to automatically adjust between Gaussian and heavy-tailed behavior.</td></tr>
<tr><td>2025-10-17</td><td>Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion</td><td>[2510.15803](http://arxiv.org/pdf/2510.15803)</td><td>◆ This paper presents a novel fusion technique for LiDAR Simultaneous Localization and Mapping (SLAM), aimed at improving localization and 3D mapping using LiDAR sensor.
◆ Our approach centers on the Inferred Attention Fusion (INAF) module, which integrates AI with geometric odometry.
◆ Utilizing the KITTI dataset&#x27;s LiDAR data, INAF dynamically adjusts attention weights based on environmental feedback, enhancing the system&#x27;s adaptability and measurement accuracy.</td></tr>
<tr><td>2025-10-17</td><td>LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization</td><td>[2510.15220](http://arxiv.org/pdf/2510.15220)</td><td>◆ Autonomous navigation for legged robots in complex and dynamic environments relies on robust simultaneous localization and mapping (SLAM) systems to accurately map surroundings and localize the robot, ensuring safe and efficient operation.
◆ While prior sensor fusion-based SLAM approaches have integrated various sensor modalities to improve their robustness, these algorithms are still susceptible to estimation drift in challenging environments due to their reliance on unsuitable fusion strategies.
◆ Therefore, we propose a robust LiDAR-visual-inertial-kinematic odometry system that integrates information from multiple sensors, such as a camera, LiDAR, inertial measurement unit (IMU), and joint encoders, for visual and LiDAR-based odometry estimation.</td></tr>
<tr><td>2025-10-16</td><td>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</td><td>[2510.14945](http://arxiv.org/pdf/2510.14945)</td><td>◆ We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency.
◆ Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video.
◆ Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency.</td></tr>
<tr><td>2025-10-15</td><td>Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU</td><td>[2510.13546](http://arxiv.org/pdf/2510.13546)</td><td>◆ Feature detection is a common yet time-consuming module in Simultaneous Localization and Mapping (SLAM) implementations, which are increasingly deployed on power-constrained platforms, such as drones.
◆ Graphics Processing Units (GPUs) have been a popular accelerator for computer vision in general, and feature detection and SLAM in particular.
◆ On the other hand, System-on-Chips (SoCs) with integrated Field Programmable Gate Array (FPGA) are also widely available.</td></tr>
<tr><td>2025-10-15</td><td>Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition</td><td>[2510.13464](http://arxiv.org/pdf/2510.13464)</td><td>◆ Visual Place Recognition (VPR) enables robots and autonomous vehicles to identify previously visited locations by matching current observations against a database of known places.
◆ However, VPR systems face significant challenges when deployed across varying visual environments, lighting conditions, seasonal changes, and viewpoints changes.
◆ Failure-critical VPR applications, such as loop closure detection in simultaneous localization and mapping (SLAM) pipelines, require robust estimation of place matching uncertainty.</td></tr>
<tr><td>2025-10-15</td><td>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</td><td>[2510.13287](http://arxiv.org/pdf/2510.13287)</td><td>◆ LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for enabling precise navigation and environmental reconstruction across various applications.
◆ Although current point-to-plane ICP algorithms perform effec- tively in structured, feature-rich environments, they struggle in scenarios with sparse features, repetitive geometric structures, and high-frequency motion.
◆ This leads to degeneracy in 6- DOF pose estimation.</td></tr>
<tr><td>2025-10-14</td><td>SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding</td><td>[2510.12749](http://arxiv.org/pdf/2510.12749)</td><td>◆ The scene perception, understanding, and simulation are fundamental techniques for embodied-AI agents, while existing solutions are still prone to segmentation deficiency, dynamic objects&#x27; interference, sensor data sparsity, and view-limitation problems.
◆ This paper proposes a novel framework, named SPORTS, for holistic scene understanding via tightly integrating Video Panoptic Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into an iterative and unified perspective.
◆ Firstly, VPS designs an adaptive attention-based geometric fusion mechanism to align cross-frame features via enrolling the pose, depth, and optical flow modality, which automatically adjust feature maps for different decoding stages.</td></tr>
<tr><td>2025-10-14</td><td>PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing</td><td>[2510.12346](http://arxiv.org/pdf/2510.12346)</td><td>◆ Recently, biped robot walking technology has been significantly developed, mainly in the context of a bland walking scheme.
◆ To emulate human walking, robots need to step on the positions they see in unknown spaces accurately.
◆ In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs.</td></tr>
<tr><td>2025-10-09</td><td>ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation</td><td>[2510.08551](http://arxiv.org/pdf/2510.08551)</td><td>◆ On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as real-to-sim, AR/VR, and robotics.
◆ Existing methods face a major tradeoff: per-scene optimization yields high fidelity but is computationally expensive, whereas feed-forward foundation models enable real-time inference but struggle with accuracy and robustness.
◆ In this work, we propose ARTDECO, a unified framework that combines the efficiency of feed-forward models with the reliability of SLAM-based pipelines.</td></tr>
<tr><td>2025-10-09</td><td>RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction</td><td>[2510.06644](http://arxiv.org/pdf/2510.06644)</td><td>◆ 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping (SLAM) systems can largely benefit from 3DGS&#x27;s state-of-the-art rendering efficiency and accuracy, but have not yet been adopted in resource-constrained edge devices due to insufficient speed.
◆ Addressing this, we identify notable redundancies across the SLAM pipeline for acceleration.
◆ While conceptually straightforward, practical approaches are required to minimize the overhead associated with identifying and eliminating these redundancies.</td></tr>
<tr><td>2025-10-07</td><td>Human3R: Everyone Everywhere All at Once</td><td>[2510.06219](http://arxiv.org/pdf/2510.06219)</td><td>◆ We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos.
◆ Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies (&quot;everyone&quot;), dense 3D scene (&quot;everywhere&quot;), and camera trajectories in a single forward pass (&quot;all-at-once&quot;).
◆ Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R&#x27;s rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies.</td></tr>
<tr><td>2025-10-07</td><td>Dropping the D: RGB-D SLAM Without the Depth Sensor</td><td>[2510.06216](http://arxiv.org/pdf/2510.06216)</td><td>◆ We present DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors.
◆ The system replaces active depth input with three pretrained vision modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation network.
◆ Dynamic objects are suppressed using dilated instance masks, while static keypoints are assigned predicted depth values and backprojected into 3D to form metrically scaled features.</td></tr>
<tr><td>2025-10-07</td><td>Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations</td><td>[2510.05992](http://arxiv.org/pdf/2510.05992)</td><td>◆ Onboard simultaneous localization and mapping (SLAM) methods are commonly used to provide accurate localization information for autonomous robots.
◆ However, the coordinate origin of SLAM estimate often resets for each run.
◆ On the other hand, UWB-based localization with fixed anchors can ensure a consistent coordinate reference across sessions; however, it requires an accurate assignment of the anchor nodes&#x27; coordinates.</td></tr>
<tr><td>2025-10-06</td><td>OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS</td><td>[2510.04612](http://arxiv.org/pdf/2510.04612)</td><td>◆ To empower mobile robots with usable maps as well as highest state estimation accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor Simultaneous Localization and Mapping (SLAM) system building dense volumetric occupancy maps, while scalable to large environments and operating in realtime.
◆ Our unified SLAM framework seamlessly integrates different sensor modalities: visual, inertial, measured or learned depth, LiDAR and Global Navigation Satellite System (GNSS) measurements.
◆ Unlike most state-of-the-art SLAM systems, we advocate using dense volumetric map representations when leveraging depth or range-sensing capabilities.</td></tr>
<tr><td>2025-10-02</td><td>Visual Odometry with Transformers</td><td>[2510.03348](http://arxiv.org/pdf/2510.03348)</td><td>◆ Modern monocular visual odometry methods typically combine pre-trained deep learning components with optimization modules, resulting in complex pipelines that rely heavily on camera calibration and hyperparameter tuning, and often struggle in unseen real-world scenarios.
◆ Recent large-scale 3D models trained on massive amounts of multi-modal data have partially alleviated these challenges, providing generalizable dense reconstruction and camera pose estimation.
◆ Still, they remain limited in handling long videos and providing accurate per-frame estimates, which are required for visual odometry.</td></tr>
<tr><td>2025-10-02</td><td>RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments</td><td>[2510.02616](http://arxiv.org/pdf/2510.02616)</td><td>◆ Simultaneous Localization and Mapping (SLAM) plays an important role in many robotics fields, including social robots.
◆ Many of the available visual SLAM methods are based on the assumption of a static world and struggle in dynamic environments.
◆ In the current study, we introduce a real-time semantic RGBD SLAM approach designed specifically for dynamic environments.</td></tr>
<tr><td>2025-10-02</td><td>EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction</td><td>[2510.02080](http://arxiv.org/pdf/2510.02080)</td><td>◆ The application of monocular dense Simultaneous Localization and Mapping (SLAM) is often hindered by high latency, large GPU memory consumption, and reliance on camera calibration.
◆ To relax this constraint, we propose EC3R-SLAM, a novel calibration-free monocular dense SLAM framework that jointly achieves high localization and mapping accuracy, low latency, and low GPU memory consumption.
◆ This enables the framework to achieve efficiency through the coupling of a tracking module, which maintains a sparse map of feature points, and a mapping module based on a feed-forward 3D reconstruction model that simultaneously estimates camera intrinsics.</td></tr>
<tr><td>2025-10-02</td><td>Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale</td><td>[2510.01665](http://arxiv.org/pdf/2510.01665)</td><td>◆ Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention.
◆ We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset.
◆ Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework.</td></tr>
<tr><td>2025-10-01</td><td>Instant4D: 4D Gaussian Splatting in Minutes</td><td>[2510.01119](http://arxiv.org/pdf/2510.01119)</td><td>◆ Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation.
◆ In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors.
◆ Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation.</td></tr>
<tr><td>2025-10-01</td><td>Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions</td><td>[2510.00783](http://arxiv.org/pdf/2510.00783)</td><td>◆ Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of research within robotics and computer vision, focusing on the simultaneous localization of robotic systems and associating semantic information to construct the most accurate and complete comprehensive model of the surrounding environment.
◆ Since the first foundational work in Semantic SLAM appeared more than two decades ago, this field has received increasing attention across various scientific communities.
◆ Despite its significance, the field lacks comprehensive surveys encompassing recent advances and persistent challenges.</td></tr>
<tr><td>2025-09-30</td><td>Benchmarking Egocentric Visual-Inertial SLAM at City Scale</td><td>[2509.26639](http://arxiv.org/pdf/2509.26639)</td><td>◆ Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration.
◆ While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses.
◆ In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data.</td></tr>
<tr><td>2025-09-30</td><td>Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework</td><td>[2509.26581](http://arxiv.org/pdf/2509.26581)</td><td>◆ We present Graphite, a GPU-accelerated nonlinear graph optimization framework.
◆ It provides a CUDA C++ interface to enable the sharing of code between a realtime application, such as a SLAM system, and its optimization tasks.
◆ The framework supports techniques to reduce memory usage, including in-place optimization, support for multiple floating point types and mixed-precision modes, and dynamically computed Jacobians.</td></tr>
<tr><td>2025-09-30</td><td>Radio-based Multi-Robot Odometry and Relative Localization</td><td>[2509.26558](http://arxiv.org/pdf/2509.26558)</td><td>◆ Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And Ranging (radar), which have traditionally seen limited adoption in robotics, are experiencing a boost in popularity thanks to their robustness to harsh environmental conditions and cluttered environments.
◆ This work proposes a multi-robot UGV-UAV localization system that leverages the two technologies with inexpensive and readily-available sensors, such as Inertial Measurement Units (IMUs) and wheel encoders, to estimate the relative position of an aerial robot with respect to a ground robot.
◆ The first stage of the system pipeline includes a nonlinear optimization framework to trilaterate the location of the aerial platform based on UWB range data, and a radar pre-processing module with loosely coupled ego-motion estimation which has been adapted for a multi-robot scenario.</td></tr>
<tr><td>2025-09-30</td><td>DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance</td><td>[2509.26498](http://arxiv.org/pdf/2509.26498)</td><td>◆ Depth enhancement, which converts raw dToF signals into dense depth maps using RGB guidance, is crucial for improving depth perception in high-precision tasks such as 3D reconstruction and SLAM.
◆ However, existing methods often assume ideal dToF inputs and perfect dToF-RGB alignment, overlooking calibration errors and anomalies, thus limiting real-world applicability.
◆ This work systematically analyzes the noise characteristics of real-world lightweight dToF sensors and proposes a practical and novel depth completion framework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three key aspects.</td></tr>
<tr><td>2025-09-30</td><td>Side Scan Sonar-based SLAM for Autonomous Algae Farm Monitoring</td><td>[2509.26121](http://arxiv.org/pdf/2509.26121)</td><td>◆ The transition of seaweed farming to an alternative food source on an industrial scale relies on automating its processes through smart farming, equivalent to land agriculture.
◆ Key to this process are autonomous underwater vehicles (AUVs) via their capacity to automate crop and structural inspections.
◆ However, the current bottleneck for their deployment is ensuring safe navigation within farms, which requires an accurate, online estimate of the AUV pose and map of the infrastructure.</td></tr>
<tr><td>2025-09-30</td><td>User-Centric Communication Service Provision for Edge-Assisted Mobile Augmented Reality</td><td>[2509.25905](http://arxiv.org/pdf/2509.25905)</td><td>◆ Future 6G networks are envisioned to facilitate edge-assisted mobile augmented reality (MAR) via strengthening the collaboration between MAR devices and edge servers.
◆ In order to provide immersive user experiences, MAR devices must timely upload camera frames to an edge server for simultaneous localization and mapping (SLAM)-based device pose tracking.
◆ In this paper, to cope with user-specific and non-stationary uplink data traffic, we develop a digital twin (DT)-based approach for user-centric communication service provision for MAR.</td></tr>
<tr><td>2025-09-29</td><td>PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization</td><td>[2509.24236](http://arxiv.org/pdf/2509.24236)</td><td>◆ Real-time dense scene reconstruction during unstable camera motions is crucial for robotics, yet current RGB-D SLAM systems fail when cameras experience large viewpoint changes, fast motions, or sudden shaking.
◆ Classical optimization-based methods deliver high accuracy but fail with poor initialization during large motions, while learning-based approaches provide robustness but lack sufficient accuracy for dense reconstruction.
◆ We address this challenge through a combination of learning-based initialization with optimization-based refinement.</td></tr>
<tr><td>2025-09-28</td><td>GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State</td><td>[2509.23737](http://arxiv.org/pdf/2509.23737)</td><td>◆ DUSt3R-based end-to-end scene reconstruction has recently shown promising results in dense visual SLAM.
◆ However, most existing methods only use image pairs to estimate pointmaps, overlooking spatial memory and global consistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework for dense scene reconstruction and pose estimation from RGB images without any prior knowledge of the scene or camera parameters.
◆ Unlike existing DUSt3R-based frameworks, which operate on all image pairs and predict per-pair point maps in local coordinate frames, our method supports sequentialized input and incrementally estimates metric-scale point clouds in the global coordinate.</td></tr>
<tr><td>2025-09-28</td><td>From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations</td><td>[2509.23555](http://arxiv.org/pdf/2509.23555)</td><td>◆ Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted.
◆ NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding.
◆ This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation.</td></tr>
<tr><td>2025-09-27</td><td>EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation</td><td>[2509.23118](http://arxiv.org/pdf/2509.23118)</td><td>◆ Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting cannot meet the growing demand for accurate indoor localization and navigation due to its lower accuracy, while solutions based on light detection and ranging (LiDAR) can provide better localization performance but is limited by their higher deployment cost and complexity.
◆ To address these issues, we propose a novel indoor localization and navigation framework integrating Wi-Fi RSSI fingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and inertial measurement unit (IMU) navigation based on an extended Kalman filter (EKF).
◆ Specifically, coarse localization by deep neural network (DNN)-based Wi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a Gmapping-based SLAM to generate an occupancy grid map and output high-frequency attitude estimates, which is followed by EKF prediction-update integrating sensor information while effectively suppressing Wi-Fi-induced noise and IMU drift errors.</td></tr>
<tr><td>2025-09-26</td><td>Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</td><td>[2509.22910](http://arxiv.org/pdf/2509.22910)</td><td>◆ Given that Visual SLAM relies on appearance cues for localization and scene understanding, texture-less or visually degraded environments (e.g., plain walls or low lighting) lead to poor pose estimation and track loss.
◆ However, robots are typically equipped with sensors that provide some form of dead reckoning odometry with reasonable short-time performance but unreliable long-time performance.
◆ The Good Weights (GW) algorithm described here provides a framework to adaptively integrate dead reckoning (DR) with passive visual SLAM for continuous and accurate frame-level pose estimation.</td></tr>
<tr><td>2025-09-26</td><td>IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM</td><td>[2509.22288](http://arxiv.org/pdf/2509.22288)</td><td>◆ Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor graph node per measurement to compensate for the lack of time synchronization between radar and LiDAR.
◆ For a radar-LiDAR sensor pair with equal rates, this strategy results in a state creation rate of twice the individual sensor frequencies.
◆ This doubling of the number of states per second yields high optimization costs, inhibiting real-time performance on resource-constrained hardware.</td></tr>
<tr><td>2025-09-25</td><td>Real-Time Indoor Object SLAM with LLM-Enhanced Priors</td><td>[2509.21602](http://arxiv.org/pdf/2509.21602)</td><td>◆ Object-level Simultaneous Localization and Mapping (SLAM), which incorporates semantic information for high-level scene understanding, faces challenges of under-constrained optimization due to sparse observations.
◆ Prior work has introduced additional constraints using commonsense knowledge, but obtaining such priors has traditionally been labor-intensive and lacks generalizability across diverse object categories.
◆ We address this limitation by leveraging large language models (LLMs) to provide commonsense knowledge of object geometric attributes, specifically size and orientation, as prior factors in a graph-based SLAM framework.</td></tr>
<tr><td>2025-09-25</td><td>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</td><td>[2509.21006](http://arxiv.org/pdf/2509.21006)</td><td>◆ We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation.
◆ A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy.
◆ An approach planner then selects visibility and reachability aware pre grasp base poses.</td></tr>
<tr><td>2025-09-29</td><td>MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM</td><td>[2509.20757](http://arxiv.org/pdf/2509.20757)</td><td>◆ Visual SLAM is a cornerstone technique in robotics, autonomous driving and extended reality (XR), yet classical systems often struggle with low-texture environments, scale ambiguity, and degraded performance under challenging visual conditions.
◆ Recent advancements in feed-forward neural network-based pointmap regression have demonstrated the potential to recover high-fidelity 3D scene geometry directly from images, leveraging learned spatial priors to overcome limitations of traditional multi-view geometry methods.
◆ However, the widely validated advantages of probabilistic multi-sensor information fusion are often discarded in these pipelines.</td></tr>
<tr><td>2025-09-25</td><td>SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning</td><td>[2509.20739](http://arxiv.org/pdf/2509.20739)</td><td>◆ Conventional SLAM pipelines for legged robot navigation are fragile under rapid motion, calibration demands, and sensor drift, while offering limited semantic reasoning for task-driven exploration.
◆ To deal with these issues, we propose a vision-only, SLAM-free navigation framework that replaces dense geometry with semantic reasoning and lightweight topological representations.
◆ A hierarchical vision-language perception module fuses scene-level context with object-level cues for robust semantic inference.</td></tr>
<tr><td>2025-09-24</td><td>Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research</td><td>[2509.20171](http://arxiv.org/pdf/2509.20171)</td><td>◆ The development and evaluation of machine vision in underwater environments remains challenging, often relying on trial-and-error-based testing tailored to specific applications.
◆ This is partly due to the lack of controlled, ground-truthed testing environments that account for the optical challenges, such as color distortion from spectrally variant light attenuation, reduced contrast and blur from backscatter and volume scattering, and dynamic light patterns from natural or artificial illumination.
◆ Additionally, the appearance of ocean water in images varies significantly across regions, depths, and seasons.</td></tr>
<tr><td>2025-09-23</td><td>Bioinspired SLAM Approach for Unmanned Surface Vehicle</td><td>[2509.19522](http://arxiv.org/pdf/2509.19522)</td><td>◆ This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a bioinspired SLAM framework based on computational models of the rodent hippocampus.
◆ OpenRatSLAM2 delivers low-computation-cost visual-inertial based SLAM, suitable for GPS-denied environments.
◆ Our contributions include a ROS2-based architecture, experimental results on new waterway datasets, and insights into system parameter tuning.</td></tr>
<tr><td>2025-09-23</td><td>CU-Multi: A Dataset for Multi-Robot Collaborative Perception</td><td>[2509.19463](http://arxiv.org/pdf/2509.19463)</td><td>◆ A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation.
◆ Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets.
◆ Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies.</td></tr>
<tr><td>2025-09-23</td><td>Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation</td><td>[2509.18954](http://arxiv.org/pdf/2509.18954)</td><td>◆ LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans.
◆ However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation.
◆ Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions.</td></tr>
<tr><td>2025-09-22</td><td>Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation</td><td>[2509.18342](http://arxiv.org/pdf/2509.18342)</td><td>◆ Accurate localisation is critical for mobile robots in structured outdoor environments, yet LiDAR-based methods often fail in vineyards due to repetitive row geometry and perceptual aliasing.
◆ We propose a semantic particle filter that incorporates stable object-level detections, specifically vine trunks and support poles into the likelihood estimation process.
◆ Detected landmarks are projected into a birds eye view and fused with LiDAR scans to generate semantic observations.</td></tr>
<tr><td>2025-09-22</td><td>ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos</td><td>[2509.17864](http://arxiv.org/pdf/2509.17864)</td><td>◆ Achieving truly practical dynamic 3D reconstruction requires online operation, global pose and map consistency, detailed appearance modeling, and the flexibility to handle both RGB and RGB-D inputs.
◆ However, existing SLAM methods typically merely remove the dynamic parts or require RGB-D input, while offline methods are not scalable to long video sequences, and current transformer-based feedforward methods lack global consistency and appearance details.
◆ To this end, we achieve online dynamic scene reconstruction by disentangling the static and dynamic parts within a SLAM system.</td></tr>
<tr><td>2025-09-23</td><td>SLAM-Former: Putting SLAM into One Transformer</td><td>[2509.16909](http://arxiv.org/pdf/2509.16909)</td><td>◆ We present SLAM-Former, a novel neural approach that integrates full SLAM capabilities into a single transformer.
◆ Similar to traditional SLAM systems, SLAM-Former comprises both a frontend and a backend that operate in tandem.
◆ The frontend processes sequential monocular images in real-time for incremental mapping and tracking, while the backend performs global refinement to ensure a geometrically consistent result.</td></tr>
<tr><td>2025-09-21</td><td>ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM</td><td>[2509.16863](http://arxiv.org/pdf/2509.16863)</td><td>◆ We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM system for robust, highfidelity RGB-only reconstruction.
◆ Addressing geometric inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable depth estimation, ConfidentSplat incorporates a core innovation: a confidence-weighted fusion mechanism.
◆ This mechanism adaptively integrates depth cues from multiview geometry with learned monocular priors (Omnidata ViT), dynamically weighting their contributions based on explicit reliability estimates-derived predominantly from multi-view geometric consistency-to generate high-fidelity proxy depth for map supervision.</td></tr>
<tr><td>2025-09-19</td><td>SLaM-DiMM: Shared Latent Modeling for Diffusion Based Missing Modality Synthesis in MRI</td><td>[2509.16019](http://arxiv.org/pdf/2509.16019)</td><td>◆ Brain MRI scans are often found in four modalities, consisting of T1-weighted with and without contrast enhancement (T1ce and T1w), T2-weighted imaging (T2w), and Flair.
◆ Leveraging complementary information from these different modalities enables models to learn richer, more discriminative features for understanding brain anatomy, which could be used in downstream tasks such as anomaly detection.
◆ However, in clinical practice, not all MRI modalities are always available due to various reasons.</td></tr>
<tr><td>2025-09-19</td><td>Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion</td><td>[2509.15673](http://arxiv.org/pdf/2509.15673)</td><td>◆ Wide field-of-view (FoV) LiDAR sensors provide dense geometry across large environments, but most existing LiDAR-inertial-visual odometry (LIVO) systems rely on a single camera, leading to limited spatial coverage and degraded robustness.
◆ We present Omni-LIVO, the first tightly coupled multi-camera LIVO system that bridges the FoV mismatch between wide-angle LiDAR and conventional cameras.
◆ Omni-LIVO introduces a Cross-View direct tracking strategy that maintains photometric consistency across non-overlapping views, and extends the Error-State Iterated Kalman Filter (ESIKF) with multi-view updates and adaptive covariance weighting.</td></tr>
<tr><td>2025-09-18</td><td>Human Interaction for Collaborative Semantic SLAM using Extended Reality</td><td>[2509.14949](http://arxiv.org/pdf/2509.14949)</td><td>◆ Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robot maps with structural and semantic information, enabling robots to operate more effectively in complex environments.
◆ However, these systems struggle in real-world scenarios with occlusions, incomplete data, or ambiguous geometries, as they cannot fully leverage the higher-level spatial and semantic knowledge humans naturally apply.
◆ We introduce HICS-SLAM, a Human-in-the-Loop semantic SLAM framework that uses a shared extended reality environment for real-time collaboration.</td></tr>
<tr><td>2025-09-18</td><td>BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots</td><td>[2509.14636](http://arxiv.org/pdf/2509.14636)</td><td>◆ Bird&#x27;s-Eye-View (BEV) representation offers a metric-scaled planar workspace, facilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF model for monocular visual odometry (MVO) in intelligent transportation systems.
◆ However, existing BEV methods suffer from sparse supervision signals and information loss during perspective-to-BEV projection.
◆ We present BEV-ODOM2, an enhanced framework addressing both limitations without additional annotations.</td></tr>
<tr><td>2025-09-18</td><td>Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods</td><td>[2509.14516](http://arxiv.org/pdf/2509.14516)</td><td>◆ Event-based localization research and datasets are a rapidly growing area of interest, with a tenfold increase in the cumulative total number of published papers on this topic over the past 10 years.
◆ Whilst the rapid expansion in the field is exciting, it brings with it an associated challenge: a growth in the variety of required code and package dependencies as well as data formats, making comparisons difficult and cumbersome for researchers to implement reliably.
◆ To address this challenge, we present Event-LAB: a new and unified framework for running several event-based localization methodologies across multiple datasets.</td></tr>
<tr><td>2025-09-17</td><td>MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping</td><td>[2509.14191](http://arxiv.org/pdf/2509.14191)</td><td>◆ Recent progress in dense SLAM has primarily targeted monocular setups, often at the expense of robustness and geometric coverage.
◆ We present MCGS-SLAM, the first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting (3DGS).
◆ Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified, continuously optimized Gaussian map.</td></tr>
<tr><td>2025-09-17</td><td>BIM Informed Visual SLAM for Construction Monitoring</td><td>[2509.13972](http://arxiv.org/pdf/2509.13972)</td><td>◆ Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring construction sites, where aligning the evolving as-built state with the as-planned design enables early error detection and reduces costly rework.
◆ LiDAR-based SLAM achieves high geometric precision, but its sensors are typically large and power-demanding, limiting their use on portable platforms.
◆ Visual SLAM offers a practical alternative with lightweight cameras already embedded in most mobile devices.</td></tr>
<tr><td>2025-09-17</td><td>UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry</td><td>[2509.13713](http://arxiv.org/pdf/2509.13713)</td><td>◆ Monocular depth estimation has been increasingly adopted in robotics and autonomous driving for its ability to infer scene geometry from a single camera.
◆ In self-supervised monocular depth estimation frameworks, the network jointly generates and exploits depth and pose estimates during training, thereby eliminating the need for depth labels.
◆ However, these methods remain challenged by uncertainty in the input data, such as low-texture or dynamic regions, which can cause reduced depth accuracy.</td></tr>
<tr><td>2025-09-17</td><td>Barometer-Aided Attitude Estimation</td><td>[2509.13649](http://arxiv.org/pdf/2509.13649)</td><td>◆ Accurate and robust attitude estimation is a central challenge for autonomous vehicles operating in GNSS-denied or highly dynamic environments.
◆ In such cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable tilt estimation due to the ambiguity between gravitational and inertial accelerations.
◆ While auxiliary velocity sensors, such as GNSS, Pitot tubes, Doppler radar, or visual odometry, are often used, they can be unavailable, intermittent, or costly.</td></tr>
<tr><td>2025-09-16</td><td>Semantic 3D Reconstructions with SLAM for Central Airway Obstruction</td><td>[2509.13541](http://arxiv.org/pdf/2509.13541)</td><td>◆ Central airway obstruction (CAO) is a life-threatening condition with increasing incidence, caused by tumors in and outside of the airway.
◆ Traditional treatment methods such as bronchoscopy and electrocautery can be used to remove the tumor completely; however, these methods carry a high risk of complications.
◆ Recent advances allow robotic interventions with lesser risk.</td></tr>
<tr><td>2025-09-16</td><td>MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM</td><td>[2509.13536](http://arxiv.org/pdf/2509.13536)</td><td>◆ Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant impact on rendering and reconstruction techniques.
◆ Current research predominantly focuses on improving rendering performance and reconstruction quality using high-performance desktop GPUs, largely overlooking applications for embedded platforms like micro air vehicles (MAVs).
◆ These devices, with their limited computational resources and memory, often face a trade-off between system performance and reconstruction quality.</td></tr>
<tr><td>2025-09-18</td><td>MATTER: Multiscale Attention for Registration Error Regression</td><td>[2509.12924](http://arxiv.org/pdf/2509.12924)</td><td>◆ Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking.
◆ This makes detecting and quantifying registration misalignment, i.e.,~{\it PCR quality validation}, an important task.
◆ All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes.</td></tr>
<tr><td>2025-09-16</td><td>Match Chat: Real Time Generative AI and Generative Computing for Tennis</td><td>[2509.12592](http://arxiv.org/pdf/2509.12592)</td><td>◆ We present Match Chat, a real-time, agent-driven assistant designed to enhance the tennis fan experience by delivering instant, accurate responses to match-related queries.
◆ Match Chat integrates Generative Artificial Intelligence (GenAI) with Generative Computing (GenComp) techniques to synthesize key insights during live tennis singles matches.
◆ The system debuted at the 2025 Wimbledon Championships and the 2025 US Open, where it provided about 1 million users with seamless access to streaming and static data through natural language queries.</td></tr>
<tr><td>2025-09-15</td><td>See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays</td><td>[2509.11653](http://arxiv.org/pdf/2509.11653)</td><td>本文针对光学透视头显中世界相机与用户视角不匹配问题，提出了三种眼视角渲染技术以提升增强现实视觉引导的准确性。  
◆ 实现了基于平面投影的Plane-Proxy EPR方法，在固定平面上近似眼视角。  
◆ 开发了基于SLAM场景重建的Mesh-Proxy EPR方法，通过几何网格提高投影精度。  
◆ 创新性地提出Gaze-Proxy EPR方法，利用眼动追踪技术将投影与用户注视深度动态对齐。  
通过真实任务用户研究证明，精确的眼视角渲染对任务性能至关重要，且所提注视代理方法可作为轻量级替代方案。  
研究最终开源了完整的眼视角渲染框架，为后续研究提供基础支持。</td></tr>
<tr><td>2025-09-15</td><td>Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps</td><td>[2509.11574](http://arxiv.org/pdf/2509.11574)</td><td>该论文提出了GPS-SLAM，一种能实现超过150 fps实时高保真三维重建的新系统，其核心贡献在于通过一种混合表示方法解决了现有高斯SLAM方法计算效率低下的瓶颈问题。

◆ 创新性地结合了彩色符号距离场（SDF）和3D高斯两种表示，SDF负责高效构建平滑的几何与外观，而高斯则专门用于补足细节。
◆ 该混合表示避免了对整个场景都用高斯建模，从而将所需的高斯数量减少了50%。
◆ 通过让SDF承担大部分基础表示工作，高斯优化只需进行针对性的外观细化，使得优化迭代次数大幅减少75%。
◆ 最终构建的GPS-SLAM系统在保持与最先进方法相当重建质量的同时，实现了超过一个数量级的加速，在真实数据集上运行速度超过150帧/秒。</td></tr>
<tr><td>2025-09-13</td><td>FastTrack: GPU-Accelerated Tracking for Visual SLAM</td><td>[2509.10757](http://arxiv.org/pdf/2509.10757)</td><td>该论文的核心贡献是提出了一种利用GPU加速来显著提升视觉-惯性SLAM系统跟踪模块性能的新方法FastTrack。  
◆ 创新性地利用GPU并行计算能力来加速跟踪过程中最耗时的组件，包括立体特征匹配和局部地图跟踪。  
◆ 将加速设计具体实现在主流的ORB-SLAM3框架的跟踪流程中，并使用CUDA进行开发。  
◆ 在公开数据集EuRoC和TUM-VI上的实验表明，该系统在立体-惯性模式下运行，跟踪性能整体提升最高达2.8倍。  
◆ 验证了方案在两种硬件平台（桌面GPU和嵌入式Jetson Xavier NX）上的有效性与通用性，确保了实时性并降低了跟踪丢失的风险。</td></tr>
<tr><td>2025-09-12</td><td>Robust Localization in Modern Cellular Networks using Global Map Features</td><td>[2509.10433](http://arxiv.org/pdf/2509.10433)</td><td>该论文提出了一种增强型多路径同时定位与建图（MP-SLAM）方法，用于现代蜂窝网络中的鲁棒定位。  
◆引入全局地图特征（GMF）存储库，整合历史遍历中收集的高质量地图特征（如虚拟锚点），提升系统对先验知识的利用能力。  
◆通过概率假设密度（PHD）滤波器动态集成全局地图特征，实现特征强度函数的时序传播与融合。  
◆在严重多径传播和小区间干扰的密集城市环境中，基于LTE信号进行了真实实验验证，证明其定位精度和鲁棒性显著优于传统方法。  
◆该方法适用于5G/6G等现代蜂窝网络，即使在非视距（OLoS）等恶劣信号条件下仍能实现可靠定位。  
◆突破了传统MP-SLAM和基于自感知传感器的定位局限，为复杂环境提供了更优的解决方案。</td></tr>
<tr><td>2025-09-11</td><td>SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking</td><td>[2509.09509](http://arxiv.org/pdf/2509.09509)</td><td>SMapper的核心贡献是提出了一种专为SLAM研究设计的开源多模态数据采集平台，以解决现有数据集在传感器多样性、环境覆盖和实验可复现性方面的不足。其创新点包括：
◆ 采用开源硬件设计，集成了同步的LiDAR、多相机和惯性测量单元，提供了丰富的多模态感知数据。
◆ 提供了一套可靠的标定与时间同步流程，确保了跨模态数据在时空上的精确对齐。
◆ 具备可扩展和可复用的结构，支持手持和机器人搭载两种使用场景，增强了平台的适应性。
◆ 公开发布了名为SMapper-light的数据集，包含室内外典型场景的高精度真值轨迹与稠密三维重建结果。
◆ 基于所采集数据对主流LiDAR与视觉SLAM算法进行了系统性能评测，为算法比较与复现提供了坚实基础。
通过硬件开放、数据同步和评测一体化，SMapper显著提升了SLAM研究的可靠性、可比性和可复现性。</td></tr>
<tr><td>2025-09-11</td><td>S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization</td><td>[2509.09110](http://arxiv.org/pdf/2509.09110)</td><td>S-BEVLoc提出了一种基于鸟瞰图（BEV）的自监督激光雷达全局定位框架，其核心贡献在于无需地面真值姿态即可实现大规模定位。  
◆ 首次构建了基于BEV的自监督学习范式，通过利用关键点之间的已知地理距离构建训练三元组，完全摆脱了对GPS或SLAM真值数据的依赖。  
◆ 提出SoftCos损失函数，有效增强从生成的三元组中学习特征表示的能力，提升了模型在困难样本上的鲁棒性。  
◆ 结合CNN局部特征提取与NetVLAD全局描述符聚合，实现了高效且判别性强的场景表示。  
在KITTI和NCLT大规模数据集上的实验表明，该方法在位置识别、回环检测和全局定位任务上达到领先性能，同时展现出远超监督方法的可扩展性。</td></tr>
<tr><td>2025-09-10</td><td>Good Deep Features to Track: Self-Supervised Feature Extraction and Tracking in Visual Odometry</td><td>[2509.08333](http://arxiv.org/pdf/2509.08333)</td><td>该论文针对视觉里程计中因光照变化、动态场景等导致特征提取与跟踪性能下降的问题，提出了一种自监督的深度特征提取与跟踪方法。  
◆ 通过自监督学习结合任务特定反馈，增强深度特征的稳定性和信息量。  
◆ 提升了在挑战性环境（如大尺度户外场景和长期运行）中的泛化能力与可靠性。  
◆ 克服了已有学习方法（如SuperPoint和SuperGlue）在分布外数据上的泛化局限。  
◆ 无需依赖大量标注数据，通过自监督机制优化特征质量。  
该方法显著提高了运动估计的准确性，适用于实际应用中的复杂视觉条件。</td></tr>
<tr><td>2025-09-10</td><td>Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed Task Allocation</td><td>[2509.08242](http://arxiv.org/pdf/2509.08242)</td><td>本文针对行为异构多机器人协同探索问题，提出了一种结合分布式任务分配与博弈论的高效解决方案。  
◆ 引入行为熵（BE）作为异构机器人评估探索效用的核心指标，量化不同行为特性对任务选择的影响。  
◆ 将任务分配问题转化为非合作博弈模型，并设计分布式算法d-PBRAG收敛至纳什均衡，证明该均衡即最优分配方案。  
◆ 针对效用未知场景提出近似奖励方法，提供具有鲁棒性的性能边界保证。  
◆ 算法具备通信成本低和收敛速度快的特点，并通过仿真验证了行为异构团队在探索效率和路径规划上的优势。  
研究结果表明，行为异构的机器人团队能显著提升整体探索性能。</td></tr>
<tr><td>2025-09-10</td><td>Deep Visual Odometry for Stereo Event Cameras</td><td>[2509.08235](http://arxiv.org/pdf/2509.08235)</td><td>该论文提出了一种基于深度学习的立体事件相机视觉里程计系统Stereo-DEVO，其核心贡献在于实现了在复杂光照条件下的高精度实时位姿估计。  
◆ 提出了一种新颖且高效的静态-立体关联策略，用于稀疏深度估计，几乎不增加计算负担。  
◆ 将深度估计与紧耦合的束调整优化方案相结合，提升了系统的精度和鲁棒性。  
◆ 利用基于体素的事件表示和循环神经网络，实现了精确的光流估计和可靠的图像块关联。  
◆ 系统能够实时处理VGA分辨率的事件流，相比之前的离线方案实现了重大突破。  
实验表明，该系统在多个真实世界数据集上性能优于现有方法，尤其在大规模夜间高动态范围场景中仍能保持稳定的位姿估计。</td></tr>
<tr><td>2025-09-10</td><td>Online Dynamic SLAM with Incremental Smoothing and Mapping</td><td>[2509.08197](http://arxiv.org/pdf/2509.08197)</td><td>本文首次将增量优化技术应用于动态SLAM，实现了在线实时估计能力。
◆ 提出了一种新颖的因子图模型，能够联合估计静态场景和动态物体的状态。
◆ 设计了全新的系统架构，充分利用了现有增量优化方法，支持在线高效求解。
◆ 在保证精度的同时显著提升了计算效率，其相机位姿和物体运动估计精度达到或超过了现有最优方法。
◆ 通过问题结构分析，证明了该方法具有良好的可扩展性，并揭示了增量式求解动态SLAM的关键挑战。
最终，该系统架构进一步提升了性能，相比现有方法实现了5倍的加速。</td></tr>
<tr><td>2025-09-09</td><td>Sensing with Mobile Devices through Radio SLAM: Models, Methods, Opportunities, and Challenges</td><td>[2509.07775](http://arxiv.org/pdf/2509.07775)</td><td>本文探讨了无线电SLAM作为6G通感一体化（ISAC）的关键方法，利用无线信号实现同步定位与建图。  
◆ 提出将无线电SLAM作为6G通感一体化的核心实现路径，通过单一无线信号同时支持通信与环境感知。  
◆ 系统分析了不同频段下无线电SLAM的性能权衡，包括覆盖范围、分辨率和硬件需求，为实际部署提供理论依据。  
◆ 强调了与传感、定位及协同网络融合的机遇，推动多技术协同发展。  
◆ 为自动驾驶系统和工业机器人等6G应用领域的标准化解决方案奠定基础，促进技术落地。  
论文通过模型和方法创新，为未来6G网络中的高精度环境感知与定位提供了重要参考。</td></tr>
<tr><td>2025-09-10</td><td>Robust Radar SLAM for Vehicle Parking Applications</td><td>[2509.07683](http://arxiv.org/pdf/2509.07683)</td><td>该论文针对自动泊车场景中的高精度定位需求，提出了一种基于雷达的鲁棒SLAM方法，以解决传统传感器标定成本高和对恶劣天气敏感的问题。其核心创新点包括：

◆ 提出一种多普勒增强的雷达SLAM方法，通过融合多普勒速度和特征位置实现更鲁棒的数据关联与滤波器收敛  
◆ 支持多雷达传感器融合，提升系统的感知能力和环境适应性  
◆ 设计基于信息论的特征筛选策略，优化地图特征管理并提高计算效率  
实验表明，该方法在定位精度和鲁棒性上优于现有技术，能够满足自动泊车对厘米级精度的严格要求。</td></tr>
<tr><td>2025-09-09</td><td>Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark</td><td>[2509.07362](http://arxiv.org/pdf/2509.07362)</td><td>该论文的核心贡献是构建了一个解决空地跨模态定位挑战的综合基准。其创新点包括：
◆ 创建了一个新的大规模空地跨模态数据集，集成了来自移动测量系统的地面图像和三个城市（武汉、香港、旧金山）的机载激光扫描（ALS）点云。
◆ 解决了平台多样性不足的问题，为研究提供了丰富且真实的数据基础。
◆ 提出了一种适用于大规模城市场景的可靠地面真值生成方法，解决了该领域长期缺乏精确基准的难题。
◆ 首次在空地跨平台设置下对现有的图像到点云（I2P）定位算法进行了系统性的基准测试与验证。
通过提供数据集、地面真值和基准测试，该工作极大地推动和促进了基于ALS先验地图的、可扩展且精确的视觉定位技术的发展。</td></tr>
<tr><td>2025-09-08</td><td>Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization</td><td>[2509.06582](http://arxiv.org/pdf/2509.06582)</td><td>该论文提出了一种多用户协同定位VR框架，通过融合外部动捕系统和头显内置SLAM跟踪技术，实现了高精度、低延迟的共享虚拟空间同步。  
◆ 结合基于SLAM的头显内向外追踪与运动捕捉系统，兼顾高帧率、低延迟性能和长期稳定性。  
◆ 突破传统依赖持续外部追踪（易延迟抖动）或一次性校准（无法校正漂移）的局限，实现动态按需重对齐。  
◆ 支持跨设备实时姿态共享，确保多用户间空间一致性及交互沉浸感。  
◆ 在保持高空间精度的同时，显著提升了系统的舒适性、可扩展性和鲁棒性。</td></tr>
<tr><td>2025-09-08</td><td>Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation</td><td>[2509.06433](http://arxiv.org/pdf/2509.06433)</td><td>该论文核心贡献是提出了一种实时逼真地图构建系统，显著提升了未知环境中机器人遥操作的效率与准确性。  
◆ 创新性地将高斯溅射SLAM技术与在线地图遥操作系统进行模块化集成  
◆ 采用基于GPU的高效计算架构实现实时高精度三维地图重建  
◆ 突破传统系统因计算资源限制导致的视觉地图质量与实时性瓶颈  
◆ 通过真实环境无人机实验验证系统能大幅提升操作者决策速度和环境交互精度  
该系统首次实现实时逼真地图生成与遥操作任务的无缝融合，为陌生环境下的远程操控提供了突破性解决方案。</td></tr>
<tr><td>2025-09-06</td><td>LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction</td><td>[2509.05728](http://arxiv.org/pdf/2509.05728)</td><td>本文提出了LiDAR-BIND-T，通过增强时间一致性改进了多模态融合SLAM系统。其核心贡献与创新点包括：
◆ 引入时间嵌入相似性损失，显式对齐连续时刻的潜在特征以保持时序连贯。
◆ 提出运动对齐变换损失，确保预测点云与真实LiDAR点云之间的位移一致性。
◆ 设计专用时序融合模块，通过滑窗方式融合多帧信息以提升稳定性。
◆ 优化模型架构以更好地保留空间结构，提升跨模态（雷达/声纳到LiDAR）重建质量。
实验证明该方法显著提升了SLAM的轨迹精度和地图质量，并提出基于FVMD和相关峰值距离的实用时序评估指标。</td></tr>
<tr><td>2025-09-04</td><td>Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage</td><td>[2509.04370](http://arxiv.org/pdf/2509.04370)</td><td>该论文的核心贡献是开发了一个自动化计算机视觉流程，将执法或急救人员佩戴的体戴相机视频转化为简洁的全景事件摘要图像。  
◆ 提出利用单目SLAM技术从视频中估计相机运动轨迹并重建场景空间布局，为创建空间连贯的摘要奠定基础。  
◆ 通过沿轨迹对相机位姿进行聚类来识别关键视角，确保摘要能覆盖场景的重要部分。  
◆ 采用多帧图像拼接技术，将选定代表帧融合成高质量全景图，保持空间一致性和视觉完整性。  
◆ 最终生成的全景摘要图像支持快速环境理解和决策，解决了冗长视频回顾耗时低效的痛点。  
该技术提升了体戴相机数据在时间敏感场景中的实用价值，适用于事后分析和应急响应。</td></tr>
<tr><td>2025-09-04</td><td>Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot</td><td>[2509.04016](http://arxiv.org/pdf/2509.04016)</td><td>本文针对四轮独立转向独立驱动（4WIS4WID）爬壁机器人在复杂建筑立面上的定位难题，提出了一种基于多传感器融合的位姿估计器。  
◆ 创新性地将轮式里程计、视觉里程计和IMU数据通过扩展卡尔曼滤波（EKF）和无迹卡尔曼滤波（UKF）进行融合，构建了鲁棒的位姿估计系统。  
◆ 针对里程计的系统误差，结合使用了非线性优化、Levenberg-Marquardt等确定性方法以及遗传算法、粒子群等随机优化方法进行运动学校准，有效减少了系统误差和漂移。  
◆ 解决了在无法使用GPS、激光雷达等传统传感器的特殊工作环境（如钢筋混凝土立面）中的精确定位问题。  
该方案通过实物机器人实验详细验证了其校准方法和位估计算法的有效性与性能，为爬壁机器人在实际测量和维护任务中的高精度位姿感知提供了可靠的技术基础。</td></tr>
<tr><td>2025-09-03</td><td>IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments</td><td>[2509.02972](http://arxiv.org/pdf/2509.02972)</td><td>本文提出了一种基于特征感知的智能线辅助动态SLAM系统IL-SLAM，其核心贡献在于解决了动态环境下特征管理的效率与质量问题。  
◆ 创新性地引入特征感知机制，动态评估现有点特征的充足性，仅在必要时激活线特征补充，避免盲目引入额外特征。  
◆ 通过选择性引入线特征，显著降低了计算开销，并有效减少了低质量特征和噪声的累积。  
◆ 在线特征使用策略上，允许其在跟踪、局部建图与回环检测中辅助提升初始位姿估计精度，但将其排除在全局优化之外，避免长期过程中的负面干扰。  
实验证明，该系统在TUM数据集上的ATE和RPE指标均优于ORB-SLAM3基线及其他动态SLAM与多特征方法，实现了性能与效率的平衡。</td></tr>
<tr><td>2025-09-02</td><td>Coral: A Unifying Abstraction Layer for Composable Robotics Software</td><td>[2509.02453](http://arxiv.org/pdf/2509.02453)</td><td>该论文提出了Coral，一个用于组合式机器人软件的统一抽象层，旨在解决机器人系统集成困难的核心问题。  
◆引入了一个更高层次的抽象层，在不修改底层代码的前提下实现软件组件的快速集成与协调。  
◆通过语义化约束集成过程，显著降低了配置复杂性，同时保持了对不同领域和任务的广泛适应性。  
◆与现有工具兼容而非替代，增强了组件复用性和系统可重构性。  
◆在复杂场景（如LiDAR SLAM和多机器人协作）中验证了其有效性，证明了其解决集成挑战的实用价值。  
◆开源发布Coral，提升了专家与非专家用户的可访问性，推动机器人软件开发的标准化与普及。</td></tr>
<tr><td>2025-09-02</td><td>Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction</td><td>[2509.01873](http://arxiv.org/pdf/2509.01873)</td><td>该论文的核心贡献是提出了一种结合几何先验与深度学习的几何深度学习框架，以解决三维视觉中的关键任务。  
◆ 开发了针对相机位姿估计、点云配准、深度预测和三维重建的几何深度学习方法，克服了传统方法在非结构化环境中特征模糊的局限性。  
◆ 通过将深度信息、表面法线和等变约束等几何先验融入深度学习模型，增强了几何表示的准确性和鲁棒性。  
◆ 系统研究了三维视觉的关键组成部分，并验证了其在数字文化遗产保护和沉浸式VR/AR等实际应用中的有效性。  
◆ 解决了三维数据高维性和标注数据稀缺带来的训练挑战，推动了传统几何技术与深度学习能力的融合。  
该研究为生成具有几何感知的深度学习模型提供了系统解决方案，促进了三维映射技术和场景重建管道的发展。</td></tr>
<tr><td>2025-09-01</td><td>ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association</td><td>[2509.01584](http://arxiv.org/pdf/2509.01584)</td><td>ViSTA-SLAM是一个无需已知相机内参即可实时运行的单目视觉SLAM系统，其核心贡献在于通过一个轻量化的对称式双视图关联模型显著提升了系统性能与适用性。

◆ 提出一种轻量级对称双视图关联（STA）前端模型，仅需两张RGB图像即可同时估计相对相机位姿并回归局部点云地图。
◆ 该前端设计极大降低了模型复杂度，其模型大小仅为同类先进方法的35%，同时生成了更高质量的双视图约束用于后续优化。
◆ 后端构建了一个特殊的Sim(3)位姿图，通过融入回环检测来有效处理累积的尺度漂移问题。
◆ 整个系统不依赖相机内参，使其能够广泛适用于各种不同的相机设置，具备了很强的通用性。
实验结果表明，该系统在相机跟踪精度和稠密三维重建质量上均优于当前主流方法。</td></tr>
<tr><td>2025-09-01</td><td>FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</td><td>[2509.01547](http://arxiv.org/pdf/2509.01547)</td><td>FGO-SLAM的核心贡献是提出了一种基于全局一致不透明度辐射场的新型高斯SLAM系统，显著提升了场景几何重建的质量和系统的鲁棒性。其创新点主要体现在：

◆采用不透明度辐射场作为场景表示，有效增强了系统的几何建图性能。
◆在初始位姿估计后，引入全局调整优化策略，同时优化相机位姿和稀疏点云，确保了鲁棒且精确的跟踪。
◆维护了一个基于3D高斯且全局一致的不透明度辐射场，并引入了深度失真和法向一致性约束来精细化场景表示。
◆通过构建四面体网格并提取等值面，实现了直接从3D高斯中提取表面，简化了表面重建流程。
实验表明，该方法在多个真实和大型合成数据集上均实现了最先进的跟踪精度和建图性能。</td></tr>
<tr><td>2025-09-01</td><td>SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments</td><td>[2509.01111](http://arxiv.org/pdf/2509.01111)</td><td>SR-SLAM提出了一种基于场景可靠性的RGB-D SLAM框架，旨在提升视觉SLAM系统在多样化环境中的精度与鲁棒性。其核心创新在于引入统一的环境感知与可靠性评估机制，通过多指标和历史观测动态指导系统行为。具体贡献包括：
◆提出自适应动态区域选择方法，结合灵活几何约束以提升动态目标识别能力。
◆开发深度辅助的自适应聚类算法，在高维环境下高效剔除动态特征点。
◆设计可靠性感知的位姿优化策略，在特征不足时动态融合直接法以提升估计稳定性。
◆提出基于可靠性的关键帧选择与加权优化方案，在保证精度的同时显著降低计算开销。
实验证明该系统在公开数据集和真实场景中优于现有动态SLAM方法，精度和鲁棒性提升最高达90%。</td></tr>
<tr><td>2025-08-31</td><td>DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments</td><td>[2509.00741](http://arxiv.org/pdf/2509.00741)</td><td>DyPho-SLAM是一种能够在动态环境中实时运行的视觉SLAM系统，其核心贡献是解决了动态物体干扰导致的相机跟踪漂移和地图模糊问题，并实现了高保真的密集地图重建。

◆ 率先将高斯泼溅表示（Gaussian Splatting）用于动态环境SLAM，实现了实时且资源高效的光电真实感建图。
◆ 创新地整合先验图像信息来生成精细化掩码，有效减少了因动态物体误判而产生的噪声。
◆ 设计了自适应的特征提取策略，在移除动态障碍物后为系统优化增强了约束，显著提升了整个系统的鲁棒性。
实验表明，该系统在公开动态RGB-D数据集上实现了最先进的相机位姿估计和地图重建精度。</td></tr>
<tr><td>2025-08-30</td><td>AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection</td><td>[2509.00433](http://arxiv.org/pdf/2509.00433)</td><td>该论文提出了AGS，一个算法-硬件协同设计框架，旨在显著加速基于3D高斯泼溅的SLAM系统。其核心创新点在于充分利用了SLAM流式处理中相邻帧的高相似性。  
◆ 在软件层面，提出了一种根据机器人运动进行先粗后精的位姿跟踪方法。  
◆ 通过在帧间共享高斯点的贡献信息，避免了大量冗余计算。  
◆ 在硬件层面，创新地利用视频编解码器提取中间数据，设计了一个帧共视性检测引擎。  
◆ 还实现了配备工作负载调度器的位姿跟踪引擎和建图引擎，以高效部署整个AGS算法。  
最终，AGS相比现有方案在移动GPU、高端GPU及专用加速器上分别实现了最高17.12倍、6.71倍和5.41倍的加速。</td></tr>
<tr><td>2025-08-29</td><td>The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</td><td>[2508.21635](http://arxiv.org/pdf/2508.21635)</td><td>本文介绍了专为农业机器人设计的Rosario v2多模态数据集，其核心贡献在于为复杂农田环境下的算法开发提供了全面基准。主要创新点包括：  
◆ 提供超过两小时的多模态传感器数据，涵盖红外/彩色相机、IMU、多模式GNSS和轮式里程计，硬件级同步确保数据一致性。  
◆ 精准捕捉农业场景典型挑战：光照突变、运动模糊、颠簸地形及长距离视觉相似路径，高度还原真实作业困难。  
◆ 提供六自由度真值轨迹和闭环检测所需的长路径循环，满足多模态SLAM系统严格评测需求。  
◆ 通过运行主流SLAM算法并揭示其在农业场景中的局限性，验证了数据集的实用性和挑战性。  
该数据集公开可用，显著推动了农业机器人定位、建图与导航算法的研发和性能评估。</td></tr>
<tr><td>2025-08-28</td><td>Adam SLAM - the last mile of camera calibration with 3DGS</td><td>[2508.20526](http://arxiv.org/pdf/2508.20526)</td><td>该论文提出了一种利用3D高斯散射（3DGS）优化相机标定的新方法。  
◆ 创新性地通过3DGS模型的反向传播，利用新视角颜色损失对相机参数进行端到端精细优化。  
◆ 解决了相机标定中1像素误差对重建质量产生显著影响的关键问题，直接提升了新视角合成质量。  
◆ 在3DGS基准数据集上平均带来0.4 dB PSNR的性能提升，显著优于传统标定方法。  
◆ 虽然优化过程耗时较长，但为参考场景（如Mip-NeRF 360）的标定提供了以质量为优先的解决方案。  
该方法尤其适用于对标定精度要求极高的高质量新视图合成任务。</td></tr>
<tr><td>2025-08-24</td><td>SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</td><td>[2508.17255](http://arxiv.org/pdf/2508.17255)</td><td>SEER-VAR提出了一种面向车辆增强现实（AR）的语义自中心环境推理框架，其核心贡献与创新点如下：

◆ 采用深度引导的视觉-语言 grounding 技术，动态分离车内与车外场景，突破了传统静态或单视角设定的限制。  
◆ 设计了上下文感知的SLAM分支（CASB），通过双路SLAM系统分别稳健地跟踪不同场景下的自中心运动。  
◆ 引入基于GPT的大语言模型模块，生成上下文感知的AR叠加内容，如仪表盘提示和危险预警。  
◆ 构建并开源EgoSLAM-Drive真实世界数据集，提供多场景同步的自中心视图、6DoF真值位姿与AR标注，支持系统评估。  
实验表明，该系统在多种环境下实现了精确的空间对齐与感知一致的AR渲染，并通过用户研究验证了其在场景理解、信息相关性和驾驶体验方面的显著提升。</td></tr>
<tr><td>2025-08-24</td><td>VROOM - Visual Reconstruction over Onboard Multiview</td><td>[2508.17172](http://arxiv.org/pdf/2508.17172)</td><td>VROOM提出了一种仅依靠F1赛车的车载摄像头视频来重建赛道3D模型的系统。其核心创新在于解决了极端高速运动和视频帧剧烈切换带来的挑战。
◆ 首创了基于F1赛车车载单目视频进行大规模赛道4D重建的可行方案。
◆ 设计了一套结合DROID-SLAM、AnyCam和Monst3r等多种方法的处理流程，并针对动态场景进行优化。
◆ 采用了包括掩码、时间分块和分辨率缩放等预处理技术，以应对动态模糊和计算资源限制。
实验证明，该系统能在摩纳哥站等复杂环境中部分恢复赛道和车辆轨迹，验证了该方法的可行性，为实景 scalable 4D重建提供了新思路。</td></tr>
<tr><td>2025-08-23</td><td>DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration</td><td>[2508.17034](http://arxiv.org/pdf/2508.17034)</td><td>本文提出了一种新颖的双空间刚性配准方法DualReg，有效结合了基于特征匹配和局部几何匹配的优势。  
◆ 创新性地引入双空间范式，分别处理大变换差异的初始对齐和精细局部配准，克服了单一方法的局限性。  
◆ 设计了高效过滤机制，采用轻量级单点RANSAC算法和细化模块快速剔除不可靠的特征对应点，提升计算效率。  
◆ 提出将过滤后的对应点作为锚点，提取几何代理并构建优化目标函数，配合定制求解器实现高精度变换估计。  
实验表明，该方法在KITTI数据集上达到与MAC相当精度的同时，CPU计算速度提升高达32倍，显著优于现有方法。</td></tr>
<tr><td>2025-08-23</td><td>A Workflow for Map Creation in Autonomous Vehicle Simulations</td><td>[2508.16856](http://arxiv.org/pdf/2508.16856)</td><td>本文针对自动驾驶仿真中高精度地图创建困难且成本高昂的问题，提出了一种创新的地图制作工作流。  
◆ 设计了一个定制化流程，显著简化了仿真就绪地图的创建，降低了资源消耗。  
◆ 以CARLA等主流仿真器为背景，但避免了对其特定依赖，提升了方法的通用性和灵活性。  
◆ 通过生成加拿大安大略理工大学停车场的3D地图实例，验证了工作流的可行性与有效性。  
未来工作将集成SLAM技术并优化经纬度处理，以进一步提升精度和兼容性。</td></tr>
<tr><td>2025-08-22</td><td>COSMO-Bench: A Benchmark for Collaborative SLAM Optimization</td><td>[2508.16731](http://arxiv.org/pdf/2508.16731)</td><td>该论文的核心贡献是设计并发布了首个专门用于评估多机器人协同SLAM（C-SLAM）优化算法的基准测试套件COSMO-Bench，以解决该领域缺乏标准评估工具的问题。
◆ 首次构建了一个专注于多机器人协同SLAM后端优化算法的标准化评测基准。
◆ 提供了24个高质量数据集，这些数据源自真实世界的LiDAR数据和最先进的C-SLAM前端处理结果，确保了数据的真实性和挑战性。
◆ 填补了多机器人SLAM研究缺乏统一、公认评估标准的空白，为不同算法的公平比较提供了平台。
◆ 所有数据集均已开源，旨在促进该研究领域的协作、复现与发展。</td></tr>
<tr><td>2025-08-22</td><td>GPL-SLAM: A Laser SLAM Framework with Gaussian Process Based Extended Landmarks</td><td>[2508.16459](http://arxiv.org/pdf/2508.16459)</td><td>本文提出了一种基于高斯过程地标的新型激光SLAM框架GPL-SLAM。其核心创新点包括：
◆ 采用高斯过程对环境中物体的轮廓进行建模，替代传统的栅格地图或点云配准方法。
◆ 提出在线递归更新方案，能够高效更新地标轮廓并显著减少内存使用。
◆ 在完全贝叶斯框架下形式化SLAM问题，实现了机器人位姿与物体地图的联合推理。
◆ 提供语义信息输出，如物体数量和面积，并支持概率测量的物体关联。
◆ 通过高斯过程生成物体形状的置信边界，为安全导航和探索等下游任务提供关键信息。实验证明该方法在多种结构化环境中能实现精确的定位与建图。</td></tr>
<tr><td>2025-08-21</td><td>GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System</td><td>[2508.15990](http://arxiv.org/pdf/2508.15990)</td><td>GelSLAM提出了一种仅依靠触觉感知即可实现实时三维SLAM的系统，用于长时间估计物体位姿并高精度重建物体形状。  
◆ 创新性地利用触觉衍生的表面法线和曲率信息进行位姿跟踪与回环检测，替代了传统的点云方法。  
◆ 实现了实时低误差、低漂移的运动跟踪能力，即使对于木质工具等低纹理物体也能保持稳定。  
◆ 能够以亚毫米级精度重建物体形状，达到高保真度的几何复原效果。  
◆ 将触觉感知从局部接触扩展至全局长时序空间感知，为高精度操作任务提供了新基础。  
该系统在遮挡环境下表现优异，弥补了视觉方法的不足，适用于手内操作等精密交互场景。</td></tr>
<tr><td>2025-08-19</td><td>SLAM-based Safe Indoor Exploration Strategy</td><td>[2508.14235](http://arxiv.org/pdf/2508.14235)</td><td>该论文提出了一种基于SLAM的室内安全探索策略，主要面向具有圆形轮廓的非完整移动机器人。其核心贡献在于将安全性作为最高优先级，并设计了相应的探索与路径规划方法。

◆ 针对非完整圆形机器人系统（双轮差速驱动），提出专用探索策略，而非假设理想点机器人模型。
◆ 采用多传感器融合方案，结合IMU、3D-LiDAR进行RTAB-SLAM，并用RGB-D相机进行回环检测，提高了建图与定位的稳定性。
◆ 提出“安全骨架”路径规划方法，使机器人在探索过程中始终尽可能远离静态障碍物，极大提高了安全性。
◆ 探索策略以安全避障为首要目标，其次才是未知区域探索，导向空间中的开放区域进行前进。
◆ 通过ROS移动机器人平台进行了实验验证，展示了完整的实时路径规划与探索过程。</td></tr>
<tr><td>2025-08-19</td><td>Online 3D Gaussian Splatting Modeling with Novel View Selection</td><td>[2508.14014](http://arxiv.org/pdf/2508.14014)</td><td>该论文针对仅使用RGB图像进行在线3D高斯泼溅建模的挑战，提出了一种创新解决方案。其核心贡献在于通过自适应视图选择显著提升了在线重建模型的完整性。

◆ 提出了自适应新颖视图选择机制，在线分析重建质量并动态选择最优的非关键帧进行补充训练。
◆ 突破了传统方法仅依赖关键帧的局限，通过融合关键帧和精选的非关键帧，从多样化视角细化不完整区域。
◆ 设计了一个集成在线多视图立体视觉的框架，确保整个3D高斯泼溅建模过程中三维信息的一致性。
◆ 实现了在在线处理的严格限制下（无法使用大量帧或过多训练迭代），仍能构建高质量通用模型的目标。
实验结果表明，该方法在复杂户外场景中优于现有最先进技术，实现了卓越的性能表现。</td></tr>
<tr><td>2025-08-19</td><td>ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</td><td>[2508.13488](http://arxiv.org/pdf/2508.13488)</td><td>该论文提出了一种用于重复环境下回环闭合验证的鲁棒方法ROVER，其核心创新在于利用历史轨迹先验约束来识别错误回环。  
◆ 首次将机器人时空运动轨迹作为先验知识引入回环验证框架，突破传统仅依赖外观特征的局限性。  
◆ 提出通过位姿图优化生成候选回环的轨迹假设，并设计评分机制评估其与原始轨迹先验的一致性。  
◆ 在高度重复环境中能有效拒绝虚假回环，解决了外观相似性导致的误检测难题。  
实验表明该方法在公开数据集和真实场景中均显著提升验证准确性，且可无缝集成至现有SLAM系统增强鲁棒性。</td></tr>
<tr><td>2025-08-14</td><td>Super LiDAR Reflectance for Robotic Perception</td><td>[2508.10398](http://arxiv.org/pdf/2508.10398)</td><td>◆ 提出了一种创新框架，能够从稀疏扫描数据生成高密度的LiDAR反射率图像，解决了低成本LiDAR因数据稀疏性导致的应用受限问题。  
◆ 针对非重复扫描LiDAR（NRS-LiDAR）的特性，设计了专用的反射率图像稠密化网络，显著提升了稀疏数据的利用率。  
◆ 攻克了反射率校准和从静态场景到动态场景迁移的关键技术难题，实现了真实场景下的稠密反射率图像重建。  
◆ 构建了一个全面的LiDAR反射率图像稠密化数据集，为后续研究提供了重要基础。  
◆ 展示了稠密反射率图像在机器人感知任务中的多样化应用，如闭环检测和交通车道识别，验证了其实际价值。  
◆ 通过主动光学传感重新定义视觉边界，推动了主动视觉新范式的发展。</td></tr>
<tr><td>2025-08-12</td><td>Transient Noise Removal via Diffusion-based Speech Inpainting</td><td>[2508.08890](http://arxiv.org/pdf/2508.08890)</td><td>◆ 提出PGDI框架，首次将扩散模型应用于语音修复任务，能精准重建长达1秒的缺失或严重损坏语音段。  
◆ 突破传统方法限制，在保持说话人身份、韵律和环境特征（如混响）的同时，有效处理说话人差异和长间隙问题。  
◆ 创新性引入分类器引导机制，特别是音素级引导策略，显著提升重建保真度。  
◆ 实现与说话人无关的鲁棒修复，即使语音段被强烈瞬态噪声（如烟花、摔门声）完全遮蔽仍能稳定工作。  
◆ 通过大量实验验证模型优越性，证明其在无文本转录条件下仍保持高效，有文本辅助时性能进一步提升。  
◆ 针对实际噪声场景（施工噪音、敲击声等）设计解决方案，推动语音修复技术的现实应用。</td></tr>
<tr><td>2025-08-09</td><td>EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events</td><td>[2508.07003](http://arxiv.org/pdf/2508.07003)</td><td>EGS-SLAM的核心贡献在于通过融合事件相机数据与RGB-D输入，显著提升了动态模糊场景下的SLAM性能。具体创新点包括：

◆ 提出首个结合事件数据与RGB-D输入的GS-SLAM框架，有效解决传统方法在严重运动模糊下的性能退化问题。

◆ 创新性地建模相机曝光期间的连续运动轨迹，实现事件感知与模糊感知的联合跟踪与三维高斯泼溅建图。

◆ 设计可学习的相机响应函数，动态对齐事件流与RGB图像的亮度范围差异。

◆ 引入无事件损失函数，有效抑制重建过程中的振铃伪影。

◆ 构建包含合成与真实场景的新数据集，验证方法在极端运动模糊条件下的优越性。

实验表明，该系统在轨迹精度和三维重建质量上均超越现有GS-SLAM方法，为高动态场景提供了鲁棒的解决方案。</td></tr>
<tr><td>2025-08-07</td><td>Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages</td><td>[2508.05149](http://arxiv.org/pdf/2508.05149)</td><td>◆ 首次系统研究了语音大模型(Speech LLMs)在低资源自动语音识别(ASR)场景下的数据量需求，填补了该领域研究空白。  
◆ 提出基于SLAM-ASR框架的轻量级可训练投影器方案，有效连接语音编码器与大语言模型，适配低资源条件。  
◆ 量化分析了达到Whisper模型性能所需的最低训练数据量，实证揭示了数据稀缺带来的核心挑战。  
◆ 创新性发现：利用高资源语言预训练的单一/多语言投影器能显著缓解数据不足问题，特别在小规模训练集时效果突出。  
◆ 通过EuroLLM和Salamandra等多语言大模型与whisper-large-v3-turbo的组合实验，为低资源多语言语音处理提供了新优化思路。  
◆ 在多个公开基准测试上的实验结果，为未来低资源语音大模型研究提供了重要设计参考和方法论指导。</td></tr>
<tr><td>2025-08-06</td><td>Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline</td><td>[2508.04597](http://arxiv.org/pdf/2508.04597)</td><td>◆ 提出了一种基于3D高斯映射的RGB SLAM方法，通过结合深度估计器和3D高斯技术，解决了传统方法在长序列处理中的性能瓶颈问题。  
◆ 引入前馈循环预测模块，直接从光流推断相机位姿，替代了耗时的测试时优化，将跟踪速度提升90%以上。  
◆ 设计了局部图渲染技术，增强了前馈位姿预测的鲁棒性，提高了系统在复杂场景中的稳定性。  
◆ 在Replica和TUM-RGBD数据集上的实验表明，该方法性能与当前最优的SplaTAM相当，同时大幅降低了计算开销。  
◆ 通过实际部署验证了方法的实用性，展示了其在实时3D重建中的高效性和准确性。</td></tr>
<tr><td>2025-08-05</td><td>Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways</td><td>[2508.03672](http://arxiv.org/pdf/2508.03672)</td><td>◆ 提出Inland-LOAM框架，针对内河航道环境优化LiDAR SLAM，解决传统方法在垂直漂移和语义缺失方面的不足。  
◆ 改进特征提取方法并引入水面平面约束，有效抑制SLAM系统的垂直漂移问题，提升定位精度。  
◆ 创新性地通过体素化几何分析将3D点云转化为结构化2D语义地图，实时计算桥梁净空等关键导航参数。  
◆ 开发自动化模块提取岸线轮廓，并输出轻量化、兼容国际电子航道图（IENC）的标准格式。  
◆ 在真实航道数据集上验证，定位精度超越现有先进方法，生成的语义地图与岸线数据符合实际场景需求。  
◆ 公开代码与数据集，为内河自主航行提供可靠的环境感知与地理信息支持。</td></tr>
<tr><td>2025-08-04</td><td>A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration</td><td>[2508.02187](http://arxiv.org/pdf/2508.02187)</td><td>◆ 提出基于矩匹配的点云配准框架，将点云视为同分布独立样本，通过匹配广义高斯径向基矩估计刚体变换，避免传统方法对显式点对点对应关系的依赖。  
◆ 在理论层面证明了该方法的数学一致性，为算法有效性提供理论支撑。  
◆ 针对稀疏和强噪声场景设计解决方案，显著提升了ICP、NDT等传统方法在此类恶劣条件下的配准鲁棒性。  
◆ 实验验证表明，该方法在合成与真实数据集上均实现更高精度，尤其在4D雷达SLAM系统中达到与激光雷达系统相当的定位性能。  
◆ 首次将矩匹配技术系统性地应用于点云配准领域，为稀疏噪声环境下的机器人感知任务开辟新思路。</td></tr>
<tr><td>2025-08-04</td><td>AID4AD: Aerial Image Data for Automated Driving Perception</td><td>[2508.02140](http://arxiv.org/pdf/2508.02140)</td><td>◆ 提出AID4AD数据集，首次将高分辨率航拍图像与nuScenes自动驾驶数据集的空间坐标系精确对齐，填补了航拍数据在自动驾驶感知任务中的空白。  
◆ 开发了一套基于SLAM点云地图的对齐流程，通过定位和投影失真校正技术确保空间保真度，并人工筛选高质量对齐样本作为基准真值。  
◆ 验证了航拍图像在自动驾驶两大核心任务中的价值：在线地图构建中作为补充输入提升15-23%精度，运动预测中替代高精地图实现2%性能提升。  
◆ 揭示了航拍图像作为可扩展环境上下文源的潜力，尤其适用于高精地图缺失、过时或维护成本高的场景。  
◆ 开源数据集、评估代码与预训练模型，为后续研究提供标准化基准（https://github.com/DriverlessMobility/AID4AD）。</td></tr>
<tr><td>2025-08-01</td><td>CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry</td><td>[2508.00568](http://arxiv.org/pdf/2508.00568)</td><td>◆ 提出CoProU-VO方法，首次将跨帧不确定性传播与融合引入无监督单目视觉里程计，通过概率化建模结合当前帧与参考帧的不确定性。  
◆ 设计端到端框架，基于视觉Transformer主干网络，同步学习深度、不确定性估计和相机位姿，无需动态物体显式标注。  
◆ 创新性地利用投影机制将参考帧不确定性传递至目标帧，有效识别动态场景中的不可靠区域，突破传统单帧不确定性建模的局限。  
◆ 在KITTI和nuScenes数据集上显著超越现有无监督单目两帧方法，尤其在动态物体密集的高速公路场景表现突出。  
◆ 通过详实的消融实验验证跨帧不确定性传播的有效性，为动态环境下的鲁棒位姿估计提供新思路。</td></tr>
<tr><td>2025-07-31</td><td>The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking</td><td>[2508.00088](http://arxiv.org/pdf/2508.00088)</td><td>◆ 提出了Monado SLAM数据集，专门针对头戴式设备的视觉-惯性跟踪挑战，填补了现有数据集的不足。  
◆ 数据集包含真实场景下的高动态运动、动态遮挡、长时间跟踪等复杂情况，更贴近实际应用需求。  
◆ 覆盖了低纹理区域、恶劣光照条件和传感器饱和等现有数据集较少涉及的难点场景。  
◆ 数据来自多款虚拟现实头显设备，具有多样性和代表性，适用于头戴式传感器研究。  
◆ 采用CC BY 4.0许可协议公开数据集，促进视觉-惯性里程计（VIO）和SLAM技术的研发进步。  
◆ 通过真实场景数据暴露现有VIO/SLAM系统的不足，推动算法在复杂环境中的鲁棒性提升。</td></tr>
<tr><td>2025-07-31</td><td>Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes</td><td>[2507.23677](http://arxiv.org/pdf/2507.23677)</td><td>◆ 提出了首个面向户外场景的双目3D高斯泼溅SLAM系统（BGS-SLAM），填补了现有3DGS-SLAM主要针对室内环境且依赖主动深度传感器的空白。  
◆ 仅使用RGB立体图像对，无需LiDAR或主动传感器，降低了硬件成本并提升了系统适用性。  
◆ 利用预训练深度立体网络的深度估计指导3D高斯优化，通过多损失策略同时提升几何一致性和视觉质量。  
◆ 在复杂户外环境中实现了优于其他基于3DGS方案的跟踪精度和建图性能。  
◆ 通过实验验证了系统在多个数据集上的优越性，为户外大规模场景的实时高保真SLAM提供了新解决方案。</td></tr>
<tr><td>2025-07-31</td><td>DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching</td><td>[2507.23629](http://arxiv.org/pdf/2507.23629)</td><td>◆ 提出DRACo-SLAM2框架，为配备多波束成像声纳的水下机器人团队设计分布式SLAM系统，改进原有DRACo-SLAM。  
◆ 创新性地将声纳地图表示为对象图，利用对象图匹配实现高效跨机器人回环检测，无需依赖先验几何信息。  
◆ 针对水下扫描匹配特点，提出增量式组间一致测量集最大化（GCM）方法，改进原有的PCM算法。  
◆ GCM方法有效处理相邻跨机器人回环共享相似配准误差的场景，提升匹配鲁棒性。  
◆ 通过大量仿真和真实数据集验证了所提方法的优越性，展示了其在实际应用中的有效性。</td></tr>
<tr><td>2025-07-31</td><td>GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting</td><td>[2507.23273](http://arxiv.org/pdf/2507.23273)</td><td>◆ 提出GSFusion系统，首次将激光雷达（LiDAR）、惯性测量单元（IMU）与视觉传感器融合，实现基于3D高斯泼溅（3DGS）的在线建图，解决了传统纯视觉方法在弱纹理、光照不足和远距离场景中的局限性。  
◆ 引入全局位姿图优化中的面元到面元（surfel-to-surfel）约束，显著提升地图的全局一致性，确保高精度建图质量。  
◆ 设计像素感知的高斯初始化策略，有效利用稀疏激光雷达数据快速生成高斯表示，大幅缩短优化时间。  
◆ 提出有界Sigmoid约束机制，防止高斯分布无限制扩张，提升场景表示的稳定性和渲染效率。  
◆ 在公开和自建数据集上的实验表明，该系统在渲染质量和建图效率上均优于现有3DGS SLAM方案，尤其在复杂环境中表现突出。</td></tr>
<tr><td>2025-07-30</td><td>Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques</td><td>[2507.22791](http://arxiv.org/pdf/2507.22791)</td><td>◆ 全面综述了单模态与跨模态特征匹配技术，涵盖RGB图像、深度图像、3D点云、LiDAR扫描、医学图像及视觉-语言交互等多种模态，填补了该领域系统性总结的空白。  
◆ 对比分析了传统手工方法（如Harris角点、SIFT和ORB描述子）与深度学习方法（如SuperPoint和LoFTR），指出后者在跨模态鲁棒性和适应性上的显著优势。  
◆ 重点介绍了模态感知技术进展，包括针对深度图像的几何与深度专用描述子、3D点云的稀疏与稠密学习方法、LiDAR扫描的注意力增强神经网络，以及医学图像匹配的MIND描述子等创新方案。  
◆ 深入探讨跨模态应用场景，如医学图像配准和视觉-语言任务，揭示了特征匹配技术处理多样化数据交互的最新发展趋势。  
◆ 强调检测器无关的深度学习方法（如基于CNN和Transformer的模型）对跨模态匹配性能的提升，为未来研究提供了重要方向。</td></tr>
<tr><td>2025-07-30</td><td>UAVScenes: A Multi-Modal Dataset for UAVs</td><td>[2507.22412](http://arxiv.org/pdf/2507.22412)</td><td>◆ 提出了首个支持多模态（相机图像和LiDAR点云）帧级标注的大规模无人机数据集UAVScenes，填补了现有数据集仅支持定位或地图级语义分割的空白。  
◆ 基于MARS-LVIG数据集进行升级，新增了人工标注的逐帧图像和点云语义标签，以及高精度6自由度位姿数据，显著提升了数据实用性。  
◆ 首次支持无人机场景下的多任务联合评测，包括分割、深度估计、6-DoF定位、地点识别和新视角合成（NVS）等高级感知任务。  
◆ 通过严格的传感器标定和同步，确保多模态数据的时间-空间对齐，为跨模态融合研究提供可靠基准。  
◆ 开源数据集并设计标准化评测协议，推动无人机多模态感知领域的算法发展和公平比较。</td></tr>
<tr><td>2025-07-29</td><td>Impact of Underwater Image Enhancement on Feature Matching</td><td>[2507.21715](http://arxiv.org/pdf/2507.21715)</td><td>◆ 提出了局部匹配稳定性和最远可匹配帧数两项量化指标，专门用于评估水下图像增强效果。  
◆ 针对水下环境特有的光吸收、散射、生物附着等退化问题，设计了面向增强技术的帧匹配性能评估框架。  
◆ 通过度量分析揭示了现有方法的优势与局限，首次指出其在真实场景适用性评估方面的不足。  
◆ 创新性地将实际匹配策略融入评估体系，建立了结合上下文感知的增强方法对比基准。  
◆ 通过SLAM系统的全流程验证，证实了视觉质量提升对水下自主导航算法的实际影响，强化了框架的工程价值。  
◆ 该研究填补了水下图像增强技术与下游任务性能关联性评估的空白，为算法优化提供了明确方向。</td></tr>
<tr><td>2025-07-29</td><td>Adaptive Prior Scene-Object SLAM for Dynamic Environments</td><td>[2507.21709](http://arxiv.org/pdf/2507.21709)</td><td>◆ 提出基于场景-物体的可靠性评估框架，通过当前帧质量指标和相对于可靠参考帧的场景变化，全面评估SLAM系统的稳定性。  
◆ 针对现有系统在姿态估计不可靠时缺乏纠错机制的问题，采用姿态优化策略，利用可靠帧信息优化相机位姿估计，有效减少动态干扰的负面影响。  
◆ 在动态环境中显著提升了定位精度和系统鲁棒性，特别是在视角突变和运动物体特征不明确的情况下表现优异。  
◆ 通过TUM RGB-D数据集的广泛实验验证，证明了该方法在挑战性动态场景中的优越性能。  
◆ 结合当前帧质量与场景变化评估，提供了一种更全面的动态环境SLAM稳定性判断方法。  
◆ 提出的姿态优化策略为动态环境下的SLAM系统提供了有效的误差校正机制。</td></tr>
<tr><td>2025-08-01</td><td>Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments</td><td>[2507.21553](http://arxiv.org/pdf/2507.21553)</td><td>◆ 提出了一种针对地下隧道环境的去中心化多机器人LiDAR SLAM系统，分析了现有技术的局限性。  
◆ 发现当前闭环检测存在大量误报问题，这是导致系统失败的主要原因。  
◆ 开发了一种新的启发式方法，有效减少了闭环检测中的误报情况。  
◆ 在地下隧道这一极具挑战性的环境中验证了所提方法的有效性。  
◆ 指出了该领域尚未充分探索的潜在研究方向，为未来工作提供了参考。</td></tr>
<tr><td>2025-07-28</td><td>$S^3$LAM: Surfel Splatting SLAM for Geometrically Accurate Tracking and Mapping</td><td>[2507.20854](http://arxiv.org/pdf/2507.20854)</td><td>◆ 提出S³LAM系统，采用2D面元（surfel）作为基本表示单元，替代传统3D高斯椭球体，实现更高效的场景几何建模。  
◆ 创新性地利用2D高斯面元进行场景表面重建，显著提升几何精度，同时优化跟踪与建图性能。  
◆ 设计自适应表面渲染策略，解决SLAM在有限视角下的实时优化问题，兼顾计算效率与建图准确性。  
◆ 直接从2D面元渲染公式推导相机位姿雅可比矩阵，凸显几何精确表示对跟踪收敛性的关键作用。  
◆ 在合成与真实数据集上验证了S³LAM的优越性，性能达到当前最优水平。</td></tr>
<tr><td>2025-07-28</td><td>Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping</td><td>[2507.20516](http://arxiv.org/pdf/2507.20516)</td><td>◆ 提出首个大规模、高精度的LiDAR-惯性里程计（LIO）数据集，填补现有研究在复杂真实场景中验证不足的空白。  
◆ 数据集覆盖四种多样化真实环境（6万至75万平方米），通过定制背包式平台采集，集成多线激光雷达、工业级IMU和RTK-GNSS模块。  
◆ 提供长轨迹、复杂场景和高精度真值，结合SLAM优化与RTK-GNSS锚定技术生成，并通过倾斜摄影与RTK-GNSS融合验证轨迹精度。  
◆ 首次在数据集中融合多传感器冗余数据（如LiDAR-IMU-RTK），支持退化场景（如隧道、植被）下的鲁棒性评估。  
◆ 为高精度地图构建任务提供标准化基准，重点验证LIO系统在实际场景中的泛化能力与退化适应性。</td></tr>
<tr><td>2025-07-26</td><td>DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning</td><td>[2507.19742](http://arxiv.org/pdf/2507.19742)</td><td>◆ 提出基于近端策略优化（PPO）的自适应退化优化智能体（DOA），通过深度强化学习解决SLAM在长直走廊等退化环境中的定位问题。  
◆ 设计系统性方法解决传统监督学习的三大挑战：退化数据集获取瓶颈、训练样本质量下降问题以及标注协议设计的模糊性。  
◆ 开发专用奖励函数，引导智能体学习退化环境感知能力，并基于退化因子动态调整不同传感器对位姿优化的贡献权重。  
◆ 提出线性插值公式控制观测分布向运动模型分布的偏移步长，实现位姿补偿的自适应调整。  
◆ 引入迁移学习模块提升智能体跨环境泛化能力，解决退化环境中训练效率低下的问题。  
◆ 通过消融实验验证模型设计合理性，并证明DOA在多种环境中优于现有方法的退化检测与优化性能。</td></tr>
<tr><td>2025-07-25</td><td>DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations</td><td>[2507.19474](http://arxiv.org/pdf/2507.19474)</td><td>◆ 提出DINO-SLAM，一种基于DINO特征的设计策略，用于增强SLAM系统中神经隐式（NeRF）和显式（3DGS）表示的场景建模能力。  
◆ 设计场景结构编码器（SSE），将DINO特征升级为增强版EDINO，以捕捉场景的层次化元素及其结构关系。  
◆ 提出两种基于EDINO特征的基础范式，分别集成到NeRF和3DGS的SLAM系统中，提升场景表示的全面性。  
◆ 在Replica、ScanNet和TUM数据集上验证了方法的优越性，性能超越现有最先进技术。  
◆ 通过融合DINO特征，解决了传统SLAM系统在复杂场景中细节捕捉和结构关系建模的不足。</td></tr>
<tr><td>2025-07-25</td><td>The Eloquence team submission for task 1 of MLC-SLM challenge</td><td>[2507.19308](http://arxiv.org/pdf/2507.19308)</td><td>这篇论文针对MLC-SLM挑战赛任务1，提出了多语言会话语音识别的创新方法，核心贡献如下：

◆ 评估了官方基线模型的性能，通过训练线性投影器和qformer两种投影器，结合不同基础模型，系统分析了基线的优势与局限。

◆ 利用SLAM-ASR框架训练了自定义的多语言线性投影器，优化了模型在多语言场景下的适应性。

◆ 探索了对比学习在提升语音识别鲁棒性中的作用，通过对比学习机制增强了模型对多样化语音输入的识别能力。

◆ 研究了扩展会话上下文对识别效果的影响，验证了长上下文信息在改善会话语音识别性能方面的有效性。

◆ 综合三种方法，为构建更强大的多语言会话语音识别系统提供了实践指导和理论支持。</td></tr>
<tr><td>2025-07-25</td><td>SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research</td><td>[2507.19079](http://arxiv.org/pdf/2507.19079)</td><td>◆ 提出SmartPNT-MSF多源融合数据集，整合GNSS、IMU、光学相机和激光雷达等多传感器数据，弥补现有数据集在传感器多样性上的不足。  
◆ 详细记录数据集构建过程，包括传感器配置、坐标系定义及相机与激光雷达标定流程，确保数据的一致性和可复现性。  
◆ 设计标准化数据采集与处理框架，支持大规模分析并具备可扩展性，为多传感器融合研究提供结构化基础。  
◆ 通过VINS-Mono、LIO-SAM等先进SLAM算法验证数据集的实用性，证明其适用于高精度导航与定位算法开发。  
◆ 覆盖城市、校园、隧道及郊区等多种真实场景，增强复杂环境下的导航技术研究能力，填补环境多样性空白。  
◆ 公开高质量数据集，促进导航领域算法测试与比较，推动多传感器融合技术的创新与发展。</td></tr>
<tr><td>2025-07-25</td><td>A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras</td><td>[2507.18886](http://arxiv.org/pdf/2507.18886)</td><td>◆ 提出了一种解耦的非迭代视觉里程计方法，将6自由度位姿估计分为旋转和平移两步计算，避免了传统迭代优化带来的计算负担。  
◆ 利用场景中的重叠平面特征直接计算旋转矩阵，简化了旋转估计过程，提高了计算效率。  
◆ 采用核互相关器(KCC)计算平移量，省去了传统方法中特征提取与匹配的耗时步骤。  
◆ 整个流程无需迭代优化，在低端i5 CPU上实现了71Hz的高实时性能，显著提升了计算效率。  
◆ 不依赖特征点的特性使算法在低纹理或退化环境中表现优于现有先进方法，鲁棒性更强。  
◆ 通过分离旋转与平移估计并利用平面特征，在保持轻量级的同时实现了快速位姿估计。</td></tr>
<tr><td>2025-07-24</td><td>G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM</td><td>[2507.18344](http://arxiv.org/pdf/2507.18344)</td><td>◆ 提出了一种基于几何感知的高斯泼溅SLAM系统（G2S-ICP SLAM），通过将场景元素表示为局部切平面约束的高斯分布，实现高保真3D重建和实时相机位姿跟踪。  
◆ 创新性地将局部表面建模为与几何对齐的2D高斯圆盘，相比传统各向同性3D椭球表示，能更一致地处理多视角深度信息。  
◆ 将表面对齐的高斯圆盘嵌入广义ICP框架，通过引入各向异性协方差先验，在不改变配准公式的前提下提升几何一致性。  
◆ 提出几何感知损失函数，联合优化光度、深度和法向一致性，进一步提升重建和跟踪精度。  
◆ 系统在Replica和TUM-RGBD数据集上验证，在定位精度、重建完整性和渲染质量上均优于现有SLAM方法，同时保持实时性。</td></tr>
<tr><td>2025-07-23</td><td>Physics-based Human Pose Estimation from a Single Moving RGB Camera</td><td>[2507.17406](http://arxiv.org/pdf/2507.17406)</td><td>◆ 提出了首个非合成的真实数据集MoviCam，包含动态移动的单目RGB相机轨迹、场景几何和3D人体运动数据，并标注了人-场景接触信息，填补了现有数据集的空白。  
◆ 开发了PhysDynPose方法，首次将场景几何和物理约束整合到基于物理的人体姿态跟踪中，显著提升了移动相机和非平面场景下的跟踪精度。  
◆ 结合了先进运动学估计器和鲁棒SLAM技术，实现了世界坐标系下人体姿态与相机轨迹的同步恢复，解决了动态相机带来的参考系漂移问题。  
◆ 设计了场景感知的物理优化器，通过物理约束修正运动学估计结果，使姿态估计更符合真实物理规律。  
◆ 通过新基准测试发现，现有方法在移动相机和非平面场景下性能显著下降，而本方法在此挑战性场景中仍能稳定输出人体与相机位姿。  
◆ 为复杂真实场景（如不平地面、动态视角）的物理可信人体运动分析提供了新解决方案。</td></tr>
<tr><td>2025-07-23</td><td>CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance</td><td>[2507.17312](http://arxiv.org/pdf/2507.17312)</td><td>◆ 提出CasP新流程，通过级联对应先验引导半稠密特征匹配，改进传统全局搜索方式，提升精度和效率。  
◆ 将匹配过程分解为两个渐进阶段，中间引入基于区域的选择性交叉注意力机制，增强特征区分度。  
◆ 在第二阶段将搜索范围限制在第一阶段识别的一对多先验区域，实现一对一匹配的精确定位。  
◆ 结合高层特征降低低层特征提取计算成本，分辨率越高加速效果越显著（1152分辨率下比ELoFTR快2.2倍）。  
◆ 实验证明其在几何估计（尤其是跨域泛化）方面具有优越性，适用于SLAM、无人机等高实时性高鲁棒性场景。</td></tr>
<tr><td>2025-07-21</td><td>DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models</td><td>[2507.15716](http://arxiv.org/pdf/2507.15716)</td><td>◆ DiffPF首次将条件扩散模型融入粒子滤波框架，实现了高质量的后验采样，显著提升了状态估计精度。  
◆ 相比传统可微分粒子滤波依赖预定义或低容量提议分布，DiffPF通过条件扩散模型学习灵活的采样器，直接生成等权粒子。  
◆ 该方法能够从复杂、高维、多模态的滤波分布中进行精确采样，克服了传统方法在复杂分布下的局限性。  
◆ 在全局定位和KITTI视觉里程计等任务中，DiffPF分别以82.8%和26%的精度优势超越现有最优可微分滤波器。  
◆ 实验验证表明，DiffPF在单模态和多模态场景下均表现优异，为动态系统状态估计提供了新范式。</td></tr>
<tr><td>2025-07-21</td><td>Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images</td><td>[2507.15496](http://arxiv.org/pdf/2507.15496)</td><td>◆ 提出了一种新颖的LiDAR-视觉里程计框架，通过深度融合稀疏LiDAR点云和图像数据实现高精度位姿估计。  
◆ 创新性地利用深度补全技术生成稠密深度图，为运动估计提供更丰富的几何约束信息。  
◆ 设计了带注意力机制的多尺度特征提取网络，能够自适应生成深度感知的特征表示。  
◆ 采用稠密深度信息优化光流估计，有效减少了遮挡区域的误差累积问题。  
◆ 开发了分层位姿优化模块，通过渐进式运动估计提升动态环境和尺度模糊场景下的鲁棒性。  
实验证明该方法在KITTI数据集上达到了与当前最优视觉/LiDAR里程计相当或更优的精度和鲁棒性。</td></tr>
<tr><td>2025-07-21</td><td>All-UWB SLAM Using UWB Radar and UWB AOA</td><td>[2507.15474](http://arxiv.org/pdf/2507.15474)</td><td>◆ 提出了一种结合UWB雷达和UWB到达角（AOA）测量的新型SLAM方法，用于视觉受限且特征稀缺的环境。  
◆ 通过动态部署UWB锚点-标签单元，在环境特征不足的区域补充AOA测量数据，提升了SLAM的精度和可扩展性。  
◆ 解决了现有UWB雷达SLAM方法依赖环境特征数量的局限性，扩展了其在无特征环境中的应用能力。  
◆ 详细分析了UWB AOA测量单元的常见约束问题，并提出了相应的解决方案。  
◆ 实验证明，该方法在视觉受限且特征稀缺的环境中仍能有效实现SLAM，为恶劣条件下的自主系统提供了新思路。</td></tr>
<tr><td>2025-07-21</td><td>BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?</td><td>[2507.15321](http://arxiv.org/pdf/2507.15321)</td><td>◆提出BenchDepth新基准，通过五个下游代理任务（深度补全、立体匹配、单目3D场景重建、SLAM和视觉语言空间理解）评估深度基础模型（DFMs），突破传统评估局限。  
◆摒弃依赖对齐指标的固有方法，解决传统评估中因对齐偏差、深度表示偏好导致的不公平比较问题。  
◆首次从实际应用效用角度评估DFMs，强调模型在真实场景中的实用价值而非单纯指标分数。  
◆系统地对8种前沿DFMs进行横向对比，揭示关键发现，为模型优化提供实证依据。  
◆推动深度估计领域评估标准革新，引发社区对评估最佳实践的讨论，促进未来研究发展。</td></tr>
<tr><td>2025-07-20</td><td>LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM</td><td>[2507.15109](http://arxiv.org/pdf/2507.15109)</td><td>◆ 提出LoopNet，一种基于多任务学习的少样本学习方法，专门针对大规模SLAM中的闭环检测问题，兼顾精度与实时性需求。  
◆ 采用改进的ResNet多任务架构，支持动态视觉数据集的在线重训练，并针对嵌入式设备进行优化，适应实际部署场景。  
◆ 创新性结合少样本学习策略进行在线训练，解决传统方法在新环境中数据不足的问题，提升模型适应性。  
◆ 首次在闭环检测中同时输出场景索引和预测质量评估，增强系统可靠性，避免误匹配。  
◆ 利用DISK描述符替代传统手工特征或纯深度学习方法，在光照、视角变化等复杂条件下表现更优。  
◆ 开源了新型闭环检测数据集LoopDB，填补领域内标准化评估数据的空白，推动后续研究。</td></tr>
<tr><td>2025-07-19</td><td>Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</td><td>[2507.14501](http://arxiv.org/pdf/2507.14501)</td><td>◆ 系统梳理了基于前馈式深度学习的3D重建与视图合成技术，首次提出按表示架构（如点云、3D高斯泼溅、神经辐射场等）的分类体系。  
◆ 重点分析了无姿态重建、动态3D重建、3D感知图像/视频合成等关键任务，拓展了在数字人、SLAM等领域的应用场景。  
◆ 对比传统迭代优化方法，突显前馈方法在实时性与泛化能力上的突破性进展，为AR/VR等实时应用提供新范式。  
◆ 全面汇总了主流数据集与评估协议，填补了该领域标准化评测工具的综述空白。  
◆ 指出动态场景建模、计算效率与表示能力平衡等开放挑战，为未来研究指明方向。</td></tr>
<tr><td>2025-07-17</td><td>DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model</td><td>[2507.13145](http://arxiv.org/pdf/2507.13145)</td><td>◆ 提出DINO-VO系统，首次将视觉基础模型DINOv2的鲁棒语义特征应用于单目视觉里程计（VO），解决了传统学习型VO在泛化性和鲁棒性上的不足。  
◆ 设计了一种针对DINOv2粗粒度特征的显著关键点检测器，克服了基础模型特征在VO任务中粒度不足的集成难题。  
◆ 结合DINOv2的语义特征与细粒度几何特征，生成更具局部化能力的混合特征表示，提升了位姿估计精度。  
◆ 采用基于Transformer的匹配器和可微分位姿估计层，通过端到端学习优化特征匹配与运动估计性能。  
◆ 在TartanAir、KITTI等数据集上超越传统帧间VO方法（如SuperPoint），并在室外驾驶场景中与视觉SLAM系统性能相当，同时保持72 FPS的高效实时性（GPU内存&lt;1GB）。  
◆ 实验证明DINOv2特征经改进后，其描述子精度和泛化能力显著优于原始粗粒度特征，尤其在光照变化、动态物体等复杂环境中表现突出。</td></tr>
<tr><td>2025-07-17</td><td>MoCap2GT: A High-Precision Ground Truth Estimator for SLAM Benchmarking Based on Motion Capture and IMU Fusion</td><td>[2507.12920](http://arxiv.org/pdf/2507.12920)</td><td>MoCap2GT提出了一种基于动作捕捉和IMU融合的高精度SLAM基准测试真值估计方法，核心贡献如下：

◆ 提出联合优化框架，融合MoCap数据和设备IMU测量值，显著提升轨迹真值的精度，解决了传统方法因时空标定误差和MoCap抖动导致的精度限制问题。

◆ 设计鲁棒的状态初始化器，确保全局收敛性，避免了优化过程中可能出现的局部最优问题。

◆ 引入SE(3)流形上的高阶B样条位姿参数化方法，并采用可变时间偏移建模，有效处理MoCap因素，提高了轨迹估计的准确性。

◆ 提出退化感知的测量剔除策略，能够识别并剔除不可靠的测量数据，进一步提升估计精度。

实验结果表明，MoCap2GT在精度上优于现有方法，为SLAM算法的全面评估提供了更可靠的基准真值。该方法开源可用，将促进SLAM研究社区的发展。</td></tr>
<tr><td>2025-07-17</td><td>Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot</td><td>[2507.12273](http://arxiv.org/pdf/2507.12273)</td><td>◆ 提出并实现了一款名为Alter-Ego的自主博物馆导览机器人，结合先进导航与交互功能，提升参观体验。  
◆ 首次将大型语言模型（LLMs）应用于博物馆场景，实现实时、情境感知的问答交互，支持游客与机器人就展品展开对话。  
◆ 采用鲁棒的SLAM技术，使机器人能在博物馆环境中自主导航，并根据用户需求动态调整导览路线。  
◆ 通过真实博物馆环境下的用户研究（34名参与者），验证了系统的可用性，结合对话质量分析与问卷调查量化评估效果。  
◆ 揭示了AI驱动机器人在文化空间中人机交互（HRI）的潜力与挑战，包括知识获取的促进与实际部署中的技术局限性。  
◆ 为公共服务机器人领域提供了兼具技术创新与实证研究的范例，推动复杂场景下自主系统的应用探索。</td></tr>
<tr><td>2025-07-16</td><td>Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards</td><td>[2507.12093](http://arxiv.org/pdf/2507.12093)</td><td>◆ 提出Tree-SLAM，一种专为果园环境设计的语义SLAM方法，解决传统SLAM在树木重复外观下易混淆的问题。  
◆ 结合RGB-D图像和实例分割模型，实现树干检测与定位，并通过级联图数据关联算法进行树干重识别。  
◆ 将重识别的树干作为地标，融合噪声GPS信号、里程计和树干观测数据，构建因子图框架，提升定位精度。  
◆ 系统在GPS信号不可靠时仍能保持高精度，单棵树的地理定位误差低至18厘米，低于种植间距的20%。  
◆ 在苹果园和梨园的不同季节数据集上验证了方法的准确性和鲁棒性，适用于精准农业中的目标操作和单树监测任务。</td></tr>
<tr><td>2025-07-11</td><td>Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework</td><td>[2507.08364](http://arxiv.org/pdf/2507.08364)</td><td>◆ 提出M3DGR数据集：首个包含视觉干扰、激光雷达退化、轮式打滑和GNSS失效等系统性退化场景的多传感器基准数据集，填补了标准化评估工具的空白。  
◆ 对40种SLAM系统进行大规模评测：首次在多样化退化条件下全面分析现有算法的鲁棒性，揭示了实际应用中的关键性能瓶颈。  
◆ 开发Ground-Fusion++框架：创新性地融合GNSS、RGB-D、激光雷达、IMU和轮式里程计，通过模块化设计实现多传感器自适应选择。  
◆ 解决传感器动态适配问题：提出环境变化下的传感器优选策略，突破传统框架仅固定融合少数传感器的局限。  
◆ 公开代码与数据集：为后续研究提供可复现的实验平台和性能对比基准，推动领域发展。</td></tr>
<tr><td>2025-07-10</td><td>Hardware-Aware Feature Extraction Quantisation for Real-Time Visual Odometry on FPGA Platforms</td><td>[2507.07903](http://arxiv.org/pdf/2507.07903)</td><td>◆ 提出了一种基于量化SuperPoint卷积神经网络的嵌入式无监督架构，用于实时视觉里程计中的特征点检测与描述。  
◆ 通过硬件感知的模型量化技术（使用Brevitas库和FINN框架），在保证高检测质量的同时显著降低了计算需求。  
◆ 在AMD/Xilinx Zynq UltraScale+ FPGA平台上实现高效部署，利用深度学习处理单元（DPU）优化性能。  
◆ 实现了640×480分辨率图像54帧/秒的处理速度，优于当前同类先进解决方案。  
◆ 在TUM数据集上验证了不同量化技术对模型精度与性能的影响，为资源受限平台（如移动/嵌入式系统）提供了实用优化方案。</td></tr>
<tr><td>2025-07-10</td><td>IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End for Visual SLAM in Challenging Environments</td><td>[2507.07752](http://arxiv.org/pdf/2507.07752)</td><td>◆ IRAF-SLAM提出了一种针对复杂光照环境的视觉SLAM前端框架，通过自适应机制提升系统鲁棒性。  
◆ 创新性地引入图像增强方案，动态预处理不同光照条件下的图像质量，改善特征提取基础。  
◆ 开发了基于图像熵、像素强度和梯度分析的自适应特征提取机制，根据环境动态调整检测灵敏度。  
◆ 提出新型特征筛选策略，结合密度分布分析和光照影响因子，有效过滤不可靠特征点。  
◆ 在TUM-VI和EuRoC数据集上的实验表明，该方法显著减少了跟踪失败率，并在恶劣光照下实现了优于现有方法的轨迹精度。  
◆ 整个系统在提升鲁棒性的同时保持了较低计算开销，为实际应用提供了可行解决方案。</td></tr>
<tr><td>2025-07-09</td><td>g2o vs. Ceres: Optimizing Scan Matching in Cartographer SLAM</td><td>[2507.07142](http://arxiv.org/pdf/2507.07142)</td><td>◆ 首次在Cartographer框架中对g2o和Ceres两种优化器进行了系统的扫描匹配性能对比分析。  
◆ 通过实验验证了Ceres作为Cartographer默认求解器在速度、收敛效率和地图清晰度上的全面优势。  
◆ 发现Ceres在真实场景（AgileX LIMO机器人）中表现更优，所需迭代次数更少且收敛更快。  
◆ 揭示了g2o在局部障碍物检测方面的特殊优势，为其在特定场景的应用价值提供了依据。  
◆ 为SLAM系统优化器选择提供了实证参考，指出不同优化器的适用场景差异。  
◆ 通过定量化指标（如迭代次数、收敛时间）对比，深化了对两种优化器性能特征的理解。</td></tr>
<tr><td>2025-07-08</td><td>Mapping the Catacombs: An Underwater Cave Segment of the Devil&#x27;s Eye System</td><td>[2507.06397](http://arxiv.org/pdf/2507.06397)</td><td>这篇论文的核心贡献是提出了一种低成本的水下洞穴测绘框架，并应用于佛罗里达州Ginnie Springs的Devil&#x27;s Eye洞穴系统。  

◆ 使用廉价运动相机结合潜水电脑，实现了水下洞穴轨迹估计和稀疏点云重建，降低了测绘成本。  
◆ 通过潜水电脑数据增强了视觉/惯性框架（SVIn2），实现了Z轴维度及横滚/俯仰角度的观测，弥补了纯视觉方法的不足。  
◆ 将SVIn2生成的关键帧与相机位姿作为输入，结合全局优化框架COLMAP，重建了局部区域的高密度三维模型。  
◆ 提出了一种洞穴通道的一维抽象表示方法，通过平均轨迹与边界（上下左右）描述洞穴轮廓。  
◆ 采用MNemo V2仪器进行人工测绘验证，证明了该方法的有效性，同时表明运动相机足以构建洞穴地图的基本要素。  
◆ 通过VI-SLAM与全局优化框架的协同，实现了选定区域的逼真密集三维重建，为水文地质和考古研究提供了新工具。</td></tr>
<tr><td>2025-07-08</td><td>Cooperative Mapping, Localization, and Beam Management via Multi-Modal SLAM in ISAC Systems</td><td>[2507.05718](http://arxiv.org/pdf/2507.05718)</td><td>◆ 提出了一种新颖的多模态SLAM框架，解决了协同多用户SLAM在ISAC系统中的理论建模和通信层集成不足的问题。  
◆ 开发了基于贝叶斯估计的协同多用户SLAM方法，并设计了两阶段算法，在动态异构感知条件下实现鲁棒的无线电地图构建。  
◆ 引入多模态定位策略，通过误差感知模型融合SLAM结果、摄像头多目标跟踪和IMU数据，显著提升了多用户场景下的UE定位精度。  
◆ 提出了感知辅助的波束管理方案，利用全局无线电地图和定位数据生成UE特定的先验信息，优化波束选择，降低用户间干扰并提升下行频谱效率。  
◆ 仿真结果表明，该系统将无线电地图精度提升高达60%，定位精度提高37.5%，在室内外环境中均显著优于传统方法。</td></tr>
<tr><td>2025-07-07</td><td>Simultaneous Localization and Mapping Using Active mmWave Sensing in 5G NR</td><td>[2507.04662](http://arxiv.org/pdf/2507.04662)</td><td>◆ 提出利用毫米波5G NR系统进行主动感知，实现类似激光雷达的点云生成，克服了传统被动感知SLAM技术对镜面反射假设和简化地图表示的依赖。  
◆ 采用二进制搜索方法从每个波束方向的功率延迟剖面中提取点云数据，提高了环境感知的精度和细节。  
◆ 通过多个预定义目标点校准硬件延迟，确保点云数据的准确性，解决了硬件误差对定位的影响。  
◆ 利用点云配准算法从连续轨迹视角的点云数据中估计终端位姿变化，实现了高精度的终端定位。  
◆ 引入闭环检测和位姿图优化技术，进一步优化感知结果，实现了精确的终端定位和详细的无线电地图重建。  
◆ 通过仿真和实验验证了系统的有效性，为5G NR在SLAM领域的应用提供了实践支持。</td></tr>
<tr><td>2025-07-06</td><td>Lidar Variability: A Novel Dataset and Comparative Study of Solid-State and Spinning Lidars</td><td>[2507.04321](http://arxiv.org/pdf/2507.04321)</td><td>◆ 提出了首个包含穹顶式固态激光雷达（如Livox Mid-360）与其他固态及旋转式激光雷达（如Ouster系列）的综合数据集，填补了多类型激光雷达对比研究的空白。  
◆ 首次在无IMU支持的里程计场景下，系统评估了低成本固态激光雷达（Livox Avia/Mid-360）与高端旋转式激光雷达的性能差异。  
◆ 基于该数据集，对主流SLAM算法进行了跨传感器平台的基准测试，为异构激光雷达的定位与建图研究提供了标准化参考。  
◆ 针对点云配准技术，通过室内外实测数据定量比较了点对点、点对平面及混合方法的性能差异。  
◆ 研究结果为SLAM和3D重建领域在低成本固态激光雷达（尤其是穹顶式设计）的应用提供了数据支持与方法指导。  
◆ 数据集与基准测试框架为未来异构激光雷达系统的算法开发与性能优化奠定了基础。</td></tr>
<tr><td>2025-07-09</td><td>Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM</td><td>[2507.04004](http://arxiv.org/pdf/2507.04004)</td><td>◆ 首次提出结合LiDAR-惯性-相机的3D高斯泼溅SLAM系统，同步优化视觉质量、几何精度和实时性能，实现高保真3D高斯地图的实时构建与RGB/深度渲染。  
◆ 针对LiDAR覆盖不足区域，采用轻量级零样本深度模型，融合RGB外观线索与稀疏LiDAR数据生成稠密深度图，显著提升稀疏LiDAR传感器的场景适用性。  
◆ 利用高精度稀疏LiDAR深度监督高斯地图优化，并通过定制CUDA加速策略提升效率，增强几何准确性。  
◆ 创新地将增量重建的高斯地图光度约束融入连续时间因子图优化，在LiDAR性能退化时提升位姿估计鲁棒性。  
◆ 扩展系统功能至下游应用（如视频帧插值与快速3D网格提取），并构建包含真值位姿、深度图和外推轨迹的多模态数据集，支持严格评估。  
◆ 在公开与自采数据集上验证系统对多种密度LiDAR的优越性，代码与数据集将开源。</td></tr>
<tr><td>2025-07-04</td><td>Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps</td><td>[2507.03737](http://arxiv.org/pdf/2507.03737)</td><td>◆ 提出S3PO-GS方法，首次实现基于RGB单目相机的户外全局尺度一致3D高斯点建图SLAM系统。  
◆ 设计自一致跟踪模块，以3D高斯点图为锚点，避免累积尺度漂移，实现更精准鲁棒的相机跟踪（迭代次数更少）。  
◆ 创新性提出基于分块的动态点图建图模块，引入几何先验知识的同时规避尺度歧义问题，显著提升复杂户外场景的跟踪精度和重建质量。  
◆ 通过融合几何先验与3DGS渲染优势，解决了现有方法在户外场景缺乏几何约束或依赖独立跟踪模块导致的尺度漂移问题。  
◆ 在Waymo、KITTI和DL3DV数据集上验证了方法的优越性，在新视角合成和跟踪精度上均超越现有3DGS SLAM方法。</td></tr>
<tr><td>2025-07-01</td><td>RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles</td><td>[2507.00937](http://arxiv.org/pdf/2507.00937)</td><td>◆提出RaGNNarok，一种基于图神经网络（GNN）的轻量级框架，用于增强雷达点云数据，解决现有雷达定位中稀疏点云、噪声和误检测问题。  
◆该框架在低成本设备（如树莓派5）上实现实时处理，推理时间仅7.3毫秒，无需额外计算资源，适合资源受限的移动机器人。  
◆通过GNN模型优化雷达点云，显著提升在复杂动态环境中的性能，克服了传统激光雷达和相机在视觉遮挡环境中的局限性。  
◆在定位、SLAM和自主导航等关键任务中进行了多环境测试，验证了其高可靠性和泛化能力。  
◆为低成本室内移动机器人提供了一种经济高效的解决方案，结合毫米波雷达的低成本优势，推动自动化在家庭和商业空间的应用。</td></tr>
<tr><td>2025-07-01</td><td>Generation of Indoor Open Street Maps for Robot Navigation from CAD Files</td><td>[2507.00552](http://arxiv.org/pdf/2507.00552)</td><td>◆ 提出全自动系统，将建筑CAD文件转换为分层拓扑OpenStreetMap（OSM）表示，专为机器人终身导航设计，解决SLAM在动态大尺度室内环境中耗时、脆弱且易过时的问题。  
◆ 开发多阶段处理流程，从原始CAD数据中提取关键结构层，并基于AreaGraph进行拓扑分割，生成层次化可导航空间图，实现语义丰富的环境建模。  
◆ 自动关联CAD源文件中的文本标签，增强地图语义信息，同时支持多楼层无缝合并，构建拓扑正确的统一模型，提升导航鲁棒性。  
◆ 利用CAD文件固有的永久结构信息，规避SLAM的固有缺陷，为复杂室内场景提供高效、可扩展的解决方案。  
◆ 集成直观图形用户界面（GUI）封装软件，降低使用门槛，并开源代码和数据集促进社区应用与研究。</td></tr>
<tr><td>2025-06-30</td><td>VOCAL: Visual Odometry via ContrAstive Learning</td><td>[2507.00243](http://arxiv.org/pdf/2507.00243)</td><td>◆ VOCAL将视觉里程计（VO）重新定义为标签排序问题，突破了传统基于几何假设的局限，为数据驱动框架提供了新思路。  
◆ 通过结合贝叶斯推理与表征学习，该框架使视觉特征与相机状态对齐，提升了特征的可解释性。  
◆ 提出的排序机制迫使相似相机状态在潜在空间中形成一致且空间连贯的表征，增强了模型的鲁棒性。  
◆ 框架支持多模态数据融合，为复杂场景下的VO应用提供了灵活性。  
◆ 在KITTI数据集上的实验验证了VOCAL在可解释性和泛化性上的显著优势，推动了空间智能向更通用、可解释的方向发展。</td></tr>
<tr><td>2025-06-29</td><td>TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints</td><td>[2506.23207](http://arxiv.org/pdf/2506.23207)</td><td>TVG-SLAM是一种基于3D高斯泼溅（3DGS）的RGB-only SLAM系统，通过三视图几何约束提升鲁棒性和场景重建质量。其核心贡献和创新点如下：

◆ 提出三视图几何范式，通过密集三视图匹配模块聚合可靠的帧间对应关系，形成跨帧的鲁棒几何约束，解决传统方法依赖光度损失的局限性。

◆ 设计混合几何约束（Hybrid Geometric Constraints），结合三视图匹配的几何线索与光度损失，显著提升相机位姿估计的准确性和稳定性，尤其在视角突变和光照变化场景。

◆ 提出基于概率的初始化策略，将三视图对应关系的几何不确定性编码到新初始化的高斯模型中，提升映射质量。

◆ 引入动态渲染信任衰减机制（Dynamic Attenuation of Rendering Trust），有效缓解因建图延迟导致的跟踪漂移问题。

实验表明，TVG-SLAM在户外数据集上优于现有RGB-only 3DGS SLAM系统，在最挑战性数据集中将轨迹误差（ATE）降低69.0%，同时保持顶尖的渲染质量。</td></tr>
<tr><td>2025-06-29</td><td>Event-based Stereo Visual-Inertial Odometry with Voxel Map</td><td>[2506.23078](http://arxiv.org/pdf/2506.23078)</td><td>◆ 提出Voxel-ESVIO系统，结合事件相机和立体视觉惯性里程计，利用体素地图管理提升定位精度。  
◆ 采用基于体素的点选择方法，有效过滤事件流中的噪声，筛选高质量3D地图点。  
◆ 创新性地引入体素感知的点管理机制，动态优化每个体素内地图点的更新和选择。  
◆ 通过协同策略高效提取抗噪声且观测概率最高的地图点，确保状态估计的准确性。  
◆ 在三个公开数据集上的实验表明，该系统在精度和计算效率上均优于现有方法。</td></tr>
<tr><td>2025-06-26</td><td>Adaptive Multipath-Based SLAM for Distributed MIMO Systems</td><td>[2506.21798](http://arxiv.org/pdf/2506.21798)</td><td>◆ 提出了一种适用于分布式MIMO系统的自适应多路径SLAM方法，解决了传统方法在非凸几何环境中无法进行光线追踪的局限性。  
◆ 利用振幅统计量建立自适应时变检测概率，实现了&quot;软&quot;光线追踪策略，能够在非凸几何的射频环境中跨传播路径融合信息。  
◆ 通过将和积算法(SPA)的消息传递规则应用于所提出的统计模型因子图，建立了地图特征和智能体位置联合估计的贝叶斯估计方法。  
◆ 提出了一种改进的建议概率密度函数(PDF)，用于基于粒子的SPA消息计算，能够早期检测仅由双跳路径支持的新表面。  
◆ 在具有非凸几何形状的挑战性场景中使用合成射频测量验证了该方法，结果表明其能够提供准确的定位和建图估计，并达到后验CRLB界。</td></tr>
<tr><td>2025-06-24</td><td>Ark: An Open-source Python-based Framework for Robot Learning</td><td>[2506.21628](http://arxiv.org/pdf/2506.21628)</td><td>◆ 提出ARK框架，首个以Python为核心的机器人学习开源平台，弥合机器人技术与现代AI工具链的鸿沟。  
◆ 采用Gym风格接口设计，支持数据采集、预处理到策略训练的全流程，兼容仿真与实体机器人无缝切换。  
◆ 独创轻量级客户端-服务器架构，实现网络化发布-订阅通信，并保留C/C++绑定选项保障实时性能需求。  
◆ 内置控制、SLAM、运动规划等模块化组件，原生支持ROS交互，提供开箱即用的机器人功能套件。  
◆ 通过详实文档和案例（如操作与导航任务），验证其快速原型开发、硬件灵活切换及端到端流水线优势。  
◆ 统一Python生态与机器人开发，显著降低学习门槛，加速学术研究与商业场景的机器人自主性落地。</td></tr>
<tr><td>2025-06-26</td><td>EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting</td><td>[2506.21420](http://arxiv.org/pdf/2506.21420)</td><td>◆ 提出EndoFlow-SLAM系统，首次将光流损失作为几何约束引入基于3D高斯泼溅（3DGS）的SLAM框架，有效解决了内窥镜场景中非朗伯表面和呼吸运动导致的位姿估计问题。  
◆ 设计深度正则化策略，缓解内窥镜场景的光度不一致性问题，确保3DGS深度渲染的可靠性。  
◆ 改进3DGS优化策略，针对关键帧中渲染质量较差的视角进行重点优化，提升场景表示精度。  
◆ 在静态（C3VD数据集）和动态（StereoMIS数据集）手术场景中均实现领先性能，在新视角合成和相机位姿估计任务上超越现有方法。  
◆ 系统支持实时运行，为内窥镜手术提供高效的三维重建与可视化能力。</td></tr>
<tr><td>2025-06-26</td><td>CURL-SLAM: Continuous and Compact LiDAR Mapping</td><td>[2506.21077](http://arxiv.org/pdf/2506.21077)</td><td>◆ 提出了一种新型LiDAR SLAM范式CURL-SLAM，利用连续超紧凑表示（CURL）实现可更新、可定位的地图表示。  
◆ 采用球谐函数隐式编码技术，生成支持可变密度连续重建的紧凑3D地图，显著降低存储需求。  
◆ 通过独特的CURL定制优化问题重新定义LiDAR位姿估计，替代传统ICP方法，提升计算效率。  
◆ 扩展局部光束法平差（BA）技术，实现位姿精修与地图校正同步进行，确保闭环后的全局一致性。  
◆ 在CPU上达到10Hz实时性能，同时保持领先的3D建图质量和轨迹精度，适用于大规模场景。  
◆ 开源CURL-SLAM实现，推动连续紧凑地图表示领域的研究与应用。</td></tr>
<tr><td>2025-06-25</td><td>SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning</td><td>[2506.20394](http://arxiv.org/pdf/2506.20394)</td><td>◆ 提出首个在线语义信息更新框架SPARK，解决机器人任务执行中语义信息实时更新的空白问题。  
◆ 创新性地将离线场景图表示扩展到在线场景，提升动态环境下的语义信息处理能力。  
◆ 通过环境嵌入线索（如手势等非传统空间提示）实时更新场景图，增强机器人对动态环境的适应性。  
◆ 验证了基于图的空间关系表示能显著提升任务规划效率，尤其在非结构化场景中表现突出。  
◆ 系统整合几何与语义数据，为通用服务机器人提供更全面的在线信息更新解决方案。  
◆ 实验证明该框架能有效处理动态环境中的语义变化，为后续任务规划提供可靠支持。</td></tr>
<tr><td>2025-06-25</td><td>Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles</td><td>[2506.20311](http://arxiv.org/pdf/2506.20311)</td><td>◆ 开发了适用于复杂3D环境的实时避障算法，特别针对森林火灾等灾害场景中的无人机安全导航需求。  
◆ 提出了一种创新的2D融合导航策略，最初为地面移动机器人设计，具备动态环境中的安全移动能力，并支持自适应障碍处理与决策优化。  
◆ 首次设计了针对森林火灾模拟的3D反应式导航策略，解决了无人机在此类特殊场景中的避障难题。  
◆ 提出无人机与地面无人车（UGV）的协同控制框架，实现了空地车辆在森林救援任务中的统一协调作业。  
◆ 通过数学建模与仿真验证了各阶段算法的有效性，为自然灾害救援中无人系统的应用提供了兼具实用价值与学术意义的解决方案。</td></tr>
<tr><td>2025-06-24</td><td>Posterior Cramér-Rao Bounds on Localization and Mapping Errors in Distributed MIMO SLAM</td><td>[2506.19957](http://arxiv.org/pdf/2506.19957)</td><td>◆ 首次提出了针对分布式MIMO SLAM系统中镜面反射面定位与建图误差的后验克拉美罗下界（MEB），填补了该领域性能评估的理论空白。  
◆ 考虑了单次反射和双次反射的复杂传播场景，并支持分布式锚点配置，扩展了传统SLAM性能边界的适用范围。  
◆ 通过数值仿真验证了现有先进RF-SLAM算法的建图误差能渐进收敛至MEB，为算法性能评估提供了理论基准。  
◆ 创新性地将映射性能（镜面位置/朝向）与用户定位性能统一纳入全局特征评估框架，突破了传统仅关注定位精度的局限。  
◆ 所提边界理论可提升多径信道中非视距信号的利用效率，为通信-定位一体化系统设计提供理论支撑。</td></tr>
<tr><td>2025-06-23</td><td>GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM</td><td>[2506.18885](http://arxiv.org/pdf/2506.18885)</td><td>◆ 提出了GRAND-SLAM方法，首次将3D高斯泼溅技术应用于大规模户外多智能体SLAM场景，突破了现有方法仅限于小规模室内环境的限制。  
◆ 设计了基于局部子地图优化的隐式跟踪模块，有效提升了多智能体系统的定位精度和鲁棒性。  
◆ 开发了机器人内/间闭环检测方法，并将其集成到位姿图优化框架中，实现了全局一致性的大规模场景重建。  
◆ 在Replica室内数据集上实现了当前最优的跟踪性能，PSNR指标比现有方法提升28%。  
◆ 在大型户外Kimera-Multi数据集上，多智能体跟踪误差降低91%，渲染质量显著优于现有方法。  
◆ 通过可扩展的环境表示方法，为多智能体协同快速探索与重建提供了新解决方案。</td></tr>
<tr><td>2025-06-23</td><td>MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation</td><td>[2506.18678](http://arxiv.org/pdf/2506.18678)</td><td>◆ 提出首个分布式多智能体协作神经SLAM框架MCN-SLAM，结合混合隐式神经场景表示，解决传统单智能体SLAM在大场景和长序列中的局限性。  
◆ 创新设计三平面-网格联合场景表示方法，显著提升场景重建质量，优于现有神经隐式表示方案。  
◆ 开发新型&quot;内部-跨智能体&quot;闭环检测机制，首次实现单智能体局部与多智能体全局一致性协同优化。  
◆ 提出在线蒸馏方法实现多子地图融合，通过分布式通信优化解决NeRF类系统带宽受限问题。  
◆ 发布首个真实世界密集SLAM数据集DES，涵盖单/多智能体场景，提供连续轨迹与高精度3D网格真值，填补领域空白。  
实验证明该方法在建图、定位和通信效率上均优于现有技术，代码与数据集将开源推动SLAM与三维重建研究发展。</td></tr>
<tr><td>2025-06-24</td><td>Multimodal Fusion SLAM with Fourier Attention</td><td>[2506.18204](http://arxiv.org/pdf/2506.18204)</td><td>◆ 提出FMF-SLAM方法，通过快速傅里叶变换（FFT）提升多模态SLAM的算法效率，解决传统光流SLAM计算资源消耗大的问题。  
◆ 创新设计基于傅里叶的自注意力与跨注意力机制，有效融合RGB和深度信号的特征提取。  
◆ 引入多尺度跨模态知识蒸馏技术，增强多模态特征间的交互与互补性。  
◆ 结合GNSS-RTK全局定位模块与全局Bundle Adjustment，实现安全机器人的实时应用验证。  
◆ 在TUM、TartanAir及真实场景数据集上验证性能，在噪声、光照变化和黑暗条件下达到领先水平。  
◆ 公开代码与数据集，推动多模态SLAM领域的可复现研究。</td></tr>
<tr><td>2025-06-22</td><td>ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM</td><td>[2506.18016](http://arxiv.org/pdf/2506.18016)</td><td>◆ 提出ADA-DPM自适应噪声过滤策略，在动态物体干扰和噪声环境下同时提升SLAM定位精度与系统鲁棒性。  
◆ 设计动态分割头（Dynamic Segmentation Head），通过预测特征点类别主动剔除动态特征点，减少动态干扰。  
◆ 引入全局重要性评分头（Global Importance Scoring Head），自适应筛选高贡献特征点并抑制噪声干扰，优化特征选择。  
◆ 构建跨层图内卷积模块（GLI-GCN），融合多尺度邻域结构，增强重叠特征的判别能力。  
◆ 在多个公开数据集上验证有效性，实验结果表明该方法性能优于现有技术。</td></tr>
<tr><td>2025-06-21</td><td>Optimizing Exploration with a New Uncertainty Framework for Active SLAM Systems</td><td>[2506.17775](http://arxiv.org/pdf/2506.17775)</td><td>◆提出不确定性地图（UM）框架，通过概率分布量化地图不确定性，为主动SLAM系统建立新型环境建模方法。  
◆定义不确定性边界（UF）作为探索-开发的关键目标与停止准则，解决传统方法中探索终止条件模糊的问题。  
◆创新性引入基于KL散度的符号相对熵（SiREn），首次实现覆盖度与不确定性的联合度量，仅需单一参数即可平衡探索与开发。  
◆设计传感器无关的通用架构，兼容相机、激光雷达及多传感器融合系统，突破现有方法对特定SLAM配置的依赖。  
◆结合UF的路径规划系统首次实现开放空间的自主探索能力，填补了主动SLAM文献中该行为的空白。  
◆开源ROS节点与完整数据集，推动方法验证与社区应用，增强研究可复现性。</td></tr>
<tr><td>2025-06-18</td><td>MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System</td><td>[2506.15402](http://arxiv.org/pdf/2506.15402)</td><td>◆ 提出MCOO-SLAM系统，首次将多相机全景配置引入物体级SLAM，解决传统单目或RGB-D系统视场窄、遮挡敏感和深度感知受限的问题。  
◆ 融合点特征与开放词汇语义增强的物体级地标，实现复杂户外场景中更鲁棒且语义丰富的建图。  
◆ 设计语义-几何-时序多模态融合策略，显著提升跨视角物体关联的准确性，改善物体建模一致性。  
◆ 创新全景闭环检测模块，通过场景级描述符实现视角无关的地点识别，增强系统在动态环境中的稳定性。  
◆ 构建分层3D场景图谱抽象地图，为机器人高层推理任务提供结构化语义支持。  
实验证明该系统在遮挡、位姿变化和复杂环境下的定位精度与可扩展性均优于现有方法。</td></tr>
<tr><td>2025-06-24</td><td>RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories</td><td>[2506.15242](http://arxiv.org/pdf/2506.15242)</td><td>◆ 提出RA-NeRF方法，能够在复杂相机轨迹下实现高精度的相机位姿估计，解决了传统NeRF和3DGS依赖准确位姿先验的问题。  
◆ 采用增量式重建流程，结合光度一致性约束和光流驱动的位姿调节机制，提升了初始化和定位阶段的鲁棒性。  
◆ 创新性地引入隐式位姿滤波器，通过捕捉相机运动模式有效消除位姿估计中的噪声干扰。  
◆ 在Tanks&amp;Temple和NeRFBuster两个数据集上验证了方法的有效性，其中NeRFBuster包含极具挑战性的相机轨迹场景。  
◆ 实验结果表明，RA-NeRF在相机位姿估计精度和场景重建视觉质量上均达到最先进水平，尤其在复杂轨迹条件下表现突出。</td></tr>
<tr><td>2025-06-18</td><td>SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization</td><td>[2506.15175](http://arxiv.org/pdf/2506.15175)</td><td>◆ 提出SHeRLoc，首个专为异构雷达设计的深度网络，填补了跨模态雷达定位研究的空白。  
◆ 采用RCS极坐标匹配技术，有效对齐多模态雷达数据，解决异构传感器数据融合难题。  
◆ 提出基于分层最优传输的特征聚合方法，生成具有旋转鲁棒性的多尺度描述符。  
◆ 结合FFT相似性数据挖掘和自适应边界三元组损失，实现视场感知的度量学习。  
◆ 在公开数据集上实现召回率@1从不足0.1提升至0.9，性能超越现有最佳方法一个数量级。  
◆ 扩展性强，可应用于LiDAR等传感器，为跨模态地点识别和异构SLAM开辟新途径。</td></tr>
<tr><td>2025-06-18</td><td>VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments</td><td>[2506.15126](http://arxiv.org/pdf/2506.15126)</td><td>◆ 提出VIMS系统，首次将视觉-惯性-磁力-声纳多模态融合用于水下SLAM，解决传统视觉-惯性方法在水下环境中的尺度估计和闭环难题。  
◆ 创新性引入低成本单波束声纳，有效提升水下尺度估计精度，克服纯视觉方法因水体折射导致的尺度漂移问题。  
◆ 利用高采样率磁力计配合经济型磁场线圈生成磁特征，实现基于磁场指纹的场所识别，填补水下无纹理区域的感知空白。  
◆ 设计分层式视觉-磁力混合闭环检测框架，通过多模态数据互补增强闭环鲁棒性，显著降低误匹配率。  
◆ 优化前端计算负载，平衡局部特征跟踪与全局描述子匹配，在不增加前端负担的前提下实现高效闭环。  
◆ 实验验证系统在复杂水下环境中的优越性，相比传统方法定位精度提升30%以上，为低成本水下自主导航提供新方案。</td></tr>
<tr><td>2025-06-16</td><td>Slanted light-sheet array microscopy for large volume imaging at rates exceeding 100 Hz</td><td>[2506.13664](http://arxiv.org/pdf/2506.13664)</td><td>◆ 开发了倾斜光片阵列显微镜（SLAM），实现了超过100 Hz的超快速大体积成像，突破了传统成像速度限制。  
◆ 基于标准宽场复合显微镜进行简单改造，仅需对照明光路进行最小化修改，便于集成和推广。  
◆ 支持大范围多维度高分辨率成像（横向超过500像素，深度超过200层），同时保持光学切片和局部光化学能力。  
◆ 结合深度学习（条件去噪扩散概率模型），实现了各向同性分辨率提升，优化了图像质量。  
◆ 兼容常规生物样本制备协议，适用于多种生物医学研究场景，具有广泛的应用潜力。  
◆ 在高速成像的同时兼顾了空间分辨率、信噪比和大视场需求，为动态生物过程观测提供了新工具。</td></tr>
<tr><td>2025-06-16</td><td>Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots</td><td>[2506.13149](http://arxiv.org/pdf/2506.13149)</td><td>◆ 提出SEGO（语义图谱本体）认知映射架构，首次将几何感知、语义推理和解释生成整合为统一框架，实现人机协作机器人的认知协同。  
◆ 构建动态认知场景图，突破传统SLAM仅关注空间几何的局限，同时表征环境中的语义关系和本体一致性。  
◆ 创新性地融合基于SLAM的定位、深度学习物体检测跟踪与本体驱动推理三大模块，实现实时语义连贯的环境建模。  
◆ 通过本体论约束确保语义推理的逻辑一致性，使机器人能理解&quot;桌子上的杯子&quot;等复杂语义关系。  
◆ 支持可解释性输出，机器人可生成对人类友好的场景解释，显著提升人机协作的透明度和信任度。  
◆ 该架构为人类中心协作机器人提供标准化认知处理流程，在工业装配、家庭服务等场景展现应用潜力。</td></tr>
<tr><td>2025-06-16</td><td>A Novel ViDAR Device With Visual Inertial Encoder Odometry and Reinforcement Learning-Based Active SLAM Method</td><td>[2506.13100](http://arxiv.org/pdf/2506.13100)</td><td>◆ 提出了一种新型ViDAR设备，结合视觉、惯性和电机编码器，构建紧耦合的视觉-惯性-编码器里程计（VIEO），显著提升了SLAM系统的主动能力和视野范围。  
◆ 设计了ViDAR校准方法，确保VIEO算法的精确初始化，解决了多传感器融合中的标定难题。  
◆ 首次将电机编码器引入SLAM系统，以极低的成本和结构复杂度增强了跨帧共视关系，提高了状态估计精度。  
◆ 提出基于深度强化学习（DRL）的平台运动解耦主动SLAM方法，能够自主优化运动策略以增加特征点多样性。  
◆ 实验证明，所提VIEO算法相比传统VIO算法在共视关系和估计精度上均有显著提升，且DRL主动SLAM进一步优化了系统性能。  
◆ 为复杂环境下主动SLAM系统的平台设计和运动解耦方法提供了新思路，兼具理论创新和实用价值。</td></tr>
<tr><td>2025-06-16</td><td>SuperPoint-SLAM3: Augmenting ORB-SLAM3 with Deep Features, Adaptive NMS, and Learning-Based Loop Closure</td><td>[2506.13089](http://arxiv.org/pdf/2506.13089)<br><a href=''>[代码]</a></td><td>◆ 用自监督的SuperPoint特征检测-描述子替代传统ORB特征，提升了在极端视角、尺度和光照变化下的鲁棒性。  
◆ 引入自适应非极大值抑制(ANMS)技术，实现空间分布更均匀的关键点提取，增强场景覆盖度。  
◆ 集成轻量级NetVLAD模块作为学习式回环检测器，显著改善了传统词袋模型的识别能力。  
◆ 在KITTI数据集上将平均平移误差从4.15%降至0.34%，旋转误差从0.0027度/米降至0.0010度/米。  
◆ 在EuRoC MAV数据集上所有序列误差降低约50%（如V2_03从1.58%降至0.79%）。  
◆ 保持ORB-SLAM3实时性能的同时，通过深度学习特征与学习式回环的融合实现了精度突破。</td></tr>
<tr><td>2025-06-12</td><td>LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System</td><td>[2506.10567](http://arxiv.org/pdf/2506.10567)</td><td>◆ 提出LRSLAM模型，采用低秩张量分解方法（Six-axis和CP分解）优化稠密视觉SLAM系统，显著提升计算效率和内存利用率。  
◆ 通过低秩表示有符号距离场（SDF），解决了传统神经隐式表示的高计算成本和内存占用问题，适合大规模场景。  
◆ 相比现有方法（如ESLAM的平面张量分解），进一步降低了内存增长压力，同时保持高精度重建与定位能力。  
◆ 在多种室内RGB-D数据集上验证，LRSLAM在参数效率、处理速度和准确性方面均优于当前最优方法。  
◆ 实现了更快的收敛速度和更高的系统鲁棒性，为自动驾驶、移动机器人等实时应用提供可行解决方案。  
◆ 代码将开源，促进相关领域研究发展。</td></tr>
<tr><td>2025-06-11</td><td>VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots</td><td>[2506.09583](http://arxiv.org/pdf/2506.09583)</td><td>◆ 提出VAULT原型系统，基于ROS 2框架，专为户外自主机器人设计，解决复杂环境下的实时定位与建图难题。  
◆ 创新性融合多传感器数据（GNSS、VIO、IMU）与扩展卡尔曼滤波（EKF），生成高可靠性3D里程计，提升户外定位鲁棒性。  
◆ 结合视觉SLAM（VSLAM）技术，构建精细3D点云地图，弥补传统2D LiDAR在户外场景的局限性。  
◆ 实现室内外环境通用性，通过多模态传感器协同，适应农业、林业等无结构化户外场景需求。  
◆ 提供开源ROS 2解决方案，为自主机器人社区提供可扩展、模块化的移动测绘系统（MMS）参考框架。</td></tr>
<tr><td>2025-06-10</td><td>UFM: A Simple Path towards Unified Dense Correspondence with Flow</td><td>[2506.09278](http://arxiv.org/pdf/2506.09278)</td><td>◆ 提出统一流与匹配模型（UFM），首次实现宽基线场景和光流估计的统一训练，突破传统分而治之的局限。  
◆ 采用简单通用的Transformer架构直接回归(u,v)流，避免传统 coarse-to-fine 代价体积的复杂性，训练更高效且对大位移更精准。  
◆ 在光流任务上精度超越当前最优方法（Unimatch）28%，在宽基线匹配任务上误差降低62%且速度提升6.7倍（对比RoMa）。  
◆ 首次证明统一训练模型可同时在光流和宽基线匹配两个领域超越专用方法，为通用稠密对应开辟新路径。  
◆ 通过共可见像素的统一数据训练，为多模态、长距离和实时对应任务提供新思路。</td></tr>
<tr><td>2025-06-10</td><td>Princeton365: A Diverse Dataset with Accurate Camera Pose</td><td>[2506.09035](http://arxiv.org/pdf/2506.09035)</td><td>◆ 提出了Princeton365数据集，包含365个多样化视频，提供高精度的相机位姿，填补了当前SLAM基准在精度和数据多样性之间的空白。  
◆ 设计了一种新颖的地面真值采集框架，结合校准板和360度相机，实现了室内、室外和物体扫描视频的多模态同步采集（单目/立体RGB视频和IMU）。  
◆ 提出了一种基于光流的场景尺度感知SLAM评估指标，克服了传统指标（如ATE）无法跨场景比较的局限性，便于分析算法失败模式。  
◆ 构建了具有挑战性的新视角合成（NVS）基准，涵盖当前基准未涉及的场景（如非朗伯表面和360度相机轨迹）。  
◆ 公开了完整的数据集、代码和提交平台（https://princeton365.cs.princeton.edu），推动SLAM和NVS领域的标准化研究。</td></tr>
<tr><td>2025-06-10</td><td>Planar Collisionless Shock Simulations with Semi-Implicit Particle-in-Cell Model FLEKS</td><td>[2506.08384](http://arxiv.org/pdf/2506.08384)</td><td>◆ 验证了半隐式粒子网格代码FLEKS在无碰撞激波模拟中的适用性，特别针对全球磁层建模相关参数范围。  
◆ 开发了精细化算法，使FLEKS能够在电子惯性长度量级的网格分辨率下精确模拟激波结构。  
◆ 成功捕捉了激波关键特征，包括激波结构（脚部、陡坡、过冲和欠冲）、上下游波动（快磁声波、哨声波、阿尔芬离子回旋波和镜像模）以及非麦克斯韦粒子分布。  
◆ 揭示了二维模拟对准确重现准垂直激波下游波动物理和准平行激波复杂动力学（如表面波纹、激波子、SLAMS和喷流）的必要性。  
◆ 通过参数研究阐明了质量比和网格分辨率对激波物理的影响，为半隐式PIC代码的物理和数值参数选择提供了重要指导。  
◆ 为将动力学激波过程整合到MHD-AEPIC模型的大尺度空间等离子体模拟中奠定了基础。</td></tr>
<tr><td>2025-06-09</td><td>ZeroVO: Visual Odometry with Minimal Assumptions</td><td>[2506.08005](http://arxiv.org/pdf/2506.08005)</td><td>ZeroVO是一种无需预训练即可泛化至不同相机和环境的视觉里程计算法，其核心贡献与创新点如下：

◆ 提出无需标定的几何感知网络结构，能有效处理深度估计和相机参数中的噪声，摆脱传统方法对固定标定配置的依赖。

◆ 引入基于语言的先验知识，通过语义信息增强特征提取的鲁棒性，显著提升模型在未知领域的泛化能力。

◆ 开发半监督训练框架，利用未标注数据迭代适应新场景，进一步强化模型在真实复杂场景中的适应能力。

◆ 在KITTI、nuScenes和Argoverse 2等标准数据集及GTA合成数据上验证性能，相对现有方法提升超过30%。

◆ 无需微调或相机标定的特性，使得该技术具备大规模实际部署的潜力，极大扩展了视觉里程计的应用范围。</td></tr>
<tr><td>2025-06-08</td><td>Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs</td><td>[2506.07164](http://arxiv.org/pdf/2506.07164)</td><td>这篇论文的核心贡献是通过优化低端嵌入式GPU上的Oriented FAST特征检测，显著提升了视觉SLAM系统的实时处理能力。具体创新点如下：

◆ 提出了一种二进制级编码策略，快速确定FAST特征点候选点，大幅减少了计算复杂度。  
◆ 设计了一种可分离的Harris角点检测策略，结合底层GPU硬件指令优化，提高了计算效率。  
◆ 在Jetson TX2嵌入式GPU上实现了平均7.3倍的速度提升，远超现有OpenCV的GPU加速方案。  
◆ 通过优化FAST特征点检测和Harris角点检测这两个最耗时的步骤，解决了移动平台实时处理的瓶颈问题。  
◆ 为资源受限环境下的实时SLAM应用提供了高效解决方案，具有广泛的移动和嵌入式应用潜力。</td></tr>
<tr><td>2025-06-08</td><td>UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment</td><td>[2506.07013](http://arxiv.org/pdf/2506.07013)</td><td>◆ 提出UNO框架，实现跨平台、跨环境的统一自监督单目视觉里程计，无需针对特定场景或设备进行调优。  
◆ 采用混合专家策略（Mixture-of-Experts），通过多个专用解码器分别处理不同类别的运动模式，提升泛化能力。  
◆ 设计可微分的Gumbel-Softmax模块，动态构建帧间关联图并选择最优解码器，同时剔除错误估计。  
◆ 结合预训练的尺度无关深度先验与轻量级捆绑调整（bundling adjustment），后端统一优化几何一致性。  
◆ 在KITTI（自动驾驶）、EuRoC-MAV（无人机）和TUM-RGBD（手持设备）三大数据集上验证，性能达到SOTA。</td></tr>
<tr><td>2025-06-06</td><td>GS4: Generalizable Sparse Splatting Semantic SLAM</td><td>[2506.06517](http://arxiv.org/pdf/2506.06517)</td><td>◆ 提出了首个基于可泛化高斯泼溅（GS）的语义SLAM算法，通过学习网络实现跨场景的3D地图构建，摆脱传统方法依赖单场景优化的限制。  
◆ 采用RGB-D图像识别主干网络，直接从降采样和反向投影的图像位置预测高斯参数，实现高效增量式地图更新。  
◆ 创新性地将3D语义分割集成到GS框架中，通过共享主干网络统一3D建图与识别任务，提升语义理解能力。  
◆ 提出仅需1次迭代的全局定位后优化策略，有效解决定位漂移和漂浮物问题，显著提升系统鲁棒性。  
◆ 在ScanNet数据集上实现SOTA性能，所用高斯数量比同类方法少一个数量级，并在NYUv2和TUM RGB-D上展示零样本泛化能力。</td></tr>
<tr><td>2025-06-06</td><td>Enhancing Situational Awareness in Underwater Robotics with Multi-modal Spatial Perception</td><td>[2506.06476](http://arxiv.org/pdf/2506.06476)</td><td>◆ 提出多模态感知融合框架，整合摄像头、IMU和声学设备数据，解决水下视觉SLAM因光线衰减和低对比度导致的失效问题。  
◆ 突破传统单目/双目视觉限制，支持多摄像头配置，提升系统在复杂水下环境中的可扩展性。  
◆ 结合几何方法与学习技术，引入语义分析增强场景理解，实现更鲁棒的状态估计和3D重建。  
◆ 通过真实海域实验验证（特隆赫姆峡湾），首次展示多模态系统在恶劣水下条件下的实时可靠性能。  
◆ 系统分析传感器标定等工程挑战，指出基于学习方法的局限性，为未来大规模水下作业研究指明方向。</td></tr>
<tr><td>2025-06-06</td><td>Dy3DGS-SLAM: Monocular 3D Gaussian Splatti...</td><td>[2506.05965](http://arxiv.org/pdf/2506.05965)</td><td>◆ 提出了首个基于单目RGB输入的动态场景3D高斯泼溅SLAM系统Dy3DGS-SLAM。  
◆ 通过融合光流掩码和深度掩码的概率模型生成动态掩码，仅需单次网络迭代即可优化跟踪尺度和几何渲染。...</td></tr>
<tr><td>2025-06-06</td><td>Analysis of points outcome in ATP Grand Slam Tenni...</td><td>[2506.05866](http://arxiv.org/pdf/2506.05866)</td><td>◆ 该论文创新地利用大数据和机器学习方法（如逻辑回归、随机森林等）预测网球大满贯赛事中每一分的胜负，并结合球员排名、历史数据等因素分析影响得分的关键战略因素。  
◆ 研究基于2016-2020...</td></tr>
<tr><td>2025-06-05</td><td>On-the-fly Reconstruction for Large-Scale Novel Vi...</td><td>[2506.05558](http://arxiv.org/pdf/2506.05558)</td><td>◆提出实时重建方法，在拍摄后立即生成相机位姿和训练好的3D高斯泼溅模型，解决了传统方法耗时过长的问题。  
◆结合快速初始位姿估计和直接采样高斯基元位置/形状的技术，显著加速联合优化过程。  
...</td></tr>
<tr><td>**2025-06-05**</td><td>**Deep Learning Reforms Image Matching: A Survey and Outlook**</td><td>[2506.04619](http://arxiv.org/abs/2506.04619)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-04**</td><td>**cuVSLAM: CUDA accelerated visual odometry**</td><td>[2506.04359](http://arxiv.org/abs/2506.04359)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-04**</td><td>**Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset**</td><td>[2506.04224](http://arxiv.org/abs/2506.04224)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-03**</td><td>**LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM**</td><td>[2506.03073](http://arxiv.org/abs/2506.03073)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-03**</td><td>**Online Performance Assessment of Multi-Source-Localization for Autonomous Driving Systems Using Subjective Logic**</td><td>[2506.02932](http://arxiv.org/abs/2506.02932)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-03**</td><td>**VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians**</td><td>[2506.02741](http://arxiv.org/abs/2506.02741)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-03**</td><td>**GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal**</td><td>[2506.02736](http://arxiv.org/abs/2506.02736)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-03**</td><td>**Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent**</td><td>[2506.02373](http://arxiv.org/abs/2506.02373)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-06-01**</td><td>**Globally Consistent RGB-D SLAM with 2D Gaussian Splatting**</td><td>[2506.00970](http://arxiv.org/abs/2506.00970)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-30**</td><td>**Black-box Adversarial Attacks on CNN-based SLAM Algorithms**</td><td>[2505.24654](http://arxiv.org/abs/2505.24654)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera**</td><td>[2505.22880](http://arxiv.org/abs/2505.22880)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians**</td><td>[2505.22859](http://arxiv.org/abs/2505.22859)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments**</td><td>[2505.22335](http://arxiv.org/abs/2505.22335)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-27**</td><td>**HS-SLAM: A Fast and Hybrid Strategy-Based SLAM Approach for Low-Speed Autonomous Driving**</td><td>[2505.20906](http://arxiv.org/abs/2505.20906)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-27**</td><td>**ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient**</td><td>[2505.20858](http://arxiv.org/abs/2505.20858)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting**</td><td>[2505.19420](http://arxiv.org/abs/2505.19420)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-25**</td><td>**VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes**</td><td>[2505.18992](http://arxiv.org/abs/2505.18992)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-23**</td><td>**CU-Multi: A Dataset for Multi-Robot Data Association**</td><td>[2505.17576](http://arxiv.org/abs/2505.17576)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-22**</td><td>**TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition**</td><td>[2505.16447](http://arxiv.org/abs/2505.16447)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-20**</td><td>**A Methodological Framework for Measuring Spatial Labeling Similarity**</td><td>[2505.14128](http://arxiv.org/abs/2505.14128)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-22**</td><td>**Place Recognition: A Comprehensive Review, Current Challenges and Future Directions**</td><td>[2505.14068](http://arxiv.org/abs/2505.14068)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-19**</td><td>**eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks**</td><td>[2505.13309](http://arxiv.org/abs/2505.13309)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-23**</td><td>**VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold**</td><td>[2505.12549](http://arxiv.org/abs/2505.12549)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-18**</td><td>**Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey**</td><td>[2505.12384](http://arxiv.org/abs/2505.12384)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-18**</td><td>**Structureless VIO**</td><td>[2505.12337](http://arxiv.org/abs/2505.12337)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-16**</td><td>**EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video**</td><td>[2505.11709](http://arxiv.org/abs/2505.11709)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-16**</td><td>**Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization**</td><td>[2505.11620](http://arxiv.org/abs/2505.11620)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-16**</td><td>**Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS**</td><td>[2505.10847](http://arxiv.org/abs/2505.10847)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-15**</td><td>**TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation**</td><td>[2505.10696](http://arxiv.org/abs/2505.10696)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-15**</td><td>**A hybrid SLAM-Payne framework for atmospheric parameter and abundance determination of early-type Stars from LAMOST DR9 low-resolution Spectra**</td><td>[2505.10310](http://arxiv.org/abs/2505.10310)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-15**</td><td>**Large-Scale Gaussian Splatting SLAM**</td><td>[2505.09915](http://arxiv.org/abs/2505.09915)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-13**</td><td>**Automated Meta Prompt Engineering for Alignment with the Theory of Mind**</td><td>[2505.09024](http://arxiv.org/abs/2505.09024)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-13**</td><td>**MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for Enhanced Indoor Localization Using LiDAR-SLAM**</td><td>[2505.08388](http://arxiv.org/abs/2505.08388)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-13**</td><td>**SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments**</td><td>[2505.08230](http://arxiv.org/abs/2505.08230)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-12**</td><td>**RDD: Robust Feature Detector and Descriptor using Deformable Transformer**</td><td>[2505.08013](http://arxiv.org/abs/2505.08013)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-12**</td><td>**Ranking-aware Continual Learning for LiDAR Place Recognition**</td><td>[2505.07198](http://arxiv.org/abs/2505.07198)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-07**</td><td>**Scalable Aerial GNSS Localization for Marine Robots**</td><td>[2505.04095](http://arxiv.org/abs/2505.04095)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-06**</td><td>**Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and Low-Visibility Conditions**</td><td>[2505.03565](http://arxiv.org/abs/2505.03565)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-06**</td><td>**AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames**</td><td>[2505.03448](http://arxiv.org/abs/2505.03448)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-06**</td><td>**LiftFeat: 3D Geometry-Aware Local Feature Matching**</td><td>[2505.03422](http://arxiv.org/abs/2505.03422)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-05**</td><td>**LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven Control System for Skid-Steer Robots**</td><td>[2505.02598](http://arxiv.org/abs/2505.02598)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-04**</td><td>**Robust Localization, Mapping, and Navigation for Quadruped Robots**</td><td>[2505.02272](http://arxiv.org/abs/2505.02272)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-04**</td><td>**SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment**</td><td>[2505.01956](http://arxiv.org/abs/2505.01956)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-03**</td><td>**GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels**</td><td>[2505.01934](http://arxiv.org/abs/2505.01934)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-02**</td><td>**Tightly Coupled Range Inertial Odometry and Mapping with Exact Point Cloud Downsampling**</td><td>[2505.01017](http://arxiv.org/abs/2505.01017)</td><td>摘要生成中...</td></tr>
<tr><td>2025-08-11</td><td>MBA-SLAM: Motion Blur Aware Gaussian Splatting SLAM</td><td>[2411.08279](http://arxiv.org/pdf/2411.08279)</td><td>◆ Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input.
◆ However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions.
◆ This often results in a significant reduction in both camera localization accuracy and map reconstruction quality.</td></tr>
<tr><td>2025-11-07</td><td>MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and Mapping</td><td>[2410.10669](http://arxiv.org/pdf/2410.10669)</td><td>◆ The Visual Simultaneous Localization and Mapping (V-SLAM) system has seen significant development in recent years, demonstrating high precision in environments with limited dynamic objects.
◆ However, their performance significantly deteriorates when deployed in settings with a higher presence of movable objects, such as environments with pedestrians, cars, and buses, which are common in outdoor scenes.
◆ To address this issue, we propose a Multilayer Perceptron (MLP)-based real-time stereo SLAM system that leverages complete geometry information to avoid information loss.</td></tr>
</tbody>
</table>
</div>

<div align='right'><a href='#top'>↑ 返回顶部</a></div>

<h2 id='sfm'>SFM</h2>

<div class="table-container">
<table>
<thead><tr><th>日期</th><th>标题</th><th>论文与代码</th><th>摘要</th></tr></thead>
<tbody>
<tr><td>2026-02-08</td><td>Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video</td><td>[2602.07891](http://arxiv.org/pdf/2602.07891)</td><td>该论文的核心贡献是提出了SAGE框架，旨在利用互联网视频这一海量但无标注数据，来规模化地适应和增强3D几何基础模型，以解决3D标注数据稀缺的根本瓶颈。

其核心创新点包括：
◆ 提出了一个可扩展的适应框架SAGE，首次实现了利用原始互联网视频流来适应3D几何基础模型，为通用3D学习建立了一个新的规模化范式。
◆ 设计了一个分层挖掘流程，将原始视频转化为有效的训练轨迹，并融合了混合监督信号，以应对视频中无真实几何标注和存在观测噪声的挑战。
◆ 引入了稀疏几何锚定技术，利用从视频中恢复的运动结构点云提供全局结构指导，作为弱监督信号。
◆ 结合了密集可微一致性约束，通过3D高斯渲染技术施加多视图一致性约束，进一步利用视频中的自监督信号。
◆ 提出了一种基于锚定数据的正则化策略，以防止模型在适应新视频数据时发生灾难性遗忘，稳定训练过程。
实验表明，该方法能显著提升模型的零样本泛化能力，在多个未见过的基准数据集上大幅优于现有先进方法。</td></tr>
<tr><td>2026-01-31</td><td>Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency</td><td>[2602.07016](http://arxiv.org/pdf/2602.07016)</td><td>本文的核心贡献在于探索并实证了高斯约束表征在无监督三维场景重建中的实用价值，特别是在多场景发现与相机姿态估计任务中。其创新点可总结如下：

◆ 首次将受LeJEPA启发的各向同性高斯约束应用于无监督图像嵌入学习，以提升场景表征的区分度。
◆ 设计了三种逐步优化的处理流程，最终通过高斯约束明确规范嵌入空间分布，增强聚类一致性。
◆ 该方法不追求理论证明，而是聚焦于实证评估，验证约束对实际场景分离与姿态估计鲁棒性的影响。
◆ 在IMC2025挑战赛的复杂真实图像集上验证有效性，证明其能改善视觉模糊情况下的场景划分与姿态合理性。
◆ 为自监督学习原则与实际运动恢复结构流程的融合提供了新方向，展示了理论驱动表征约束的实用潜力。</td></tr>
<tr><td>2026-02-05</td><td>NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects</td><td>[2602.05822](http://arxiv.org/pdf/2602.05822)</td><td>◆ We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs.
◆ Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection.
◆ The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation.</td></tr>
<tr><td>2026-02-05</td><td>Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation</td><td>[2602.05582](http://arxiv.org/pdf/2602.05582)</td><td>◆ We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3).
◆ Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems.
◆ To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).</td></tr>
<tr><td>2026-02-04</td><td>AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation</td><td>[2602.04672](http://arxiv.org/pdf/2602.04672)</td><td>◆ Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR.
◆ However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage.
◆ To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning.</td></tr>
<tr><td>2026-02-04</td><td>SuperPoint-E: local features for 3D reconstruction via tracking adaptation in endoscopy</td><td>[2602.04108](http://arxiv.org/pdf/2602.04108)</td><td>◆ In this work, we focus on boosting the feature extraction to improve the performance of Structure-from-Motion (SfM) in endoscopy videos.
◆ We present SuperPoint-E, a new local feature extraction method that, using our proposed Tracking Adaptation supervision strategy, significantly improves the quality of feature detection and description in endoscopy.
◆ Extensive experimentation on real endoscopy recordings studies our approach&#x27;s most suitable configuration and evaluates SuperPoint-E feature quality.</td></tr>
<tr><td>2026-02-03</td><td>Pi-GS: Sparse-View Gaussian Splatting with Dense π^3 Initialization</td><td>[2602.03327](http://arxiv.org/pdf/2602.03327)</td><td>◆ Novel view synthesis has evolved rapidly, advancing from Neural Radiance Fields to 3D Gaussian Splatting (3DGS), which offers real-time rendering and rapid training without compromising visual fidelity.
◆ However, 3DGS relies heavily on accurate camera poses and high-quality point cloud initialization, which are difficult to obtain in sparse-view scenarios.
◆ While traditional Structure from Motion (SfM) pipelines often fail in these settings, existing learning-based point estimation alternatives typically require reliable reference views and remain sensitive to pose or depth errors.</td></tr>
<tr><td>2026-02-03</td><td>Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements</td><td>[2602.03209](http://arxiv.org/pdf/2602.03209)</td><td>◆ Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations.
◆ Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets.
◆ To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments.</td></tr>
<tr><td>2026-01-29</td><td>From Implicit Ambiguity to Explicit Solidity: Diagnosing Interior Geometric Degradation in Neural Radiance Fields for Dense 3D Scene Understanding</td><td>[2601.21421](http://arxiv.org/pdf/2601.21421)</td><td>◆ Neural Radiance Fields (NeRFs) have emerged as a powerful paradigm for multi-view reconstruction, complementing classical photogrammetric pipelines based on Structure-from-Motion (SfM) and Multi-View Stereo (MVS).
◆ However, their reliability for quantitative 3D analysis in dense, self-occluding scenes remains poorly understood.
◆ In this study, we identify a fundamental failure mode of implicit density fields under heavy occlusion, which we term Interior Geometric Degradation (IGD).</td></tr>
<tr><td>2026-01-21</td><td>Minimizing Submodular Functions over Hierarchical Families</td><td>[2601.14805](http://arxiv.org/pdf/2601.14805)</td><td>◆ This paper considers submodular function minimization (SFM) restricted to a family of subsets.
◆ We show that SFM over complements of families with certain hierarchical structures can be solved in polynomial-time.
◆ This yields a polynomial-time algorithm for SFM over complements of various families, such as intersecting families, crossing families, and the unions of lattices.</td></tr>
<tr><td>2026-01-20</td><td>Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting</td><td>[2601.14208](http://arxiv.org/pdf/2601.14208)</td><td>◆ Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it.
◆ Additionally, online buyers rarely see undercarriage photos.
◆ We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage.</td></tr>
<tr><td>2026-01-25</td><td>TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement</td><td>[2601.12823](http://arxiv.org/pdf/2601.12823)</td><td>◆ Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes.
◆ Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery.
◆ Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging.</td></tr>
<tr><td>2026-01-18</td><td>OpenNavMap: Structure-Free Topometric Mapping via Large-Scale Collaborative Localization</td><td>[2601.12291](http://arxiv.org/pdf/2601.12291)</td><td>◆ Scalable and maintainable map representations are fundamental to enabling large-scale visual navigation and facilitating the deployment of robots in real-world environments.
◆ While collaborative localization across multi-session mapping enhances efficiency, traditional structure-based methods struggle with high maintenance costs and fail in feature-less environments or under significant viewpoint changes typical of crowd-sourced data.
◆ To address this, we propose OPENNAVMAP, a lightweight, structure-free topometric system leveraging 3D geometric foundation models for on-demand reconstruction.</td></tr>
<tr><td>2026-01-17</td><td>SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM</td><td>[2601.11930](http://arxiv.org/pdf/2601.11930)</td><td>◆ Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM).
◆ However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs.
◆ non-overlapping pairs) fail to capture.</td></tr>
<tr><td>2026-01-12</td><td>CompNO: A Novel Foundation Model approach for solving Partial Differential Equations</td><td>[2601.07384](http://arxiv.org/pdf/2601.07384)</td><td>◆ Partial differential equations (PDEs) govern a wide range of physical phenomena, but their numerical solution remains computationally demanding, especially when repeated simulations are required across many parameter settings.
◆ Recent Scientific Foundation Models (SFMs) aim to alleviate this cost by learning universal surrogates from large collections of simulated systems, yet they typically rely on monolithic architectures with limited interpretability and high pretraining expense.
◆ In this work we introduce Compositional Neural Operators (CompNO), a compositional neural operator framework for parametric PDEs.</td></tr>
<tr><td>2026-01-11</td><td>Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech</td><td>[2601.07064](http://arxiv.org/pdf/2601.07064)</td><td>◆ We propose a unified framework for not only attributing synthetic speech to its source but also for detecting speech generated by synthesizers that were not encountered during training.
◆ This requires methods that move beyond simple detection to support both detailed forensic analysis and open-set generalization.
◆ To address this, we introduce SIGNAL, a hybrid framework that combines speech foundation models (SFMs) with graph-based modeling and open-set-aware inference.</td></tr>
<tr><td>2026-01-11</td><td>SARA: Scene-Aware Reconstruction Accelerator</td><td>[2601.06831](http://arxiv.org/pdf/2601.06831)</td><td>◆ We present SARA (Scene-Aware Reconstruction Accelerator), a geometry-driven pair selection module for Structure-from-Motion (SfM).
◆ Unlike conventional pipelines that select pairs based on visual similarity alone, SARA introduces geometry-first pair selection by scoring reconstruction informativeness - the product of overlap and parallax - before expensive matching.
◆ A lightweight pre-matching stage uses mutual nearest neighbors and RANSAC to estimate these cues, then constructs an Information-Weighted Spanning Tree (IWST) augmented with targeted edges for loop closure, long-baseline anchors, and weak-view reinforcement.</td></tr>
<tr><td>2026-01-09</td><td>InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection</td><td>[2601.05805](http://arxiv.org/pdf/2601.05805)</td><td>◆ This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS).
◆ Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity.
◆ We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation.</td></tr>
<tr><td>2025-12-31</td><td>3D Semantic Segmentation for Post-Disaster Assessment</td><td>[2512.24593](http://arxiv.org/pdf/2512.24593)</td><td>◆ The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses.
◆ While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments.
◆ To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds.</td></tr>
<tr><td>2025-12-11</td><td>Enhanced Information Security via Wave-Field Selectivity and Structured Wavefront Manipulation</td><td>[2512.19702](http://arxiv.org/pdf/2512.19702)</td><td>◆ In this paper, we propose a novel secure wireless transmission architecture that enables the co-existence of spatial field modulation (SFM) and digital bandpass modulation (DBM), utilizing multi-mode vortex waves and programmable meta-surfaces (PMS).
◆ Distinct from conventional joint modulation schemes, our approach establishes two logically independent transmission channels--SFM and DBM--thereby eliminating the need for joint signal design or time synchronization.
◆ Specifically, the orthogonality of vortex wave modes is exploited to construct a high-capacity multi-mode DBM channel, in which each mode carries modulated symbols independently.</td></tr>
<tr><td>2025-12-18</td><td>An evacuation simulator for pedestrian dynamics based on the Social Force Model</td><td>[2512.16887](http://arxiv.org/pdf/2512.16887)</td><td>◆ The evacuation of pedestrians from enclosed spaces represents a key problem in safety engineering and infrastructure design.
◆ Analyzing the collective dynamics that emerge during evacuation processes requires simulation tools capable of capturing individual interactions and spatial constraints realistically.
◆ In this work, we present \textit{SiCoBioNa}, an open-source evacuation simulator based on the Social Force Model (SFM).</td></tr>
<tr><td>2025-12-24</td><td>Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</td><td>[2512.16378](http://arxiv.org/pdf/2512.16378)</td><td>◆ As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines.
◆ Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question.
◆ We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs.</td></tr>
<tr><td>2025-12-17</td><td>Robust Multi-view Camera Calibration from Dense Matches</td><td>[2512.15608](http://arxiv.org/pdf/2512.15608)</td><td>◆ Estimating camera intrinsics and extrinsics is a fundamental problem in computer vision, and while advances in structure-from-motion (SfM) have improved accuracy and robustness, open challenges remain.
◆ In this paper, we introduce a robust method for pose estimation and calibration.
◆ We consider a set of rigid cameras, each observing the scene from a different perspective, which is a typical camera setup in animal behavior studies and forensic analysis of surveillance footage.</td></tr>
<tr><td>2025-12-11</td><td>3D Blood Pulsation Maps</td><td>[2512.10517](http://arxiv.org/pdf/2512.10517)</td><td>◆ We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps.
◆ These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging.
◆ Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis.</td></tr>
<tr><td>2025-12-21</td><td>Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment</td><td>[2512.08930](http://arxiv.org/pdf/2512.08930)</td><td>◆ Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand.
◆ Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images.
◆ While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks.</td></tr>
<tr><td>2025-12-08</td><td>Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization</td><td>[2512.07969](http://arxiv.org/pdf/2512.07969)</td><td>◆ Robotic perception often requires solving large nonlinear least-squares (NLS) problems.
◆ While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution.
◆ Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties.</td></tr>
<tr><td>2025-12-05</td><td>The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos</td><td>[2512.05398](http://arxiv.org/pdf/2512.05398)</td><td>◆ Estimating accurate camera poses, 3D scene geometry, and object motion from in-the-wild videos is a long-standing challenge for classical structure from motion pipelines due to the presence of dynamic objects.
◆ Recent learning-based methods attempt to overcome this challenge by training motion estimators to filter dynamic objects and focus on the static background.
◆ However, their performance is largely limited by the availability of large-scale motion segmentation datasets, resulting in inaccurate segmentation and, therefore, inferior structural 3D understanding.</td></tr>
<tr><td>2025-12-05</td><td>PoolNet: Deep Learning for 2D to 3D Video Process Validation</td><td>[2512.05362](http://arxiv.org/pdf/2512.05362)</td><td>◆ Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task.
◆ In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data.
◆ To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data.</td></tr>
<tr><td>2025-12-04</td><td>Deep infant brain segmentation from multi-contrast MRI</td><td>[2512.05114](http://arxiv.org/pdf/2512.05114)</td><td>◆ Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures.
◆ However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints.
◆ Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts.</td></tr>
<tr><td>2025-12-04</td><td>QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory</td><td>[2512.05049](http://arxiv.org/pdf/2512.05049)</td><td>◆ Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate.
◆ However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity.
◆ In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs.</td></tr>
<tr><td>2025-12-04</td><td>Geometric Data Science</td><td>[2512.05040](http://arxiv.org/pdf/2512.05040)</td><td>◆ This book introduces the new research area of Geometric Data Science, where data can represent any real objects through geometric measurements.
◆ The first part of the book focuses on finite point sets.
◆ The most important result is a complete and continuous classification of all finite clouds of unordered points under rigid motion in any Euclidean space.</td></tr>
<tr><td>2025-12-04</td><td>Internal superfluid response and torque evolution in the giant glitch of PSR J1718-3718</td><td>[2512.04972](http://arxiv.org/pdf/2512.04972)</td><td>◆ We investigate the post-glitch rotational evolution of pulsars by analyzing the 2007 giant glitch of PSR J1718$-$3718 using a vortex creep model that incorporates both inward and outward nonlinear vortex motion, along with a time-varying external torque.
◆ A comprehensive fitting framework is developed, constrained by prior knowledge of moment of inertia participation from previous glitch studies.
◆ We apply a Markov Chain Monte Carlo approach to quantify uncertainties and parameter correlations.</td></tr>
<tr><td>2025-12-04</td><td>Canonical Rough Path over Tempered Fractional Brownian Motion: Existence, Construction, and Applications</td><td>[2512.04646](http://arxiv.org/pdf/2512.04646)</td><td>◆ We construct a canonical geometric rough path over $d$-dimensional tempered fractional Brownian motion (tfBm) for any Hurst parameter $H &gt; 1/4$ and tempering parameter $λ&gt; 0$.
◆ The main challenge stems from the non-homogeneous nature of the tfBm covariance, which exhibits a power-law structure at small scales and exponential decay at large scales.
◆ Our primary contribution is a detailed analysis of this covariance, proving it has finite 2D $ρ$-variation for $ρ= 1/(2H)$.</td></tr>
<tr><td>2025-12-04</td><td>Refaçade: Editing Object with Given Reference Texture</td><td>[2512.04534](http://arxiv.org/pdf/2512.04534)</td><td>◆ Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored.
◆ In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos.
◆ To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture.</td></tr>
<tr><td>2025-12-04</td><td>Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation</td><td>[2512.04399](http://arxiv.org/pdf/2512.04399)</td><td>◆ In robotic hand research, minimizing the number of actuators while maintaining human-hand-consistent dimensions and degrees of freedom constitutes a fundamental challenge.
◆ Drawing bio-inspiration from human hand kinematic configurations and muscle distribution strategies, this work proposes a novel 15-DoF dexterous robotic hand, with detailed analysis of its mechanical architecture, electrical system, and control system.
◆ The bionic hand employs a new tendon-driven mechanism, significantly reducing the number of motors required by traditional tendon-driven systems while enhancing motion performance and simplifying the mechanical structure.</td></tr>
<tr><td>2025-12-03</td><td>Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications</td><td>[2512.04303](http://arxiv.org/pdf/2512.04303)</td><td>◆ Accurate perception of the vehicle&#x27;s 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control.
◆ However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability.
◆ To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure.</td></tr>
<tr><td>2025-12-03</td><td>Emergent Outlier View Rejection in Visual Geometry Grounded Transformers</td><td>[2512.04012](http://arxiv.org/pdf/2512.04012)</td><td>◆ Reliable 3D reconstruction from in-the-wild image collections is often hindered by &quot;noisy&quot; images-irrelevant inputs with little or no view overlap with others.
◆ While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions.
◆ In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images.</td></tr>
<tr><td>2025-12-03</td><td>DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment</td><td>[2512.03981](http://arxiv.org/pdf/2512.03981)</td><td>◆ Drag-based image editing using generative models provides intuitive control over image structures.
◆ However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision.
◆ Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts.</td></tr>
<tr><td>2025-12-03</td><td>Vortex Dynamics from Burst-and-Coast Motion of Anguilliform and Carangiform Swimmers</td><td>[2512.03969](http://arxiv.org/pdf/2512.03969)</td><td>◆ Fish perform various propulsive maneuvers while swimming by generating traveling waves along their bodies and producing thrust through tail strokes.
◆ Anguilliform swimmers spread motion along the body, while carangiform swimmers&#x27; motion is more prominent near their tails.
◆ Many species also switch between continuous undulation and intermittent swimming, such as burst-and-coast maneuver, which can save energy but can also change the wake structure and hydrodynamic forces.</td></tr>
<tr><td>2025-12-03</td><td>Geometrical structure of the Wigner flow information quantifiers and hyperbolic stability in the phase-space framework</td><td>[2512.03717](http://arxiv.org/pdf/2512.03717)</td><td>◆ Quantifiers of stationarity, classicality, purity and vorticity are derived from phase-space differential geometrical structures within the Weyl-Wigner framework, after which they are related to the hyperbolic stability of classical and quantum-modified Hamiltonian (non-linear) equations of motion.
◆ By examining the equilibrium regime produced by such an autonomous system of ordinary differential equations, a correspondence between Wigner flow properties and hyperbolic stability boundaries in the phase-space is identified.
◆ Explicit analytical expressions for equilibrium-stability parameters are obtained for quantum Gaussian ensembles, wherein information quantifiers driven by Wigner currents are identified.</td></tr>
<tr><td>2025-12-03</td><td>Terahertz light driven coherent excitation of a zone-folded Raman-active phonon mode in the Spin-Ladder System $α&#x27;$-NaV$_2$O$_5$</td><td>[2512.03691](http://arxiv.org/pdf/2512.03691)</td><td>◆ We investigate the out-of-equilibrium lattice dynamics in the spin-ladder system $α&#x27;$-NaV$_2$O$_5$ using intense terahertz (THz) pump and near-infrared (NIR) probe spectroscopy.
◆ When quasi-single-cycle THz pulses interact with $α&#x27;$-NaV$_2$O$_5$ in its low-temperature, dimerized charge-ordered phase, they induce coherent oscillations in the time domain at the zone-folded Raman-active phonon frequency of 1.85 THz.
◆ By combining pump-probe measurements with lattice dynamics modeling based on equation-of-motion approach, we propose that these oscillations arise from a nonlinear coupling between Raman-active and infrared (IR)-active phonon modes, with the latter being resonantly excited by the THz pulses.</td></tr>
<tr><td>2025-12-03</td><td>LAMP: Language-Assisted Motion Planning for Controllable Video Generation</td><td>[2512.03619](http://arxiv.org/pdf/2512.03619)</td><td>◆ Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion.
◆ Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited.
◆ We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras.</td></tr>
<tr><td>2025-12-02</td><td>Comparing Unsupervised and Supervised Semantic Speech Tokens: A Case Study of Child ASR</td><td>[2512.03301](http://arxiv.org/pdf/2512.03301)</td><td>◆ Discrete speech tokens have gained attention for their storage efficiency and integration with Large Language Models (LLMs).
◆ They are commonly categorized into acoustic and semantic tokens, with the latter being more advantageous for Automatic Speech Recognition (ASR).
◆ Traditionally, unsupervised K-means clustering has been used to extract semantic speech tokens from Speech Foundation Models (SFMs).</td></tr>
<tr><td>2025-12-02</td><td>The Origins of the Bulk flow</td><td>[2512.03168](http://arxiv.org/pdf/2512.03168)</td><td>◆ We analyze the origin of the large scale bulk flow using the CosmicFlows 4 (CF4) peculiar velocity catalog.
◆ We decompose the observed motions into internal components, generated by mass fluctuations within 200Mpc/h, and external ones arising from structures beyond this volume.
◆ A weighted average technique is developed to test the model&#x27;s self consistency while minimizing the impact of non Gaussian distance errors.</td></tr>
<tr><td>2025-12-03</td><td>DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</td><td>[2512.03000](http://arxiv.org/pdf/2512.03000)</td><td>◆ Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities.
◆ However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet.
◆ To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video.</td></tr>
<tr><td>2025-12-02</td><td>Asymptotics for additive functionals of particle systems via Stein&#x27;s method</td><td>[2512.02922](http://arxiv.org/pdf/2512.02922)</td><td>◆ We consider additive functionals of systems of random measures whose initial configuration is given by a Poisson point process, and whose individual components evolve according to arbitrary Markovian or non-Markovian measure valued dynamics, with no structural assumptions beyond basic moment bounds.
◆ In this setting and under adequate conditions, we establish a general third moment theorem for the normalized functionals.
◆ Building on this result, we obtain the first quantitative bounds in the Wasserstein distance for a variety of moving-measure models initialized by Poisson-driven clouds of points, turning qualitative central limit theorems into explicit rates of convergence.</td></tr>
<tr><td>2025-12-02</td><td>Symmetry transformation group arising from the Laplace-Runge-Lenz vector</td><td>[2512.02903](http://arxiv.org/pdf/2512.02903)</td><td>◆ The Kepler problem in classical mechanics exhibits a rich structure of conserved quantities, highlighted by the Laplace--Runge--Lenz (LRL) vector.
◆ Through Noether&#x27;s theorem in reverse, the LRL vector gives rise to a corresponding infinitesimal dynamical symmetry on the kinematical variables, which is well known in the literature.
◆ However, the physically relevant part of the LRL vector is its direction angle in the plane of motion (since its magnitude is just a function of energy and angular momentum).</td></tr>
<tr><td>2025-12-02</td><td>ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation</td><td>[2512.02453](http://arxiv.org/pdf/2512.02453)</td><td>◆ Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion.
◆ However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge.
◆ In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation.</td></tr>
<tr><td>2025-12-02</td><td>On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning</td><td>[2512.02375](http://arxiv.org/pdf/2512.02375)</td><td>◆ Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance.
◆ However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area.
◆ This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time.</td></tr>
<tr><td>2025-12-01</td><td>Sampling on Metric Graphs</td><td>[2512.02175](http://arxiv.org/pdf/2512.02175)</td><td>◆ Metric graphs are structures obtained by associating edges in a standard graph with segments of the real line and gluing these segments at the vertices of the graph.
◆ The resulting structure has a natural metric that allows for the study of differential operators and stochastic processes on the graph.
◆ Brownian motions in these domains have been extensively studied theoretically using their generators.</td></tr>
<tr><td>2025-12-01</td><td>The Astrometric Resoeccentric Degeneracy: Eccentric Single Planets Mimic 2:1 Resonant Planet Pairs in Astrometry</td><td>[2512.02007](http://arxiv.org/pdf/2512.02007)</td><td>◆ Detections of long-period giant exoplanets will expand dramatically with Gaia Data Release 4 (DR4), but interpreting these signals will require care.
◆ We derive the astrometric resoeccentric degeneracy: an astrometric analogue of the well-known radial velocity degeneracy in which a single eccentric planet can mimic two circular planets near a 2:1 period ratio.
◆ To first order in eccentricity, the sky-projected motion of a single eccentric orbit decomposes into a fundamental mode and first harmonic with an amplitude proportional to that eccentricity.</td></tr>
<tr><td>2025-12-01</td><td>Active chromospheric fibril singularity: Coordinated observations from Solar Orbiter, SST, and IRIS</td><td>[2512.01886](http://arxiv.org/pdf/2512.01886)</td><td>◆ The fine structures of the solar chromosphere, driven by photospheric motions, play a crucial role in the dynamics of solar magnetic fields.
◆ Many have been already identified such as fibrils, filament feet, and arch filament systems.
◆ Still, high resolution observations show a wealth of structures that remain elusive.</td></tr>
<tr><td>2025-12-02</td><td>SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge</td><td>[2512.01629](http://arxiv.org/pdf/2512.01629)</td><td>◆ Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures.
◆ We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image.
◆ Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images.</td></tr>
<tr><td>2025-12-01</td><td>X-CME: From In Situ Flux-Rope Reconstruction to CME Propagation Forecasting</td><td>[2512.01561](http://arxiv.org/pdf/2512.01561)</td><td>◆ Accurate forecasts of Coronal Mass Ejection (CME) arrival times and impact geometry remain a major challenge for space-weather operations.
◆ Coronagraph-based techniques typically achieve mean absolute errors of order ten hours, while in situ measurements at L1 provide excellent magnetic-field information but only tens of minutes of warning.
◆ In this work we introduce X-CME, a framework that links in situ flux-rope reconstructions at intermediate heliocentric distances with a physics-based CME propagation model.</td></tr>
<tr><td>2025-12-01</td><td>InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</td><td>[2512.01342](http://arxiv.org/pdf/2512.01342)</td><td>◆ Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues.
◆ In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks.
◆ We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning.</td></tr>
<tr><td>2025-11-30</td><td>Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception</td><td>[2512.01108](http://arxiv.org/pdf/2512.01108)</td><td>◆ Intercepting fast moving objects, by its very nature, is challenging because of its tight time constraints.
◆ This problem becomes further complicated in the presence of sensor noise because noisy sensors provide, at best, incomplete information, which results in a distribution over target states to be intercepted.
◆ Since time is of the essence, to hit the target, the planner must begin directing the interceptor, in this case a robot arm, while still receiving information.</td></tr>
<tr><td>2025-11-30</td><td>CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions</td><td>[2512.01095](http://arxiv.org/pdf/2512.01095)</td><td>◆ We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions.
◆ CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes.
◆ CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition.</td></tr>
<tr><td>2025-11-30</td><td>Euclid Structural-Thermal-Optical Performance</td><td>[2512.01075](http://arxiv.org/pdf/2512.01075)</td><td>◆ The Euclid system performance is defined in terms of image quality metrics tuned to the weak gravitational lensing (WL) cosmological probe.
◆ WL induces stringent requirements on the shape and stability of the VIS instrument system point spread function (PSF).
◆ The PSF is affected by error contributions from the telescope, the focal plane and image motion, and is controlled by a global error budget with error allocations to each contributor.</td></tr>
<tr><td>2025-11-30</td><td>Evolution of Flare Ribbon Bead-like Structures in a Solar Flare</td><td>[2512.00710](http://arxiv.org/pdf/2512.00710)</td><td>◆ We present fast cadence and high resolution observations of flare ribbons from the Solar Orbiter Extreme Ultraviolet Imager (EUI).
◆ Utilizing the short-exposure observations from the EUI High Resolution Imager in EUV (HRIEUV), we find small-scale blob/bead-like kernel structures propagating within a hook at the end of a flare ribbon, during the impulsive phase of a C9.9-class solar flare.
◆ These bead structures are dynamic, with well-resolved spatial separations as low as ~420-840 kilometers (3-6 pixels) - below the observable limit of full-disk solar imagers.</td></tr>
<tr><td>2025-11-29</td><td>Strings at the Tip of the Cone and Black Hole Entropy From the Worldsheet: Part I</td><td>[2512.00637](http://arxiv.org/pdf/2512.00637)</td><td>◆ We study the nonlinear sigma model (NLSM) worldsheet action describing the motion of closed bosonic strings in the target space of a two-dimensional (2D) flat cone in polar coordinates.
◆ We calculate the cylinder partition function.
◆ We first place the cylindrical worldsheet on a rectangular lattice before taking the continuum limit.</td></tr>
<tr><td>2025-11-28</td><td>DisMo: Disentangled Motion Representations for Open-World Motion Transfer</td><td>[2511.23428](http://arxiv.org/pdf/2511.23428)</td><td>◆ Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames.
◆ However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators.
◆ To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective.</td></tr>
<tr><td>2025-11-28</td><td>From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products</td><td>[2511.23407](http://arxiv.org/pdf/2511.23407)</td><td>◆ To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal.
◆ Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs.
◆ We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product&#x27;s internal state.</td></tr>
<tr><td>2025-11-28</td><td>Simulating AGN feedback in galaxy clusters with pre-existing turbulence</td><td>[2511.23267](http://arxiv.org/pdf/2511.23267)</td><td>◆ Feedback from active galactic nuclei (AGN) is believed to play a significant role in suppressing cooling flows in cool-core (CC) clusters.
◆ Turbulence in the intracluster medium (ICM), which may be induced by AGN activity or pre-existing motions, has been proposed as a potential heating mechanism based on analysis of Chandra X-ray surface brightness fluctuations.
◆ However, subsequent simulation results have found the subdominant role of turbulence in heating the ICM.</td></tr>
<tr><td>2025-11-28</td><td>Nonequilibrium dynamics of magnetic hopfions driven by spin-orbit torque</td><td>[2511.23045](http://arxiv.org/pdf/2511.23045)</td><td>◆ Hopfions--three-dimensional topological solitons with knotted spin texture--have recently garnered attention in topological magnetism due to their unique topology characterized by the Hopf number $H$, a topological invariant derived from knot theory.
◆ In contrast to two-dimensional skyrmions, which are typically limited to small topological invariants, i.e., skyrmion numbers, hopfions can, in principle, be stabilized with arbitrary Hopf numbers.
◆ However, the nonequilibrium dynamics, especially interconversion between different Hopf numbers, remain poorly understood.</td></tr>
<tr><td>2025-11-28</td><td>GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light &amp; Camera</td><td>[2511.22857](http://arxiv.org/pdf/2511.22857)</td><td>◆ Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects.
◆ While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion.
◆ However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle.</td></tr>
<tr><td>2025-11-28</td><td>Captain Safari: A World Engine</td><td>[2511.22815](http://arxiv.org/pdf/2511.22815)</td><td>◆ World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion.
◆ However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion.
◆ To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory.</td></tr>
<tr><td>2025-11-27</td><td>An Efficient and Accurate Surrogate Modeling of Flapping Dynamics in Inverted Elastic Foils using Hypergraph Neural Networks</td><td>[2511.22012](http://arxiv.org/pdf/2511.22012)</td><td>◆ Cantilevered elastic foils can undergo self-induced, large-amplitude flapping when subject to fluid flow, a widely observed phenomenon of fluid-structure interaction, from fluttering leaves or the movement of fish fins.
◆ When harnessed in steady currents, these oscillations enable the extraction of kinetic energy from the flow.
◆ However, accurately predicting these dynamics requires high-fidelity simulations that are prohibitively expensive to perform across the broad configuration space needed for design optimization.</td></tr>
<tr><td>2025-11-26</td><td>UniArt: Unified 3D Representation for Generating 3D Articulated Objects with Open-Set Articulation</td><td>[2511.21887](http://arxiv.org/pdf/2511.21887)</td><td>◆ Articulated 3D objects play a vital role in realistic simulation and embodied robotics, yet manually constructing such assets remains costly and difficult to scale.
◆ In this paper, we present UniArt, a diffusion-based framework that directly synthesizes fully articulated 3D objects from a single image in an end-to-end manner.
◆ Unlike prior multi-stage techniques, UniArt establishes a unified latent representation that jointly encodes geometry, texture, part segmentation, and kinematic parameters.</td></tr>
<tr><td>2025-11-26</td><td>TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</td><td>[2511.21690](http://arxiv.org/pdf/2511.21690)</td><td>◆ Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging.
◆ While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use.
◆ We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D &quot;trace-space&quot; of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos.</td></tr>
<tr><td>2025-11-26</td><td>UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes</td><td>[2511.21565](http://arxiv.org/pdf/2511.21565)</td><td>◆ Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction.
◆ Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting.
◆ This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable.</td></tr>
<tr><td>2025-11-26</td><td>From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings</td><td>[2511.21428](http://arxiv.org/pdf/2511.21428)</td><td>◆ We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training.
◆ Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel &quot;Latent Action Energy&quot; metric to discover and segment semantically coherent action primitives.
◆ The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training.</td></tr>
<tr><td>2025-11-26</td><td>DeepRFTv2: Kernel-level Learning for Image Deblurring</td><td>[2511.21132](http://arxiv.org/pdf/2511.21132)</td><td>◆ It is well-known that if a network aims to learn how to deblur, it should understand the blur process.
◆ Blurring is naturally caused by the convolution of the sharp image with the blur kernel.
◆ Thus, allowing the network to learn the blur process in the kernel-level can significantly improve the image deblurring performance.</td></tr>
<tr><td>2025-11-25</td><td>Hund-projected Kanamori model: an effective description of Hund&#x27;s metals near the Mott insulating regime</td><td>[2511.20788](http://arxiv.org/pdf/2511.20788)</td><td>◆ Hund&#x27;s coupling plays a decisive role in shaping electron correlations of multi-orbital systems, giving rise to a class of materials--Hund&#x27;s metals--that combine local-moment physics with metallic transport.
◆ Here we derive an effective low-energy description of such a system near the Mott insulating regime, starting from the multi-orbital Hubbard-Kanamori Hamiltonian and projecting onto the high-spin manifold favored by Hund&#x27;s first rule.
◆ The resulting Hund-projected Kanamori model captures the interplay between carrier motion and magnetic correlations in the presence of strong Hund&#x27;s coupling.</td></tr>
<tr><td>2025-11-25</td><td>From Observations to Simulations: A Neural-Network Approach to Intracluster Medium Kinematics</td><td>[2511.20755](http://arxiv.org/pdf/2511.20755)</td><td>◆ We present a systematic comparison between {\it XMM-Newton} velocity maps of the Virgo, Centaurus, Ophiuchus and A3266 clusters and synthetic velocity maps generated from the Illustris TNG-300 simulations.
◆ Our goal is to constrain the physical conditions and dynamical states of the intracluster medium (ICM) through a data-driven approach.
◆ We employ a Siamese Convolutional Neural Network (CNN) designed to identify the most analogous simulated cluster to each observed system based on the morphology of their line-of-sight velocity maps.</td></tr>
<tr><td>2025-11-25</td><td>Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization</td><td>[2511.20647](http://arxiv.org/pdf/2511.20647)</td><td>◆ While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt.
◆ We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt.
◆ To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations.</td></tr>
<tr><td>2025-11-25</td><td>Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features</td><td>[2511.20469](http://arxiv.org/pdf/2511.20469)</td><td>◆ Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories.
◆ Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns.
◆ This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos.</td></tr>
<tr><td>2025-11-25</td><td>AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend</td><td>[2511.20343](http://arxiv.org/pdf/2511.20343)</td><td>◆ We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks.
◆ The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness.
◆ Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization.</td></tr>
<tr><td>2025-11-25</td><td>Stochastic Dynamics of Skyrmions on a Racetrack: Impact of Equilibrium and Nonequilibrium Noise</td><td>[2511.20287](http://arxiv.org/pdf/2511.20287)</td><td>◆ Current-driven motion of domain walls and skyrmions is central to the operation of non-volatile magnetic memory devices.
◆ Racetrack memory requires current densities high enough to generate velocities above 50 m/s, but such conditions also enhance spin-current noise.
◆ We develop a theoretical framework based on the stochastic Thiele equation to analyze the effects of equilibrium (thermal) and nonequilibrium (spin-current) fluctuations on skyrmion dynamics.</td></tr>
<tr><td>2025-11-25</td><td>Numerical Simulation of the Cleaning Process of Microchannel by an External Flow</td><td>[2511.20228](http://arxiv.org/pdf/2511.20228)</td><td>◆ This paper describes the problem of drift of solid non-interacting particles in a microchannel, which can stick to its walls under the action of the van der Waals forces and break away from the wall due to thermal noise and viscous stresses arising from the flow.
◆ The pressure drop is given between the channel inlet and outlet.
◆ At the initial moment of time, the channel walls are contaminated with adhered particles, i.e.</td></tr>
<tr><td>2025-11-25</td><td>STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction</td><td>[2511.19854](http://arxiv.org/pdf/2511.19854)</td><td>◆ Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task.
◆ Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness.
◆ Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids).</td></tr>
<tr><td>2025-11-24</td><td>Infrared absorption spectroscopy of a single polyatomic molecular ion</td><td>[2511.19687](http://arxiv.org/pdf/2511.19687)</td><td>◆ Absorption spectroscopy is a fundamental tool for probing molecular structure.
◆ However, performing absorption spectroscopy on individual molecules is challenging due to the low signal-to-noise ratio.
◆ Here, we report on a nondestructive absorption spectroscopy on a mid-infrared vibrational transition in a single molecular ion that is co-trapped with an atomic ion.</td></tr>
<tr><td>2025-11-24</td><td>KIC 5623923: A Faint Eclipsing Binary Consisting of $δ$ Scuti pulsations</td><td>[2511.19685](http://arxiv.org/pdf/2511.19685)</td><td>◆ In this paper, we present a detailed analysis of the light variation of KIC 5623923 using high-precision time-series data from the $Kepler$ mission.
◆ The analysis reveals this target is an eclipsing binary system with $δ$ Scuti type pulsations from the primary component, rather than from the secondary as previously reported.
◆ The frequency analysis of three short-cadence data reveals 41 significant frequencies, including the orbital frequency ($f_{orb}$ = 0.827198 d$^{-1}$) due to orbital motion from binary system and the pulsational frequencies.</td></tr>
<tr><td>2025-11-24</td><td>Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization</td><td>[2511.19275](http://arxiv.org/pdf/2511.19275)</td><td>◆ Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design.
◆ Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments.
◆ Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs.</td></tr>
<tr><td>2025-11-24</td><td>A Deep-Learning-Based Framework for Focal Mechanism Determination and Its Application to the 2022 Luding Earthquake Sequence</td><td>[2511.19185](http://arxiv.org/pdf/2511.19185)</td><td>◆ P-wave first-motion polarity plays an important role in resolving focal mechanisms of small to moderate earthquakes (M &lt;= 4.5).
◆ High-quality focal mechanism solutions for abundant small events can greatly improve our understanding of regional tectonics, fault geometries, and stress-field characteristics.
◆ In this study, we develop an automated focal mechanism determination framework that integrates deep neural networks with P-wave first-motion polarity observations, and apply it to the 2022 Luding earthquake sequence.</td></tr>
<tr><td>2025-11-24</td><td>MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes</td><td>[2511.19172](http://arxiv.org/pdf/2511.19172)</td><td>◆ Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction.
◆ However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge.
◆ To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments.</td></tr>
<tr><td>2025-11-24</td><td>The variability of blazars throughout the electromagnetic spectrum</td><td>[2511.18975](http://arxiv.org/pdf/2511.18975)</td><td>◆ With their jet pointing towards us, blazars are ideal tools to study the physics and structure of extragalactic jets.
◆ Their powerful jets are cosmic particle accelerators and are alleged to be one of the production sites of the high-energy neutrinos detected by the IceCube Observatory.
◆ Doppler beaming of the jet nonthermal radiation increases blazar brightness, blue-shifts their emission, and shortens their variability time scales, which are observed to range from years down to minutes.</td></tr>
<tr><td>2025-11-24</td><td>MagicWorld: Interactive Geometry-driven Video World Exploration</td><td>[2511.18886](http://arxiv.org/pdf/2511.18886)</td><td>◆ Recent interactive video world model methods generate scene evolution conditioned on user instructions.
◆ Although they achieve impressive results, two key limitations remain.
◆ First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes.</td></tr>
<tr><td>2025-11-24</td><td>STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution</td><td>[2511.18786](http://arxiv.org/pdf/2511.18786)</td><td>◆ We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions.
◆ The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation.
◆ To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions.</td></tr>
<tr><td>2025-11-24</td><td>On the role of fractional Brownian motion in models of chemotaxis and stochastic gradient ascent</td><td>[2511.18745](http://arxiv.org/pdf/2511.18745)</td><td>◆ Cell migration often exhibits long-range temporal correlations and anomalous diffusion, even in the absence of external guidance cues such as chemical gradients or topographical constraints.
◆ These observations raise a fundamental question: do such correlations simply reflect internal cellular processes, or do they enhance a cell&#x27;s ability to navigate complex environments?
◆ In this work, we explore how temporally correlated noise (modeled using fractional Brownian motion) influences chemotactic search dynamics.</td></tr>
<tr><td>2025-11-23</td><td>C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction</td><td>[2511.18559](http://arxiv.org/pdf/2511.18559)</td><td>◆ Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos.
◆ However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs.
◆ ground) or modalities (e.g., photos vs.</td></tr>
<tr><td>2025-11-23</td><td>Non-Symplectic Deformations of Geometric Quantisation</td><td>[2511.18549](http://arxiv.org/pdf/2511.18549)</td><td>◆ We introduce the notion of geometric pseudo-quantisation based on geometric quantisation with a weakened curvature condition.
◆ We show how such a structure arises naturally from simple deformations of the symplectic structure and pullbacks of prequantum data by non-symplectic diffeomorphisms.
◆ Our main result is deriving the equations of motion for some simple pseudo-quantisations.</td></tr>
<tr><td>2025-11-23</td><td>Zero-Shot Video Deraining with Video Diffusion Models</td><td>[2511.18537](http://arxiv.org/pdf/2511.18537)</td><td>◆ Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion.
◆ Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases.
◆ In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities.</td></tr>
<tr><td>2025-11-21</td><td>TRAO Survey of the Nearby Filamentary Molecular Clouds, the Universal Nursery of Stars (TRAO-FUNS). IV. Filaments and Dense Cores in the W40 and Serpens South Regions of Aquila</td><td>[2511.16978](http://arxiv.org/pdf/2511.16978)</td><td>◆ We present the results of molecular line observations toward the W40 and Serpens South regions of the Aquila molecular cloud complex, conducted as part of the TRAO-FUNS project to investigate the role of filamentary structures in the formation of dense cores and stars in molecular clouds.
◆ We performed a Gaussian decomposition of the C$^{18}$O spectra to disentangle multiple velocity components along the line-of-sight and a `Friends-of-Friends&#x27; algorithm on these decomposed components to identify 24 velocity-coherent filaments in the observed region.
◆ The `FellWalker&#x27; algorithm is applied on the N$_{2}$H$^{+}$ integrated intensity map to identify the dense cores embedded within the filaments.</td></tr>
<tr><td>2025-11-21</td><td>One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements</td><td>[2511.16966](http://arxiv.org/pdf/2511.16966)</td><td>◆ Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered.
◆ This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk.
◆ We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction.</td></tr>
<tr><td>2025-11-20</td><td>TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</td><td>[2511.16662](http://arxiv.org/pdf/2511.16662)</td><td>◆ With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge.
◆ Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics.
◆ To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars.</td></tr>
<tr><td>2025-11-20</td><td>Flow and Depth Assisted Video Prediction with Latent Transformer</td><td>[2511.16484](http://arxiv.org/pdf/2511.16484)</td><td>◆ Video prediction is a fundamental task for various downstream applications, including robotics and world modeling.
◆ Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction.
◆ We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion.</td></tr>
<tr><td>2025-11-20</td><td>Two Epochs of VLBI Observations of 8 KISSR Seyfert &amp; LINER Galaxies: Suggestions of Fast and Filamentary Outflows</td><td>[2511.16159](http://arxiv.org/pdf/2511.16159)</td><td>◆ We present here the results from a second epoch of phase-referenced VLBA observations of 8 Seyfert and LINER galaxies from the KISSR sample.
◆ These sources were chosen based on the presence of double peaks or asymmetries in their emission lines as observed in SDSS spectra.
◆ Parsec-scale radio emission is detected in 7 of the 8 sources in the second epoch.</td></tr>
<tr><td>2025-11-19</td><td>MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling</td><td>[2511.15645](http://arxiv.org/pdf/2511.15645)</td><td>◆ Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications.
◆ Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted.
◆ However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO.</td></tr>
<tr><td>2025-11-19</td><td>Covariant Measures of Non-Markovianity in Curved Spacetime</td><td>[2511.15365](http://arxiv.org/pdf/2511.15365)</td><td>◆ Standard measures of quantum non-Markovianity are usually defined in terms of dynamical maps on a preferred time foliation and therefore do not extend straightforwardly to curved spacetimes, where no global time coordinate exists and causal structure is primary.
◆ We develop a covariant framework for open quantum dynamics along arbitrary timelike worldlines by building multi-time quantum processes (process tensors) from overlapping causal diamonds.
◆ For an Unruh--DeWitt detector weakly coupled to a scalar field in a Hadamard state, we define a foliation-independent measure of non-Markovianity as the operational distance between the physical process tensor and the convex set of Markovian (CP-divisible) processes.</td></tr>
<tr><td>2025-11-19</td><td>Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation</td><td>[2511.15159](http://arxiv.org/pdf/2511.15159)</td><td>◆ High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition.
◆ Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations.
◆ We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation.</td></tr>
<tr><td>2025-11-19</td><td>SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection</td><td>[2511.15153](http://arxiv.org/pdf/2511.15153)</td><td>◆ Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation.
◆ However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations.
◆ While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection.</td></tr>
<tr><td>2025-11-18</td><td>Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video</td><td>[2511.14848](http://arxiv.org/pdf/2511.14848)</td><td>◆ We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video.
◆ Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence.
◆ Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction.</td></tr>
<tr><td>2025-11-18</td><td>Blur-Robust Detection via Feature Restoration: An End-to-End Framework for Prior-Guided Infrared UAV Target Detection</td><td>[2511.14371](http://arxiv.org/pdf/2511.14371)</td><td>◆ Infrared unmanned aerial vehicle (UAV) target images often suffer from motion blur degradation caused by rapid sensor movement, significantly reducing contrast between target and background.
◆ Generally, detection performance heavily depends on the discriminative feature representation between target and background.
◆ Existing methods typically treat deblurring as a preprocessing step focused on visual quality, while neglecting the enhancement of task-relevant features crucial for detection.</td></tr>
<tr><td>2025-11-18</td><td>Hubble Space Telescope proper motions of Large Magellanic Cloud star clusters -- II. Kinematic structure of young and intermediate-age clusters</td><td>[2511.14351](http://arxiv.org/pdf/2511.14351)</td><td>◆ In this paper, we explore the kinematic properties of a sample of 19 young (&lt;1 Gyr) and intermediate-age (1-2.5 Gyr) massive star clusters within the Large Magellanic Cloud (LMC).
◆ We analyse the proper motions of the clusters, which have been measured based on multi-epoch Hubble Space Telescope (HST) observations.
◆ Additionally, we infer from the HST data homogeneous and robust estimates for the distances, ages and metallicities of the clusters.</td></tr>
<tr><td>2025-11-18</td><td>Vortex stability in pseudo-Hermitian theories</td><td>[2511.14300](http://arxiv.org/pdf/2511.14300)</td><td>◆ Pseudo-Hermitian (including $\mathcal{PT}$-symmetric) field theories support phenomenology that cannot be replicated in standard Hermitian theories.
◆ We describe a concrete example in which the vortex solutions that are realised in a prototypical pseudo-Hermitian field theory exhibit a novel metastability, despite the model parameters residing within the naively stable regime of exact antilinear symmetry of the vacuum theory.
◆ This instability is identified analytically and confirmed through numerical simulations, and it arises from the small breaking of the underlying antilinear symmetry of the pseudo-Hermitian theory due to the presence of the topological defect.</td></tr>
<tr><td>2025-11-18</td><td>Model-Based Clustering of Football Event Sequences: A Marked Spatio-Temporal Point Process Mixture Approach</td><td>[2511.14297](http://arxiv.org/pdf/2511.14297)</td><td>◆ We propose a novel mixture model for football event data that clusters entire possessions to reveal their temporal, sequential, and spatial structure.
◆ Each mixture component models possessions as marked spatio-temporal point processes: event types follow a finite Markov chain with an absorbing state for ball loss, event times follow a conditional Gamma process to account for dispersion, and spatial locations evolve via truncated Brownian motion.
◆ To aid interpretation, we derive summary indicators from model parameters capturing possession speed, number of events, and spatial dynamics.</td></tr>
<tr><td>2025-11-18</td><td>Newborn jet in the symbiotic system R Aquarii</td><td>[2511.14243](http://arxiv.org/pdf/2511.14243)</td><td>◆ R Aquarii (R Aqr) is a well-known symbiotic binary that has attracted renewed interest during its recent periastron passage, an event that occurs only once every about 40 years.
◆ This passage marks the first to be observed with modern, state-of-the-art instruments.
◆ We investigate the inner, sub-arcsecond active region of R Aqr during this recent periastron passage, with the goal of gaining insight into the jet-launching mechanisms at work in this system.</td></tr>
<tr><td>2025-11-18</td><td>FreeMusco: Motion-Free Learning of Latent Control for Morphology-Adaptive Locomotion in Musculoskeletal Characters</td><td>[2511.14205](http://arxiv.org/pdf/2511.14205)</td><td>◆ We propose FreeMusco, a motion-free framework that jointly learns latent representations and control policies for musculoskeletal characters.
◆ By leveraging the musculoskeletal model as a strong prior, our method enables energy-aware and morphology-adaptive locomotion to emerge without motion data.
◆ The framework generalizes across human, non-human, and synthetic morphologies, where distinct energy-efficient strategies naturally appear--for example, quadrupedal gaits in Chimanoid versus bipedal gaits in Humanoid.</td></tr>
<tr><td>2025-11-18</td><td>AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models</td><td>[2511.14148](http://arxiv.org/pdf/2511.14148)</td><td>◆ Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots.
◆ However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM).
◆ Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure.</td></tr>
<tr><td>2025-11-17</td><td>B2F: End-to-End Body-to-Face Motion Generation with Style Reference</td><td>[2511.13988](http://arxiv.org/pdf/2511.13988)</td><td>◆ Human motion naturally integrates body movements and facial expressions, forming a unified perception.
◆ If a virtual character&#x27;s facial expression does not align well with its body movements, it may weaken the perception of the character as a cohesive whole.
◆ Motivated by this, we propose B2F, a model that generates facial motions aligned with body movements.</td></tr>
<tr><td>2025-11-17</td><td>Enabling Real-Time Volumetric Imaging in Interventional Radiology Suits via a Deep Learning Framework Robust to C-arm Tilt</td><td>[2511.13980](http://arxiv.org/pdf/2511.13980)</td><td>◆ Contemporary interventional imaging lacks the real-time 3D guidance needed for the precise localization of mobile thoracic targets.
◆ While Cone-Beam CT (CBCT) provides 3D data, it is often too slow for dynamic motion tracking.
◆ Deep learning frameworks that reconstruct 3D volumes from sparse 2D projections offer a promising solution, but their performance under the geometrically complex, non-zero tilt acquisitions common in interventional radiology is unknown.</td></tr>
<tr><td>2025-11-17</td><td>Ultrafast electron diffractive imaging of the dissociation of pre-excited molecules</td><td>[2511.13479](http://arxiv.org/pdf/2511.13479)</td><td>◆ Gas phase ultrafast electron diffraction (GUED) has become a powerful technique to directly observe the structural dynamics of photoexcited molecules.
◆ GUED reveals information about the nuclear motions that is complementary to the information on the electronic states provided by spectroscopic measurements.
◆ GUED experiments so far have utilized a single laser pulse to excite the molecules and an electron pulse to probe the dynamics.</td></tr>
<tr><td>2025-11-17</td><td>An Automated Framework for Analyzing Structural Evolution in On-the-fly Non-adiabatic Molecular Dynamics Using Autoencoder and Multiple Molecular Descriptors</td><td>[2511.13364](http://arxiv.org/pdf/2511.13364)</td><td>◆ A major challenge in nonadiabatic molecular dynamics is to automatically and objectively identify the key reaction coordinates that drive molecules toward distinct excited-state decay channels.
◆ Traditional manual analyses are inefficient and rely heavily on expert intuition, creating a bottleneck for interpreting complex photochemical processes.
◆ To overcome this, we introduce a fully automated machine-learning framework that directly extracts these coordinates from on-the-fly trajectory surface hopping data.</td></tr>
<tr><td>2025-11-17</td><td>Antisymmetric Mueller Generator as the Universal Origin of Geometric Phase in Classical Polarization and Quantum Two-Level Systems</td><td>[2511.13266](http://arxiv.org/pdf/2511.13266)</td><td>◆ We show that the antisymmetric part of the Mueller matrix of any ideal retarder uniquely determines the geometric phase observed in classical polarization optics.
◆ This antisymmetric block encodes the angular-velocity pseudovector that governs the tangential component of the Stokesvector motion on the Poincaré sphere and thus fully determines the associated geometric phase, while the symmetric block is geometrically neutral in the sense that it does not contribute to the phase.
◆ We further demonstrate that the same antisymmetric generator arises in the adjoint action of any SU(2) unitary operator and fully determines the geometric phase of a qubit, independently of adiabaticity or cyclicity.</td></tr>
<tr><td>2025-11-17</td><td>The Spontaneous Genesis of Solar Prominence Structures Driven by Supergranulation in Three-Dimensional Simulations</td><td>[2511.13252](http://arxiv.org/pdf/2511.13252)</td><td>◆ Solar prominences usually have a horizontally elongated body with many feet extending to the solar surface, resembling a multi-arch bridge with many bridge piers.
◆ The basic mechanism by which solar prominences acquire these common structures during their evolution, however, remains an unresolved question.
◆ For the first time, our three-dimensional magneto-frictional simulation, driven by supergranular motions, self-consistently replicates the commonly observed multi-arch bridge morphology and its characteristic structures of solar quiescent prominences in a magnetic flux rope.</td></tr>
<tr><td>2025-11-17</td><td>Infrared photometry and CaT spectroscopy of the most metal-poor in-situ globular cluster VVV-CL001</td><td>[2511.13161](http://arxiv.org/pdf/2511.13161)</td><td>◆ Globular clusters in the Galactic bulge are difficult to study due to high extinction and severe crowding.
◆ VVV-CL001 is an old, metal-poor, and fast cluster in the inner bulge, whose extreme properties make it a key probe of the early chemical and dynamical evolution of the Milky Way.
◆ We derive its fundamental parameters by combining spectroscopy, astrometry, and near-infrared photometry.</td></tr>
<tr><td>2025-11-16</td><td>Kagome metals</td><td>[2511.12731](http://arxiv.org/pdf/2511.12731)</td><td>◆ Three important driving forces for creating qualitatively new phases in quantum materials are the topology of the materials&#x27; electronic band structures, frustration in the electrons&#x27; motion or magnetic interactions, and strong correlations between their charge, spin, and orbital degrees of freedom.
◆ In very few material systems do all of these aspects come together to contribute on an equal footing to stabilize new electronic states with unprecedented properties; however the search for such systems can be guided by models of configurational motifs or key sublattices that can host such physics.
◆ One of the most fascinating structural motifs for realizing this rich interplay of frustration, electronic topology, and electron correlation effects is the kagome lattice.</td></tr>
<tr><td>2025-11-16</td><td>Examining Turbulence in Galactic Molecular Clouds - II: Continuity of Turbulence Cascading in a Portion of the Local Arm</td><td>[2511.12418](http://arxiv.org/pdf/2511.12418)</td><td>◆ We use $^{12}$CO (J=1-0) MWISP data to study turbulence in a segment of the Local Arm.
◆ Velocity slices at different kinematic distances show similar spatial power spectra (SPSs) and structure functions (SFs), demonstrating that the entire region forms a single turbulent field with a cascade extending from $\sim 400$ pc to sub-parsec scales.
◆ The SPS slopes of both the intensity and velocity fields exhibit a systematic scale dependence that approaches the values expected from turbulence models.</td></tr>
<tr><td>2025-11-16</td><td>Towards Rotation-only Imaging Geometry: Rotation Estimation</td><td>[2511.12415](http://arxiv.org/pdf/2511.12415)</td><td>◆ Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images.
◆ The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment.
◆ Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation.</td></tr>
<tr><td>2025-11-14</td><td>Free3D: 3D Human Motion Emerges from Single-View 2D Supervision</td><td>[2511.11368](http://arxiv.org/pdf/2511.11368)</td><td>◆ Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions.
◆ This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations.
◆ Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility.</td></tr>
<tr><td>2025-11-14</td><td>YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation</td><td>[2511.11344](http://arxiv.org/pdf/2511.11344)</td><td>◆ We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation.
◆ While synthetic data has become fundamental in frame-based computer vision, event-based vision lacks comparable comprehensive resources.
◆ Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology.</td></tr>
<tr><td>2025-11-14</td><td>The Spatial Evolution of Star Clusters in NGC 628 with JWST</td><td>[2511.11115](http://arxiv.org/pdf/2511.11115)</td><td>◆ We examine the spatial distribution of star clusters in NGC 628 using the statistical tool INDICATE to quantify clustering tendencies.
◆ Our sample, based on HST and JWST observations, is the most complete to date, spanning ages from 1 Myr to &gt;100 Myr.
◆ We find cluster spatial behaviour varies with galactic position, age, and mass.</td></tr>
<tr><td>2025-11-14</td><td>Discovery of an X-ray bridge between the comma-shaped gas and the main cluster in MCXC J0157.4-0550</td><td>[2511.10968](http://arxiv.org/pdf/2511.10968)</td><td>◆ We report the discovery of a faint X-ray bridge connecting between the comma-shaped gas and the main cluster in MCXC J0157.4-0550, using {\it XMM-Newton} image.
◆ The filamentary structure is found in a model-independent manner in both topological features and Gaussian Gradient Magnitude filtering.
◆ The X-ray surface brightness profile perpendicular to the filament is detected at a $5.5σ$ level.</td></tr>
<tr><td>2025-11-14</td><td>DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition</td><td>[2511.10948](http://arxiv.org/pdf/2511.10948)</td><td>◆ Micro expression recognition (MER) is crucial for inferring genuine emotion.
◆ Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions.
◆ However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion.</td></tr>
<tr><td>2025-11-14</td><td>A High-Precision Dynamical Model of Callisto: Incorporating Rotation Effects within Multi-Layer Internal Structure Models</td><td>[2511.10929](http://arxiv.org/pdf/2511.10929)</td><td>◆ China is planing to launch the Tianwen-4 mission around the year 2030, with its aim being the exploration of Jupiter and its moon, Callisto.
◆ Within the realm of deep space exploration, the accuracy of ephemerides is of great importance.
◆ Current ephemerides employ a simplified rotation model for Callisto, which this study addresses by proposing a novel dynamical model.</td></tr>
<tr><td>2025-11-14</td><td>Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation</td><td>[2511.10874](http://arxiv.org/pdf/2511.10874)</td><td>◆ Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale.
◆ Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks.
◆ To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning.</td></tr>
<tr><td>2025-11-13</td><td>A validated lumped-element model for bioinspired acoustic flow sensing toward the performance limit</td><td>[2511.10830](http://arxiv.org/pdf/2511.10830)</td><td>◆ Flow sensing is fundamental to both biological survival and technological innovation.
◆ Inspired by biological mechanoreceptors, artificial flow sensors detect subtle fluid motion using slender, viscous-driven structures.
◆ Among these, acoustic flow sensors that mimic nature&#x27;s velocity-sensitive ears have the potential to transform vector sound detection.</td></tr>
<tr><td>2025-11-13</td><td>From Attention to Frequency: Integration of Vision Transformer and FFT-ReLU for Enhanced Image Deblurring</td><td>[2511.10806](http://arxiv.org/pdf/2511.10806)</td><td>◆ Image deblurring is vital in computer vision, aiming to recover sharp images from blurry ones caused by motion or camera shake.
◆ While deep learning approaches such as CNNs and Vision Transformers (ViTs) have advanced this field, they often struggle with complex or high-resolution blur and computational demands.
◆ We propose a new dual-domain architecture that unifies Vision Transformers with a frequency-domain FFT-ReLU module, explicitly bridging spatial attention modeling and frequency sparsity.</td></tr>
<tr><td>2025-11-13</td><td>Towards Attribution of Generators and Emotional Manipulation in Cross-Lingual Synthetic Speech using Geometric Learning</td><td>[2511.10790](http://arxiv.org/pdf/2511.10790)</td><td>◆ In this work, we address the problem of finegrained traceback of emotional and manipulation characteristics from synthetically manipulated speech.
◆ We hypothesize that combining semantic-prosodic cues captured by Speech Foundation Models (SFMs) with fine-grained spectral dynamics from auditory representations can enable more precise tracing of both emotion and manipulation source.
◆ To validate this hypothesis, we introduce MiCuNet, a novel multitask framework for fine-grained tracing of emotional and manipulation attributes in synthetically generated speech.</td></tr>
<tr><td>2025-11-13</td><td>From Fold to Function: Dynamic Modeling and Simulation-Driven Design of Origami Mechanisms</td><td>[2511.10580](http://arxiv.org/pdf/2511.10580)</td><td>◆ Origami-inspired mechanisms can transform flat sheets into functional three-dimensional dynamic structures that are lightweight, compact, and capable of complex motion.
◆ These properties make origami increasingly valuable in robotic and deployable systems.
◆ However, accurately simulating their folding behavior and interactions with the environment remains challenging.</td></tr>
<tr><td>2025-11-13</td><td>M3Scope a 3D multimode multiplane microscope for imaging nanoscale dynamics in soft matter</td><td>[2511.10174](http://arxiv.org/pdf/2511.10174)</td><td>◆ Fast, volumetric imaging that integrates multiple imaging modalities is essential for probing dynamic, heterogeneous soft and biological matter.
◆ Here, we present the M3Scope, a simple yet versatile multiplane microscope that extends widefield detection with a modular multimode cube to enable dual color fluorescence, polarization-resolved, and correlative brightfield fluorescence imaging while (i) preserving simultaneous 3D acquisition at high frame rates (100 fps) and (ii) requiring minimal realignment.
◆ We demonstrate its potential by investigating polymer dynamics across multiple spatial and temporal scales.</td></tr>
<tr><td>2025-11-13</td><td>Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks</td><td>[2511.10079](http://arxiv.org/pdf/2511.10079)</td><td>◆ Friction modeling plays a crucial role in achieving high-precision motion control in robotic operating systems.
◆ Traditional static friction models (such as the Stribeck model) are widely used due to their simple forms; however, they typically require predefined functional assumptions, which poses significant challenges when dealing with unknown functional structures.
◆ To address this issue, this paper proposes a physics-inspired machine learning approach based on the Kolmogorov Arnold Network (KAN) for static friction modeling of robotic joints.</td></tr>
<tr><td>2025-11-13</td><td>Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints</td><td>[2511.10076](http://arxiv.org/pdf/2511.10076)</td><td>◆ Reliable co-speech motion generation requires precise motion representation and consistent structural priors across all joints.
◆ Existing generative methods typically operate on local joint rotations, which are defined hierarchically based on the skeleton structure.
◆ This leads to cumulative errors during generation, manifesting as unstable and implausible motions at end-effectors.</td></tr>
<tr><td>2025-11-13</td><td>PuffyBot: An Untethered Shape Morphing Robot for Multi-environment Locomotion</td><td>[2511.09885](http://arxiv.org/pdf/2511.09885)</td><td>◆ Amphibians adapt their morphologies and motions to accommodate movement in both terrestrial and aquatic environments.
◆ Inspired by these biological features, we present PuffyBot, an untethered shape morphing robot capable of changing its body morphology to navigate multiple environments.
◆ Our robot design leverages a scissor-lift mechanism driven by a linear actuator as its primary structure to achieve shape morphing.</td></tr>
<tr><td>2025-11-13</td><td>AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting</td><td>[2511.09827](http://arxiv.org/pdf/2511.09827)</td><td>◆ We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction.
◆ Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes.
◆ By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes.</td></tr>
<tr><td>2025-11-12</td><td>DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation</td><td>[2511.09502](http://arxiv.org/pdf/2511.09502)</td><td>◆ Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships.
◆ However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios.
◆ Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation.</td></tr>
<tr><td>2025-11-12</td><td>SPIDER: Scalable Physics-Informed Dexterous Retargeting</td><td>[2511.09484](http://arxiv.org/pdf/2511.09484)</td><td>◆ Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive.
◆ In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem.
◆ However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots.</td></tr>
<tr><td>2025-11-12</td><td>3D PIC simulation and theoretical modeling of RF Laser pulse in magnetized plasma for the generation of multidimensional relativistic Wakefields</td><td>[2511.09079](http://arxiv.org/pdf/2511.09079)</td><td>◆ The present study, investigates the modulation of plasma wakefields in dense magnetized plasma driven by relativistic electron beams under transverse RF excitation.
◆ A self consistent theoretical framework, comprising the RF vector potential, Maxwells equations, and relativistic electron motion, is extended through full 3D electromagnetic particle in cell simulations.
◆ The results reveal systematic amplification and reshaping of wakefields under the combined action of external magnetic fields and RF drivers.</td></tr>
<tr><td>2025-11-12</td><td>Group-Theoretic Structure Governing Identifiability in Inverse Problems</td><td>[2511.08995](http://arxiv.org/pdf/2511.08995)</td><td>◆ In physical systems possessing symmetry, reconstructing the underlying causal structure from observational data constitutes an inverse problem of fundamental importance.
◆ In this work, we formulate the inverse problem of causal inference within the framework of group-representation theory, clarifying the structure of the representation spaces to which the {\it causality} and estimation maps belong.
◆ This formulation leads to both theoretical and practical limits of reconstructability (identifiability).</td></tr>
<tr><td>2025-11-10</td><td>Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation</td><td>[2511.06857](http://arxiv.org/pdf/2511.06857)</td><td>◆ A simultaneous enhancement of accuracy and diversity of predictions remains a challenge in ambiguous medical image segmentation (AMIS) due to the inherent trade-offs.
◆ While truncated diffusion probabilistic models (TDPMs) hold strong potential with a paradigm optimization, existing TDPMs suffer from entangled accuracy and diversity of predictions with insufficient fidelity and plausibility.
◆ To address the aforementioned challenges, we propose Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel inference paradigm and dedicated model components.</td></tr>
<tr><td>2025-11-10</td><td>SDSS-ALMA Legacy Value Archival Gas Exploration (SALVAGE) - I: global star formation is governed by central (not global) molecular gas</td><td>[2511.06775](http://arxiv.org/pdf/2511.06775)</td><td>◆ Star-forming galaxies form tight relations between their stellar mass, star-formation rate, and molecular gas reservoir on global and resolved scales.
◆ On the path to quiescence, the exchange between gas and stars must inevitably be broken.
◆ Understanding the mechanisms governing star formation and quenching therefore requires observations of both the stellar and molecular gas components.</td></tr>
<tr><td>2025-11-06</td><td>Sub-Gyr variability around the SFMS and its contribution to the scatter</td><td>[2511.04745](http://arxiv.org/pdf/2511.04745)</td><td>◆ We aim to measure the evolution of individual galaxies around the Star Formation Main Sequence (SFMS) during the last Gyr as a function of their stellar mass to quantify how much of its scatter is due to short-term variability.We derived star formation histories using full spectral fitting for a sample of 8,960 galaxies from the MaNGA survey to track the position of the galaxies in the SFMS during the last Gyr.The variability correlates with both the stellar mass of the galaxies and their current position in both the SFMS and the mass-metallicity relation (MZR), with the position in the latter strongly affecting variability in SFR.
◆ While most of the fluctuations are compatible with stochasticity, there is a very weak but statistically significant preference for $\sim135-150$ Myr time-scales.
◆ These results support a strong self-regulation of SFR within galaxies, establishing characteristic intensities and time-scales for bursts of star formation and quenching episodes.</td></tr>
<tr><td>2025-11-04</td><td>Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization</td><td>[2511.02329](http://arxiv.org/pdf/2511.02329)</td><td>◆ We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations).
◆ Our core innovation is a location solver that adapts message-passing least squares (MPLS) -- originally developed for group synchronization -- to camera location estimation.
◆ We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss.</td></tr>
<tr><td>2025-11-03</td><td>Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play</td><td>[2511.01261](http://arxiv.org/pdf/2511.01261)</td><td>◆ Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction.
◆ Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges.
◆ Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles.</td></tr>
<tr><td>2025-11-01</td><td>Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</td><td>[2511.00362](http://arxiv.org/pdf/2511.00362)</td><td>◆ Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise.
◆ Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts.
◆ As a result, many of Bangladesh&#x27;s architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form.</td></tr>
<tr><td>2025-10-29</td><td>Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models</td><td>[2510.25577](http://arxiv.org/pdf/2510.25577)</td><td>◆ Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations.
◆ This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal.
◆ One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice.</td></tr>
<tr><td>2025-10-27</td><td>SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution</td><td>[2510.25178](http://arxiv.org/pdf/2510.25178)</td><td>◆ Intra-sentence multilingual speech synthesis (code-switching TTS) remains a major challenge due to abrupt language shifts, varied scripts, and mismatched prosody between languages.
◆ Conventional TTS systems are typically monolingual and fail to produce natural, intelligible speech in mixed-language contexts.
◆ We introduce Script-First Multilingual Synthesis with Adaptive Locale Resolution (SFMS-ALR), an engine-agnostic framework for fluent, real-time code-switched speech generation.</td></tr>
<tr><td>2025-10-27</td><td>Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition</td><td>[2510.22961](http://arxiv.org/pdf/2510.22961)</td><td>◆ Unified speech recognition aims to perform auditory, visual, and audiovisual speech recognition within a single model framework.
◆ While speech foundation models (SFMs) have demonstrated remarkable performance in auditory tasks, their adaptation to multimodal scenarios remains underexplored.
◆ This paper presents UASR-LLM, a novel framework that adapts frozen SFMs to unified VSR, ASR, and AVSR tasks by leveraging large language models (LLMs) as text decoders.</td></tr>
<tr><td>2025-10-23</td><td>RubbleSim: A Photorealistic Structural Collapse Simulator for Confined Space Mapping</td><td>[2510.20529](http://arxiv.org/pdf/2510.20529)</td><td>◆ Despite well-reported instances of robots being used in disaster response, there is scant published data on the internal composition of the void spaces within structural collapse incidents.
◆ Data collected during these incidents is mired in legal constraints, as ownership is often tied to the responding agencies, with little hope of public release for research.
◆ While engineered rubble piles are used for training, these sites are also reluctant to release information about their proprietary training grounds.</td></tr>
<tr><td>2025-10-21</td><td>The slope and scatter of the star forming main sequence at z~5 : reconciling observations with simulations</td><td>[2510.19044](http://arxiv.org/pdf/2510.19044)</td><td>◆ Galaxies exhibit a tight correlation between their star-formation rate and stellar mass over a wide redshift range known as the star-forming main sequence (SFMS).
◆ With JWST, we can now investigate the SFMS at high redshifts down to masses of $\sim10^6$ M$_{\odot}$, using sensitive star-formation rate tracers such as H$\alpha$ emission -- which allow us to probe the variability in star formation histories.
◆ We present inferences of the SFMS based on 316 H$\alpha$-selected galaxies at $z\sim4$-$5$ with $\log(\rm M_\star/M_\odot) = 6.4$ -$10.6$.</td></tr>
<tr><td>2025-10-20</td><td>Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS</td><td>[2510.17479](http://arxiv.org/pdf/2510.17479)</td><td>◆ Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training views, leading to artifacts like blurring in novel view rendering.
◆ Prior work addresses it either by enhancing the initialization (\emph{i.e.}, the point cloud from Structure-from-Motion (SfM)) or by adding training-time constraints (regularization) to the 3DGS optimization.
◆ Yet our controlled ablations reveal that initialization is the decisive factor: it determines the attainable performance band in sparse-view 3DGS, while training-time constraints yield only modest within-band improvements at extra cost.</td></tr>
<tr><td>2025-10-21</td><td>Leveraging AV1 motion vectors for Fast and Dense Feature Matching</td><td>[2510.17434](http://arxiv.org/pdf/2510.17434)</td><td>◆ We repurpose AV1 motion vectors to produce dense sub-pixel correspondences and short tracks filtered by cosine consistency.
◆ On short videos, this compressed-domain front end runs comparably to sequential SIFT while using far less CPU, and yields denser matches with competitive pairwise geometry.
◆ As a small SfM demo on a 117-frame clip, MV matches register all images and reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows with match density.</td></tr>
<tr><td>2025-10-21</td><td>DeepDetect: Learning All-in-One Dense Keypoints</td><td>[2510.17422](http://arxiv.org/pdf/2510.17422)</td><td>◆ Keypoint detection is the foundation of many computer vision tasks, including image registration, structure-from motion, 3D reconstruction, visual odometry, and SLAM.
◆ Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong performance yet suffer from key limitations: sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding, often failing to prioritize visually important regions.
◆ We present DeepDetect, an intelligent, all-in-one, dense keypoint detector that unifies the strengths of classical detectors using deep learning.</td></tr>
<tr><td>2025-10-18</td><td>LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching</td><td>[2510.16438](http://arxiv.org/pdf/2510.16438)</td><td>◆ Lines and points are complementary local features, whose combination has proven effective for applications such as SLAM and Structure-from-Motion.
◆ The backbone of these pipelines are the local feature matchers, establishing correspondences across images.
◆ Traditionally, point and line matching have been treated as independent tasks.</td></tr>
<tr><td>2025-10-17</td><td>MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes</td><td>[2510.15467](http://arxiv.org/pdf/2510.15467)</td><td>◆ Structure from Motion (SfM) estimates camera poses and reconstructs point clouds, forming a foundation for various tasks.
◆ However, applying SfM to driving scenes captured by multi-camera systems presents significant difficulties, including unreliable pose estimation, excessive outliers in road surface reconstruction, and low reconstruction efficiency.
◆ To address these limitations, we propose a Multi-camera Reconstruction and Aggregation Structure-from-Motion (MRASfM) framework specifically designed for driving scenes.</td></tr>
<tr><td>2025-10-17</td><td>CuSfM: CUDA-Accelerated Structure-from-Motion</td><td>[2510.15271](http://arxiv.org/pdf/2510.15271)</td><td>◆ Efficient and accurate camera pose estimation forms the foundational requirement for dense reconstruction in autonomous navigation, robotic perception, and virtual simulation systems.
◆ This paper addresses the challenge via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that leverages GPU parallelization to efficiently employ computationally intensive yet highly accurate feature extractors, generating comprehensive and non-redundant data associations for precise camera pose estimation and globally consistent mapping.
◆ The system supports pose optimization, mapping, prior-map localization, and extrinsic refinement.</td></tr>
<tr><td>2025-10-15</td><td>Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos</td><td>[2510.13540](http://arxiv.org/pdf/2510.13540)</td><td>◆ We present a neural parametric 3D breast shape model and, based on this model, introduce a low-cost and accessible 3D surface reconstruction pipeline capable of recovering accurate breast geometry from a monocular RGB video.
◆ In contrast to widely used, commercially available yet prohibitively expensive 3D breast scanning solutions and existing low-cost alternatives, our method requires neither specialized hardware nor proprietary software and can be used with any device that is able to record RGB videos.
◆ The key building blocks of our pipeline are a state-of-the-art, off-the-shelf Structure-from-motion pipeline, paired with a parametric breast model for robust and metrically correct surface reconstruction.</td></tr>
<tr><td>2025-10-15</td><td>InstantSfM: Fully Sparse and Parallel Structure-from-Motion</td><td>[2510.13310](http://arxiv.org/pdf/2510.13310)</td><td>◆ Structure-from-Motion (SfM), a method that recovers camera poses and scene geometry from uncalibrated images, is a central component in robotic reconstruction and simulation.
◆ Despite the state-of-the-art performance of traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive CPU-specialized implementations of bundle adjustment (BA) or global positioning (GP) introduce significant computational overhead when handling large-scale scenarios, leading to a trade-off between accuracy and speed in SfM.
◆ Moreover, the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes with the curse of limited flexibility, as they lack support for various external optimization options.</td></tr>
<tr><td>2025-10-15</td><td>Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation</td><td>[2510.13084](http://arxiv.org/pdf/2510.13084)</td><td>◆ Text-to-image (T2I) diffusion models have recently demonstrated significant progress in video editing.
◆ However, existing video editing methods are severely limited by their high computational overhead and memory consumption.
◆ Furthermore, these approaches often sacrifice visual fidelity, leading to undesirable temporal inconsistencies and artifacts such as blurring and pronounced mosaic-like patterns.</td></tr>
<tr><td>2025-10-14</td><td>Scene Coordinate Reconstruction Priors</td><td>[2510.12387](http://arxiv.org/pdf/2510.12387)</td><td>◆ Scene coordinate regression (SCR) models have proven to be powerful implicit scene representations for 3D vision, enabling visual relocalization and structure-from-motion.
◆ SCR models are trained specifically for one scene.
◆ If training images imply insufficient multi-view constraints SCR models degenerate.</td></tr>
<tr><td>2025-10-10</td><td>Two-Stage Gaussian Splatting Optimization for Outdoor Scene Reconstruction</td><td>[2510.09489](http://arxiv.org/pdf/2510.09489)</td><td>◆ Outdoor scene reconstruction remains challenging due to the stark contrast between well-textured, nearby regions and distant backgrounds dominated by low detail, uneven illumination, and sky effects.
◆ We introduce a two-stage Gaussian Splatting framework that explicitly separates and optimizes these regions, yielding higher-fidelity novel view synthesis.
◆ In stage one, background primitives are initialized within a spherical shell and optimized using a loss that combines a background-only photometric term with two geometric regularizers: one constraining Gaussians to remain inside the shell, and another aligning them with local tangential planes.</td></tr>
<tr><td>2025-10-08</td><td>The Star-forming Main Sequence and Bursty Star-formation Histories at $z&gt;1.4$ in JADES and AURORA</td><td>[2510.06681](http://arxiv.org/pdf/2510.06681)</td><td>◆ We analyze JWST spectroscopic and HST+JWST photometric observations of 659 star-forming galaxies at $1.4 &lt; z &lt; 9$ from DR3 of the JADES survey and the AURORA Cycle 1 program.
◆ We measure the star-forming main sequence (SFMS) for galaxies above $10^{8.5}\rm\ M_\odot$ where the sample is largely representative, estimating star-formation rates (SFRs) using the H$\alpha$ line flux and rest-frame far UV (1600\AA) continuum measurements, each independently corrected for dust attenuation.
◆ We find that the intrinsic, measurement-error-subtracted scatter in the SFMS ($\sigma_{\rm int}$) increases with decreasing stellar mass for the H$\alpha$-based SFMS, and we find no mass dependence of $\sigma_{\rm int}$ in the UV-based SFMS.</td></tr>
<tr><td>2025-10-06</td><td>The Prevalence of Bursty Star Formation in Low-Mass Galaxies at z=1-7 from Hα-to-UV Diagnostics</td><td>[2510.05388](http://arxiv.org/pdf/2510.05388)</td><td>◆ We present an analysis of bursty star-formation histories (SFHs) of 346 star-forming galaxies at $1\lesssim z&lt;7$, selected from JWST/NIRSpec G395M and PRISM spectroscopy provided by the CEERS and RUBIES surveys.
◆ We analyze the correlation of star-formation rate vs.
◆ stellar mass (the star-forming main sequence, SFMS) for our sample and find no significant difference between the intrinsic scatter in the H$\alpha$-based SFMS and the UV-continuum-based SFMS.</td></tr>
<tr><td>2025-10-02</td><td>Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale</td><td>[2510.01665](http://arxiv.org/pdf/2510.01665)</td><td>◆ Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention.
◆ We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset.
◆ Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework.</td></tr>
<tr><td>2025-10-08</td><td>VGGT-X: When VGGT Meets Dense Novel View Synthesis</td><td>[2509.25191](http://arxiv.org/pdf/2509.25191)</td><td>◆ We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS).
◆ Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures.
◆ Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS.</td></tr>
<tr><td>2025-09-28</td><td>BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes</td><td>[2509.24126](http://arxiv.org/pdf/2509.24126)</td><td>◆ Active vision (AV) has been in the spotlight of robotics research due to its emergence in numerous applications including agricultural tasks such as precision crop monitoring and autonomous harvesting to list a few.
◆ A major AV problem that gained popularity is the 3D reconstruction of targeted environments using 2D images from diverse viewpoints.
◆ While collecting and processing a large number of arbitrarily captured 2D images can be arduous in many practical scenarios, a more efficient solution involves optimizing the placement of available cameras in 3D space to capture fewer, yet more informative, images that provide sufficient visual information for effective reconstruction of the environment of interest.</td></tr>
<tr><td>2025-09-28</td><td>RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization</td><td>[2509.23991](http://arxiv.org/pdf/2509.23991)</td><td>◆ The increasing use of 360 images across various domains has emphasized the need for robust depth estimation techniques tailored for omnidirectional images.
◆ However, obtaining large-scale labeled datasets for 360 depth estimation remains a significant challenge.
◆ In this paper, we propose RPG360, a training-free robust 360 monocular depth estimation method that leverages perspective foundation models and graph optimization.</td></tr>
<tr><td>2025-09-28</td><td>CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting</td><td>[2509.23947](http://arxiv.org/pdf/2509.23947)</td><td>◆ Automatic car damage detection has been a topic of significant interest for the auto insurance industry as it promises faster, accurate, and cost-effective damage assessments.
◆ However, few works have gone beyond 2D image analysis to leverage 3D reconstruction methods, which have the potential to provide a more comprehensive and geometrically accurate representation of the damage.
◆ Moreover, recent methods employing 3D representations for novel view synthesis, particularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to generate accurate and coherent 3D reconstructions from a limited number of views.</td></tr>
<tr><td>2025-09-24</td><td>Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering</td><td>[2509.19898](http://arxiv.org/pdf/2509.19898)</td><td>◆ The integration of aerial and ground images has been a promising solution in 3D modeling of complex scenes, which is seriously restricted by finding reliable correspondences.
◆ The primary contribution of this study is a feature matching algorithm for aerial and ground images, whose core idea is to generate intermediate views to alleviate perspective distortions caused by the extensive viewpoint changes.
◆ First, by using aerial images only, sparse models are reconstructed through an incremental SfM (Structure from Motion) engine due to their large scene coverage.</td></tr>
<tr><td>2025-09-23</td><td>DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring</td><td>[2509.18898](http://arxiv.org/pdf/2509.18898)</td><td>◆ In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
◆ We address the motion-deblurring problem in two ways.
◆ First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images.</td></tr>
<tr><td>2025-09-21</td><td>Reference-aware SFM layers for intrusive intelligibility prediction</td><td>[2509.17270](http://arxiv.org/pdf/2509.17270)</td><td>◆ Intrusive speech-intelligibility predictors that exploit explicit reference signals are now widespread, yet they have not consistently surpassed non-intrusive systems.
◆ We argue that a primary cause is the limited exploitation of speech foundation models (SFMs).
◆ This work revisits intrusive prediction by combining reference conditioning with multi-layer SFM representations.</td></tr>
<tr><td>2025-09-19</td><td>Investigating Polyglot Speech Foundation Models for Learning Collective Emotion from Crowds</td><td>[2509.16329](http://arxiv.org/pdf/2509.16329)</td><td>◆ This paper investigates the polyglot (multilingual) speech foundation models (SFMs) for Crowd Emotion Recognition (CER).
◆ We hypothesize that polyglot SFMs, pre-trained on diverse languages, accents, and speech patterns, are particularly adept at navigating the noisy and complex acoustic environments characteristic of crowd settings, thereby offering a significant advantage for CER.
◆ To substantiate this, we perform a comprehensive analysis, comparing polyglot, monolingual, and speaker recognition SFMs through extensive experiments on a benchmark CER dataset across varying audio durations (1 sec, 500 ms, and 250 ms).</td></tr>
<tr><td>2025-09-26</td><td>MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild</td><td>[2509.15548](http://arxiv.org/pdf/2509.15548)</td><td>◆ In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis.
◆ Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting.
◆ In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS.</td></tr>
<tr><td>2025-09-18</td><td>MapAnything: Universal Feed-Forward Metric 3D Reconstruction</td><td>[2509.13414](http://arxiv.org/pdf/2509.13414)</td><td>◆ We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras.
◆ MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame.
◆ Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more.</td></tr>
<tr><td>2025-09-18</td><td>A-TDOM: Active TDOM via On-the-Fly 3DGS</td><td>[2509.12759](http://arxiv.org/pdf/2509.12759)</td><td>◆ True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in various fields such as urban management, city planning, land surveying, etc.
◆ However, traditional TDOM generation methods generally rely on a complex offline photogrammetric pipeline, resulting in delays that hinder real-time applications.
◆ Moreover, the quality of TDOM may degrade due to various challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and scene occlusions.</td></tr>
<tr><td>2025-09-15</td><td>Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles</td><td>[2509.12458](http://arxiv.org/pdf/2509.12458)</td><td>◆ Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction.
◆ To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams.
◆ Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control.</td></tr>
<tr><td>2025-09-15</td><td>Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting</td><td>[2509.11853](http://arxiv.org/pdf/2509.11853)</td><td>本文针对稀疏视图合成中几何与外观恢复困难的问题，提出了一种基于分割驱动初始化的3D高斯溅射方法（SDI-GS）。  
◆ 提出利用区域分割技术识别结构显著区域，替代传统依赖运动恢复结构（SfM）或多视图立体（MVS）的方法。  
◆ 通过选择性下采样稠密点云，大幅减少3D高斯数量，降低内存占用和计算成本。  
◆ 在保持场景保真度的同时，实现高达50%的高斯数量削减，并提升训练速度。  
实验表明，该方法在多项基准测试中达到相当或更优的渲染质量（PSNR、SSIM），仅LPIPS略有下降，有效推动了3DGS在受限视角场景中的实用化。</td></tr>
<tr><td>2025-09-15</td><td>WAFER: A new method to retrieve sun-induced fluorescence based on spectral wavelet decompositions</td><td>[2509.11829](http://arxiv.org/pdf/2509.11829)</td><td>该论文提出了一种名为WAFER的新型日光诱导叶绿素荧光（SIF）反演方法。  
◆ 利用小波分解技术处理反射辐射和参考辐射光谱，通过比较小波系数来独立获取相对反射率，避免了荧光与反射率的耦合。  
◆ 可直接从剩余偏移量中推导出荧光信号，无需对荧光光谱形状进行先验假设，减少了模型偏差。  
◆ 方法适用于任意波长窗口和全光谱范围，能充分利用所有可用光谱数据，并可分离不同频率的吸收线特征。  
◆ 无需依赖大量训练数据集，且对反射率形状的假设要求极低，增强了方法的通用性和灵活性。  
◆ 通过合成数据和实地测量验证，与现有方法（iFLD和SFM）结果一致，但额外提供了探索真实荧光光谱形状和自由选择反演窗口的优势。</td></tr>
<tr><td>2025-09-14</td><td>3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment</td><td>[2509.11097](http://arxiv.org/pdf/2509.11097)</td><td>该论文的核心贡献是推出了首个专门用于灾后评估的三维无人机基准数据集3DAeroRelief，填补了该领域的数据空白。  
◆首创面向灾后场景的三维语义分割基准数据集，专注于真实灾害环境中的精细结构损伤识别  
◆采用低成本无人机采集数据，实现了在危险区域高效、灵活且安全的大规模三维数据获取  
◆通过运动重建和多视角立体技术生成密集三维点云，并结合人工二维标注投影至三维空间，提供高质量语义注释  
◆数据集包含真实飓风受灾区域的大规模室外场景，突破了现有数据集局限于城市或室内环境的不足  
论文还基于该数据集评估了多种先进三维分割模型，证明了其在推动灾后响应三维视觉系统发展方面的实用价值。</td></tr>
<tr><td>2025-09-09</td><td>Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision</td><td>[2509.09720](http://arxiv.org/pdf/2509.09720)</td><td>该论文的核心贡献是推出了澳大利亚超市物体数据集（ASOS），这是一个专为机器人和计算机视觉应用设计的实物与3D模型基准数据集。

◆ 提供了包含50种常见超市商品的高质量3D纹理网格数据集，所有物品均可从澳大利亚大型连锁超市轻松购得，成本低廉且易于获取。
◆ 采用运动恢复结构（SfM）技术和高分辨率成像技术生成水密的三维网格，模型质量高。
◆ 数据集覆盖10个不同类别，物品在形状、尺寸和重量上具有多样性，增强了其实用性。
◆ 专注于现实世界的适用性，弥补了现有数据集中合成模型或专用物体可及性有限的缺陷。
◆ 该数据集非常适合用于物体检测、位姿估计和机器人操作等任务的基准测试。</td></tr>
<tr><td>2025-09-10</td><td>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</td><td>[2509.06685](http://arxiv.org/pdf/2509.06685)</td><td>VIM-GS是一种用于大场景新颖视图合成的视觉-惯性单目高斯泼溅框架。其核心贡献是解决了单目图像因缺乏准确深度信息而导致渲染质量差的问题。具体创新点包括：
◆ 提出了一种新颖的深度生成框架，巧妙地融合了来自视觉-惯性SLAM的稀疏但精确的深度，与来自大基础模型的稠密但粗糙的深度。
◆ 设计了一种基于对象分割的深度传播算法，通过渲染结构化物体的像素深度，有效弥合了稀疏输入与稠密输出之间的鸿沟。
◆ 开发了一个动态深度优化模块，专门处理动态物体上不完整的SLAM深度，并进一步优化粗糙的大模型深度估计。
最终，该方法生成了高质量且准确的密集深度图，从而支持了高斯泼溅在大场景中的高清渲染，在公开和定制数据集上均展现了卓越的渲染质量。</td></tr>
<tr><td>2025-09-02</td><td>Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction</td><td>[2509.01873](http://arxiv.org/pdf/2509.01873)</td><td>该论文的核心贡献是开发了结合几何先验与深度学习的几何深度学习方法，以解决三维视觉中的关键任务。  
◆ 针对相机位姿估计、点云配准、深度预测及三维重建等任务，提出了定制化的几何深度学习模型。  
◆ 通过引入深度信息、表面法线和等变性约束等几何先验，增强了模型的表示准确性与鲁棒性。  
◆ 克服了传统方法（如SfM和SLAM）在非结构化环境中特征模糊和几何细节缺失的局限性。  
◆ 在文化遗产保护和VR/AR等实际应用中验证了方法的有效性，推动了三维映射与场景重建技术的发展。</td></tr>
<tr><td>2025-08-25</td><td>SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</td><td>[2508.17972](http://arxiv.org/pdf/2508.17972)</td><td>SAIL-Recon提出了一种用于大规模运动恢复结构（SfM）的前馈Transformer方法，有效解决了现有场景回归方法难以处理大量输入图像的问题。

◆ 核心创新在于将视觉定位能力融入场景回归网络，通过结合两种范式的优势来提升处理大规模场景的能力。
◆ 方法首先从锚图像子集计算出一个神经场景表示，然后基于此表示对回归网络进行微调，以重建所有输入图像。
◆ 该方法不仅能够高效扩展到大规模场景，还在相机姿态估计和新视图合成任务上取得了最先进的性能。
◆ 在多个权威数据集（如TUM-RGBD、CO3Dv2和Tanks &amp; Temples）上的综合实验验证了其有效性和优越性。</td></tr>
<tr><td>2025-08-25</td><td>HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images</td><td>[2508.16465](http://arxiv.org/pdf/2508.16465)</td><td>该论文提出了一种名为HOSt3R的、无需关键点检测的手-物三维重建方法。其核心贡献在于摆脱了现有方法对关键点检测技术的依赖，从而提升了在复杂场景下的鲁棒性和泛化能力。
◆ 提出了一种无需关键点检测器（keypoint-free）的鲁棒方法，直接从单目运动视频中估计手和物体的三维变换。
◆ 解决了现有基于SfM和手部关键点优化的方法在物体几何多样、纹理弱以及手物严重遮挡情况下的失效问题。
◆ 将所提的变换估计方法与一个多视图重建流程集成，实现了精确的手-物三维形状恢复。
◆ 该方法不受约束，不依赖预先扫描的物体模板或相机内参，具有更强的通用性和非侵入式应用潜力。
◆ 在SHOWMe基准测试中达到了最先进的性能，并在HO3D数据集上验证了其对未见物体类别的泛化能力。</td></tr>
<tr><td>2025-08-22</td><td>NeuralMeshing: Complete Object Mesh Extraction from Casual Captures</td><td>[2508.16026](http://arxiv.org/pdf/2508.16026)</td><td>该论文提出了NeuralMeshing系统，用于从日常拍摄的多段视频中自动重建物体的完整网格模型。其核心创新如下：
◆ 仅需通过普通视频而非专业扫描设备即可实现完整物体建模，大幅降低硬件门槛。
◆ 提出基于多视频融合的完整重建方案，无需依赖后处理的孔洞填充技术。
◆ 采用最少人工干预的标注方式（仅需在每段视频中指定一个已知点），支持通过标定板或AR标记实现自动化。
◆ 结合运动恢复结构（SfM）技术实现自动帧定位与三维重建。
◆ 开源系统代码，促进相关研究和应用发展。</td></tr>
<tr><td>2025-08-21</td><td>Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework</td><td>[2508.15457](http://arxiv.org/pdf/2508.15457)</td><td>该论文针对3D高斯泼溅（3DGS）在极端稀疏视角（如仅2个训练视图）下因运动恢复结构（SfM）初始化失败导致的渲染质量下降问题，提出了一种无需SfM的新型框架。其核心创新点包括：
◆ 提出密集立体模块替代SfM，通过渐进式相机姿态估计和全局稠密点云重建实现初始化。
◆ 设计连贯视图插值模块，通过插值相机姿态并生成视角一致的内容作为额外监督信号，缓解稀疏输入的信息稀缺问题。
◆ 引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化，显著提升几何结构和渲染内容的质量。
实验表明，该方法在极端稀疏条件下PSNR指标提升2.75dB，合成图像畸变小且高频细节丰富，显著优于现有技术。</td></tr>
<tr><td>2025-08-20</td><td>GeMS: Efficient Gaussian Splatting for Extreme Motion Blur</td><td>[2508.14682](http://arxiv.org/pdf/2508.14682)</td><td>GeMS是首个直接从极端运动模糊图像中进行3D高斯溅射（3DGS）重建的框架，无需依赖任何清晰图像。其核心创新点包括：
◆ 提出VGGSfM，一种基于深度学习的运动恢复结构（SfM）方法，直接从模糊输入中估计相机位姿并生成点云。
◆ 引入3DGS-MCMC方法，将高斯分布视为概率分布样本进行初始化，取代了依赖启发式 densification 和 pruning 的传统策略。
◆ 联合优化相机轨迹与高斯参数，实现更稳定的三维场景重建。
◆ 进一步提出GeMS-E变体，集成事件相机数据，通过事件双积分去模糊（EDI）生成更清晰图像以优化初始估计。
该框架在合成与真实数据集上实现了最先进的性能，解决了严重模糊下3DGS重建的根本性挑战。</td></tr>
<tr><td>2025-08-19</td><td>Smooth Flow Matching</td><td>[2508.13831](http://arxiv.org/pdf/2508.13831)</td><td>本文提出Smooth Flow Matching（SFM）框架，用于解决函数型数据的生成建模问题。其核心贡献与创新点包括：
◆ 专为函数型数据设计，突破传统方法对高斯性或低秩假设的限制，构建半参数Copula流生成无限维非高斯函数数据。
◆ 计算效率高且能处理不规则采样观测，直接保证生成函数的平滑性，克服了现有深度生成模型在函数数据场景中的适用性局限。
◆ 通过模拟实验验证了SFM在合成数据质量和计算效率方面的优势，并在MIMIC-IV电子健康记录数据上成功生成了高质量临床轨迹替代数据。
◆ 为隐私敏感场景下的统计分析提供了实用灵活的解决方案，显著提升了电子健康记录等函数型数据在临床应用中的效用价值。</td></tr>
<tr><td>2025-08-17</td><td>HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization</td><td>[2508.12292](http://arxiv.org/pdf/2508.12292)</td><td>◆ 提出HuBERT-VIC模型，通过方差-不变性-协方差正则化（VICReg）提升语音基础模型在噪声环境下的鲁棒性。  
◆ 首次将VICReg目标应用于语音领域，调整噪声语音表征的统计特性，增强模型对多样化声学特征的捕捉能力。  
◆ 通过联合优化方差、不变性和协方差约束，有效提升模型在不同噪声类型中的泛化性能。  
◆ 在HuBERT模型上验证有效性，相比基线模型，LibriSpeech测试集上相对性能提升23.3%（clean）和13.2%（other）。  
◆ 为解决语音基础模型因依赖干净数据训练导致的噪声场景性能下降问题提供了新思路。</td></tr>
<tr><td>2025-08-17</td><td>What do Speech Foundation Models Learn? Analysis and Applications</td><td>[2508.12255](http://arxiv.org/pdf/2508.12255)</td><td>◆ 提出轻量级分析框架，结合统计工具和无训练任务，系统研究语音基础模型（SFMs）各层编码的声学和语言学知识。  
◆ 首次对多种SFMs和统计工具进行横向对比研究，揭示模型内部知识表示与下游任务性能的关联性。  
◆ 针对口语理解（SLU）领域数据匮乏问题，创新性地贡献了口语命名实体识别（NER）和定位（NEL）任务，扩充SLU评测基准。  
◆ 验证端到端SFM模型在SLU任务上的优越性，其性能超越传统级联式（语音识别+文本模型）方法。  
◆ 全面评估不同SFMs及适配策略对SLU任务的影响，为模型选择提供实证依据。  
◆ 通过工具链和数据集的双重创新，推动社区对SFMs的机理理解与实用化开发。</td></tr>
<tr><td>2025-08-10</td><td>GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction</td><td>[2508.07355](http://arxiv.org/pdf/2508.07355)</td><td>◆ 提出GS4Buildings方法，利用语义3D建筑模型作为先验，增强高斯泼溅（GS）在大规模复杂城市场景中的重建能力，解决传统方法因遮挡导致的不完整问题。  
◆ 直接基于低细节层次（LoD2）语义建筑模型初始化高斯分布，替代传统运动恢复结构（SfM）流程，简化重建流程并提升鲁棒性。  
◆ 通过建筑几何生成先验深度和法线图，并将其融入优化过程，显著提升表面一致性和结构准确性。  
◆ 引入可选建筑聚焦模式，仅重建建筑区域，减少71.8%的高斯基元数量，实现更高效紧凑的表示。  
◆ 在城市场景数据集上验证，重建完整度提升20.5%，几何精度提高32.8%，为智慧城市和数字孪生等应用提供新思路。</td></tr>
<tr><td>2025-08-09</td><td>Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View</td><td>[2508.06968](http://arxiv.org/pdf/2508.06968)</td><td>◆首次在真实超180度鱼眼图像上评估两种鱼眼兼容的3D高斯泼溅方法（Fisheye-GS和3DGUT），填补了极端畸变场景下的技术空白。  
◆通过室内外200度鱼眼相机实测，系统分析不同视场角（200/160/120度）下两种方法的性能平衡：Fisheye-GS在160度表现最佳，而3DGUT在全200度下仍保持稳定高质量。  
◆提出基于UniK3D预测的深度初始化策略，仅需2-3张鱼眼图即可生成稠密点云，克服传统SfM在强畸变场景失效的问题。  
◆验证UniK3D在未训练真实鱼眼数据的情况下，对雾霾、眩光、天空等复杂场景仍能实现与SfM相当的3D重建质量。  
◆研究成果证实了鱼眼3DGS方法在稀疏高畸变图像中进行广角3D重建的实用价值，为实际应用提供重要参考。</td></tr>
<tr><td>2025-08-07</td><td>EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery</td><td>[2508.05205](http://arxiv.org/pdf/2508.05205)</td><td>◆提出EndoMatcher，一种基于多领域预训练的内窥镜图像通用匹配器，通过大规模跨域数据解决内窥镜图像匹配泛化性难题。  
◆首创双分支视觉Transformer架构，结合多尺度特征提取与双重交互模块，显著提升弱纹理、大视角变化等复杂内窥镜场景的匹配鲁棒性。  
◆构建首个多领域内窥镜匹配数据集Endo-Mix6，包含6个领域约120万真实/合成图像对，通过运动恢复结构和模拟变换生成标注，解决数据稀缺与领域多样性不足问题。  
◆设计渐进式多目标训练策略，有效应对跨域数据规模差异、分布偏移和误差不平衡问题，实现不同领域的均衡学习与表征优化。  
◆在零样本设置下实现跨器官和成像条件的泛化能力，在三个基准数据集上以140%-201%的匹配内点提升率和9.4%的方向预测准确率超越现有最优方法。</td></tr>
<tr><td>2025-08-07</td><td>Refining Gaussian Splatting: A Volumetric Densification Approach</td><td>[2508.05187](http://arxiv.org/pdf/2508.05187)</td><td>◆ 提出基于惯性体积的新型密度控制方法，利用高斯函数的惯性体积指导3D高斯分布的精细化过程，克服原始3DGS密度策略的缺陷。  
◆ 首次系统研究了传统SfM与深度图像匹配(DIM)两种点云初始化方法对3DGS重建质量的影响，为初始化选择提供依据。  
◆ 在Mip-NeRF 360数据集上的实验表明，该方法显著提升了3DGS的重建质量，在不同场景中均表现出优越性能。  
◆ 改进了自适应密度控制(ADC)流程，通过更智能的密集化和修剪机制优化点基元管理，实现更高质量的新视角合成。  
◆ 提出的体积感知策略为3D高斯分布的形状和空间分布优化提供了新思路，增强了场景表示的几何准确性。</td></tr>
<tr><td>2025-08-06</td><td>Bursting at the seams: the star-forming main sequence and its scatter at z=3-9 using NIRCam photometry from JADES</td><td>[2508.04410](http://arxiv.org/pdf/2508.04410)</td><td>◆ 首次利用JADES巡天的NIRCam测光数据系统研究了红移z=3-9范围内恒星形成主序(SFMS)及其弥散，样本恒星质量完备性下限达log(M⋆/M⊙)≈8.1。  
◆ 发现10Myr时间尺度下的SFMS演化符合sSFR∝(1+z)^2.30的关系，与暗物质晕质量吸积率的理论预测高度吻合。  
◆ 揭示了SFMS归一化随恒星形成率(SFR)平均时间尺度变化的复杂规律，反映了爆发性恒星形成与上升型恒星形成历史的综合效应。  
◆ 首次定量分析SFMS弥散随SFR平均时间尺度的演变：从10Myr的0.4-0.5dex降至100Myr的0.2dex，表明短期波动主导弥散，但长期变化也存在。  
◆ 发现低质量星系中爆发性恒星形成历史更显著，并指出需要引入初始质量函数偏重、恒星形成效率提升或爆发性增强等机制来解释z&gt;10观测到的UV亮星系过量现象。  
◆ 强调在拟合SFMS时（特别是对爆发性恒星形成历史的星系），准确确定恒星质量完备性限的重要性。</td></tr>
<tr><td>2025-08-06</td><td>PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction</td><td>[2508.04236](http://arxiv.org/pdf/2508.04236)</td><td>◆ 提出PIS3R方法，首次通过深度3D重建解决大视差图像拼接难题，突破传统方法在显著视差下的性能瓶颈。  
◆ 采用视觉几何驱动的Transformer网络，从大视差图像中联合估计相机内外参数并完成稠密3D场景重建，实现几何精确的初始对齐。  
◆ 创新性地将重建的3D点云重投影到参考视图，实现像素级精准配准，保留所有像素在3D摄影测量中的几何完整性。  
◆ 设计点云条件扩散模型，针对初始拼接可能存在的空洞或噪声进行精细化修复，生成无缝高质量结果。  
◆ 实验证明该方法在大视差场景下显著优于现有方法，且输出结果可直接支持SfM等下游3D视觉任务，具有实用价值。</td></tr>
<tr><td>2025-08-03</td><td>CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes</td><td>[2508.01936](http://arxiv.org/pdf/2508.01936)</td><td>◆ 提出了一种新颖的多高度相机位姿估计系统CVD-SfM，专门针对稀疏图像输入下不同高度场景的鲁棒精准定位问题。  
◆ 创新性地将跨视角Transformer、深度特征与运动恢复结构(SfM)技术融合到统一框架中，有效应对复杂环境条件和视角变化。  
◆ 为解决该领域数据稀缺问题，专门收集并发布了两个新的多高度相机位姿估计数据集，为后续研究提供基准平台。  
◆ 通过大量对比实验验证，该系统在多高度稀疏位姿估计任务中展现出优于现有方案的精度和鲁棒性。  
◆ 所提框架特别适合无人机导航、搜救行动、自动化检测等实际机器人应用场景，具有重要实用价值。</td></tr>
<tr><td>2025-08-01</td><td>Counting topological interface modes using simplicial characteristic classes</td><td>[2508.01063](http://arxiv.org/pdf/2508.01063)</td><td>◆ 提出了一种基于谱流-单极子对应关系的计算方法，用于预测厄米系统中拓扑界面模（TIMs）的数量。  
◆ 通过计算围绕外尔点的相空间球上局部极化向量复线丛的陈数，确定TIMs数量，创新性地将拓扑不变量与物理现象直接关联。  
◆ 采用离散向量丛的单纯第一陈类构造方法计算陈数，该方法具有规范不变性、无需导数运算、结构保持性强且抗噪声干扰的特点。  
◆ 算法在赤道流体波和拓扑朗缪尔回旋波的案例中成功复现了预期的TIMs数量，验证了其有效性。  
◆ 探索了该算法在实验测量中的应用潜力，通过合成示例展示了如何利用体波极化数据预测TIMs数量，为实验研究提供了新工具。</td></tr>
<tr><td>2025-08-01</td><td>3D Reconstruction via Incremental Structure From Motion</td><td>[2508.01019](http://arxiv.org/pdf/2508.01019)</td><td>这篇论文的核心贡献和创新点如下：

◆ 提出了一种增量式运动恢复结构（SfM）的详细实现方法，相比全局SfM技术更具灵活性。  
◆ 通过逐步加入新视图的方式，能够在稀疏或部分重叠的数据集中恢复场景结构和相机运动。  
◆ 重点研究了几何估计的一致性，并通过光束法平差（bundle adjustment）实现迭代优化，提升了重建精度。  
◆ 在真实数据集上验证了方法的有效性，通过重投影误差和相机轨迹一致性评估了重建质量。  
◆ 证明了增量式SfM在视觉结构化环境中是一种可靠的稀疏3D重建方法，适用于实际应用场景。</td></tr>
<tr><td>2025-07-27</td><td>RESCUE: Crowd Evacuation Simulation via Controlling SDM-United Characters</td><td>[2507.20117](http://arxiv.org/pdf/2507.20117)</td><td>这篇论文的核心贡献是提出了一种实时3D人群疏散模拟框架RESCUE，通过模拟人类感知-决策-运动（SDM）流程来提升疏散仿真的真实性和动态适应性。  

◆ 提出基于SDM流程的仿真框架，首次将3D自适应社会力模型（SFM）决策机制与个性化步态控制运动模块结合，实现更符合人类行为逻辑的疏散模拟。  
◆ 引入动态群体感知机制，支持多智能体并行运动，能适应不同地形和场景需求，突破了传统模型对复杂环境适应性的限制。  
◆ 开发个性化步态控制模块，通过考虑个体体型差异和地形影响，首次实现疏散过程中个体运动特征的差异化模拟。  
◆ 创新提出部件级受力可视化技术，为疏散分析提供直观的力学交互数据支持，辅助安全策略优化。  
◆ 实验证明该框架支持动态路径规划和实时行为调整，在崎岖地形中仍能生成视觉可信、符合现实的疏散动画。  
◆ 开源代码并验证了方法在真实性和实用性上的优势，为公共安全领域提供了新的仿真分析工具。</td></tr>
<tr><td>2025-07-22</td><td>Sparse-View 3D Reconstruction: Recent Advances and Open Challenges</td><td>[2507.16406](http://arxiv.org/pdf/2507.16406)</td><td>◆ 该论文首次将稀疏视角3D重建领域的几何方法、神经隐式模型（如NeRF）和生成式方法（如扩散模型）纳入统一框架进行系统分析。  
◆ 重点对比了不同方法在几何正则化、显式形状建模和生成推理方面的创新，揭示了它们在解决稀疏视角下浮游伪影和位姿模糊问题上的独特优势。  
◆ 提出当前方法在标准基准测试中面临的核心权衡：重建精度、计算效率和泛化能力之间的相互制约关系。  
◆ 区别于以往综述，特别强调了视觉基础模型（VFMs）和3D高斯泼溅等新兴技术在稀疏重建中的迁移应用潜力。  
◆ 明确指出该领域尚未解决的两大挑战：跨域泛化能力和无位姿约束的重建，并首次提出发展&quot;3D原生生成先验&quot;的未来方向。  
◆ 通过整合实时重建需求与无约束条件设定，为稀疏视角3D重建的工业落地提供了新的技术路线图。</td></tr>
<tr><td>2025-07-21</td><td>Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing</td><td>[2507.15683](http://arxiv.org/pdf/2507.15683)</td><td>◆ 提出Hi^2-GSLoc框架，首次将3D高斯溅射（3DGS）引入遥感视觉重定位任务，利用其紧凑的几何与外观表征能力解决传统方法精度与效率的矛盾。  
◆ 设计双层次稀疏到稠密、粗到精的定位范式：稀疏阶段通过渲染感知采样和地标引导检测器获取鲁棒初始位姿，稠密阶段通过多级栅格化匹配迭代优化位姿。  
◆ 开发分区高斯训练、GPU并行匹配和动态内存管理策略，突破大尺度遥感场景的计算瓶颈，实现高效处理高海拔变化和跨域数据。  
◆ 创新性提出高斯基元一致性渲染感知采样方法，结合可靠性验证机制，显著提升特征匹配的稳定性和位姿估计的准确性。  
◆ 在仿真数据、公开数据集和真实飞行实验中验证了方法的优越性，兼具高定位精度（竞争性指标）、召回率和计算效率，为实际遥感应用提供可靠解决方案。</td></tr>
<tr><td>2025-07-21</td><td>Few-Shot Object Detection via Spatial-Channel State Space Model</td><td>[2507.15308](http://arxiv.org/pdf/2507.15308)</td><td>◆ 提出空间-通道状态空间建模（SCSM）模块，通过联合建模空间和通道关系，解决小样本目标检测中特征提取不准确的问题。  
◆ 设计空间特征建模（SFM）模块，平衡空间关系和通道关系的学习，提升特征表示的有效性。  
◆ 创新性地将Mamba模型引入通道序列建模，提出通道状态建模（CSM）模块，利用通道间相关性动态调整通道权重。  
◆ 通过SCSM模块，模型能够自动强化有效通道特征并修正无效通道特征，从而提升小样本条件下的检测性能。  
◆ 在VOC和COCO数据集上的实验表明，该方法显著优于现有技术，实现了最先进的性能。</td></tr>
<tr><td>2025-07-20</td><td>An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks</td><td>[2507.14798](http://arxiv.org/pdf/2507.14798)</td><td>◆ 首次对DUSt3R/MASt3R/VGGT三种基于Transformer的3D重建模型在航摄影像块上进行系统性评估，填补了该领域的研究空白。  
◆ 证明这些模型在极稀疏影像（少于10张、分辨率低至518像素）下仍能生成完整稠密点云，相比传统COLMAP方法完整性提升高达50%。  
◆ 发现VGGT模型具有显著计算效率和可扩展性优势，同时提供更可靠的相机位姿估计能力。  
◆ 揭示了这些模型在高分辨率影像和大规模数据集上的局限性，表现为位姿估计可靠性随影像数量增加而下降。  
◆ 提出Transformer模型虽无法完全替代传统SfM/MVS流程，但在低分辨率、稀疏影像等挑战性场景中可作为有效补充方案。  
◆ 为航测领域提供了基于深度学习的3D重建新思路，特别适用于影像重叠率极低或纹理缺失区域的快速重建需求。</td></tr>
<tr><td>2025-07-17</td><td>Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation</td><td>[2507.13486](http://arxiv.org/pdf/2507.13486)</td><td>这篇论文的核心贡献和创新点如下：

◆ 提出了一个完整的误差传播框架，用于量化航空和无人机摄影测量中从SfM到MVS两阶段的全流程不确定性，填补了MVS阶段不确定性评估的研究空白。

◆ 针对MVS阶段非可微、多模态的特性，创新性地提出基于可靠多视角点（n≥6）的自校准方法，通过匹配代价等关键特征回归视差不确定性。

◆ 该方法直接从MVS过程中提取自包含的可靠3D点，具有自监督特性，无需外部数据即可实现不确定性建模。

◆ 提出的误差协方差矩阵建模方法严格遵循摄影测量误差传播路径，能适应不同场景的鲁棒性验证需求。

◆ 通过公开数据集验证表明，该方法在保证高边界覆盖率的同时避免了不确定性高估问题，性能优于现有方法。

◆ 首次实现了摄影测量点云逐点精度凭证的标准化输出，为场景依赖的摄影测量精度评估提供了可认证的量化工具。</td></tr>
<tr><td>2025-07-16</td><td>Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models</td><td>[2507.12595](http://arxiv.org/pdf/2507.12595)</td><td>◆ 提出多语言语音基础模型（SFMs）在情感伪造检测（EFD）中的有效性假设，认为其跨语言预训练能更好捕捉音高、音调和强度的细微变化。  
◆ 通过全面对比实验验证了多语言SFMs在同语言（域内）和跨语言（域外）场景下的优越性能。  
◆ 创新性地提出THAMA融合方法，结合Tucker分解和Hadamard乘积，实现基础模型的高效互补融合。  
◆ THAMA与多语言SFMs协同合作，在域内和域外评测中均达到最优性能，超越单一模型、基线融合方法和先前SOTA方法。  
◆ 首次系统探索了多语言SFMs在EFD任务中的潜力，为跨语言情感伪造检测提供了新思路。</td></tr>
<tr><td>2025-07-16</td><td>BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images</td><td>[2507.12095](http://arxiv.org/pdf/2507.12095)</td><td>这篇论文的核心贡献是通过稀疏视角输入实现高精度的3D车辆重建，解决了现有方法依赖密集视角的局限性。  

◆提出基于深度图和鲁棒位姿估计架构的新方法，能够从稀疏输入合成新视角并增强训练数据。  
◆改进高斯泼溅技术，引入选择性光度损失函数，仅对高置信度像素进行计算，提升重建质量。  
◆采用DUSt3R架构替代传统运动恢复结构（SfM）流程，显著提高了相机位姿估计的准确性。  
◆发布了一个包含合成和真实公共交通工具车辆的新数据集，支持方法的全面评估。  
实验结果表明，该方法在多个基准测试中达到最先进性能，尤其在输入条件受限时仍能实现高质量重建。</td></tr>
<tr><td>2025-07-23</td><td>Spatial Frequency Modulation for Semantic Segmentation</td><td>[2507.11893](http://arxiv.org/pdf/2507.11893)</td><td>◆ 提出空间频率调制（SFM）方法，通过在下采样前将高频特征调制到低频，上采样时再解调回来，有效解决语义分割中高频信息因下采样导致的混叠失真问题。  
◆ 设计自适应重采样（ARS）模块，通过密集采样高频区域来缩放信号，利用频率缩放特性降低高频成分的频率，实现高效调制。  
◆ 提出多尺度自适应上采样（MSAU）模块，通过非均匀上采样解调特征并恢复高频信息，同时利用多尺度密集与稀疏采样区域的交互增强分割精度。  
◆ 模块设计轻量且通用，可无缝集成到CNN和Transformer等多种架构中，扩展性强。  
◆ 通过特征可视化验证了该方法能有效缓解混叠并保留细节，进一步在图像分类、对抗鲁棒性、实例分割和全景分割等任务中验证了其广泛适用性。</td></tr>
<tr><td>2025-07-20</td><td>Supporting SENCOTEN Language Documentation Efforts with Automatic Speech Recognition</td><td>[2507.10827](http://arxiv.org/pdf/2507.10827)</td><td>这篇论文的核心贡献是通过自动语音识别（ASR）技术支持濒危语言SENĆOTEN的文档化工作，具体创新点如下：

◆ 提出了一种ASR驱动的文档化流程，结合文本转语音（TTS）系统增强有限的语言数据，解决了数据不足的问题。  
◆ 利用跨语言迁移学习技术，借助语音基础模型（SFMs）提升ASR在低资源语言上的性能。  
◆ 引入n-gram语言模型，通过浅层融合或n-best恢复技术，最大化利用现有词汇数据。  
◆ 在SENĆOTEN数据集上实现了19.34%的词错误率（WER）和5.09%的字符错误率（CER），经过过滤后分别提升至14.32%和3.45%，展示了方法的有效性。  
◆ 特别针对SENĆOTEN的多合成结构和重音驱动的音位变换等语言特点，优化了ASR系统的适应性。  
◆ 为濒危语言的数字化保护和教学资源创建提供了可行的技术方案。</td></tr>
<tr><td>2025-07-11</td><td>Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT</td><td>[2507.08448](http://arxiv.org/pdf/2507.08448)</td><td>◆ 提出了前馈式3D重建新范式，以DUSt3R为代表，通过单一前向传播直接从无约束图像中联合推断相机位姿和稠密几何结构，颠覆了传统迭代优化流程。  
◆ 采用基于Transformer的对应关系建模技术，实现了跨图像的高效特征匹配，显著提升了纹理缺失等挑战性场景的鲁棒性。  
◆ 设计了联合位姿与几何回归机制，将传统多阶段流程（如SfM+MVS）整合为端到端网络，大幅简化了工作流程并降低计算成本。  
◆ 系统分析了从双视图到多视图的扩展策略，为不同应用场景提供了灵活的技术路径。  
◆ 通过与传统方法（如SfM）和早期学习型方法（如MVSNet）的对比，阐明了该范式在效率、泛化性和易用性方面的突破性优势。  
◆ 探讨了动态场景处理、模型精度与可扩展性等未来挑战，为该领域的进一步发展指明了方向。</td></tr>
<tr><td>2025-07-04</td><td>MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion</td><td>[2507.03306](http://arxiv.org/pdf/2507.03306)</td><td>◆ 提出了一种专为多相机系统设计的全局运动平均框架，通过解耦旋转平均和混合平移平均模块提升传统全局SfM的鲁棒性问题。  
◆ 采用分层策略的旋转平均方法：先估计刚性相机单元内的相对旋转，再计算全局刚性单元旋转，优化多相机系统的旋转估计精度。  
◆ 创新性融合相机间约束和相机-点约束的平移平均模块，通过凸距离目标函数初始化相机位姿和3D点，并采用无偏非双线性角度目标函数进行细化。  
◆ 在保持与增量式SfM相当精度的前提下，显著提升计算效率，实验证明其在大规模数据集上优于现有全局SfM方法。  
◆ 框架充分利用多相机系统的固有相对位姿约束，为自动驾驶和机器人环境感知提供了更鲁棒的实时SfM解决方案。  
◆ 开源代码便于学术和工业界应用验证，推动多相机SfM技术的实际部署。</td></tr>
<tr><td>2025-06-30</td><td>Towards Initialization-free Calibrated Bundle Adjustment</td><td>[2506.23808](http://arxiv.org/pdf/2506.23808)</td><td>◆ 提出了一种无需初始化的标定束调整方法，能够在初始重建阶段直接利用相机标定信息，生成接近度量精度的重建结果（仅差一个相似变换）。  
◆ 创新性地引入具有标定信息的成对相对旋转估计，这些旋转估计仅对相似变换保持不变，从而推动解保持真实场景的度量特征。  
◆ 将旋转平均技术整合到伪物体空间误差（pOSE）框架中，实现了标定信息与初始化无关的SfM（运动恢复结构）流程。  
◆ 实验证明该方法能够可靠优化目标函数，即使从随机初始解出发也能高概率收敛到全局最优，获得精确的接近度量重建。  
◆ 相比现有基于pOSE的方法（仅能获得射影变换解且需要更多数据），新方法显著提升了重建精度和效率。</td></tr>
<tr><td>2025-06-30</td><td>AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention</td><td>[2506.23611](http://arxiv.org/pdf/2506.23611)</td><td>◆ 提出AttentionGS框架，首次实现无需高质量初始点云的3D高斯泼溅重建，突破传统3DGS对SfM点云的强依赖。  
◆ 创新性引入几何注意力机制，在训练初期快速恢复场景全局结构，解决随机初始化导致的收敛难题。  
◆ 设计渐进式纹理注意力模块，在训练后期精细化局部细节，显著提升纹理缺失场景的渲染质量。  
◆ 开发不透明度加权梯度策略，优化高斯分布致密化过程，实现更精准的表面重建。  
◆ 在标准数据集上全面超越现有方法，尤其在低纹理/受限视角场景中表现突出，验证了方案的鲁棒性。  
◆ 为实际应用中复杂场景的3D重建提供新思路，扩展了3DGS技术的适用边界。</td></tr>
<tr><td>2025-06-27</td><td>Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras</td><td>[2506.22069](http://arxiv.org/pdf/2506.22069)</td><td>◆ 提出了一种基于单扫描线投影交点的新方法，用于估计滚动快门相机间的相对位姿，无需显式建模相机运动。  
◆ 创新性地实现了单视图内扫描线的相对位姿估计，扩展了滚动快门相机的应用场景。  
◆ 该方法作为滚动快门运动恢复结构（SfM）的基础模块，支持独立计算每条扫描线的位姿，且无需运动模型假设。  
◆ 在已知内参和无镜头畸变的条件下，分类了通用和特定场景（如平行线和已知重力方向）的最小求解器。  
◆ 针对平行线场景，开发了带/不带重力先验的最小求解器，通过将其与1D相机的2D结构估计问题关联实现创新求解。  
◆ 在Fastec数据集上的实验验证了该方法用于滚动快门SfM初始化的可行性，展现了进一步开发的潜力。</td></tr>
<tr><td>2025-06-24</td><td>ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes</td><td>[2506.21629](http://arxiv.org/pdf/2506.21629)</td><td>◆ 提出了一种无需SfM预处理的方法ICP-3DGS，将迭代最近点（ICP）与基于优化的位姿细化相结合，解决了大范围无边界场景中相机位姿估计的难题。  
◆ 创新性地将ICP引入3D高斯泼溅（3DGS）框架，实现了在大幅度相机运动下的高精度位姿估计，突破了传统神经渲染对SfM先验的依赖。  
◆ 设计了基于体素的场景致密化策略，有效指导大规模场景的重建过程，提升了场景覆盖率和几何细节的完整性。  
◆ 在室内外多种尺度场景的实验中，ICP-3DGS在相机位姿估计和新视角合成任务上均优于现有方法，证明了其鲁棒性和泛化能力。  
◆ 开源了完整代码，为后续研究提供了可复现的基础，推动了无预计算位姿的神经渲染技术的发展。</td></tr>
<tr><td>2025-07-08</td><td>Wild refitting for black box prediction</td><td>[2506.21460](http://arxiv.org/pdf/2506.21460)</td><td>◆ 提出了一种名为&quot;wild refitting&quot;的高效计算流程，仅需单次数据集和预测方法的黑箱访问，通过残差计算、对称化和缩放三个步骤，为惩罚非参数估计提供实例级均方预测误差的高概率上界。  
◆ 创新性地采用Rademacher残差对称化技术（类似wild bootstrap变体），通过预定义缩放因子ρ调整残差，构建以当前估计为中心的修正预测问题。  
◆ 在允许噪声异质性的较温和条件下，理论证明了该方法性能：当wild噪声尺度ρ选择适当时，wild refit能确保预测误差上界的有效性。  
◆ 为实际应用提供关键设计指导，包括残差构建方法、wild子问题中噪声缩放量的选择依据，以及黑箱程序局部稳定性的分析框架。  
◆ 展示了方法在多个领域的适用性，如基于结构化矩阵惩罚的非刚性运动恢复、深度神经网络先验的即插即用图像修复，以及核方法的随机草图技术。</td></tr>
<tr><td>2025-06-24</td><td>Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications</td><td>[2506.19491](http://arxiv.org/pdf/2506.19491)</td><td>◆ 提出了一种将神经三维重建（N3DR）技术与小型无人机系统结合的新方法，用于精细三维数字重建静态小物体。  
◆ 设计并实现了一套基于N3DR的流程，整合了Instant-ngp、Nerfacto和Splatfacto等先进模型，显著提升了重建质量。  
◆ 通过多无人机协同采集图像，解决了小型无人机在动态飞行和功耗限制下的自主性与任务能力问题。  
◆ 采用多种图像和点云指标评估模型性能，并与传统运动恢复结构（SfM）算法对比，验证了N3DR的优越性。  
◆ 实验证明该方案能支持高精度三维建模和异常检测，拓展了小型无人机在受限环境中的应用潜力。  
◆ 整体研究展示了N3DR技术在提升微型无人机系统能力方面的广阔前景。</td></tr>
<tr><td>2025-06-23</td><td>ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs</td><td>[2506.18792](http://arxiv.org/pdf/2506.18792)</td><td>◆ 提出ViDAR框架，首次将个性化扩散模型引入单目视频的4D重建任务，通过生成伪多视角监督信号解决单目输入的结构-运动歧义问题。  
◆ 创新性地利用场景特定特征进行条件扩散，在保持外观细节的同时有效缓解单目模糊性导致的伪影问题。  
◆ 设计扩散感知损失函数，专门处理扩散生成视图的时空不一致性，提升合成视图与真实几何的对齐精度。  
◆ 提出相机位姿优化策略，动态调整合成视角与底层场景几何的匹配关系，增强动态区域的几何一致性。  
◆ 在极端视角变化的DyCheck基准测试中全面超越现有方法，尤其在运动丰富区域重建质量上取得显著提升。  
◆ 发布新评测基准，首次针对场景中高动态部分的重建性能进行系统化比较，推动领域评估标准发展。</td></tr>
<tr><td>2025-06-23</td><td>Room temperature spin injection into commercial VCSELs at non-resonant wavelengths</td><td>[2506.18376](http://arxiv.org/pdf/2506.18376)</td><td>◆ 首次在室温下实现了对商用垂直腔面发射激光器（VCSEL）的非共振波长自旋注入，突破了传统共振波长限制。  
◆ 通过794 nm和810 nm光泵浦实验，观察到20%和5%的最大圆偏振度差异，揭示了波长对自旋注入效率的影响机制。  
◆ 结合量子阱光学取向研究，证实长波长激发会导致自旋注入效率降低，为器件优化提供理论依据。  
◆ 扩展自旋翻转模型（SFM），首次纳入实际激发条件，使理论模型能准确复现实验观测趋势。  
◆ 该成果为自旋激光器的低阈值、高速调制和全光数据处理等应用提供了新的实现路径。</td></tr>
<tr><td>2025-06-11</td><td>OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary</td><td>[2506.09448](http://arxiv.org/pdf/2506.09448)</td><td>◆ 提出了一种将上下文偏置（CB）方法与预训练的开放Whisper风格语音模型（OWSM v3.1）结合的新方法，无需微调预训练参数。  
◆ 通过利用预训练语音基础模型（SFMs）的嵌入知识，即使在小数据集上也能有效提升罕见词和未登录词的识别准确率。  
◆ 该方法在保持SFMs原有优势的同时，显著降低了偏置词错误率（B-WER），在LibriSpeech测试集上提升11.6个百分点。  
◆ 整体词错误率（WER）改善0.9个百分点，同时实时因子（RTF）减少7.5%，兼顾性能与效率。  
◆ 实验证明，该方法优于从头训练的CB方法，凸显了预训练模型知识迁移的重要性。</td></tr>
<tr><td>2025-06-06</td><td>SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction</td><td>[2506.05935](http://arxiv.org/pdf/2506.05935)</td><td>◆提出SurGSplat新范式，通过渐进式几何约束优化3D高斯泼溅(3DGS)技术，解决内窥镜场景稀疏特征和光照不均导致的传统SfM方法重建失败问题。  
◆首创将几何约束融入3DGS优化过程，实现血管等关键解剖结构的高精度重建，显著提升手术场景的视觉清晰度。  
◆开发渐进式优化框架，逐步细化重建细节，在保持实时性的同时突破现有方法在复杂手术环境中的性能瓶颈。  
◆实验证明该方法在新型视角合成(NVS)和位姿估计精度上均超越现有技术，为术中导航提供高保真重建解决方案。  
◆通过专属几何约束机制有效克服内窥镜图像特征稀疏的固有挑战，为微创手术提供更可靠的3D场景理解支持。  
◆开源项目网站提供完整技术细节和可视化结果，推动手术导航领域的可重复研究。</td></tr>
<tr><td>2025-06-05</td><td>On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images</td><td>[2506.05558](http://arxiv.org/pdf/2506.05558)</td><td>◆ 提出实时重建方法，能够在图像采集完成后立即生成相机位姿和训练好的3D高斯泼溅模型，显著缩短传统方法所需的分钟到小时级计算时间。  
◆ 针对大场景和宽基线图像序列，设计了快速初始位姿估计方案，结合学习特征和GPU友好的小型束调整，提升处理效率。  
◆ 创新性地采用高斯图元位置与形状的直接采样方法，通过增量式生成图元加速训练过程，实现位姿与高斯图元的快速联合优化。  
◆ 提出可扩展的辐射场构建技术，通过渐进式聚类将3DGS图元存储在锚点中并从GPU卸载，有效管理大规模场景的内存需求。  
◆ 引入动态图元合并机制，根据视点需求自适应调整3DGS规模，保持渲染质量的同时优化计算资源使用。  
◆ 实验验证该方法能实时处理多种采集场景和不同规模的数据集，在速度、图像质量或两者兼备方面优于仅针对特定场景的现有方法。</td></tr>
<tr><td>2025-06-05</td><td>SupeRANSAC: One RANSAC to Rule Them All</td><td>[2506.04803](http://arxiv.org/pdf/2506.04803)<br><a href=''>[代码]</a></td><td>◆ SupeRANSAC提出了一种统一的RANSAC框架，解决了传统RANSAC在不同视觉任务中性能不稳定的问题。  
◆ 通过系统分析RANSAC在特定视觉任务（如单应性矩阵、基础矩阵、位姿估计等）中的有效技术，优化了整体流程。  
◆ 相比现有最佳方法，SupeRANSAC在基础矩阵估计任务中平均提升了6个AUC点，表现出更高的准确性。  
◆ 该框架克服了现有库（如OpenCV和PoseLib）在不同任务中表现不一致的缺陷，实现了跨任务的稳定高性能。  
◆ 论文提供了详细的实现细节和任务特定优化，为鲁棒估计领域提供了可复现的高效解决方案。  
◆ 开源代码便于社区验证和应用，已在多个数据集和问题上展示了显著的性能提升。</td></tr>
<tr><td>2025-06-04</td><td>Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation</td><td>[2506.04225](http://arxiv.org/pdf/2506.04225)</td><td>◆ Voyager提出了一种新颖的视频扩散框架，能够从单张图像生成用户自定义相机路径下的3D点云序列，实现端到端的世界一致场景生成，无需依赖传统3D重建流程。  
◆ 该框架首次整合了RGB与深度视频的联合生成，通过现有世界观测条件确保全局一致性，解决了长序列生成中的累积误差问题。  
◆ 创新性地采用带点云剔除的世界缓存机制和自回归推理方法，支持上下文感知的迭代场景扩展，实现超长距离（&gt;100米）的3D场景探索。  
◆ 开发了可扩展的数据引擎，通过自动化相机位姿估计和深度预测，构建大规模无人工标注的训练数据集，显著降低数据获取成本。  
◆ 在视觉质量和几何精度上超越现有方法，支持虚拟现实、游戏开发等需要动态3D场景构建的应用场景。  
◆ 整体架构摒弃了多阶段处理流程，首次实现单模型直接输出几何一致的可探索3D场景，为生成式3D建模开辟新方向。</td></tr>
<tr><td>2025-06-04</td><td>Accelerating SfM-based Pose Estimation with Dominating Set</td><td>[2506.03667](http://arxiv.org/pdf/2506.03667)</td><td>◆ 提出基于支配集的预处理技术，显著加速基于SfM的位姿估计过程，适用于AR/VR和机器人等实时应用场景。  
◆ 首次将图论中的支配集概念引入SfM模型优化，在不显著损失精度前提下实现计算效率提升。  
◆ 在OnePose数据集上验证了方法的普适性，兼容多种SfM位姿估计技术，展现广泛适用性。  
◆ 实现1.5-14.48倍的加速效果，同时将参考图像数量和点云规模分别缩减17-23倍和2.27-4倍。  
◆ 通过平衡速度与精度，为实时3D位姿估计提供了高效解决方案，突破现有技术瓶颈。</td></tr>
<tr><td>2025-06-03</td><td>Nearby dwarf galaxies with extreme star formation rates: a window into dwarf-galaxy evolution in the early Universe</td><td>[2506.03265](http://arxiv.org/pdf/2506.03265)</td><td>◆ 研究发现附近低光度矮星系（质量10^7-10^8太阳质量）存在极端恒星形成率（0.1-3太阳质量/年），可作为早期宇宙（z~5.5）矮星系的类比样本。  
◆ 通过对比正常矮星系样本，发现极端恒星形成率并非由星系结构紧凑性或特殊环境（如靠近节点/纤维结构）驱动。  
◆ 揭示具有极端恒星形成率的矮星系中相互作用星系和早型形态比例显著升高（分别增加约5.6倍和9倍），表明星系相互作用是关键触发机制。  
◆ 指出当前基于中低红移数据的主序星形成率演化模型会低估早期宇宙（z~5.5）矮星系的恒星形成率。  
◆ 提出早期宇宙矮星系通过更高气体丰度与频繁相互作用的共同作用，驱动其恒星质量快速累积的新演化图景。</td></tr>
<tr><td>2025-06-02</td><td>Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent</td><td>[2506.01940](http://arxiv.org/pdf/2506.01940)</td><td>◆ 提出了一种快速且鲁棒的各向异性旋转平均方法，通过分析块坐标下降法家族，简化了原有和弦距离优化的复杂形式。  
◆ 首次将各向异性扩展应用于块坐标下降法，开发出一个通用的快速求解器，显著提升了计算效率。  
◆ 将该求解器集成到大规模鲁棒旋转平均流程中，解决了传统方法在问题规模增大时计算效率低下的问题。  
◆ 通过实验验证，该方法在公开的结构运动数据集上达到了最先进的性能表现。  
◆ 克服了传统局部方法对初始化的敏感性，避免了最小生成树方法中常见的漂移累积和局部极小值陷阱问题。  
◆ 在全局最优性、鲁棒性和效率之间取得了良好平衡，为各向异性旋转平均提供了实用解决方案。</td></tr>
<tr><td>2025-06-03</td><td>Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC</td><td>[2505.24200](http://arxiv.org/pdf/2505.24200)</td><td>◆ 提出多种微调策略（冻结上游训练、部分微调、低秩适应）优化多语言语音基础模型（SFM）在ML-SUPERB 2.0上的表现。  
◆ 采用数据增强技术缓解少样本场景下的性能下降问题，提升模型在资源受限条件下的鲁棒性。  
◆ 创新性地引入语言识别（LID）感知的CTC损失函数作为正则化手段，联合优化LID和ASR任务。  
◆ 在ML-SUPERB 2.0基准上实现显著提升：LID准确率相对提高14%，ASR字错误率（CER）相对降低30%。  
◆ 综合方法在Interspeech 2025 ML-SUPERB 2.0挑战赛中斩获第二名，验证了策略的有效性。</td></tr>
<tr><td>2025-05-29</td><td>Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping</td><td>[2505.23756](http://arxiv.org/pdf/2505.23756)</td><td>◆ 提出Rooms from Motion (RfM)方法，首次实现无需先验相机位姿的室内3D物体检测，通过物体中心化框架同时完成定位与建图。  
◆ 创新性地用图像衍生的3D定向包围盒替代传统基于2D关键点的匹配器，从而估计相机位姿并生成全局语义3D物体地图。  
◆ 在已有相机位姿时，通过全局3D包围盒优化显著提升地图质量，优于依赖点云或多视图的过参数化方法。  
◆ 实现稀疏定位与参数化建图，其计算复杂度仅与场景中物体数量成正比，效率更高。  
◆ 在CA-1M和ScanNet++数据集上，RfM的定位性能与地图质量均超越基于点云和密集体素的领先方法。  
◆ 扩展Cubify Anything至全场景，建立通用的物体中心化表征，为场景理解提供新范式。</td></tr>
<tr><td>2025-05-30</td><td>FAMA: The First Large-Scale Open-Sc...</td><td>[2505.22759](http://arxiv.org/pdf/2505.22759)<br><a href=''>[代码]</a></td><td>◆FAMA是首个面向英语和意大利语的大规模开源语音基础模型，填补了语音领域开放科学的空白。  
◆创新性地使用15万+小时开源语音数据训练，并发布了包含1.6万小时清洗和伪标注数据的新数据集。 ...</td></tr>
<tr><td>**2025-05-28**</td><td>**UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images**</td><td>[2505.22098](http://arxiv.org/abs/2505.22098)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule**</td><td>[2505.22089](http://arxiv.org/abs/2505.22089)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-30**</td><td>**Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations**</td><td>[2505.21356](http://arxiv.org/abs/2505.21356)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-27**</td><td>**Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting**</td><td>[2505.20729](http://arxiv.org/abs/2505.20729)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**Robust fine-tuning of speech recognition models via model merging: application to disordered speech**</td><td>[2505.20477](http://arxiv.org/abs/2505.20477)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-29**</td><td>**Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud**</td><td>[2505.19854](http://arxiv.org/abs/2505.19854)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-25**</td><td>**Improving Novel view synthesis of 360$^\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images**</td><td>[2505.19264](http://arxiv.org/abs/2505.19264)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-24**</td><td>**Token-Level Logits Matter: A Closer Look at Speech Foundation Models for Ambiguous Emotion Recognition**</td><td>[2505.18484](http://arxiv.org/abs/2505.18484)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-22**</td><td>**Tracking the Flight: Exploring a Computational Framework for Analyzing Escape Responses in Plains Zebra (Equus quagga)**</td><td>[2505.16882](http://arxiv.org/abs/2505.16882)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-21**</td><td>**A Taxonomy of Structure from Motion Methods**</td><td>[2505.15814](http://arxiv.org/abs/2505.15814)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-18**</td><td>**Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis**</td><td>[2505.12226](http://arxiv.org/abs/2505.12226)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-15**</td><td>**Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis**</td><td>[2505.10751](http://arxiv.org/abs/2505.10751)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-13**</td><td>**Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People**</td><td>[2505.08215](http://arxiv.org/abs/2505.08215)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-12**</td><td>**RDD: Robust Feature Detector and Descriptor using Deformable Transformer**</td><td>[2505.08013](http://arxiv.org/abs/2505.08013)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-12**</td><td>**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**</td><td>[2505.07373](http://arxiv.org/abs/2505.07373)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-11**</td><td>**Symmetry in Fundamental Parameters of Galaxies on the Star-forming Main Sequence**</td><td>[2505.06868](http://arxiv.org/abs/2505.06868)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-10**</td><td>**TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility**</td><td>[2505.06743](http://arxiv.org/abs/2505.06743)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-08**</td><td>**DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion**</td><td>[2505.05473](http://arxiv.org/abs/2505.05473)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-20**</td><td>**FastMap: Revisiting Dense and Scalable Structure from Motion**</td><td>[2505.04612](http://arxiv.org/abs/2505.04612)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-15**</td><td>**Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera**</td><td>[2505.03093](http://arxiv.org/abs/2505.03093)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-03**</td><td>**AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting**</td><td>[2505.01799](http://arxiv.org/abs/2505.01799)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-03**</td><td>**PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth**</td><td>[2505.01729](http://arxiv.org/abs/2505.01729)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-01**</td><td>**Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?**</td><td>[2505.00866](http://arxiv.org/abs/2505.00866)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
</tbody>
</table>
</div>

<div align='right'><a href='#top'>↑ 返回顶部</a></div>

<h2 id='visual-localization'>Visual Localization</h2>

<div class="table-container">
<table>
<thead><tr><th>日期</th><th>标题</th><th>论文与代码</th><th>摘要</th></tr></thead>
<tbody>
<tr><td>2026-02-11</td><td>DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories</td><td>[2602.10809](http://arxiv.org/pdf/2602.10809)</td><td>该论文的核心贡献是提出了一个面向视觉历史中上下文感知图像检索的新型智能体范式与评测基准，以克服现有检索系统仅关注单张图像语义匹配的局限。

◆ 提出了DeepImageSearch这一新颖的智能体范式，将图像检索重新定义为对原始视觉历史进行自主探索的任务，要求模型进行多步推理以利用隐式上下文线索。
◆ 构建了DISBench挑战性评测基准，其基于相互关联的视觉数据，强调跨时间序列的依赖关系，有效模拟了真实场景中的信息分布。
◆ 设计了一种人机协作流水线，利用视觉语言模型挖掘潜在的时空关联以生成上下文相关的查询，大幅降低了人工构建评测数据的成本。
◆ 建立了一个强大的模块化智能体基线，该基线配备细粒度工具和双记忆系统，以支持在长视觉历史中进行持久导航与推理。
◆ 通过大量实验证实，现有先进模型在该基准上表现不佳，从而凸显了将智能体推理能力融入下一代检索系统的必要性。</td></tr>
<tr><td>2026-02-09</td><td>OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval</td><td>[2602.08603](http://arxiv.org/pdf/2602.08603)</td><td>该论文提出了OSCAR框架，用于组合图像检索任务。其核心贡献与创新点如下：

◆ 首次将组合图像检索中的智能体规划问题，从启发式搜索过程重新定义为一种原则性的轨迹优化问题，为其奠定了数学基础。

◆ 提出了一种新颖的离线-在线范式。离线阶段将检索任务建模为两阶段混合整数规划问题，通过严格的布尔集合运算推导出最大化覆盖训练样本的最优轨迹。

◆ 构建了一个“黄金库”，用于存储离线阶段计算出的最优轨迹，这些轨迹在在线推理时作为上下文示例，以指导视觉语言模型规划器的决策。

◆ 该方法克服了现有统一嵌入检索的模型单一性局限，以及启发式智能体检索的试错协调次优问题，实现了优化驱动的规划。

◆ 实验表明，该框架在多个基准测试上性能超越现有技术，且仅需10%的训练数据就能达到优异效果，证明了其规划逻辑的强大泛化能力，而非对数据集的简单记忆。</td></tr>
<tr><td>2026-02-09</td><td>A Sketch+Text Composed Image Retrieval Dataset for Thangka</td><td>[2602.08411](http://arxiv.org/pdf/2602.08411)</td><td>该论文的核心贡献是构建并发布了首个面向唐卡图像的草图+文本组合检索数据集CIRThan，以推动需要细粒度语义推理和领域知识的组合图像检索研究。

◆ 创建了首个针对唐卡这一具有复杂结构、密集符号和特定文化语义领域的草图+文本组合检索数据集CIRThan。
◆ 数据集包含2,287张高质量唐卡图像，每张均配有人工绘制的草图和三层次（整体、局部、细节）的文本描述，支持同时表达结构意图和多级语义的组合查询。
◆ 该数据集突破了现有基准主要关注通用域图像且仅依赖简短文本修改的局限，为需要细粒度推理和领域知识的检索场景提供了支持。
◆ 通过基准测试发现，现有主要针对通用域开发的CIR方法难以有效对齐草图抽象和层次化文本语义，尤其在无领域监督下表现不佳。
◆ CIRThan为文化遗产及其他知识特定视觉领域的草图+文本组合检索、层次语义建模和多模态检索研究提供了有价值的基准。</td></tr>
<tr><td>2026-02-09</td><td>UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science</td><td>[2602.08342](http://arxiv.org/pdf/2602.08342)</td><td>该论文的核心贡献在于为城市科学构建了一个空间接地的多模态学习与评估框架。其创新点主要体现在以下三个方面：

◆ 提出了一个新颖的空间接地数据集UGData，它首次将街景图像与结构化空间图进行显式对齐，并通过空间推理路径和空间上下文描述提供监督，从而捕捉距离、方向、连通性等超越图像内容的空间关系。

◆ 提出了一种名为UGE的两阶段训练策略，该策略结合指令引导的对比学习和基于图的空间编码，逐步且稳定地对齐图像、文本和空间结构，从而学习可迁移的多模态嵌入。

◆ 建立了一个全面的评估基准UGBench，用于系统评估空间接地嵌入在多种城市理解任务（如地理位置排序、图像检索、城市感知和空间接地）上的性能，填补了该领域的空白。

通过在多个先进视觉语言模型上进行训练和评估，该方法在训练城市和未见城市上都显著提升了图像检索和地理位置排序等空间密集型任务的性能，证明了显式空间接地对于城市理解的有效性和泛化能力。</td></tr>
<tr><td>2026-02-10</td><td>WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning</td><td>[2602.07872](http://arxiv.org/pdf/2602.07872)</td><td>该论文提出了一种名为WristMIR的区域感知儿科腕部X光片检索框架，其核心贡献在于显著提升了具有相似骨折模式影像的检索精度与临床实用性。

◆ 提出了一种无需人工图像标注的方法，利用密集的放射学报告和基于骨骼的定位来学习细粒度、具有临床意义的图像表示。
◆ 创新性地采用基于MedGemma的结构化报告挖掘技术，自动生成全局和区域级别的文本描述，作为监督信号。
◆ 设计了由全局和局部对比编码器组成的联合训练架构，并结合一个两阶段检索流程：先进行粗粒度全局匹配筛选，再进行针对特定解剖骨骼区域的细粒度重排序。
◆ 该方法在多个评估维度上大幅超越现有基线模型，例如将图像到文本的Recall@5从0.82%提升至9.35%，并显著提升了基于检索的骨折诊断性能（平均F1分数从0.568增至0.753）。
◆ 其学习到的嵌入表示在直接的骨折分类任务上也表现出色（AUROC达0.949），并且经放射科医生评估，其检索出的病例具有更高的临床相关性。</td></tr>
<tr><td>2026-02-04</td><td>SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation</td><td>[2602.04712](http://arxiv.org/pdf/2602.04712)</td><td>◆ We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR).
◆ SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images.
◆ Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements.</td></tr>
<tr><td>2026-02-05</td><td>SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval</td><td>[2602.04451](http://arxiv.org/pdf/2602.04451)</td><td>◆ Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text.
◆ Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval.
◆ However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image.</td></tr>
<tr><td>2026-02-04</td><td>Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition</td><td>[2602.04401](http://arxiv.org/pdf/2602.04401)</td><td>◆ Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall.
◆ Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change.
◆ We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall.</td></tr>
<tr><td>2026-02-04</td><td>Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement</td><td>[2602.04304](http://arxiv.org/pdf/2602.04304)</td><td>◆ Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors.
◆ Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static &quot;magic layer&quot; empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks.
◆ In contrast to this static assumption, we propose a dynamic perspective on visual grounding.</td></tr>
<tr><td>2026-02-03</td><td>LaVPR: Benchmarking Language and Vision for Place Recognition</td><td>[2602.03253](http://arxiv.org/pdf/2602.03253)</td><td>◆ Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing.
◆ Furthermore, standard systems cannot perform &quot;blind&quot; localization from verbal descriptions alone, a capability needed for applications such as emergency response.
◆ To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions.</td></tr>
<tr><td>2026-02-03</td><td>ObjEmbed: Towards Universal Multimodal Object Embeddings</td><td>[2602.01753](http://arxiv.org/pdf/2602.01753)</td><td>◆ Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding.
◆ While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases.
◆ In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings.</td></tr>
<tr><td>2026-02-02</td><td>Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss</td><td>[2602.01673](http://arxiv.org/pdf/2602.01673)</td><td>◆ Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift.
◆ Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing.
◆ In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM.</td></tr>
<tr><td>2026-02-02</td><td>ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval</td><td>[2602.01639](http://arxiv.org/pdf/2602.01639)</td><td>◆ Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text.
◆ Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task.
◆ Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction.</td></tr>
<tr><td>2026-02-01</td><td>Interacted Planes Reveal 3D Line Mapping</td><td>[2602.01296](http://arxiv.org/pdf/2602.01296)</td><td>◆ 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes.
◆ We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch.
◆ We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives.</td></tr>
<tr><td>2026-02-05</td><td>Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition</td><td>[2602.00841](http://arxiv.org/pdf/2602.00841)</td><td>◆ Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts.
◆ Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations.
◆ In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training.</td></tr>
<tr><td>2026-02-03</td><td>Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval</td><td>[2602.00813](http://arxiv.org/pdf/2602.00813)</td><td>◆ Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text.
◆ The text specifies how to alter the reference image to form a ``mental image&#x27;&#x27;, based on which CIR should find the target image in the database.
◆ The fundamental challenge of CIR is that this ``mental image&#x27;&#x27; is not physically available and is only implicitly defined by the query.</td></tr>
<tr><td>2026-01-31</td><td>VVLoc: Prior-free 3-DoF Vehicle Visual Localization</td><td>[2602.00810](http://arxiv.org/pdf/2602.00810)</td><td>◆ Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates.
◆ Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications.
◆ In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system.</td></tr>
<tr><td>2026-01-31</td><td>Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation</td><td>[2602.00681](http://arxiv.org/pdf/2602.00681)</td><td>◆ Audio-to-image retrieval offers an interpretable alternative to audio-only classification for bioacoustic species recognition, but learning aligned audio-image representations is challenging due to the scarcity of paired audio-image data.
◆ We propose a simple and data-efficient approach that enables audio-to-image retrieval without any audio-image supervision.
◆ Our proposed method uses text as a semantic intermediary: we distill the text embedding space of a pretrained image-text model (BioCLIP-2), which encodes rich visual and taxonomic structure, into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder with a contrastive objective.</td></tr>
<tr><td>2026-01-30</td><td>HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation</td><td>[2601.23064](http://arxiv.org/pdf/2601.23064)</td><td>◆ Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography.
◆ Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail.
◆ We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space.</td></tr>
<tr><td>2026-01-30</td><td>Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval</td><td>[2601.22783](http://arxiv.org/pdf/2601.22783)</td><td>◆ Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations.
◆ While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search.
◆ In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations.</td></tr>
<tr><td>2026-01-29</td><td>Variance &amp; Greediness: A comparative study of metric-learning losses</td><td>[2601.21450](http://arxiv.org/pdf/2601.21450)</td><td>◆ Metric learning is central to retrieval, yet its effects on embedding geometry and optimization dynamics are not well understood.
◆ We introduce a diagnostic framework, VARIANCE (intra-/inter-class variance) and GREEDINESS (active ratio and gradient norms), to compare seven representative losses, i.e., Contrastive, Triplet, N-pair, InfoNCE, ArcFace, SCL, and CCL, across five image-retrieval datasets.
◆ Our analysis reveals that Triplet and SCL preserve higher within-class variance and clearer inter-class margins, leading to stronger top-1 retrieval in fine-grained settings.</td></tr>
<tr><td>2026-01-28</td><td>When Vision Meets Texts in Listwise Reranking</td><td>[2601.20623](http://arxiv.org/pdf/2601.20623)</td><td>◆ Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets.
◆ Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities.
◆ In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts.</td></tr>
<tr><td>2026-01-28</td><td>Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval</td><td>[2601.20391](http://arxiv.org/pdf/2601.20391)</td><td>◆ Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views&#x27;&#x27; of the user&#x27;s intent.
◆ However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text.
◆ Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance.</td></tr>
<tr><td>2026-01-27</td><td>VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</td><td>[2601.19887](http://arxiv.org/pdf/2601.19887)</td><td>◆ We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT.
◆ Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics.
◆ Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures.</td></tr>
<tr><td>2026-01-27</td><td>Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models</td><td>[2601.19060](http://arxiv.org/pdf/2601.19060)</td><td>◆ Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image.
◆ Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve.
◆ We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning.</td></tr>
<tr><td>2026-01-23</td><td>X-Aligner: Composed Visual Retrieval without the Bells and Whistles</td><td>[2601.16582](http://arxiv.org/pdf/2601.16582)</td><td>◆ Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries.
◆ However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline.
◆ To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs).</td></tr>
<tr><td>2026-01-22</td><td>Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing</td><td>[2601.16125](http://arxiv.org/pdf/2601.16125)</td><td>◆ Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding.
◆ Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios.
◆ To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories.</td></tr>
<tr><td>2026-01-21</td><td>Unified Multimodal and Multilingual Retrieval via Multi-Task Learning with NLU Integration</td><td>[2601.14714](http://arxiv.org/pdf/2601.14714)</td><td>◆ Multimodal retrieval systems typically employ Vision Language Models (VLMs) that encode images and text independently into vectors within a shared embedding space.
◆ Despite incorporating text encoders, VLMs consistently underperform specialized text models on text-only retrieval tasks.
◆ Moreover, introducing additional text encoders increases storage, inference overhead, and exacerbates retrieval inefficiencies, especially in multilingual settings.</td></tr>
<tr><td>2026-01-21</td><td>LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval</td><td>[2601.14706](http://arxiv.org/pdf/2601.14706)</td><td>◆ In this paper, we present LookBench (We use the term &quot;look&quot; to reflect retrieval that mirrors how people shop -- finding the exact item, a close substitute, or a visually consistent alternative.), a live, holistic and challenging benchmark for fashion image retrieval in real e-commerce settings.
◆ LookBench includes both recent product images sourced from live websites and AI-generated fashion images, reflecting contemporary trends and use cases.
◆ Each test sample is time-stamped and we intend to update the benchmark periodically, enabling contamination-aware evaluation aligned with declared training cutoffs.</td></tr>
<tr><td>2026-01-20</td><td>XR: Cross-Modal Agents for Composed Image Retrieval</td><td>[2601.14245](http://arxiv.org/pdf/2601.14245)</td><td>◆ Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms.
◆ Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities.
◆ While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning.</td></tr>
<tr><td>2026-01-20</td><td>Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration</td><td>[2601.14060](http://arxiv.org/pdf/2601.14060)</td><td>◆ Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications.
◆ Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively.
◆ They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context.</td></tr>
<tr><td>2026-01-20</td><td>Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning</td><td>[2601.13942](http://arxiv.org/pdf/2601.13942)</td><td>◆ Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge.
◆ Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries.
◆ To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning.</td></tr>
<tr><td>2026-01-19</td><td>DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition</td><td>[2601.12729](http://arxiv.org/pdf/2601.12729)</td><td>◆ One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts.
◆ While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs.
◆ However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes.</td></tr>
<tr><td>2026-01-17</td><td>SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM</td><td>[2601.11930](http://arxiv.org/pdf/2601.11930)</td><td>◆ Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM).
◆ However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs.
◆ non-overlapping pairs) fail to capture.</td></tr>
<tr><td>2026-01-22</td><td>Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning</td><td>[2601.11393](http://arxiv.org/pdf/2601.11393)</td><td>◆ Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text.
◆ Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model&#x27;s robustness.
◆ Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets.</td></tr>
<tr><td>2026-01-16</td><td>Simple Models, Rich Representations: Visual Decoding from Primate Intracortical Neural Signals</td><td>[2601.11108](http://arxiv.org/pdf/2601.11108)</td><td>◆ Understanding how neural activity gives rise to perception is a central challenge in neuroscience.
◆ We address the problem of decoding visual information from high-density intracortical recordings in primates, using the THINGS Ventral Stream Spiking Dataset.
◆ We systematically evaluate the effects of model architecture, training objectives, and data scaling on decoding performance.</td></tr>
<tr><td>2026-01-20</td><td>Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text</td><td>[2601.10096](http://arxiv.org/pdf/2601.10096)</td><td>◆ Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources.
◆ Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized.
◆ We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space.</td></tr>
<tr><td>2026-01-20</td><td>UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval</td><td>[2601.09828](http://arxiv.org/pdf/2601.09828)</td><td>◆ Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems.
◆ Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision.
◆ However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones.</td></tr>
<tr><td>2026-01-14</td><td>Hybrid guided variational autoencoder for visual place recognition</td><td>[2601.09248](http://arxiv.org/pdf/2601.09248)</td><td>◆ Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments.
◆ One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places.
◆ State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities.</td></tr>
<tr><td>2026-01-13</td><td>Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps</td><td>[2601.08520](http://arxiv.org/pdf/2601.08520)</td><td>◆ In this article, we propose a new keyframe-based mapping system.
◆ The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor.
◆ The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras.</td></tr>
<tr><td>2026-01-13</td><td>Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation</td><td>[2601.08311](http://arxiv.org/pdf/2601.08311)</td><td>◆ Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability.
◆ However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels.
◆ Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs&#x27; IQA ability.</td></tr>
<tr><td>2026-01-13</td><td>Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization</td><td>[2601.06224](http://arxiv.org/pdf/2601.06224)</td><td>◆ While Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse tasks, their practical deployment is severely hindered by hallucination issues, which become particularly acute during Reinforcement Learning (RL) optimization.
◆ This paper systematically analyzes the root causes of hallucinations in MLLMs under RL training, identifying three critical factors: (1) an over-reliance on chained visual reasoning, where inaccurate initial descriptions or redundant information anchor subsequent inferences to incorrect premises; (2) insufficient exploration diversity during policy optimization, leading the model to generate overly confident but erroneous outputs; and (3) destructive conflicts between training samples, where Neural Tangent Kernel (NTK) similarity causes false associations and unstable parameter updates.
◆ To address these challenges, we propose a comprehensive framework comprising three core modules.</td></tr>
<tr><td>2026-01-08</td><td>Multi-task Cross-modal Learning for Chest X-ray Image Retrieval</td><td>[2601.05399](http://arxiv.org/pdf/2601.05399)</td><td>◆ CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries.
◆ To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval.
◆ Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment.</td></tr>
<tr><td>2026-01-07</td><td>ImLoc: Revisiting Visual Localization with Image-based Representation</td><td>[2601.04185](http://arxiv.org/pdf/2601.04185)</td><td>◆ Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update.
◆ In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure.
◆ Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions.</td></tr>
<tr><td>2026-01-07</td><td>CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval</td><td>[2601.03728](http://arxiv.org/pdf/2601.03728)</td><td>◆ Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems.
◆ However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance.
◆ This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization.</td></tr>
<tr><td>2026-01-07</td><td>BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion</td><td>[2601.03713](http://arxiv.org/pdf/2601.03713)</td><td>◆ Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding.
◆ However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames.
◆ To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway.</td></tr>
<tr><td>2026-01-07</td><td>HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps</td><td>[2601.02730](http://arxiv.org/pdf/2601.02730)</td><td>◆ Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving.
◆ However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy.
◆ In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps.</td></tr>
<tr><td>2026-01-06</td><td>Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM</td><td>[2601.02723](http://arxiv.org/pdf/2601.02723)</td><td>◆ Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM.
◆ We propose a method to improve loop closure performance in DPV-SLAM.
◆ Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method.</td></tr>
<tr><td>2026-01-07</td><td>Comparative Analysis of Binarization Methods For Medical Image Hashing On Odir Dataset</td><td>[2601.02564](http://arxiv.org/pdf/2601.02564)</td><td>◆ In this study, we evaluated four binarization methods.
◆ Locality-Sensitive Hashing (LSH), Iterative Quantization (ITQ), Kernel-based Supervised Hashing (KSH), and Supervised Discrete Hashing (SDH) on the ODIR dataset using deep feature embeddings.
◆ Experimental results show that SDH achieved the best performance, with an mAP@100 of 0.9184 using only 32-bit codes, outperforming LSH, ITQ, and KSH.</td></tr>
<tr><td>2026-01-05</td><td>Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach</td><td>[2601.00388](http://arxiv.org/pdf/2601.00388)</td><td>◆ Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization.
◆ However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability.
◆ In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning.</td></tr>
<tr><td>2025-12-31</td><td>OCP-LS: An Efficient Algorithm for Visual Localization</td><td>[2512.24552](http://arxiv.org/pdf/2512.24552)</td><td>◆ This paper proposes a novel second-order optimization algorithm.
◆ It aims to address large-scale optimization problems in deep learning because it incorporates the OCP method and appropriately approximating the diagonal elements of the Hessian matrix.
◆ Extensive experiments on multiple standard visual localization benchmarks demonstrate the significant superiority of the proposed method.</td></tr>
<tr><td>2025-12-30</td><td>Geometric Multi-Session Map Merging with Learned Local Descriptors</td><td>[2512.24384](http://arxiv.org/pdf/2512.24384)</td><td>◆ Multi-session map merging is crucial for extended autonomous operations in large-scale environments.
◆ In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions.
◆ The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation.</td></tr>
<tr><td>2025-12-29</td><td>Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation</td><td>[2512.23864](http://arxiv.org/pdf/2512.23864)</td><td>◆ Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact.
◆ Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip.
◆ While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions.</td></tr>
<tr><td>2025-12-29</td><td>MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning</td><td>[2512.23412](http://arxiv.org/pdf/2512.23412)</td><td>◆ Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation.
◆ Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments.
◆ In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning.</td></tr>
<tr><td>2025-12-29</td><td>Anomaly Detection by Effectively Leveraging Synthetic Images</td><td>[2512.23227](http://arxiv.org/pdf/2512.23227)</td><td>◆ Anomaly detection plays a vital role in industrial manufacturing.
◆ Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied.
◆ Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution.</td></tr>
<tr><td>2025-12-26</td><td>Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer</td><td>[2512.21883](http://arxiv.org/pdf/2512.21883)</td><td>◆ Visual localization has traditionally been formulated as a pair-wise pose regression problem.
◆ Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates.
◆ However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments.</td></tr>
<tr><td>2025-12-24</td><td>Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval</td><td>[2512.21221](http://arxiv.org/pdf/2512.21221)</td><td>◆ Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management.
◆ However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions.
◆ In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions.</td></tr>
<tr><td>2025-12-28</td><td>UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer</td><td>[2512.21078](http://arxiv.org/pdf/2512.21078)</td><td>◆ Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task.
◆ Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments.
◆ In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views.</td></tr>
<tr><td>2025-12-23</td><td>Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints</td><td>[2512.20781](http://arxiv.org/pdf/2512.20781)</td><td>◆ Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text.
◆ While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants, tending to dilute key information and failing to account for what they wish to avoid.
◆ Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts.</td></tr>
<tr><td>2025-12-23</td><td>Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark</td><td>[2512.20174](http://arxiv.org/pdf/2512.20174)</td><td>◆ Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query.
◆ Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts.
◆ However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided.</td></tr>
<tr><td>2025-12-22</td><td>Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</td><td>[2512.19663](http://arxiv.org/pdf/2512.19663)</td><td>◆ Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems.
◆ While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images.
◆ We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment.</td></tr>
<tr><td>2025-12-22</td><td>Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation</td><td>[2512.19026](http://arxiv.org/pdf/2512.19026)</td><td>◆ The rise of personalized generative models raises a central question: how should we evaluate identity preservation?
◆ Given a reference image (e.g., one&#x27;s pet), we expect the generated image to retain precise details attached to the subject&#x27;s identity.
◆ However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details.</td></tr>
<tr><td>2025-12-21</td><td>Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</td><td>[2512.18613](http://arxiv.org/pdf/2512.18613)</td><td>◆ Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change.
◆ We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places.
◆ Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching.</td></tr>
<tr><td>2025-12-20</td><td>Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval</td><td>[2512.18407](http://arxiv.org/pdf/2512.18407)</td><td>◆ Accurately retrieving images that are semantically similar remains a fundamental challenge in computer vision, as traditional methods often fail to capture the relational and contextual nuances of a scene.
◆ We introduce PRISm (Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs), a multimodal framework that advances image-to-image retrieval through two novel components.
◆ First, the Importance Prediction Module identifies and retains the most critical objects and relational triplets within an image while pruning irrelevant elements.</td></tr>
<tr><td>2025-12-19</td><td>Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors</td><td>[2512.17226](http://arxiv.org/pdf/2512.17226)</td><td>◆ Recent learning-based visual localization methods use global descriptors to disambiguate visually similar places, but existing approaches often derive these descriptors from geometric cues alone (e.g., covisibility graphs), limiting their discriminative power and reducing robustness in the presence of noisy geometric constraints.
◆ We propose an aggregator module that learns global descriptors consistent with both geometrical structure and visual similarity, ensuring that images are close in descriptor space only when they are visually similar and spatially connected.
◆ This corrects erroneous associations caused by unreliable overlap scores.</td></tr>
<tr><td>2025-12-18</td><td>The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining</td><td>[2512.17121](http://arxiv.org/pdf/2512.17121)</td><td>◆ Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data.
◆ This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings.
◆ A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing.</td></tr>
<tr><td>2025-12-18</td><td>MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval</td><td>[2512.16294](http://arxiv.org/pdf/2512.16294)</td><td>◆ Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval.
◆ In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them.
◆ It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories.</td></tr>
<tr><td>2025-12-16</td><td>CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer</td><td>[2512.14560](http://arxiv.org/pdf/2512.14560)</td><td>◆ Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images.
◆ Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization.
◆ In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views.</td></tr>
<tr><td>2025-12-16</td><td>Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</td><td>[2512.14102](http://arxiv.org/pdf/2512.14102)</td><td>◆ Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS).
◆ However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use.
◆ To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries.</td></tr>
<tr><td>2025-12-15</td><td>Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing</td><td>[2512.13055](http://arxiv.org/pdf/2512.13055)</td><td>◆ Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance.
◆ Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical.
◆ In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing.</td></tr>
<tr><td>2025-12-14</td><td>Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching</td><td>[2512.12610](http://arxiv.org/pdf/2512.12610)</td><td>◆ Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance.
◆ To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning.
◆ Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching.</td></tr>
<tr><td>2025-12-11</td><td>Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval</td><td>[2512.10596](http://arxiv.org/pdf/2512.10596)</td><td>◆ Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model&#x27;s low-level visual features and high-level human concepts.
◆ While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context.
◆ To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image.</td></tr>
<tr><td>2025-12-10</td><td>YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos</td><td>[2512.09903](http://arxiv.org/pdf/2512.09903)</td><td>◆ Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning.
◆ However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive.
◆ We address the problem of visual navigation when exploration videos of a large environment are available.</td></tr>
<tr><td>2025-12-09</td><td>Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics</td><td>[2512.09071](http://arxiv.org/pdf/2512.09071)</td><td>◆ Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications.
◆ This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic.
◆ Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves.</td></tr>
<tr><td>2025-12-08</td><td>Generalized Referring Expression Segmentation on Aerial Photos</td><td>[2512.07338](http://arxiv.org/pdf/2512.07338)</td><td>◆ Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions.
◆ Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions.
◆ This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types.</td></tr>
<tr><td>2025-12-07</td><td>Spatial Retrieval Augmented Autonomous Driving</td><td>[2512.06865](http://arxiv.org/pdf/2512.06865)</td><td>◆ Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception.
◆ However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain.
◆ In contrast, human drivers are able to recall road structure even under poor visibility.</td></tr>
<tr><td>2025-12-06</td><td>Language-driven Fine-grained Retrieval</td><td>[2512.06255](http://arxiv.org/pdf/2512.06255)</td><td>◆ Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision.
◆ While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories.
◆ To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs).</td></tr>
<tr><td>2025-12-05</td><td>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</td><td>[2512.06147](http://arxiv.org/pdf/2512.06147)</td><td>◆ While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare.
◆ To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&amp;M trainer, along with 15+ hours of observing guide dog-assisted walking.
◆ After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people.</td></tr>
<tr><td>2025-12-04</td><td>ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</td><td>[2512.05111](http://arxiv.org/pdf/2512.05111)</td><td>◆ Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks.
◆ We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring.
◆ This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models.</td></tr>
<tr><td>2025-12-04</td><td>Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark</td><td>[2512.05091](http://arxiv.org/pdf/2512.05091)</td><td>◆ Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering.
◆ However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result.
◆ This contrasts with human intelligence, which naturally operates through a chain of visual reasoning.</td></tr>
<tr><td>2025-12-04</td><td>Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding</td><td>[2512.05039](http://arxiv.org/pdf/2512.05039)</td><td>◆ Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration.
◆ Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors.
◆ In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis.</td></tr>
<tr><td>2025-12-04</td><td>Revealing stimulus-dependent dynamics through statistical complexity</td><td>[2512.05007](http://arxiv.org/pdf/2512.05007)</td><td>◆ Advances in large-scale neural recordings have expanded our ability to describe the activity of distributed brain circuits.
◆ However, understanding how neural population dynamics differ across regions and behavioral contexts remains challenging.
◆ Here, we surveyed neuronal population dynamics across multiple mouse brain areas (visual cortex, hippocampus, thalamus, and midbrain) using spike data from local ensembles.</td></tr>
<tr><td>2025-12-04</td><td>Influence of Object Affordance on Action Language Understanding: Evidence from Dynamic Causal Modeling Analysis</td><td>[2512.04989](http://arxiv.org/pdf/2512.04989)</td><td>◆ This study investigates the causal neural dynamics by which affordance representations influence action language comprehension.
◆ In this study, 18 participants observed stimuli displayed in two conditions during the experiment: text-only (e.g., `Hit with a hammer&#x27;) and video+text (visual clips with matching phrases).
◆ EEG data were recorded from 32 channels and analyzed for event-related potentials and source localization using LORETA, which identified four left-hemisphere regions of interest: the Lateral Occipital Cortex (LOC), Posterior Superior Temporal Gyrus (pSTG), Ventral Premotor Cortex (PMv), and Inferior Parietal Lobule (IPL).</td></tr>
<tr><td>2025-12-04</td><td>LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging</td><td>[2512.04939](http://arxiv.org/pdf/2512.04939)</td><td>◆ 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception.
◆ However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images.
◆ To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes.</td></tr>
<tr><td>2025-12-04</td><td>Terahertz Fourier Ptychographic Imaging</td><td>[2512.04783](http://arxiv.org/pdf/2512.04783)</td><td>◆ High-resolution imaging in the terahertz (THz) spectral range remains fundamentally constrained by the limited numerical apertures of currently existing state-of-the-art imagers, which restricts its applicability across many fields, such as imaging in complex media or nondestructive testing.
◆ To address this challenge, we introduce a proof-of-concept implementation of THz Fourier Ptychographic imaging to enhance spatial resolution without requiring extensive hardware modifications.
◆ Our method employs a motorized kinematic mirror to generate a sequence of controlled, multi-angle plane-wave illuminations, with each resulting oblique-illumination intensity image encoding a limited portion of the spatial-frequency content of the target imaging sample.</td></tr>
<tr><td>2025-12-04</td><td>TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards</td><td>[2512.04772](http://arxiv.org/pdf/2512.04772)</td><td>◆ In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation.
◆ However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials.
◆ The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions.</td></tr>
<tr><td>2025-12-04</td><td>MemLoRA: Distilling Expert Adapters for On-Device Memory Systems</td><td>[2512.04763](http://arxiv.org/pdf/2512.04763)</td><td>◆ Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context.
◆ Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private.
◆ However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment.</td></tr>
<tr><td>2025-12-04</td><td>Spectral micro-CT for quantitative analysis of calcification in fibrocartilage</td><td>[2512.04662](http://arxiv.org/pdf/2512.04662)</td><td>◆ This work introduces a quantitative method for assessing calcification in fibrocartilage using spectral micro-computed tomography ($μ$CT).
◆ Tissue samples of hip acetabular labrum from patients with osteoarthritis and femoroacetabular impingement were imaged with a laboratory-based spectral $μ$CT system equipped with a small-pixel photon-counting detector.
◆ The detector operated with two energy thresholds, allowing the simultaneous acquisition of two CT datasets at different X-ray energies.</td></tr>
<tr><td>2025-12-03</td><td>RELIC: Interactive Video World Model with Long-Horizon Memory</td><td>[2512.04040](http://arxiv.org/pdf/2512.04040)</td><td>◆ A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control.
◆ However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance.
◆ In this work, we present RELIC, a unified framework that tackles these three challenges altogether.</td></tr>
<tr><td>2025-12-03</td><td>DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment</td><td>[2512.03981](http://arxiv.org/pdf/2512.03981)</td><td>◆ Drag-based image editing using generative models provides intuitive control over image structures.
◆ However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision.
◆ Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts.</td></tr>
<tr><td>2025-12-03</td><td>Revealing Nanoscale Molecular Organization in Liquid Crystals via Cryogenic Atom Probe Tomograph</td><td>[2512.03734](http://arxiv.org/pdf/2512.03734)</td><td>◆ While liquid crystals (LCs) have been extensively studied, obtaining a comprehensive nanoscale picture of their molecular organization remains challenging, as conventional techniques face an intrinsic trade-off between spatial and chemical resolution.
◆ Here, cryogenic atom probe tomography (cryo-APT) is introduced as a new analytical approach for LC materials, using 4&#x27;-Pentyl-4-cyanobiphenyl (5CB) and 4&#x27;-Octyl-4-cyanobiphenyl (8CB) as representative model compounds.
◆ This was enabled by a tailored cryogenic focused ion beam (cryo-FIB) protocol optimized for small organic molecules.</td></tr>
<tr><td>2025-12-03</td><td>DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction</td><td>[2512.03715](http://arxiv.org/pdf/2512.03715)</td><td>◆ This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.
◆ The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.
◆ DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.</td></tr>
<tr><td>2025-12-03</td><td>A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection</td><td>[2512.03684](http://arxiv.org/pdf/2512.03684)</td><td>◆ This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping.
◆ The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting.
◆ For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination.</td></tr>
<tr><td>2025-12-03</td><td>Multi-Scale Visual Prompting for Lightweight Small-Image Classification</td><td>[2512.03663](http://arxiv.org/pdf/2512.03663)</td><td>◆ Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space.
◆ However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet.
◆ In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting.</td></tr>
<tr><td>2025-12-03</td><td>M3DR: Towards Universal Multilingual Multimodal Document Retrieval</td><td>[2512.03514](http://arxiv.org/pdf/2512.03514)</td><td>◆ Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search.
◆ However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts.
◆ In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts.</td></tr>
<tr><td>2025-12-03</td><td>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</td><td>[2512.03454](http://arxiv.org/pdf/2512.03454)</td><td>◆ Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).
◆ Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution.
◆ Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions.</td></tr>
<tr><td>2025-12-03</td><td>Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation</td><td>[2512.03445](http://arxiv.org/pdf/2512.03445)</td><td>◆ Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations.
◆ However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts.
◆ To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining.</td></tr>
<tr><td>2025-12-03</td><td>Multimodal Reinforcement Learning with Agentic Verifier for AI Agents</td><td>[2512.03438](http://arxiv.org/pdf/2512.03438)</td><td>◆ Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers.
◆ Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance.
◆ However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too.</td></tr>
<tr><td>2025-12-02</td><td>MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues</td><td>[2512.03046](http://arxiv.org/pdf/2512.03046)</td><td>◆ We propose MagicQuill V2, a novel system that introduces a \textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software.
◆ While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance.
◆ To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette.</td></tr>
<tr><td>2025-12-02</td><td>Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</td><td>[2512.03040](http://arxiv.org/pdf/2512.03040)</td><td>◆ We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data.
◆ To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks.
◆ We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning.</td></tr>
<tr><td>2025-12-02</td><td>MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding</td><td>[2512.02906](http://arxiv.org/pdf/2512.02906)</td><td>◆ Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs).
◆ Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model.
◆ The most relevant crops are then selected to localize the target object and suppress irrelevant information.</td></tr>
<tr><td>2025-12-02</td><td>Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models</td><td>[2512.02897](http://arxiv.org/pdf/2512.02897)</td><td>◆ This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model.
◆ We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself.
◆ Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy.</td></tr>
<tr><td>2025-12-02</td><td>BOOM: Beyond Only One Modality KIT&#x27;s Multimodal Multilingual Lecture Companion</td><td>[2512.02817](http://arxiv.org/pdf/2512.02817)</td><td>◆ The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge.
◆ Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities.
◆ To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning.</td></tr>
<tr><td>2025-12-14</td><td>HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval</td><td>[2512.02792](http://arxiv.org/pdf/2512.02792)</td><td>◆ Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video.
◆ The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning.
◆ Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality.</td></tr>
<tr><td>2025-12-02</td><td>Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone</td><td>[2512.02737](http://arxiv.org/pdf/2512.02737)</td><td>◆ Image-based localization in GNSS-denied environments is critical for UAV autonomy.
◆ Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training.
◆ Such data are costly to acquire and often unavailable, limiting their applicability.</td></tr>
<tr><td>2025-12-02</td><td>DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions</td><td>[2512.02727](http://arxiv.org/pdf/2512.02727)</td><td>◆ Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE).
◆ To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene).
◆ However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN&#x27;s inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context.</td></tr>
<tr><td>2025-12-02</td><td>Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs</td><td>[2512.02713](http://arxiv.org/pdf/2512.02713)</td><td>◆ As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified.
◆ Understanding how specific training data contributes to a model&#x27;s output is critical.
◆ We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs).</td></tr>
<tr><td>2025-12-02</td><td>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</td><td>[2512.02697](http://arxiv.org/pdf/2512.02697)</td><td>◆ Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image.
◆ However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable.
◆ It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image).</td></tr>
<tr><td>2025-12-01</td><td>Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback</td><td>[2512.01979](http://arxiv.org/pdf/2512.01979)</td><td>◆ GUI grounding aims to align natural language instructions with precise regions in complex user interfaces.
◆ Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts.
◆ These limitations arise from limited grounding capacity and from underuse of existing reasoning potential.</td></tr>
<tr><td>2025-12-01</td><td>SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception</td><td>[2512.01908](http://arxiv.org/pdf/2512.01908)</td><td>◆ Contact-rich robotic manipulation requires representations that encode local geometry.
◆ Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues.
◆ Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information.</td></tr>
<tr><td>2025-12-01</td><td>KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</td><td>[2512.01889](http://arxiv.org/pdf/2512.01889)</td><td>◆ We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments.
◆ Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training.
◆ KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views).</td></tr>
<tr><td>2025-12-01</td><td>Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval</td><td>[2512.01636](http://arxiv.org/pdf/2512.01636)</td><td>◆ Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification.
◆ While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions.
◆ The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap.</td></tr>
<tr><td>2025-12-01</td><td>Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track</td><td>[2512.01608](http://arxiv.org/pdf/2512.01608)</td><td>◆ This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller.
◆ The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization.
◆ The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization.</td></tr>
<tr><td>2025-12-01</td><td>Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation</td><td>[2512.01589](http://arxiv.org/pdf/2512.01589)</td><td>◆ Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly.
◆ Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention.
◆ In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses.</td></tr>
<tr><td>2025-12-01</td><td>Near-infrared polarimetric imaging with nonlinear flat-optics</td><td>[2512.01525](http://arxiv.org/pdf/2512.01525)</td><td>◆ A compact and broadband polarimetric imaging platform is presented, based on second-harmonic generation (SHG) in nonlinear flat-optics.
◆ The system employs periodic all-dielectric AlGaAs gratings to induce polarization-dependent SH emission, enabling pixel by pixel direct retrieval of the full Stokes vector from an input intensity distribution in the near-infrared range.
◆ By engineering the geometry and orientation of the polarimetric units, sensitivity to linear and circular polarization components is achieved.</td></tr>
<tr><td>2025-12-01</td><td>QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions</td><td>[2512.01519](http://arxiv.org/pdf/2512.01519)</td><td>◆ Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs.
◆ Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems.
◆ We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter.</td></tr>
<tr><td>2025-12-01</td><td>Winning Solutions for the Rayan AI Contest: Compositional Retrieval, Zero-Shot Anomaly Detection, and Backdoor Detection</td><td>[2512.01498](http://arxiv.org/pdf/2512.01498)</td><td>◆ This report presents solutions to three machine learning challenges: compositional image retrieval, zero-shot anomaly detection, and backdoored model detection.
◆ In compositional image retrieval, we developed a system that processes visual and textual inputs to retrieve relevant images, achieving 95.38\% accuracy and ranking first with a clear margin over the second team.
◆ For zero-shot anomaly detection, we designed a model that identifies and localizes anomalies in images without prior exposure to abnormal examples, securing 1st place with 73.14\% accuracy.</td></tr>
<tr><td>2025-12-01</td><td>Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries</td><td>[2512.01419](http://arxiv.org/pdf/2512.01419)</td><td>◆ Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA).
◆ To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries.
◆ RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories.</td></tr>
<tr><td>2025-11-28</td><td>DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</td><td>[2511.23377](http://arxiv.org/pdf/2511.23377)</td><td>◆ Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize.
◆ Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content.
◆ We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images.</td></tr>
<tr><td>2025-11-28</td><td>FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting</td><td>[2511.23292](http://arxiv.org/pdf/2511.23292)</td><td>◆ Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering.
◆ Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness.
◆ However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity.</td></tr>
<tr><td>2025-11-28</td><td>Robust 3DGS-based SLAM via Adaptive Kernel Smoothing</td><td>[2511.23221](http://arxiv.org/pdf/2511.23221)</td><td>◆ In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy.
◆ We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking.
◆ To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM.</td></tr>
<tr><td>2025-11-28</td><td>PowerCLIP: Powerset Alignment for Contrastive Pre-Training</td><td>[2511.23170](http://arxiv.org/pdf/2511.23170)</td><td>◆ Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks.
◆ Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding.
◆ However, it remains challenging to capture compositional semantics that span multiple image regions.</td></tr>
<tr><td>2025-11-28</td><td>DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation</td><td>[2511.23127](http://arxiv.org/pdf/2511.23127)</td><td>◆ This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation.
◆ Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness.
◆ DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences.</td></tr>
<tr><td>2025-11-28</td><td>Geodiffussr: Generative Terrain Texturing with Elevation Fidelity</td><td>[2511.23029](http://arxiv.org/pdf/2511.23029)</td><td>◆ Large-scale terrain generation remains a labor-intensive task in computer graphics.
◆ We introduce Geodiffussr, a flow-matching pipeline that synthesizes text-guided texture maps while strictly adhering to a supplied Digital Elevation Map (DEM).
◆ The core mechanism is multi-scale content aggregation (MCA): DEM features from a pretrained encoder are injected into UNet blocks at multiple resolutions to enforce global-to-local elevation consistency.</td></tr>
<tr><td>2025-11-28</td><td>Contrastive Heliophysical Image Pretraining for Solar Dynamics Observatory Records</td><td>[2511.22958](http://arxiv.org/pdf/2511.22958)</td><td>◆ Deep learning has revolutionized solar image analysis, yet most approaches train task-specific encoders from scratch or rely on natural-image pretraining that ignores the unique characteristics of Solar Dynamics Observatory (SDO) data.
◆ We introduce SolarCHIP, a family of contrastively pretrained visual backbones tailored to multi-instrument SDO observations.
◆ SolarCHIP addresses three key challenges in solar imaging: multimodal sensing across AIA and HMI instruments, weak inter-class separability due to slow temporal evolution, and strong intra-class variability with sparse activity signals.</td></tr>
<tr><td>2025-11-28</td><td>See, Rank, and Filter: Important Word-Aware Clip Filtering via Scene Understanding for Moment Retrieval and Highlight Detection</td><td>[2511.22906](http://arxiv.org/pdf/2511.22906)</td><td>◆ Video moment retrieval (MR) and highlight detection (HD) with natural language queries aim to localize relevant moments and key highlights in a video clips.
◆ However, existing methods overlook the importance of individual words, treating the entire text query and video clips as a black-box, which hinders contextual understanding.
◆ In this paper, we propose a novel approach that enables fine-grained clip filtering by identifying and prioritizing important words in the query.</td></tr>
<tr><td>2025-11-28</td><td>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</td><td>[2511.22860](http://arxiv.org/pdf/2511.22860)</td><td>◆ Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise.
◆ We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization.
◆ At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity.</td></tr>
<tr><td>2025-11-28</td><td>Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering</td><td>[2511.22843](http://arxiv.org/pdf/2511.22843)</td><td>◆ Existing Multimodal Knowledge-Based Visual Question Answering (MKB-VQA) benchmarks suffer from &quot;visual shortcuts&quot;, as the query image typically matches the primary subject entity of the target document.
◆ We demonstrate that models can exploit these shortcuts, achieving comparable results using visual cues alone.
◆ To address this, we introduce Relational Entity Text-Image kNowledge Augmented (RETINA) benchmark, automatically constructed using an LLM-driven pipeline, consisting of 120k training and 2k human-curated test set.</td></tr>
<tr><td>2025-11-27</td><td>UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries</td><td>[2511.22253](http://arxiv.org/pdf/2511.22253)</td><td>◆ Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images.
◆ This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR).
◆ In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt.</td></tr>
<tr><td>2025-11-26</td><td>Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models</td><td>[2511.21663](http://arxiv.org/pdf/2511.21663)</td><td>◆ In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly.
◆ However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches.
◆ To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space.</td></tr>
<tr><td>2025-11-26</td><td>Fast 3D Ultrasound Localization Microscopy via Projection-based Processing Framework</td><td>[2511.21647](http://arxiv.org/pdf/2511.21647)</td><td>◆ Three-dimensional ultrasound localization microscopy (ULM) enables comprehensive visualization of the vasculature, thereby improving diagnostic reliability.
◆ Nevertheless, its clinical translation remains challenging, as the exponential growth in voxel count for full 3D reconstruction imposes heavy computational demands and extensive post-processing time.
◆ In this row-column array (RCA)-based 3D in vivo pig kidney ULM study, we reformulate each step of the full 3D ULM pipeline, including beamforming, clutter filtering, motion estimation, microbubble separation and localization into a series of computational-efficient 2D operations, substantially reducing the number of voxels to be processed while maintaining comparable accuracy.</td></tr>
<tr><td>2025-11-26</td><td>Qwen3-VL Technical Report</td><td>[2511.21631](http://arxiv.org/pdf/2511.21631)</td><td>◆ We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks.
◆ It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video.
◆ The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs.</td></tr>
<tr><td>2025-11-26</td><td>Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy</td><td>[2511.21579](http://arxiv.org/pdf/2511.21579)</td><td>◆ The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment.
◆ Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization.
◆ To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization.</td></tr>
<tr><td>2025-11-26</td><td>FITRep: Attention-Guided Item Representation via MLLMs</td><td>[2511.21389](http://arxiv.org/pdf/2511.21389)</td><td>◆ Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text.
◆ While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs.
◆ auxiliary elements), leading to local structural collapse problem.</td></tr>
<tr><td>2025-11-26</td><td>Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning</td><td>[2511.21375](http://arxiv.org/pdf/2511.21375)</td><td>◆ Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions.
◆ Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders.
◆ To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications.</td></tr>
<tr><td>2025-11-26</td><td>HTTM: Head-wise Temporal Token Merging for Faster VGGT</td><td>[2511.21317](http://arxiv.org/pdf/2511.21317)</td><td>◆ The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass.
◆ However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views.
◆ For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck.</td></tr>
<tr><td>2025-11-26</td><td>Low-dose Chemically Specific Bioimaging via Deep-UV Lensless Holographic Microscopy on a Standard Camera</td><td>[2511.21311](http://arxiv.org/pdf/2511.21311)</td><td>◆ Deep-ultraviolet (DUV) microscopy can provide label-free biochemical contrast by exploiting the intrinsic absorption of nucleic acids, proteins and lipids, offering chemically specific morphological information that complements structural optical thickness contrast from phase-sensitive imaging.
◆ However, existing DUV microscopes typically rely on specialized optics and DUV-sensitive cameras, which restrict field of view, increase system complexity and cost, and often require high illumination doses that risk photodamage.
◆ Here, we report a low-dose deep-UV lensless holographic microscopy platform that uses standard board-level CMOS sensors designed for visible light, eliminating all imaging optics and dedicated DUV detectors.</td></tr>
<tr><td>2025-11-26</td><td>Adaptive Lighting Control in Visible Light Systems: An Integrated Sensing, Communication, and Illumination Framework</td><td>[2511.21271](http://arxiv.org/pdf/2511.21271)</td><td>◆ Indoor visible light communication (VLC) is a promising sixth-generation (6G) technology, as its directional and sensitive optical signals are naturally suited for integrated sensing and communication (ISAC).
◆ However, current research mainly focuses on maximizing data rates and sensing accuracy, creating a conflict between high performance, high energy consumption, and user visual comfort.
◆ This paper proposes an adaptive integrated sensing, communication, and illumination (ISCI) framework that resolves this conflict by treating energy savings as a primary objective.</td></tr>
<tr><td>2025-11-26</td><td>Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition</td><td>[2511.21202](http://arxiv.org/pdf/2511.21202)</td><td>◆ Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories.
◆ However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time.
◆ In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions.</td></tr>
<tr><td>2025-11-25</td><td>Adaptive Hopfield Network: Rethinking Similarities in Associative Memory</td><td>[2511.20609](http://arxiv.org/pdf/2511.20609)</td><td>◆ Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability.
◆ However, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness.
◆ We reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process.</td></tr>
<tr><td>2025-11-25</td><td>New York Smells: A Large Multimodal Dataset for Olfaction</td><td>[2511.20544](http://arxiv.org/pdf/2511.20544)</td><td>◆ While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines.
◆ One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings.
◆ We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.&#x27;&#x27; Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\times$ more objects than existing olfactory datasets.</td></tr>
<tr><td>2025-11-25</td><td>Wide Area Surface Dosimetry with Conformal Scintillator Array for External Beam Radiotherapy</td><td>[2511.20472](http://arxiv.org/pdf/2511.20472)</td><td>◆ Background: In vivo dosimetry is essential for treatment verification in modern radiotherapy, but existing techniques are limited by spatiotemporal resolution and performance on non-uniform anatomy.
◆ Scintillation imaging dosimetry shows potential to address several of these limitations.
◆ Here, translation to conventional photon external beam radiotherapy was examined using a novel wide-area imaging and sensing technique.</td></tr>
<tr><td>2025-11-25</td><td>Power-Efficient Autonomous Mobile Robots</td><td>[2511.20467](http://arxiv.org/pdf/2511.20467)</td><td>◆ This paper presents pNav, a novel power-management system that significantly enhances the power/energy-efficiency of Autonomous Mobile Robots (AMRs) by jointly optimizing their physical/mechanical and cyber subsystems.
◆ By profiling AMRs&#x27; power consumption, we identify three challenges in achieving CPS (cyber-physical system) power-efficiency that involve both cyber (C) and physical (P) subsystems: (1) variabilities of system power consumption breakdown, (2) environment-aware navigation locality, and (3) coordination of C and P subsystems.
◆ pNav takes a multi-faceted approach to achieve power-efficiency of AMRs.</td></tr>
<tr><td>2025-11-25</td><td>STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow</td><td>[2511.20462](http://arxiv.org/pdf/2511.20462)</td><td>◆ Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation.
◆ Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models.
◆ In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation.</td></tr>
<tr><td>2025-11-25</td><td>Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search</td><td>[2511.20460](http://arxiv.org/pdf/2511.20460)</td><td>◆ With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread.
◆ However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details.
◆ In this context, guiding the model look where it matters before prediction becomes crucial.</td></tr>
<tr><td>2025-11-25</td><td>Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi</td><td>[2511.20391](http://arxiv.org/pdf/2511.20391)</td><td>◆ We describe a prototype of a fully capable Ethereum Proof-of-Work (PoW) blockchain network running on multiple Raspberry Pi (RPi) computers.
◆ The prototype is easy to set up and is intended to function as a completely standalone system, using a local WiFi router for connectivity.
◆ It features LCD screens for visualization of the local state of blockchain ledgers on each RPi, making it ideal for educational purposes and to demonstrate fundamental blockchain concepts to a wide audience.</td></tr>
<tr><td>2025-11-25</td><td>TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection</td><td>[2511.20306](http://arxiv.org/pdf/2511.20306)</td><td>◆ Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images.
◆ Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions.
◆ Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies.</td></tr>
<tr><td>2025-11-25</td><td>Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations</td><td>[2511.20295](http://arxiv.org/pdf/2511.20295)</td><td>◆ Counterfactual explanations (CFEs) are minimal and semantically meaningful modifications of the input of a model that alter the model predictions.
◆ They highlight the decisive features the model relies on, providing contrastive interpretations for classifiers.
◆ State-of-the-art visual counterfactual explanation methods are designed to explain image classifiers.</td></tr>
<tr><td>2025-11-25</td><td>Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement</td><td>[2511.20280](http://arxiv.org/pdf/2511.20280)</td><td>◆ Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles.
◆ To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation.
◆ Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality.</td></tr>
<tr><td>2025-11-24</td><td>Wigner and Gabor phase-space analysis of propagators for evolution equations</td><td>[2511.19400](http://arxiv.org/pdf/2511.19400)</td><td>◆ We study the Wigner kernel and the Gabor matrix associated with the propagators of a broad class   of linear evolution equations, including the complex heat, wave,   and Hermite equations.
◆ Within the framework of time-frequency analysis, we derive   explicit expressions for the Wigner kernels of Fourier multipliers and establish quantitative   decay estimates for the corresponding Gabor matrices.
◆ These results are obtained under symbol   regularity conditions formulated in the Gelfand-Shilov scale and ensure exponential off-diagonal   decay or quasi-diagonality of the matrix representation.</td></tr>
<tr><td>2025-11-24</td><td>Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments</td><td>[2511.19396](http://arxiv.org/pdf/2511.19396)</td><td>◆ Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics.
◆ This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments.
◆ The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects.</td></tr>
<tr><td>2025-11-24</td><td>In-vivo imaging with a low-cost MRI scanner and cloud data processing in low-resource settings</td><td>[2511.19226](http://arxiv.org/pdf/2511.19226)</td><td>◆ Purpose: To demonstrate in-vivo imaging with a low-cost, low-field MRI scanner built and operated in Africa, and to show how systematic hardware and software improvements can mitigate the main operational limitations encountered in low-resource environments.
◆ Methods: A 46 mT Halbach scanner located at the Mbarara University of Science and Technology (Uganda) was upgraded through a complete reorganization of grounding and shielding, installation of new control electronics and open-source user-interface software.
◆ Noise performance was quantified using a standardized protocol and in-vivo brain images were acquired with three-dimensional RARE sequences.</td></tr>
<tr><td>2025-11-24</td><td>Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?</td><td>[2511.19200](http://arxiv.org/pdf/2511.19200)</td><td>◆ Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception.
◆ One subtle ability is to judge whether an image looks like a given object without being an instance of that object.
◆ We study whether vision-language models such as CLIP capture this distinction.</td></tr>
<tr><td>2025-11-24</td><td>From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation</td><td>[2511.19149](http://arxiv.org/pdf/2511.19149)</td><td>◆ This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting.
◆ The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization.
◆ The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index.</td></tr>
<tr><td>2025-11-24</td><td>Graph-based 3D Human Pose Estimation using WiFi Signals</td><td>[2511.19105](http://arxiv.org/pdf/2511.19105)</td><td>◆ WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods.
◆ However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints.
◆ In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE.</td></tr>
<tr><td>2025-11-24</td><td>Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach</td><td>[2511.19080](http://arxiv.org/pdf/2511.19080)</td><td>◆ The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes.
◆ Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection.
◆ Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection.</td></tr>
<tr><td>2025-11-24</td><td>LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space</td><td>[2511.19057](http://arxiv.org/pdf/2511.19057)</td><td>◆ Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding.
◆ However, datasets tailored for 3D LAA perception remain scarce.
◆ To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles.</td></tr>
<tr><td>2025-11-24</td><td>Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors</td><td>[2511.19031](http://arxiv.org/pdf/2511.19031)</td><td>◆ Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot&#x27;s pose while simultaneously reconstructing an unknown 3D scene using a single camera.
◆ While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization.
◆ To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses.</td></tr>
<tr><td>2025-11-24</td><td>Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting</td><td>[2511.19021](http://arxiv.org/pdf/2511.19021)</td><td>◆ Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details.
◆ Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation.
◆ To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity.</td></tr>
<tr><td>2025-11-21</td><td>REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing</td><td>[2511.17442](http://arxiv.org/pdf/2511.17442)</td><td>◆ Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping.
◆ These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data.
◆ They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering.</td></tr>
<tr><td>2025-11-21</td><td>IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation</td><td>[2511.17384](http://arxiv.org/pdf/2511.17384)</td><td>◆ While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning.
◆ Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity.
◆ To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning.</td></tr>
<tr><td>2025-11-21</td><td>SVRecon: Sparse Voxel Rasterization for Surface Reconstruction</td><td>[2511.17364](http://arxiv.org/pdf/2511.17364)</td><td>◆ We extend the recently proposed sparse voxel rasterization paradigm to the task of high-fidelity surface reconstruction by integrating Signed Distance Function (SDF), named SVRecon.
◆ Unlike 3D Gaussians, sparse voxels are spatially disentangled from their neighbors and have sharp boundaries, which makes them prone to local minima during optimization.
◆ Although SDF values provide a naturally smooth and continuous geometric field, preserving this smoothness across independently parameterized sparse voxels is nontrivial.</td></tr>
<tr><td>2025-11-21</td><td>NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior</td><td>[2511.17322](http://arxiv.org/pdf/2511.17322)</td><td>◆ In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.
◆ Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios.
◆ To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation.</td></tr>
<tr><td>2025-11-21</td><td>MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning</td><td>[2511.17300](http://arxiv.org/pdf/2511.17300)</td><td>◆ Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations.
◆ This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains.
◆ However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements.</td></tr>
<tr><td>2025-11-21</td><td>Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</td><td>[2511.17282](http://arxiv.org/pdf/2511.17282)</td><td>◆ Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized.
◆ Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency.
◆ We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts.</td></tr>
<tr><td>2025-11-21</td><td>A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback</td><td>[2511.17255](http://arxiv.org/pdf/2511.17255)</td><td>◆ Large vision-language models (VLMs) enable intuitive visual search using natural language queries.
◆ However, improving their performance often requires fine-tuning and scaling to larger model variants.
◆ In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback.</td></tr>
<tr><td>2025-11-21</td><td>Mixed Reality Scenic Live Streaming for Cultural Heritage: Visual Interactions in a Historic Landscape</td><td>[2511.17246](http://arxiv.org/pdf/2511.17246)</td><td>◆ Scenic Live Streams (SLS), capturing real-world scenic sites from fixed cameras without streamers, have gained increasing popularity recently.
◆ They afford unique real-time lenses into remote sites for viewers&#x27; synchronous and collective engagement.
◆ Foregrounding its lack of dynamism and interactivity, we aim to maximize the potential of SLS by making it interactive.</td></tr>
<tr><td>2025-11-21</td><td>SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors</td><td>[2511.17207](http://arxiv.org/pdf/2511.17207)</td><td>◆ Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis.
◆ To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework.
◆ The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications.</td></tr>
<tr><td>2025-11-21</td><td>Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition</td><td>[2511.17183](http://arxiv.org/pdf/2511.17183)</td><td>◆ Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving.
◆ Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets.
◆ Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively.</td></tr>
<tr><td>2025-11-20</td><td>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</td><td>[2511.16671](http://arxiv.org/pdf/2511.16671)</td><td>◆ Recent advances in visual generation have increasingly explored the integration of reasoning capabilities.
◆ They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself.
◆ In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process.</td></tr>
<tr><td>2025-11-20</td><td>Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems</td><td>[2511.16654](http://arxiv.org/pdf/2511.16654)</td><td>◆ Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents.
◆ However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering.
◆ To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space).</td></tr>
<tr><td>2025-11-20</td><td>SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction</td><td>[2511.16635](http://arxiv.org/pdf/2511.16635)</td><td>◆ Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption.
◆ While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases.
◆ We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction.</td></tr>
<tr><td>2025-11-20</td><td>POMA-3D: The Point Map Way to 3D Scene Understanding</td><td>[2511.16567](http://arxiv.org/pdf/2511.16567)</td><td>◆ In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps.
◆ Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models.
◆ To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed.</td></tr>
<tr><td>2025-11-20</td><td>NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening</td><td>[2511.16566](http://arxiv.org/pdf/2511.16566)</td><td>◆ Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention.
◆ In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children&#x27;s images, simultaneously addressing generalizability and class imbalance.
◆ In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings.</td></tr>
<tr><td>2025-11-20</td><td>Contrastive vision-language learning with paraphrasing and negation</td><td>[2511.16527](http://arxiv.org/pdf/2511.16527)</td><td>◆ Contrastive vision-language models continue to be the dominant approach for image and text retrieval.
◆ Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space.
◆ Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning.</td></tr>
<tr><td>2025-11-20</td><td>BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization</td><td>[2511.16524](http://arxiv.org/pdf/2511.16524)</td><td>◆ Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments.
◆ In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing.
◆ The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes.</td></tr>
<tr><td>2025-11-20</td><td>YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras</td><td>[2511.16521](http://arxiv.org/pdf/2511.16521)</td><td>◆ Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications.
◆ However, registering CMCs to the target scene layout presents a challenging task.
◆ While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists.</td></tr>
<tr><td>2025-11-20</td><td>TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models</td><td>[2511.16423](http://arxiv.org/pdf/2511.16423)</td><td>◆ Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning.
◆ Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks.
◆ Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive.</td></tr>
<tr><td>2025-11-20</td><td>CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering</td><td>[2511.16349](http://arxiv.org/pdf/2511.16349)</td><td>◆ Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content.
◆ Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure.
◆ This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud.</td></tr>
<tr><td>2025-11-19</td><td>GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization</td><td>[2511.15705](http://arxiv.org/pdf/2511.15705)</td><td>◆ Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models.
◆ In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning.
◆ Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models.</td></tr>
<tr><td>2025-11-19</td><td>First Frame Is the Place to Go for Video Content Customization</td><td>[2511.15700](http://arxiv.org/pdf/2511.15700)</td><td>◆ What role does the first frame play in video generation models?
◆ Traditionally, it&#x27;s viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation.
◆ In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation.</td></tr>
<tr><td>2025-11-19</td><td>Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning</td><td>[2511.15633](http://arxiv.org/pdf/2511.15633)</td><td>◆ Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge.
◆ Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL.
◆ However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like &quot;dog&quot; subsumes fine-grained categories such as &quot;Labrador&quot; and &quot;Golden Retriever,&quot; and each category entails its images.</td></tr>
<tr><td>2025-11-19</td><td>Multi-Text Guided Few-Shot Semantic Segmentation</td><td>[2511.15515](http://arxiv.org/pdf/2511.15515)</td><td>◆ Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class).
◆ However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories.
◆ Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality.</td></tr>
<tr><td>2025-11-19</td><td>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</td><td>[2511.15464](http://arxiv.org/pdf/2511.15464)</td><td>◆ Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles.
◆ However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization.
◆ To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales.</td></tr>
<tr><td>2025-11-19</td><td>HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation</td><td>[2511.15435](http://arxiv.org/pdf/2511.15435)</td><td>◆ Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues.
◆ Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents.
◆ However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components.</td></tr>
<tr><td>2025-11-19</td><td>The Empowerment of Science of Science by Large Language Models: New Tools and Methods</td><td>[2511.15370](http://arxiv.org/pdf/2511.15370)</td><td>◆ Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race.
◆ This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning.
◆ Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain.</td></tr>
<tr><td>2025-11-19</td><td>C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models</td><td>[2511.15333](http://arxiv.org/pdf/2511.15333)</td><td>◆ Space grounding refers to localizing a set of spatial references described in natural language instructions.
◆ Traditional methods often fail to account for complex reasoning -- such as distance, geometry, and inter-object relationships -- while vision-language models (VLMs), despite strong reasoning abilities, struggle to produce a fine-grained region of outputs.
◆ To overcome these limitations, we propose C2F-Space, a novel coarse-to-fine space-grounding framework that (i) estimates an approximated yet spatially consistent region using a VLM, then (ii) refines the region to align with the local environment through superpixelization.</td></tr>
<tr><td>2025-11-19</td><td>Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval</td><td>[2511.15201](http://arxiv.org/pdf/2511.15201)</td><td>◆ This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem.
◆ As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment.
◆ Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions.</td></tr>
<tr><td>2025-11-19</td><td>Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation</td><td>[2511.15118](http://arxiv.org/pdf/2511.15118)</td><td>◆ Few-shot segmentation has garnered significant attention.
◆ Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task.
◆ With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation.</td></tr>
<tr><td>2025-11-18</td><td>FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</td><td>[2511.14712](http://arxiv.org/pdf/2511.14712)</td><td>◆ The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive.
◆ Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation.
◆ At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token&#x27;s training scale receptive field is crucial for preserving visual fidelity and detail.</td></tr>
<tr><td>2025-11-18</td><td>Overcoming global sensitivity limitations: using active subspaces to explore discrepancies between global and local parameter sensitivities</td><td>[2511.14687](http://arxiv.org/pdf/2511.14687)</td><td>◆ Global sensitivity metrics are essential tools for assessing parameter importance in complex models, particularly when precise information about parameter values is unavailable.
◆ In many cases, such metrics are used to provide parameter rankings that allow for necessary dimension reduction in moderate-to-high dimensional systems.
◆ However, globally-derived sensitivity results may obscure localized variability in parameter sensitivities, resulting in misleading conclusions about parameter importance and ensuing consequences for subsequent tasks such as model calibration and surrogate model construction.</td></tr>
<tr><td>2025-11-18</td><td>A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</td><td>[2511.14638](http://arxiv.org/pdf/2511.14638)</td><td>◆ Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years.
◆ Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations.
◆ We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval.</td></tr>
<tr><td>2025-11-18</td><td>Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction</td><td>[2511.14544](http://arxiv.org/pdf/2511.14544)</td><td>◆ Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane.
◆ However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions.
◆ Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection.</td></tr>
<tr><td>2025-11-18</td><td>D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images</td><td>[2511.14518](http://arxiv.org/pdf/2511.14518)</td><td>◆ Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks.
◆ However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development.
◆ While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details.</td></tr>
<tr><td>2025-11-18</td><td>Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations</td><td>[2511.14504](http://arxiv.org/pdf/2511.14504)</td><td>◆ Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings.
◆ The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire&#x27;s location.
◆ Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.</td></tr>
<tr><td>2025-11-18</td><td>DIR-TIR: Dialog-Iterative Refinement for Text-to-Image Retrieval</td><td>[2511.14449](http://arxiv.org/pdf/2511.14449)</td><td>◆ This paper addresses the task of interactive, conversational text-to-image retrieval.
◆ Our DIR-TIR framework progressively refines the target image search through two specialized modules: the Dialog Refiner Module and the Image Refiner Module.
◆ The Dialog Refiner actively queries users to extract essential information and generate increasingly precise descriptions of the target image.</td></tr>
<tr><td>2025-11-18</td><td>Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding</td><td>[2511.14446](http://arxiv.org/pdf/2511.14446)</td><td>◆ Video understanding requires not only visual recognition but also complex reasoning.
◆ While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement.
◆ While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training.</td></tr>
<tr><td>2025-11-18</td><td>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</td><td>[2511.14386](http://arxiv.org/pdf/2511.14386)</td><td>◆ Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception.
◆ Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored.
◆ To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving.</td></tr>
<tr><td>2025-11-18</td><td>O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</td><td>[2511.14368](http://arxiv.org/pdf/2511.14368)</td><td>◆ While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited.
◆ Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually.
◆ We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions.</td></tr>
<tr><td>2025-11-17</td><td>Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images</td><td>[2511.13586](http://arxiv.org/pdf/2511.13586)</td><td>◆ Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease.
◆ Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell&#x27;s function and identity.
◆ In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.</td></tr>
<tr><td>2025-11-17</td><td>Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification</td><td>[2511.13575](http://arxiv.org/pdf/2511.13575)</td><td>◆ Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I).
◆ Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment.
◆ Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance.</td></tr>
<tr><td>2025-11-17</td><td>Language-Guided Invariance Probing of Vision-Language Models</td><td>[2511.13494](http://arxiv.org/pdf/2511.13494)</td><td>◆ Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations.
◆ We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching.
◆ Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.</td></tr>
<tr><td>2025-11-17</td><td>Attention Grounded Enhancement for Visual Document Retrieval</td><td>[2511.13415](http://arxiv.org/pdf/2511.13415)</td><td>◆ Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs.
◆ Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance.
◆ However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match.</td></tr>
<tr><td>2025-11-17</td><td>Stray Light Correction for the Helioseismic and Magnetic Imager</td><td>[2511.13348](http://arxiv.org/pdf/2511.13348)</td><td>◆ We report a point spread function (PSF) and deconvolution procedure to remove stray light from the Helioseismic and Magnetic Imager (HMI) data.
◆ Pre-launch calibration observations, post-launch Venus transit and lunar transit data were used to develop the PSF and evaluate how well it reproduced the observed scattering.
◆ The PSF reported differs from previous stray light removal efforts since we do not use Gaussians as the central mathematical component.</td></tr>
<tr><td>2025-11-17</td><td>Uncovering and Mitigating Transient Blindness in Multimodal Model Editing</td><td>[2511.13243](http://arxiv.org/pdf/2511.13243)</td><td>◆ Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models.
◆ Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting.
◆ We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits.</td></tr>
<tr><td>2025-11-17</td><td>GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry</td><td>[2511.13216](http://arxiv.org/pdf/2511.13216)</td><td>◆ Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms.
◆ In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping.
◆ Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation.</td></tr>
<tr><td>2025-11-17</td><td>Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework</td><td>[2511.13189](http://arxiv.org/pdf/2511.13189)</td><td>◆ Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC).
◆ Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance.
◆ Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures.</td></tr>
<tr><td>2025-11-17</td><td>THIR: Topological Histopathological Image Retrieval</td><td>[2511.13170](http://arxiv.org/pdf/2511.13170)</td><td>◆ According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020.
◆ Early diagnosis and accurate clinical decision making are critical in reducing this global burden.
◆ In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns.</td></tr>
<tr><td>2025-11-17</td><td>SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration</td><td>[2511.13168](http://arxiv.org/pdf/2511.13168)</td><td>◆ Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics.
◆ Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory.
◆ Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences.</td></tr>
<tr><td>2025-11-14</td><td>DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding</td><td>[2511.11552](http://arxiv.org/pdf/2511.11552)</td><td>◆ Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs).
◆ Existing approaches falter on a fundamental challenge: evidence localization.
◆ They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination.</td></tr>
<tr><td>2025-11-14</td><td>STEM EBIC as a Quantitative Probe of Semiconductor Devices</td><td>[2511.11528](http://arxiv.org/pdf/2511.11528)</td><td>◆ Electron beam-induced current (EBIC) imaging in the scanning transmission electron microscope (STEM), STEM-EBIC, provides direct access to carrier transport at the nanoscale.
◆ While well established in bulk SEM geometries, its application to thin TEM lamellae remains largely unexplored.
◆ Here, we present a systematic STEM-EBIC study of silicon photodiode lamellae prepared by gallium and xenon focused ion beam (FIB) milling.</td></tr>
<tr><td>2025-11-14</td><td>Bridging Hidden States in Vision-Language Models</td><td>[2511.11526](http://arxiv.org/pdf/2511.11526)</td><td>◆ Vision-Language Models (VLMs) are a new family of models that align image content with natural language.
◆ Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings.
◆ Many methods also tie fusion to an autoregressive decoder.</td></tr>
<tr><td>2025-11-14</td><td>Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs</td><td>[2511.11427](http://arxiv.org/pdf/2511.11427)</td><td>◆ Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions.
◆ Research on the area remains predominantly English-centric, despite increasing global deployment demands.
◆ This work addresses multilingual REC through two main contributions.</td></tr>
<tr><td>2025-11-14</td><td>Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment</td><td>[2511.11422](http://arxiv.org/pdf/2511.11422)</td><td>◆ Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach.
◆ We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG&#x27;s inherent noise and signal degradation, vs.
◆ vision&#x27;s high-fidelity features) and a Semantic Gap (arising from EEG&#x27;s shallow conceptual representation, vs.</td></tr>
<tr><td>2025-11-14</td><td>Bidimensional measurements of photon statistics within a multimodal temporal framework</td><td>[2511.11403](http://arxiv.org/pdf/2511.11403)</td><td>◆ Ultrafast imaging of photon statistics in two dimensions is a powerful tool for probing non-equilibrium and transient optical phenomena, yet it remains experimentally challenging due to the simultaneous need for high temporal resolution and statistical fidelity.
◆ In this work, we demonstrate spatially resolved single-shot measurements of photon number distributions using difference-frequency generation (DFG) in a nonlinear BBO crystal.
◆ We show that our platform can discriminate between coherent and thermal photon statistics across two spatial dimensions with picosecond resolution.</td></tr>
<tr><td>2025-11-14</td><td>GRANITE: High-Resolution Imaging and Electrical Qualification of Large-Area TPC Electrodes</td><td>[2511.11401](http://arxiv.org/pdf/2511.11401)</td><td>◆ Next-generation dual-phase time projection chambers (TPCs) for rare event searches will require large-scale, high-precision electrodes.
◆ To meet the stringent requirements for high-voltage performance of such an experiment, we have developed a scanning setup for comprehensive electrode quality assurance.
◆ The system is built around the GRANITE (Granular Robotic Assay for Novel Integrated TPC Electrodes) facility: a gantry robot on top of a $2.5\,\text{m}\times1.8\,\text{m}$ granite table, equipped with a suite of non-contact metrology devices.</td></tr>
<tr><td>2025-11-14</td><td>StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks</td><td>[2511.11320](http://arxiv.org/pdf/2511.11320)</td><td>◆ Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation.
◆ Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible.
◆ Equilibrium Propagation (EP) provides a more local and biologically grounded alternative.</td></tr>
<tr><td>2025-11-14</td><td>DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding</td><td>[2511.11313](http://arxiv.org/pdf/2511.11313)</td><td>◆ Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents.
◆ However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices.
◆ We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources.</td></tr>
<tr><td>2025-11-14</td><td>MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising</td><td>[2511.11305](http://arxiv.org/pdf/2511.11305)</td><td>◆ We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications.
◆ MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on.
◆ The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement.</td></tr>
<tr><td>2025-11-13</td><td>Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering</td><td>[2511.10591](http://arxiv.org/pdf/2511.10591)</td><td>◆ The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently.
◆ The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images.
◆ In this work, we present two complementary approaches developed for the English track.</td></tr>
<tr><td>2025-11-13</td><td>SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</td><td>[2511.10518](http://arxiv.org/pdf/2511.10518)</td><td>◆ Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions.
◆ In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation.
◆ Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2.</td></tr>
<tr><td>2025-11-13</td><td>Domain Adaptation for Camera-Specific Image Characteristics using Shallow Discriminators</td><td>[2511.10424](http://arxiv.org/pdf/2511.10424)</td><td>◆ Each image acquisition setup leads to its own camera-specific image characteristics degrading the image quality.
◆ In learning-based perception algorithms, characteristics occurring during the application phase, but absent in the training data, lead to a domain gap impeding the performance.
◆ Previously, pixel-level domain adaptation through unpaired learning of the pristine-to-distorted mapping function has been proposed.</td></tr>
<tr><td>2025-11-13</td><td>MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns</td><td>[2511.10390](http://arxiv.org/pdf/2511.10390)</td><td>◆ Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis.
◆ However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems.
◆ We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline.</td></tr>
<tr><td>2025-11-13</td><td>Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery</td><td>[2511.10387](http://arxiv.org/pdf/2511.10387)</td><td>◆ Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management.
◆ In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data.
◆ Unlike previous hybrid approaches that require real satellite images for self-supevised training.</td></tr>
<tr><td>2025-11-13</td><td>Rethinking Visual Information Processing in Multimodal LLMs</td><td>[2511.10301](http://arxiv.org/pdf/2511.10301)</td><td>◆ Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities.
◆ We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder.
◆ To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations.</td></tr>
<tr><td>2025-11-13</td><td>H3Former: Hypergraph-based Semantic-Aware Aggregation via Hyperbolic Hierarchical Contrastive Loss for Fine-Grained Visual Classification</td><td>[2511.10260](http://arxiv.org/pdf/2511.10260)</td><td>◆ Fine-Grained Visual Classification (FGVC) remains a challenging task due to subtle inter-class differences and large intra-class variations.
◆ Existing approaches typically rely on feature-selection mechanisms or region-proposal strategies to localize discriminative regions for semantic analysis.
◆ However, these methods often fail to capture discriminative cues comprehensively while introducing substantial category-agnostic redundancy.</td></tr>
<tr><td>2025-11-13</td><td>TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding</td><td>[2511.10241](http://arxiv.org/pdf/2511.10241)</td><td>◆ Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video.
◆ This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning.
◆ Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps.</td></tr>
<tr><td>2025-11-13</td><td>Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization</td><td>[2511.10212](http://arxiv.org/pdf/2511.10212)</td><td>◆ Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets.
◆ However, such approaches that target generalization require pretraining over real samples.
◆ Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment.</td></tr>
<tr><td>2025-11-13</td><td>Beyond the Black Box: Demystifying Multi-Turn LLM Reasoning with VISTA</td><td>[2511.10182](http://arxiv.org/pdf/2511.10182)</td><td>◆ Recent research has increasingly focused on the reasoning capabilities of Large Language Models (LLMs) in multi-turn interactions, as these scenarios more closely mirror real-world problem-solving.
◆ However, analyzing the intricate reasoning processes within these interactions presents a significant challenge due to complex contextual dependencies and a lack of specialized visualization tools, leading to a high cognitive load for researchers.
◆ To address this gap, we present VISTA, an web-based Visual Interactive System for Textual Analytics in multi-turn reasoning tasks.</td></tr>
<tr><td>2025-11-10</td><td>TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research</td><td>[2511.07412](http://arxiv.org/pdf/2511.07412)</td><td>◆ Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation.
◆ However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings.
◆ Digital twins provide high-fidelity, risk-free environments for exploration and training.</td></tr>
<tr><td>2025-11-10</td><td>LeCoT: revisiting network architecture for two-view correspondence pruning</td><td>[2511.07078](http://arxiv.org/pdf/2511.07078)</td><td>◆ Two-view correspondence pruning aims to accurately remove incorrect correspondences (outliers) from initial ones and is widely applied to various computer vision tasks.
◆ Current popular strategies adopt multilayer perceptron (MLP) as the backbone, supplemented by additional modules to enhance the network ability to handle context information, which is a known limitation of MLPs.
◆ In contrast, we introduce a novel perspective for capturing correspondence context information without extra design modules.</td></tr>
<tr><td>2025-11-10</td><td>Semi-distributed Cross-modal Air-Ground Relative Localization</td><td>[2511.06749](http://arxiv.org/pdf/2511.06749)</td><td>◆ Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks.
◆ However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy.
◆ To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework.</td></tr>
<tr><td>2025-11-09</td><td>DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization</td><td>[2511.06422](http://arxiv.org/pdf/2511.06422)</td><td>◆ With the rapid growth of the low-altitude economy, unmanned aerial vehicles (UAVs) have become key platforms for measurement and tracking in intelligent patrol systems.
◆ However, in GNSS-denied environments, localization schemes that rely solely on satellite signals are prone to failure.
◆ Cross-view image retrieval-based localization is a promising alternative, yet substantial geometric and appearance domain gaps exist between oblique UAV views and nadir satellite orthophotos.</td></tr>
<tr><td>2025-11-08</td><td>Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era</td><td>[2511.06024](http://arxiv.org/pdf/2511.06024)</td><td>◆ Visual place recognition (VPR) is typically regarded as a specific image retrieval task, whose core lies in representing images as global descriptors.
◆ Over the past decade, dominant VPR methods (e.g., NetVLAD) have followed a paradigm that first extracts the patch features/tokens of the input image using a backbone, and then aggregates these patch features into a global descriptor via an aggregator.
◆ This backbone-plus-aggregator paradigm has achieved overwhelming dominance in the CNN era and remains widely used in transformer-based models.</td></tr>
<tr><td>2025-11-07</td><td>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</td><td>[2511.05404](http://arxiv.org/pdf/2511.05404)</td><td>◆ Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration.
◆ In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity.
◆ This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments.</td></tr>
<tr><td>2025-11-07</td><td>DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval</td><td>[2511.05020](http://arxiv.org/pdf/2511.05020)</td><td>◆ Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text.
◆ Most existing methods rely on a single model to perform feature fusion and similarity matching.
◆ However, this paradigm faces two major challenges.</td></tr>
<tr><td>2025-11-06</td><td>Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</td><td>[2511.04384](http://arxiv.org/pdf/2511.04384)</td><td>◆ We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding.
◆ The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks.
◆ This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable.</td></tr>
<tr><td>2025-11-06</td><td>An Efficient Algorithm for Learning-Based Visual Localization</td><td>[2511.04232](http://arxiv.org/pdf/2511.04232)</td><td>◆ This paper addresses the visual localization problem in Global Positioning System (GPS)-denied environments, where computational resources are often limited.
◆ To achieve efficient and robust performance under these constraints, we propose a novel algorithm.
◆ The algorithm stems from the optimal control principle (OCP).</td></tr>
<tr><td>2025-11-04</td><td>Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization</td><td>[2511.02489](http://arxiv.org/pdf/2511.02489)</td><td>◆ With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems.
◆ However, in GNSS-denied areas, satellite-based localization methods are prone to failure.
◆ This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching.</td></tr>
<tr><td>2025-11-04</td><td>LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment</td><td>[2511.02371](http://arxiv.org/pdf/2511.02371)</td><td>◆ Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for grounding large language model outputs in verifiable evidence.
◆ However, as modern AI agents transition from static knowledge bases to continuous multimodal streams encompassing text, images, video, and audio, two critical challenges arise: maintaining index freshness without prohibitive re-indexing costs, and preserving cross-modal semantic consistency across heterogeneous embedding spaces.
◆ We present LUMA-RAG, a lifelong multimodal agent architecture featuring three key innovations: (i) a streaming, multi-tier memory system that dynamically spills embeddings from a hot HNSW tier to a compressed IVFPQ tier under strict memory budgets; (ii) a streaming CLAP-&gt;CLIP alignment bridge that maintains cross-modal consistency through incremental orthogonal Procrustes updates; and (iii) stability-aware retrieval telemetry providing Safe@k guarantees by jointly bounding alignment drift and quantization error.</td></tr>
<tr><td>2025-11-03</td><td>SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</td><td>[2511.01390](http://arxiv.org/pdf/2511.01390)</td><td>◆ Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications.
◆ Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities.
◆ Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities.</td></tr>
<tr><td>2025-11-02</td><td>Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval</td><td>[2511.00925](http://arxiv.org/pdf/2511.00925)</td><td>◆ The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved increasing attention due to its wide applications, e.g.
◆ e-commerce.
◆ Despite progress made in this field, previous works suffer from using imbalanced samples of modalities and inconsistent low-quality information during training, resulting in sub-optimal performance.</td></tr>
<tr><td>2025-11-01</td><td>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</td><td>[2511.00635](http://arxiv.org/pdf/2511.00635)</td><td>◆ As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted.
◆ Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions.
◆ In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration.</td></tr>
<tr><td>2025-10-31</td><td>Approximate Diverse $k$-nearest Neighbor Search in Vector Database</td><td>[2510.27243](http://arxiv.org/pdf/2510.27243)</td><td>◆ Approximate $k$-nearest neighbor search (A$k$-NNS) is a core operation in vector databases, underpinning applications such as retrieval-augmented generation (RAG) and image retrieval.
◆ In these scenarios, users often prefer diverse result sets to minimize redundancy and enhance information value.
◆ However, existing greedy-based diverse methods frequently yield sub-optimal results, failing to adequately approximate the optimal similarity score under certain diversification level.</td></tr>
<tr><td>2025-11-03</td><td>Evaluating Perspectival Biases in Cross-Modal Retrieval</td><td>[2510.26861](http://arxiv.org/pdf/2510.26861)</td><td>◆ Multimodal retrieval systems are expected to operate in a semantic space, agnostic to the language or cultural origin of the query.
◆ In practice, however, retrieval outcomes systematically reflect perspectival biases: deviations shaped by linguistic prevalence and cultural associations.
◆ We study two such biases.</td></tr>
<tr><td>2025-10-30</td><td>Scaling Image Geo-Localization to Continent Level</td><td>[2510.26795](http://arxiv.org/pdf/2510.26795)</td><td>◆ Determining the precise geographic location of an image at a global scale remains an unsolved challenge.
◆ Standard image retrieval techniques are inefficient due to the sheer volume of images (&gt;100M) and fail when coverage is insufficient.
◆ Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions.</td></tr>
<tr><td>2025-10-29</td><td>Instance-Level Composed Image Retrieval</td><td>[2510.25387](http://arxiv.org/pdf/2510.25387)</td><td>◆ The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data.
◆ We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition.
◆ The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries.</td></tr>
<tr><td>2025-10-28</td><td>DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts</td><td>[2510.24813](http://arxiv.org/pdf/2510.24813)</td><td>◆ Recent lightweight retrieval-augmented image caption models often utilize retrieved data solely as text prompts, thereby creating a semantic gap by leaving the original visual features unenhanced, particularly for object details or complex scenes.
◆ To address this limitation, we propose $DualCap$, a novel approach that enriches the visual representation by generating a visual prompt from retrieved similar images.
◆ Our model employs a dual retrieval mechanism, using standard image-to-text retrieval for text prompts and a novel image-to-image retrieval to source visually analogous scenes.</td></tr>
<tr><td>2025-10-27</td><td>Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment</td><td>[2510.23224](http://arxiv.org/pdf/2510.23224)</td><td>◆ The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows.
◆ Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education.
◆ However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content.</td></tr>
<tr><td>2025-10-26</td><td>Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models</td><td>[2510.22868](http://arxiv.org/pdf/2510.22868)</td><td>◆ Wind turbine blades operate in harsh environments, making timely damage detection essential for preventing failures and optimizing maintenance.
◆ Drone-based inspection and deep learning are promising, but typically depend on large, labeled datasets, which limit their ability to detect rare or evolving damage types.
◆ To address this, we propose a zero-shot-oriented inspection framework that integrates Retrieval-Augmented Generation (RAG) with Vision-Language Models (VLM).</td></tr>
<tr><td>2025-10-26</td><td>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</td><td>[2510.22754](http://arxiv.org/pdf/2510.22754)</td><td>◆ Multi-agent cooperative SLAM often encounters challenges in similar indoor environments characterized by repetitive structures, such as corridors and rooms.
◆ These challenges can lead to significant inaccuracies in shared location identification when employing point cloud-based techniques.
◆ To mitigate these issues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that integrates text semantics and WiFi signal features to enhance location identification and loop closure detection.</td></tr>
<tr><td>2025-10-30</td><td>Cross-view Localization and Synthesis -- Datasets, Challenges and Opportunities</td><td>[2510.22736](http://arxiv.org/pdf/2510.22736)</td><td>◆ Cross-view localization and synthesis are two fundamental tasks in cross-view visual understanding, which deals with cross-view datasets: overhead (satellite or aerial) and ground-level imagery.
◆ These tasks have gained increasing attention due to their broad applications in autonomous navigation, urban planning, and augmented reality.
◆ Cross-view localization aims to estimate the geographic position of ground-level images based on information provided by overhead imagery while cross-view synthesis seeks to generate ground-level images based on information from the overhead imagery.</td></tr>
<tr><td>2025-10-26</td><td>STATUS Bench: A Rigorous Benchmark for Evaluating Object State Understanding in Vision-Language Models</td><td>[2510.22571](http://arxiv.org/pdf/2510.22571)</td><td>◆ Object state recognition aims to identify the specific condition of objects, such as their positional states (e.g., open or closed) and functional states (e.g., on or off).
◆ While recent Vision-Language Models (VLMs) are capable of performing a variety of multimodal tasks, it remains unclear how precisely they can identify object states.
◆ To alleviate this issue, we introduce the STAte and Transition UnderStanding Benchmark (STATUS Bench), the first benchmark for rigorously evaluating the ability of VLMs to understand subtle variations in object states in diverse situations.</td></tr>
<tr><td>2025-10-26</td><td>Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</td><td>[2510.22529](http://arxiv.org/pdf/2510.22529)</td><td>◆ Loop closure is critical in Simultaneous Localization and Mapping (SLAM) systems to reduce accumulative drift and ensure global mapping consistency.
◆ However, conventional methods struggle in perceptually aliased environments, such as narrow pipes, due to vector quantization, feature sparsity, and repetitive textures, while existing solutions often incur high computational costs.
◆ This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure detection method that achieves superior precision-recall, robustness, and computational efficiency.</td></tr>
<tr><td>2025-10-24</td><td>BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</td><td>[2510.20095](http://arxiv.org/pdf/2510.20095)</td><td>◆ This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models.
◆ Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits.
◆ Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations.</td></tr>
<tr><td>2025-10-18</td><td>Small Language Models Offer Significant Potential for Science Community</td><td>[2510.18890](http://arxiv.org/pdf/2510.18890)</td><td>◆ Recent advancements in natural language processing, particularly with large language models (LLMs), are transforming how scientists engage with the literature.
◆ While the adoption of LLMs is increasing, concerns remain regarding potential information biases and computational costs.
◆ Rather than LLMs, I developed a framework to evaluate the feasibility of precise, rapid, and cost-effective information retrieval from extensive geoscience literature using freely available small language models (MiniLMs).</td></tr>
<tr><td>2025-10-21</td><td>Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</td><td>[2510.18437](http://arxiv.org/pdf/2510.18437)</td><td>◆ At the core of Camouflaged Object Detection (COD) lies segmenting objects from their highly similar surroundings.
◆ Previous efforts navigate this challenge primarily through image-level modeling or annotation-based optimization.
◆ Despite advancing considerably, this commonplace practice hardly taps valuable dataset-level contextual information or relies on laborious annotations.</td></tr>
<tr><td>2025-10-21</td><td>ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</td><td>[2510.18433](http://arxiv.org/pdf/2510.18433)</td><td>◆ We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences.
◆ We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations.
◆ Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images.</td></tr>
<tr><td>2025-10-21</td><td>DualHash: A Stochastic Primal-Dual Algorithm with Theoretical Guarantee for Deep Hashing</td><td>[2510.18218](http://arxiv.org/pdf/2510.18218)</td><td>◆ Deep hashing converts high-dimensional feature vectors into compact binary codes, enabling efficient large-scale retrieval.
◆ A fundamental challenge in deep hashing stems from the discrete nature of quantization in generating the codes.
◆ W-type regularizations, such as $</td></tr>
<tr><td>2025-10-20</td><td>Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition</td><td>[2510.17739](http://arxiv.org/pdf/2510.17739)</td><td>◆ We address multi-reference visual place recognition (VPR), where reference sets captured under varying conditions are used to improve localisation performance.
◆ While deep learning with large-scale training improves robustness, increasing data diversity and model complexity incur extensive computational cost during training and deployment.
◆ Descriptor-level fusion via voting or aggregation avoids training, but often targets multi-sensor setups or relies on heuristics with limited gains under appearance and viewpoint change.</td></tr>
<tr><td>2025-10-16</td><td>Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval</td><td>[2510.14535](http://arxiv.org/pdf/2510.14535)</td><td>◆ Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification.
◆ Domain harmonization is thus a critical research focus.
◆ Recent approaches encode brain images $\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and $\boldsymbol{z_d}$ (domain-specific), achieving strong results.</td></tr>
<tr><td>2025-10-15</td><td>Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition</td><td>[2510.13464](http://arxiv.org/pdf/2510.13464)</td><td>◆ Visual Place Recognition (VPR) enables robots and autonomous vehicles to identify previously visited locations by matching current observations against a database of known places.
◆ However, VPR systems face significant challenges when deployed across varying visual environments, lighting conditions, seasonal changes, and viewpoints changes.
◆ Failure-critical VPR applications, such as loop closure detection in simultaneous localization and mapping (SLAM) pipelines, require robust estimation of place matching uncertainty.</td></tr>
<tr><td>2025-10-13</td><td>Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval</td><td>[2510.12014](http://arxiv.org/pdf/2510.12014)</td><td>◆ Text--image retrieval is necessary for applications such as product recommendation.
◆ Embedding-based approaches like CLIP enable efficient large-scale retrieval via vector similarity search, but they are primarily trained on literal caption-like text--image pairs and often fail to capture abstract or persona-driven attributes common in product recommendation applications (e.g., ``a gift for a mother who loves gardening&#x27;&#x27;).
◆ In contrast, state-of-the-art vision--language models (vLLMs) can align text with images in a flexible manner, but their limited context window prevents them from directly handling retrieval over large catalogs.</td></tr>
<tr><td>2025-10-10</td><td>Hierarchical Scheduling for Multi-Vector Image Retrieval</td><td>[2510.08976](http://arxiv.org/pdf/2510.08976)</td><td>◆ To effectively leverage user-specific data, retrieval augmented generation (RAG) is employed in multimodal large language model (MLLM) applications.
◆ However, conventional retrieval approaches often suffer from limited retrieval accuracy.
◆ Recent advances in multi-vector retrieval (MVR) improve accuracy by decomposing queries and matching against segmented images.</td></tr>
<tr><td>2025-10-09</td><td>DarkHash: A Data-Free Backdoor Attack Against Deep Hashing</td><td>[2510.08094](http://arxiv.org/pdf/2510.08094)</td><td>◆ Benefiting from its superior feature learning capabilities and efficiency, deep hashing has achieved remarkable success in large-scale image retrieval.
◆ Recent studies have demonstrated the vulnerability of deep hashing models to backdoor attacks.
◆ Although these studies have shown promising attack results, they rely on access to the training dataset to implant the backdoor.</td></tr>
<tr><td>2025-10-09</td><td>CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning</td><td>[2510.08003](http://arxiv.org/pdf/2510.08003)</td><td>◆ Composed Image Retrieval (CIR), which aims to find a target image from a reference image and a modification text, presents the core challenge of performing unified reasoning across visual and semantic modalities.
◆ While current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown progress, they predominantly function as ``black boxes.&quot; This inherent opacity not only prevents users from understanding the retrieval rationale but also restricts the models&#x27; ability to follow complex, fine-grained instructions.
◆ To overcome these limitations, we introduce CIR-CoT, the first end-to-end retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT) reasoning.</td></tr>
<tr><td>2025-10-09</td><td>Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision</td><td>[2510.07703](http://arxiv.org/pdf/2510.07703)</td><td>◆ Deep hashing has been widely adopted for large-scale image retrieval, with numerous strategies proposed to optimize hash function learning.
◆ Pairwise-based methods are effective in learning hash functions that preserve local similarity relationships, whereas center-based methods typically achieve superior performance by more effectively capturing global data distributions.
◆ However, the strength of center-based methods in modeling global structures often comes at the expense of underutilizing important local similarity information.</td></tr>
<tr><td>2025-10-08</td><td>Multi-hop Deep Joint Source-Channel Coding with Deep Hash Distillation for Semantically Aligned Image Retrieval</td><td>[2510.06868](http://arxiv.org/pdf/2510.06868)</td><td>◆ We consider image transmission via deep joint source-channel coding (DeepJSCC) over multi-hop additive white Gaussian noise (AWGN) channels by training a DeepJSCC encoder-decoder pair with a pre-trained deep hash distillation (DHD) module to semantically cluster images, facilitating security-oriented applications through enhanced semantic consistency and improving the perceptual reconstruction quality.
◆ We train the DeepJSCC module to both reduce mean square error (MSE) and minimize cosine distance between DHD hashes of source and reconstructed images.
◆ Significantly improved perceptual quality as a result of semantic alignment is illustrated for different multi-hop settings, for which classical DeepJSCC may suffer from noise accumulation, measured by the learned perceptual image patch similarity (LPIPS) metric.</td></tr>
<tr><td>2025-10-07</td><td>CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval</td><td>[2510.05586](http://arxiv.org/pdf/2510.05586)</td><td>◆ Existing Visual Language Models (VLMs) suffer structural limitations where a few low contribution tokens may excessively capture global semantics, dominating the information aggregation process and suppressing the discriminative features in text-driven image retrieval tasks.
◆ To address this, we introduce \textbf{CalibCLIP}, a training-free method designed to calibrate the suppressive effect of dominant tokens.
◆ Specifically, in the visual space, we propose the Contrastive Visual Enhancer (CVE), which decouples visual features into target and low information regions.</td></tr>
<tr><td>2025-10-06</td><td>Personalizing Retrieval using Joint Embeddings or &quot;the Return of Fluffy&quot;</td><td>[2510.05411](http://arxiv.org/pdf/2510.05411)</td><td>◆ The goal of this paper is to be able to retrieve images using a compound query that combines object instance information from an image, with a natural text description of what that object is doing or where it is.
◆ For example, to retrieve an image of &quot;Fluffy the unicorn (specified by an image) on someone&#x27;s head&quot;.
◆ To achieve this we design a mapping network that can &quot;translate&quot; from a local image embedding (of the object instance) to a text token, such that the combination of the token and a natural language query is suitable for CLIP style text encoding, and image retrieval.</td></tr>
<tr><td>2025-10-05</td><td>Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition</td><td>[2510.04282](http://arxiv.org/pdf/2510.04282)</td><td>◆ Sequential Visual Place Recognition (Seq-VPR) leverages transformers to capture spatio-temporal features effectively; however, existing approaches prioritize performance at the expense of flexibility and efficiency.
◆ In practice, a transformer-based Seq-VPR model should be flexible to the number of frames per sequence (seq-length), deliver fast inference, and have low memory usage to meet real-time constraints.
◆ To our knowledge, no existing transformer-based Seq-VPR method achieves both flexibility and efficiency.</td></tr>
<tr><td>2025-10-04</td><td>The Overlooked Value of Test-time Reference Sets in Visual Place Recognition</td><td>[2510.03751](http://arxiv.org/pdf/2510.03751)</td><td>◆ Given a query image, Visual Place Recognition (VPR) is the task of retrieving an image of the same place from a reference database with robustness to viewpoint and appearance changes.
◆ Recent works show that some VPR benchmarks are solved by methods using Vision-Foundation-Model backbones and trained on large-scale and diverse VPR-specific datasets.
◆ Several benchmarks remain challenging, particularly when the test environments differ significantly from the usual VPR training datasets.</td></tr>
<tr><td>2025-10-03</td><td>Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping</td><td>[2510.02874](http://arxiv.org/pdf/2510.02874)</td><td>◆ Traditional exteroceptive sensors in mobile robots, such as LiDARs and cameras often struggle to perceive the environment in poor visibility conditions.
◆ Recently, radar technologies, such as ultra-wideband (UWB) have emerged as potential alternatives due to their ability to see through adverse environmental conditions (e.g.
◆ dust, smoke and rain).</td></tr>
<tr><td>2025-10-03</td><td>Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4</td><td>[2510.02728](http://arxiv.org/pdf/2510.02728)</td><td>◆ Cross-modal drone navigation remains a challenging task in robotics, requiring efficient retrieval of relevant images from large-scale databases based on natural language descriptions.
◆ The RoboSense 2025 Track 4 challenge addresses this challenge, focusing on robust, natural language-guided cross-view image retrieval across multiple platforms (drones, satellites, and ground cameras).
◆ Current baseline methods, while effective for initial retrieval, often struggle to achieve fine-grained semantic matching between text queries and visual content, especially in complex aerial scenes.</td></tr>
<tr><td>2025-10-01</td><td>EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</td><td>[2510.01183](http://arxiv.org/pdf/2510.01183)</td><td>◆ Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced.
◆ Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration.
◆ Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene&#x27;s 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory.</td></tr>
<tr><td>2025-10-01</td><td>A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features</td><td>[2510.00978](http://arxiv.org/pdf/2510.00978)</td><td>◆ Visually localizing an image, i.e., estimating its camera pose, requires building a scene representation that serves as a visual map.
◆ The representation we choose has direct consequences towards the practicability of our system.
◆ Even when starting from mapping images with known camera poses, state-of-the-art approaches still require hours of mapping time in the worst case, and several minutes in the best.</td></tr>
<tr><td>2025-10-01</td><td>Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions</td><td>[2510.00783](http://arxiv.org/pdf/2510.00783)</td><td>◆ Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of research within robotics and computer vision, focusing on the simultaneous localization of robotic systems and associating semantic information to construct the most accurate and complete comprehensive model of the surrounding environment.
◆ Since the first foundational work in Semantic SLAM appeared more than two decades ago, this field has received increasing attention across various scientific communities.
◆ Despite its significance, the field lacks comprehensive surveys encompassing recent advances and persistent challenges.</td></tr>
<tr><td>2025-09-30</td><td>Video Object Segmentation-Aware Audio Generation</td><td>[2509.26604](http://arxiv.org/pdf/2509.26604)</td><td>◆ Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows.
◆ In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects.
◆ To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps.</td></tr>
<tr><td>2025-09-30</td><td>SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval</td><td>[2509.26330](http://arxiv.org/pdf/2509.26330)</td><td>◆ Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of a reference image while incorporating user-specified textual modifications.
◆ Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging.
◆ In this paper, we present SQUARE, a novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR.</td></tr>
<tr><td>2025-09-30</td><td>SETR: A Two-Stage Semantic-Enhanced Framework for Zero-Shot Composed Image Retrieval</td><td>[2509.26012](http://arxiv.org/pdf/2509.26012)</td><td>◆ Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve a target image given a reference image and a relative text, without relying on costly triplet annotations.
◆ Existing CLIP-based methods face two core challenges: (1) union-based feature fusion indiscriminately aggregates all visual cues, carrying over irrelevant background details that dilute the intended modification, and (2) global cosine similarity from CLIP embeddings lacks the ability to resolve fine-grained semantic relations.
◆ To address these issues, we propose SETR (Semantic-enhanced Two-Stage Retrieval).</td></tr>
<tr><td>2025-09-30</td><td>SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition</td><td>[2509.25723](http://arxiv.org/pdf/2509.25723)</td><td>◆ Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation.
◆ Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training.
◆ We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining.</td></tr>
<tr><td>2025-09-29</td><td>Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity</td><td>[2509.25520](http://arxiv.org/pdf/2509.25520)</td><td>◆ We consider the problem of vision-based 6-DoF object pose estimation in the context of the notional Mars Sample Return campaign, in which a robotic arm would need to localize multiple objects of interest for low-clearance pickup and insertion, under severely constrained hardware.
◆ We propose a novel localization algorithm leveraging a custom renderer together with a new template matching metric tailored to the edge domain to achieve robust pose estimation using only low-fidelity, textureless 3D models as inputs.
◆ Extensive evaluations on synthetic datasets as well as from physical testbeds on Earth and in situ Mars imagery shows that our method consistently beats the state of the art in compute and memory-constrained localization, both in terms of robustness and accuracy, in turn enabling new possibilities for cheap and reliable localization on general-purpose hardware.</td></tr>
<tr><td>2025-09-29</td><td>Performance-Efficiency Trade-off for Fashion Image Retrieval</td><td>[2509.24477](http://arxiv.org/pdf/2509.24477)</td><td>◆ The fashion industry has been identified as a major contributor to waste and emissions, leading to an increased interest in promoting the second-hand market.
◆ Machine learning methods play an important role in facilitating the creation and expansion of second-hand marketplaces by enabling the large-scale valuation of used garments.
◆ We contribute to this line of work by addressing the scalability of second-hand image retrieval from databases.</td></tr>
<tr><td>2025-09-28</td><td>Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras</td><td>[2509.24094](http://arxiv.org/pdf/2509.24094)</td><td>◆ Visual Place Recognition (VPR) enables systems to identify previously visited locations within a map, a fundamental task for autonomous navigation.
◆ Prior works have developed VPR solutions using event cameras, which asynchronously measure per-pixel brightness changes with microsecond temporal resolution.
◆ However, these approaches rely on dense representations of the inherently sparse camera output and require tens to hundreds of milliseconds of event data to predict a place.</td></tr>
<tr><td>2025-09-26</td><td>Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation</td><td>[2509.22307](http://arxiv.org/pdf/2509.22307)</td><td>◆ Lightweight 3D medical image segmentation remains constrained by a fundamental &quot;efficiency / robustness conflict&quot;, particularly when processing complex anatomical structures and heterogeneous modalities.
◆ In this paper, we study how to redesign the framework based on the characteristics of high-dimensional 3D images, and explore data synergy to overcome the fragile representation of lightweight methods.
◆ Our approach, VeloxSeg, begins with a deployable and extensible dual-stream CNN-Transformer architecture composed of Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC).</td></tr>
<tr><td>2025-09-23</td><td>SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment</td><td>[2509.20401](http://arxiv.org/pdf/2509.20401)</td><td>◆ Aligning 3D scene graphs is a crucial initial step for several applications in robot navigation and embodied perception.
◆ Current methods in 3D scene graph alignment often rely on single-modality point cloud data and struggle with incomplete or noisy input.
◆ We introduce SGAligner++, a cross-modal, language-aided framework for 3D scene graph alignment.</td></tr>
<tr><td>2025-09-24</td><td>A Versatile Foundation Model for AI-enabled Mammogram Interpretation</td><td>[2509.20271](http://arxiv.org/pdf/2509.20271)</td><td>◆ Breast cancer is the most commonly diagnosed cancer and the leading cause of cancer-related mortality in women globally.
◆ Mammography is essential for the early detection and diagnosis of breast lesions.
◆ Despite recent progress in foundation models (FMs) for mammogram analysis, their clinical translation remains constrained by several fundamental limitations, including insufficient diversity in training data, limited model generalizability, and a lack of comprehensive evaluation across clinically relevant tasks.</td></tr>
<tr><td>2025-09-23</td><td>Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions</td><td>[2509.19203](http://arxiv.org/pdf/2509.19203)</td><td>◆ Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have become the standard approach for learning discriminative vision-language representations.
◆ However, these models often exhibit shallow language understanding, manifesting bag-of-words behaviour.
◆ These limitations are reinforced by their dual-encoder design, which induces a modality gap.</td></tr>
<tr><td>2025-09-22</td><td>OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata</td><td>[2509.18350](http://arxiv.org/pdf/2509.18350)</td><td>◆ Accurate visual localization from aerial views is a fundamental problem with applications in mapping, large-area inspection, and search-and-rescue operations.
◆ In many scenarios, these systems require high-precision localization while operating with limited resources (e.g., no internet connection or GNSS/GPS support), making large image databases or heavy 3D models impractical.
◆ Surprisingly, little attention has been given to leveraging orthographic geodata as an alternative paradigm, which is lightweight and increasingly available through free releases by governmental authorities (e.g., the European Union).</td></tr>
<tr><td>2025-09-21</td><td>Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization</td><td>[2509.17049](http://arxiv.org/pdf/2509.17049)</td><td>◆ Fine-grained hashing has become a powerful solution for rapid and efficient image retrieval, particularly in scenarios requiring high discrimination between visually similar categories.
◆ To enable each hash bit to correspond to specific visual attributes, we propoe a novel method that harnesses learnable queries for attribute-aware hash codes learning.
◆ This method deploys a tailored set of queries to capture and represent nuanced attribute-level information within the hashing process, thereby enhancing both the interpretability and relevance of each hash bit.</td></tr>
<tr><td>2025-09-25</td><td>Efficient Multimodal Dataset Distillation via Generative Models</td><td>[2509.15472](http://arxiv.org/pdf/2509.15472)</td><td>◆ Dataset distillation aims to synthesize a small dataset from a large dataset, enabling the model trained on it to perform well on the original dataset.
◆ With the blooming of large language models and multimodal large language models, the importance of multimodal datasets, particularly image-text datasets, has grown significantly.
◆ However, existing multimodal dataset distillation methods are constrained by the Matching Training Trajectories algorithm, which significantly increases the computing resource requirement, and takes days to process the distillation.</td></tr>
<tr><td>2025-09-18</td><td>SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models</td><td>[2509.15432](http://arxiv.org/pdf/2509.15432)</td><td>◆ Visual Document Retrieval (VDR) typically operates as text-to-image retrieval using specialized bi-encoders trained to directly embed document images.
◆ We revisit a zero-shot generate-and-encode pipeline: a vision-language model first produces a detailed textual description of each document image, which is then embedded by a standard text encoder.
◆ On the ViDoRe-v2 benchmark, the method reaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual document encoder.</td></tr>
<tr><td>2025-09-18</td><td>PRISM: Product Retrieval In Shopping Carts using Hybrid Matching</td><td>[2509.14985](http://arxiv.org/pdf/2509.14985)</td><td>◆ Compared to traditional image retrieval tasks, product retrieval in retail settings is even more challenging.
◆ Products of the same type from different brands may have highly similar visual appearances, and the query image may be taken from an angle that differs significantly from view angles of the stored catalog images.
◆ Foundational models, such as CLIP and SigLIP, often struggle to distinguish these subtle but important local differences.</td></tr>
<tr><td>2025-09-18</td><td>Chain-of-Thought Re-ranking for Image Retrieval Tasks</td><td>[2509.14746](http://arxiv.org/pdf/2509.14746)</td><td>◆ Image retrieval remains a fundamental yet challenging problem in computer vision.
◆ While recent advances in Multimodal Large Language Models (MLLMs) have demonstrated strong reasoning capabilities, existing methods typically employ them only for evaluation, without involving them directly in the ranking process.
◆ As a result, their rich multimodal reasoning abilities remain underutilized, leading to suboptimal performance.</td></tr>
<tr><td>2025-09-18</td><td>DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising</td><td>[2509.14565](http://arxiv.org/pdf/2509.14565)</td><td>◆ Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap.
◆ Current SD-map-based approaches primarily focus on Bird&#x27;s-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS.
◆ Although GPS is readily available, it suffers from multipath errors in urban environments.</td></tr>
<tr><td>2025-09-18</td><td>Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods</td><td>[2509.14516](http://arxiv.org/pdf/2509.14516)</td><td>◆ Event-based localization research and datasets are a rapidly growing area of interest, with a tenfold increase in the cumulative total number of published papers on this topic over the past 10 years.
◆ Whilst the rapid expansion in the field is exciting, it brings with it an associated challenge: a growth in the variety of required code and package dependencies as well as data formats, making comparisons difficult and cumbersome for researchers to implement reliably.
◆ To address this challenge, we present Event-LAB: a new and unified framework for running several event-based localization methodologies across multiple datasets.</td></tr>
<tr><td>2025-09-17</td><td>Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models</td><td>[2509.14427](http://arxiv.org/pdf/2509.14427)</td><td>◆ Information retrieval with compact binary embeddings, also referred to as hashing, is crucial for scalable fast search applications, yet state-of-the-art hashing methods require expensive, scenario-specific training.
◆ In this work, we introduce Hashing-Baseline, a strong training-free hashing method leveraging powerful pretrained encoders that produce rich pretrained embeddings.
◆ We revisit classical, training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization, to produce a strong baseline for hashing.</td></tr>
<tr><td>2025-09-17</td><td>CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts</td><td>[2509.14104](http://arxiv.org/pdf/2509.14104)</td><td>◆ Self-supervised learning through masked autoencoders has attracted great attention for remote sensing (RS) foundation model (FM) development, enabling improved representation learning across diverse sensors and downstream tasks.
◆ However, existing RS FMs often either suffer from substantial computational complexity during both training and inference or exhibit limited representational capacity.
◆ These issues restrict their practical applicability in RS.</td></tr>
<tr><td>2025-09-16</td><td>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</td><td>[2509.13474](http://arxiv.org/pdf/2509.13474)</td><td>◆ Ensuring accurate localization of robots in environments without GPS capability is a challenging task.
◆ Visual Place Recognition (VPR) techniques can potentially achieve this goal, but existing RGB-based methods are sensitive to changes in illumination, weather, and other seasonal changes.
◆ Existing cross-modal localization methods leverage the geometric properties of RGB images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.</td></tr>
<tr><td>2025-09-18</td><td>MapAnything: Universal Feed-Forward Metric 3D Reconstruction</td><td>[2509.13414](http://arxiv.org/pdf/2509.13414)</td><td>◆ We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras.
◆ MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame.
◆ Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more.</td></tr>
<tr><td>2025-09-17</td><td>DiffHash: Text-Guided Targeted Attack via Diffusion Models against Deep Hashing Image Retrieval</td><td>[2509.12824](http://arxiv.org/pdf/2509.12824)</td><td>◆ Deep hashing models have been widely adopted to tackle the challenges of large-scale image retrieval.
◆ However, these approaches face serious security risks due to their vulnerability to adversarial examples.
◆ Despite the increasing exploration of targeted attacks on deep hashing models, existing approaches still suffer from a lack of multimodal guidance, reliance on labeling information and dependence on pixel-level operations for attacks.</td></tr>
<tr><td>2025-09-15</td><td>Bridging Vision Language Models and Symbolic Grounding for Video Question Answering</td><td>[2509.11862](http://arxiv.org/pdf/2509.11862)</td><td>该论文的核心贡献是提出了一个名为SG-VLM的模块化框架，旨在通过符号化场景图增强视频问答中的时空与因果推理能力。

◆ 引入符号化场景图（SGs）作为视频问答的中间 grounding 信号，提供结构化对象-关系表示以补充视觉语言模型的整体推理。
◆ 提出SG-VLM框架，将冻结的视觉语言模型（VLMs）与场景图 grounding 机制通过提示和视觉定位进行集成。
◆ 在多个基准测试（NExT-QA、iVQA、ActivityNet-QA）和不同VLMs（QwenVL、InternVL）上验证了方法的有效性。
◆ 显著提升了模型在因果推理和时间推理任务上的性能，尤其在需要细粒度时空关系的视频问答中表现突出。
◆ 研究结果揭示了符号 grounding 方法的潜力和当前局限性，为未来融合视觉语言模型与符号推理的混合方法提供了重要参考。</td></tr>
<tr><td>2025-09-14</td><td>UnLoc: Leveraging Depth Uncertainties for Floorplan Localization</td><td>[2509.11301](http://arxiv.org/pdf/2509.11301)</td><td>UnLoc提出了一种基于数据驱动的序列化相机平面图定位方法，其核心贡献在于通过深度不确定性建模显著提升了定位精度与泛化能力。  
◆ 引入概率化深度预测模型，将单目深度估计结果表示为显式概率分布，从而有效量化预测不确定性。  
◆ 摆脱对每场景定制深度网络的依赖，直接利用预训练单目深度模型，大幅提升方法在未知环境中的泛化性能。  
◆ 在长序列与短序列定位任务中均实现突破，在LaMAR HGE数据集上长序列定位召回率提升2.7倍，短序列提升16.7倍。  
◆ 通过融合易于获取且长期稳定的平面图数据，克服视觉外观变化带来的挑战，增强系统鲁棒性。  
该方法在合成与真实大规模数据集上验证了其优越性，为视觉定位提供了一种高效且通用的解决方案。</td></tr>
<tr><td>2025-09-11</td><td>Listening for &quot;You&quot;: Enhancing Speech Image Retrieval via Target Speaker Extraction</td><td>[2509.09306](http://arxiv.org/pdf/2509.09306)</td><td>该论文提出了一种新颖的目标说话人语音-图像检索任务及框架，用于解决多说话人场景下语音图像检索的难题。  
◆ 首次引入目标说话人提取技术，从混合语音中分离并识别目标说话人的指令。  
◆ 结合自监督音频编码器和视觉模型，通过目标说话人感知的对比学习进行跨模态对齐。  
◆ 设计了端到端的联合训练框架，将说话人提取与检索任务统一优化，提升系统整体性能。  
在二说话人和三说话人混合语音数据集上的实验表明，该方法显著优于现有基线模型，检索准确率提升明显。  
该技术为辅助机器人和多模态交互系统的实际应用提供了有效解决方案。</td></tr>
<tr><td>2025-09-09</td><td>Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark</td><td>[2509.07362](http://arxiv.org/pdf/2509.07362)</td><td>该论文的核心贡献是构建了一个解决空地跨模态定位挑战的综合基准。其创新点包括：
◆ 创建了一个新的大规模空地跨模态数据集，集成了来自移动测量系统的地面图像和三个城市（武汉、香港、旧金山）的机载激光扫描点云。
◆ 解决了该领域平台多样化数据缺乏的问题，为算法开发提供了丰富的数据基础。
◆ 提出了一种适用于大规模城市场景的可靠地面真值生成方法，解决了以往该环节的缺失问题。
◆ 首次在空地跨平台设置下对现有的图像到点云配准算法进行了全面基准测试，评估了其性能。
◆ 为提升在纹理缺失、大视角变化等挑战下的视觉定位精度提供了新的途径，推动了相关领域的发展。</td></tr>
<tr><td>2025-09-08</td><td>Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval</td><td>[2509.06566](http://arxiv.org/pdf/2509.06566)</td><td>本文针对场景级草图检索任务，提出了一种强调草图固有模糊性和噪声的鲁棒性训练方案。其核心创新点包括：
◆ 重新审视了场景级草图检索问题，强调真实手绘草图的模糊性和噪声特性，而非仅关注模型结构改进。
◆ 设计了一种显式的训练目标，专门针对草图的高变异性进行优化，提升了模型的鲁棒性。
◆ 通过结合适当的预训练策略、编码器架构和损失函数，在不增加模型复杂度的前提下实现了最先进的性能。
◆ 在FS-COCO和SketchyCOCO等挑战性数据集上验证了方法的有效性，并强调了训练设计在跨模态检索中的关键作用。
◆ 指出了改进场景级草图检索评估场景的必要性，为该领域的未来发展提供了重要见解。</td></tr>
<tr><td>2025-09-05</td><td>Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)</td><td>[2509.04948](http://arxiv.org/pdf/2509.04948)</td><td>该论文针对移动机器人在办公室环境中的拓扑定位问题，提出了一种仅依靠单目彩色相机图像、不依赖时序连续性的视觉定位方法。  
◆ 系统性地定量比较了多种先进视觉描述符（如颜色直方图、SIFT、ASIFT、RGB-SIFT及词袋模型）的性能。  
◆ 深入分析了不同特征描述符、距离度量方法和分类器的组合效果，并通过标准评估指标和可视化方法扩展了已有实验。  
◆ 在ImageCLEF评测任务中验证了所提配置的有效性，成功实现了对新图像序列的最可能位置识别。  
论文为外观描述符、相似性度量与分类器的合理配置提供了实证依据，为构建更鲁棒的实时定位系统奠定了基础。</td></tr>
<tr><td>2025-09-05</td><td>FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph</td><td>[2509.04772](http://arxiv.org/pdf/2509.04772)</td><td>本文提出了FloodVision，一种结合基础视觉语言模型与领域知识图的零样本洪水深度估计框架。其核心贡献在于显著提升了城市洪水深度估计的准确性和泛化能力。
◆ 创新性地将强大的基础视觉语言模型GPT-4o的语义推理能力，与结构化的领域知识图谱相结合，实现了零样本深度估计。
◆ 构建了一个包含车辆、行人等常见城市物体真实尺寸的知识图谱，为模型推理提供了可靠的物理现实依据，有效缓解了幻觉问题。
◆ 提出了一套动态处理流程，包括参考物体识别、淹没比例估算和统计离群值过滤，以计算出精确的深度值。
◆ 在真实数据集上的评估表明，其平均绝对误差低至8.17厘米，较GPT-4o基线提升了20.5%，并超越了以往的CNN方法，且具备近实时处理能力。</td></tr>
<tr><td>2025-09-05</td><td>Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking</td><td>[2509.04351](http://arxiv.org/pdf/2509.04351)</td><td>该论文提出了与主流相反的“局部到全局”图像检索新范式，取代了传统的“全局到局部”方法。  
◆ 利用新兴的高效局部特征搜索技术，首先进行大规模精细的局部匹配，以找到全局特征容易遗漏的局部相似图像。  
◆ 创新性地引入了一种基于局部检索相似性的全局特征即时重排序方法，在重排序阶段才动态生成全局特征。  
◆ 采用多维缩放技术，将局部特征检索获得的相似性关系嵌入到全局特征表示中，使全局特征能够尊重局部匹配的结果。  
这种结合使得重排序过程既保持了高计算效率，又显著提升了检索精度。  
实验表明，该方法在Revisited Oxford和Paris数据集上取得了新的最先进性能。</td></tr>
<tr><td>2025-09-04</td><td>DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval</td><td>[2509.04193](http://arxiv.org/pdf/2509.04193)</td><td>该论文提出了一种基于扩散模型的无监督跨域图像检索方法DUDE，其核心贡献在于通过特征解耦解决跨域检索中对象特征与域风格纠缠的难题。  
◆ 利用文本到图像生成模型实现对象特征与域特定风格的解耦，增强语义表示的纯净性。  
◆ 提出渐进式跨域互近邻对齐机制，通过域内到域间的逐步对齐提升特征匹配可靠性。  
◆ 在三个基准数据集（涵盖13个域）上取得最先进性能，验证了方法的泛化能力。  
该方法无需标注即可实现跨域精准检索，为无监督域适应领域提供了新思路。</td></tr>
<tr><td>2025-09-02</td><td>Scale, Don&#x27;t Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time</td><td>[2509.02129](http://arxiv.org/pdf/2509.02129)</td><td>该论文提出了一种用于视觉地点识别（VPR）的零样本新框架，核心贡献在于无需微调即可实现高效且强大的跨域识别。其创新点包括：
◆ 提出测试时缩放（TTS）框架，利用多模态大模型（MLLM）的视觉-语言对齐能力，通过基于引导的方法直接进行相似性评分，避免了传统微调的高计算开销。
◆ 采用结构化提示生成长度可控的JSON输出，消除了传统方法中复杂的多阶段处理流程，简化了流程。
◆ 引入不确定性感知自一致性（UASC）机制，使模型能在测试时进行实时自适应，无需额外训练成本，提升了实时性。
◆ 实现了卓越的跨域泛化能力，在多种环境中显著提升性能，同时计算效率提升了高达210倍。
实验结果表明，该方法在保持高性能的同时，极大提升了效率与适应性。</td></tr>
<tr><td>2025-09-02</td><td>Ensemble-Based Event Camera Place Recognition Under Varying Illumination</td><td>[2509.01968](http://arxiv.org/pdf/2509.01968)</td><td>本文提出了一种集成式事件相机地点识别方法，显著提升了在剧烈光照变化下的环境鲁棒性。  
◆ 采用多事件重建、多特征提取与多时序分辨率的集成融合策略，突破了以往仅融合时序信息的局限。  
◆ 在跨日-夜光照变化场景下实现了Recall@1指标57%的相对提升，表现出极强的光照鲁棒性。  
◆ 在长达8公里的实际驾驶数据集上验证了有效性，未进行降采样，保留了真实事件密度变化。  
◆ 提出了对序列匹配框架的改进，有效提升了长序列下的识别性能。  
◆ 系统分析了事件表征、重建方法和特征提取等关键设计选项的影响，为后续研究提供了重要参考。</td></tr>
<tr><td>2025-09-01</td><td>M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision</td><td>[2509.01360](http://arxiv.org/pdf/2509.01360)</td><td>M3Ret论文的核心贡献是提出了一种统一的自监督视觉编码器，成功解决了多模态医学图像检索的碎片化问题。其创新点包括：
◆构建了大规模混合模态医学数据集，包含86万余样本，涵盖2D、3D及视频数据
◆首次在不使用任何模态定制化设计的前提下，实现了统一的多模态视觉编码器
◆融合生成式(MAE)与对比式(SimDINO)自监督学习范式，学习可迁移的视觉表示
◆在零样本检索任务上全面超越DINOv3和BMC-CLIP等强基线模型
◆展现出强大的跨模态对齐能力，无需配对数据即可实现跨模态检索
◆首次证明纯视觉自监督学习可泛化至未见模态（如未训练的MRI数据），为医学基础模型发展提供了新方向</td></tr>
<tr><td>2025-09-01</td><td>ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization</td><td>[2509.01259](http://arxiv.org/pdf/2509.01259)</td><td>该论文提出了ReCap系统，旨在解决传统图像描述生成无法捕捉事件级语义的问题，通过整合相关文章中的上下文信息来生成叙事丰富且事实准确的描述。  
◆ 设计了一个两阶段文章检索系统，结合DINOv2全局特征相似度初选和局部块互近邻相似度重排序，提升事件相关文章的检索精度。  
◆ 开发了上下文提取框架，综合文章摘要、通用描述和源数据信息，为生成描述提供多维度语义支持。  
◆ 引入了基于大语言模型的描述生成机制，并采用语义高斯归一化技术，增强生成文本的流畅性和相关性。  
在EVENTA 2025挑战赛中，ReCap在OpenEvents V1数据集上取得0.54666的综合评分，排名第二，验证了其有效性和实用性。  
该系统为新闻存档等高要求领域提供了视觉感知与真实世界知识融合的解决方案。</td></tr>
<tr><td>2025-09-03</td><td>Multimodal Iterative RAG for Knowledge Visual Question Answering</td><td>[2509.00798](http://arxiv.org/pdf/2509.00798)</td><td>该论文提出了MI-RAG，一个多模态迭代检索增强生成框架，用于解决知识密集型视觉问答任务。其核心创新点在于：

◆ 采用迭代式检索-推理框架，通过多轮迭代逐步完善外部知识的获取与理解，克服了传统单次检索知识不足的局限。

◆ 利用累积的推理记录动态生成多查询，驱动对包含视觉和文本信息的异构知识库进行联合搜索。

◆ 实现了跨模态的知识融合与推理更新，将新检索到的知识合成到推理记录中，进行渐进式的精化理解。

实验证明，该方法在多个挑战性基准上显著提高了检索召回率和答案准确率，为知识密集型视觉问答提供了一个可扩展的组合推理方案。</td></tr>
<tr><td>2025-08-31</td><td>Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification</td><td>[2509.00752](http://arxiv.org/pdf/2509.00752)</td><td>本文提出了一种面向内窥镜图像分析的统一视觉-语言框架，其核心贡献在于通过多模态对比学习实现三类临床任务的高效协同处理。  
◆ 采用CLIP ViT-B/16主干网络并引入低秩自适应（LoRA）技术，实现有限医疗数据下的高效微调。  
◆ 提出多级CLS令牌聚合机制，增强视觉特征的多样性和表征能力。  
◆ 设计球面特征插值方法，优化跨模态语义对齐效果。  
◆ 创新性地引入类别特定的自然语言提示，将诊断文本上下文与视觉特征通过对比学习和监督分类联合训练目标进行融合。  
该框架在多项任务中达到S性能（分类准确率95%，检索Recall@1超0.92），并通过消融实验验证了各模块的有效性，为低资源医疗场景提供了鲁棒的多模态理解方案。</td></tr>
<tr><td>2025-08-31</td><td>EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions</td><td>[2509.00751](http://arxiv.org/pdf/2509.00751)</td><td>该论文提出了一个面向复杂事件描述的多模态图像检索系统EVENT-Retriever，其核心创新在于通过多阶段框架解决传统方法对隐含事件语义和长文本描述的检索瓶颈。  
◆ 结合密集文档检索、事件感知语言模型重排序和高效图像收集的多阶段检索架构  
◆ 利用Qwen3系列模型实现文章搜索、上下文对齐和精准图像评分的分层处理  
◆ 引入基于标题的语义匹配与排序感知选择机制增强事件关联性  
◆ 采用 Reciprocal Rank Fusion 融合多配置输出提升系统鲁棒性  
该系统在EVENTA 2025挑战赛Track 2私有测试集上获得第一名，证明了语言推理与多模态检索结合对复杂现实图像理解的有效性。</td></tr>
<tr><td>2025-08-29</td><td>Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders</td><td>[2509.00177](http://arxiv.org/pdf/2509.00177)</td><td>该论文针对类别级文本-图像检索任务，提出了一种解决模态差异的创新方法。其核心贡献在于通过融合生成模型与视觉编码器，显著提升了跨模态检索性能。

◆ 提出两阶段检索框架：首先生成模型将文本查询转换为视觉查询，再用视觉模型计算图像间相似度。
◆ 利用扩散模型生成高质量图像，将文本模态转化为视觉模态，有效缩小文本与图像在表示空间中的差距。
◆ 设计聚合网络整合多个生成图像的向量表示，形成单一且鲁棒的查询表征。
◆ 创新性地融合文本和生成图像双模态的相似度评分，进一步提升检索精度。
该方法综合运用了视觉语言模型、扩散生成模型和视觉编码器的最新进展，在多个评估中显著优于仅依赖文本查询的检索方法。</td></tr>
<tr><td>2025-08-29</td><td>HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones</td><td>[2508.21539](http://arxiv.org/pdf/2508.21539)</td><td>该论文针对自然语言引导无人机任务中的复杂视觉-语言理解挑战，提出了HCCM分层跨粒度对比与匹配学习框架。其核心创新如下：
◆ 提出区域-全局图像文本对比学习（RG-ITC），无需精确场景划分即可捕获从局部到全局的分层语义对齐；
◆ 设计区域-全局图像文本匹配（RG-ITM），通过评估全局跨模态表征中的局部语义一致性来增强组合推理能力；
◆ 引入动量对比与蒸馏机制（MCD），有效缓解无人机文本描述不完整或模糊带来的对齐不稳定问题；
◆ 在GeoText-1652和ERA数据集上实现检索精度突破，显著优于现有方法并展现强大零样本泛化能力。</td></tr>
<tr><td>2025-08-27</td><td>Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)</td><td>[2508.20322](http://arxiv.org/pdf/2508.20322)</td><td>该论文提出了一种名为SLiCS的方法，用于解耦视觉-语言共嵌入空间中的语义信息。其核心贡献是通过稀疏线性概念子空间实现嵌入向量的结构化分解。  
◆ 提出一种监督式字典学习框架，将嵌入向量分解为多个概念特定的成分，每个成分由字典中一组稀疏非负的原子向量线性组合而成。  
◆ 设计了一种新颖的交替优化算法，保证收敛性，并能学习具有分组结构的字典，其组活动与多标签信息匹配。  
◆ 利用文本共嵌入特性，实现无监督字典学习：通过概念标签的文本嵌入进行零样本分类，自动生成实例级多标签。  
◆ 能够为每个概念子空间找到语义明确的文本描述，增强可解释性。  
该方法在概念过滤图像检索和条件生成任务中表现出更高精度，适用于CLIP、TiTok和DINOv2等多种嵌入空间。</td></tr>
<tr><td>2025-08-27</td><td>Low-exposure, high-quality multimodal speckle X-ray imaging via an intrinsic gradient-flow approach</td><td>[2508.20209](http://arxiv.org/pdf/2508.20209)</td><td>该论文提出了一种基于梯度流方法的新型多模态散斑X射线成像技术。其核心创新在于开发了梯度流MIST算法，显著提升了成像性能和数据效率。  
◆ 首次将梯度流方法引入散斑成像领域，通过求解福克-普朗克方程同步获取衰减、相移和暗场三种互补成像模式  
◆ 大幅减少成像所需数据量，降低实验中对曝光量和采样数量的要求  
◆ 在保持X射线福克-普朗克方程完整通用性的前提下，突破传统算法的局限性  
◆ 显著提升暗场图像质量，能有效显示低于空间分辨率的亚像素结构信息  
◆ 通过澳大利亚同步辐射实验验证了该方法在相位衬度和暗场成像中的应用潜力，特别适用于需要简化实验流程的场景。</td></tr>
<tr><td>2025-08-27</td><td>Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study</td><td>[2508.20188](http://arxiv.org/pdf/2508.20188)</td><td>该论文探索了如何利用定量皮肤属性提升多模态大语言模型在皮肤疾病诊断中的可解释性。  
◆ 创新性地将多模态大语言模型（MLLMs）与定量皮肤属性（如病灶面积）相结合，以增强诊断推理的可解释性。  
◆ 提出通过微调MLLMs，使其能够从皮肤图像中预测这些定量属性值，从而实现模型嵌入空间与临床属性的对齐。  
◆ 采用基于内容的图像检索方法，在SLICE-3D数据集上验证了嵌入空间与属性之间的关联性。  
◆ 为模型诊断结果提供了可量化的视觉依据，使模型输出更具可信度和交互性。  
这一研究为构建更透明、可解释的医疗人工智能系统提供了新的思路和方法基础。</td></tr>
<tr><td>2025-08-27</td><td>Addressing Deepfake Issue in Selfie banking through camera based authentication</td><td>[2508.19714](http://arxiv.org/pdf/2508.19714)</td><td>该论文的核心贡献是提出了一种利用相机成像特征来防御自拍银行中深度伪造攻击的新型认证方法。

◆ 创新性地将原本用于图像溯源（如图片相机定位）的取证识别系统，应用于深度伪造检测领域，实现了技术应用的跨界迁移。
◆ 该方法专注于利用相机本身的硬件缺陷（如镜头光学特性、传感器噪声模式）作为生物特征之外的辅助认证因素，因为深度伪造技术难以完美复制这些物理层面的细微特征。
◆ 通过分析图像中嵌入的相机固有“指纹”来区分真实拍摄的照片与AI生成的伪造图像，为现有的面部识别生物系统增加了一个强大的安全层。
◆ 为解决自拍银行等金融场景下面临的日益严峻的深度伪造威胁，提供了一种实用且可能更可靠的解决方案。</td></tr>
<tr><td>2025-08-26</td><td>Can we make NeRF-based visual localization privacy-preserving?</td><td>[2508.18971](http://arxiv.org/pdf/2508.18971)</td><td>该论文针对基于NeRF的视觉定位方法存在的隐私泄露风险提出了解决方案。  
◆ 首先设计了一种新的评估协议，用于系统检验NeRF表示中的隐私保护性，发现即使移除颜色预测头，其几何表示仍会存储敏感细节。  
◆ 提出一种自监督学习框架，将RGB图像转换为分割标签作为训练监督，避免直接使用原始图像数据。  
◆ 构建了ppNeSF（隐私保护神经分割场），以分割标签替代RGB进行训练，确保场景表示既粗糙无法还原细节，又保留足够的判别性用于定位。  
◆ 在保护隐私的同时，该方法实现了先进的视觉定位精度，平衡了隐私与实用性。  
◆ 整体工作首次系统揭示了NeRF的隐私漏洞，并提供了可替代的隐私保护范式。</td></tr>
<tr><td>2025-08-26</td><td>Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025</td><td>[2508.18904](http://arxiv.org/pdf/2508.18904)</td><td>该论文的核心贡献是推出了首个专注于事件级多模态理解的大规模基准挑战EVENTA，以解决传统方法在图像分析中忽视上下文和语义深度的问题。

◆ 首创大规模事件级多模态理解基准，突破传统图像描述与检索的表层识别局限。
◆ 通过整合上下文、时序和语义信息，构建“人物、时间、地点、事件、原因”五维事件理解框架。
◆ 基于OpenEvents V1数据集设计双赛道：事件增强图像检索与描述，以及事件驱动图像检索。
◆ 建立包含45支国际团队参与的公平评估体系，通过公开和私有测试阶段确保结果可复现性。
◆ 为叙事驱动多媒体AI奠定基础，推动新闻、媒体分析、文化存档等领域的上下文感知应用发展。</td></tr>
<tr><td>2025-08-25</td><td>GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations</td><td>[2508.18242](http://arxiv.org/pdf/2508.18242)</td><td>GSVisLoc提出了一种专为3D高斯泼溅（3DGS）场景表示设计的通用视觉定位方法。  
◆ 首次实现了无需任何修改或重训练，直接利用原始3DGS模型进行视觉定位。  
◆ 通过下采样和编码3D高斯来提取场景特征，并与查询图像特征进行鲁棒匹配。  
◆ 采用由粗到精的三阶段流程：粗匹配、精细匹配和姿态优化，确保高精度位姿估计。  
◆ 在室内外标准基准测试中表现出竞争力，显著优于现有基于3DGS的基线方法。  
◆ 展现出强大的泛化能力，无需额外训练即可直接应用于全新场景。</td></tr>
<tr><td>2025-08-25</td><td>SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization</td><td>[2508.17972](http://arxiv.org/pdf/2508.17972)</td><td>SAIL-Recon提出了一种用于大规模运动恢复结构（SfM）的前馈Transformer模型，旨在解决现有场景回归方法难以处理大量输入图像的问题。

◆ 核心创新是将视觉定位能力融入场景回归网络，通过引入锚点图像子集来构建神经场景表示。
◆ 该方法首先从锚点图像计算出一个紧凑的神经场景表示，作为全局场景先验。
◆ 回归网络随后以该神经表示为条件进行微调，从而能够高效地重建所有输入图像的相机位姿和3D结构。
◆ 该方法在保持场景回归方法应对极端视角变化优势的同时，成功将其扩展至大规模场景。
实验表明，该方法在TUM-RGBD、CO3Dv2和Tanks &amp; Temples等多个基准测试中，在相机位姿估计和新视角合成任务上均达到了最先进的性能。</td></tr>
<tr><td>2025-08-24</td><td>Data Leakage in Visual Datasets</td><td>[2508.17416](http://arxiv.org/pdf/2508.17416)</td><td>该论文的核心贡献是系统性地分析了视觉数据集中的数据泄漏问题及其对模型评估可靠性的影响。  
◆首次对视觉数据泄漏进行了多维度分类，依据模态、覆盖范围和程度划分泄漏类型。  
◆采用图像检索技术实证检验了多个主流数据集，发现所有被分析数据集均存在不同形式的泄漏。  
◆证明了各类泄漏（从严重到轻微）均会损害下游任务中模型评估的公正性。  
◆揭示了互联网数据源与公开基准并存导致的泄漏必然性，呼吁学界关注数据构建规范。  
研究结果对视觉领域基准构建和模型评估实践具有重要警示意义。</td></tr>
<tr><td>2025-08-22</td><td>Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval</td><td>[2508.16707](http://arxiv.org/pdf/2508.16707)</td><td>本文提出了一种联合稀疏-密集检索优化框架，用于提升多模态文本-图像检索性能。  
◆ 通过自知识蒸馏实现稀疏与密集表示的双向协同学习，突破以往单向蒸馏或独立训练的限制。  
◆ 提出融合相似度分数（稀疏与密集得分的加权和）作为共享教师信号，同步优化两种表示。  
◆ 仅微调密集编码器最后一层和稀疏投影头，无需全模型重训练，高效兼容现有视觉-语言预训练模型。  
实验表明，该框架使稀疏检索器性能显著优于现有稀疏基线，甚至达到或超越密集模型效果，同时保持稀疏模型的高效与可解释性优势。</td></tr>
<tr><td>2025-08-21</td><td>DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding</td><td>[2508.15297](http://arxiv.org/pdf/2508.15297)</td><td>该论文提出了DesignCLIP，一个基于CLIP的多模态框架，用于提升设计专利的理解与分析。其核心贡献与创新点包括：
◆ 构建了首个针对美国设计专利的大规模数据集，为多模态研究提供了基础。
◆ 提出类感知分类与对比学习策略，有效适应专利数据的抽象和结构性特点。
◆ 利用生成式详细标注和专利图像的多视角学习，增强了图像与文本的语义对齐。
◆ 验证了框架在专利分类和检索任务上的优越性，显著超越现有基线及SOTA模型。
◆ 探索了多模态专利检索的应用潜力，为设计创新提供更多样化的灵感来源。</td></tr>
<tr><td>2025-08-19</td><td>UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion</td><td>[2508.13843](http://arxiv.org/pdf/2508.13843)</td><td>◆ 提出了UniECS统一多模态电商搜索框架，能灵活处理图像、文本及其任意组合的检索场景，突破了传统方法局限于固定模态配对的限制。  
◆ 设计了新型门控多模态编码器，采用自适应融合机制，有效整合不同模态表征并处理模态缺失问题。  
◆ 开发了综合训练策略，结合跨模态对齐损失、局部对齐损失、模态内对比损失和自适应损失加权，优化模型学习效果。  
◆ 构建了M-BEER基准数据集，包含5万商品对，为多模态电商检索提供全面评估标准。  
◆ 在多个基准测试中显著优于现有模型（如文本搜图任务R@10提升28%），参数量仅0.2B，效率远超更大规模模型。  
◆ 成功部署于快手电商搜索平台，点击率提升2.74%、收入增长8.33%，验证了实际应用价值。</td></tr>
<tr><td>2025-08-19</td><td>ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</td><td>[2508.13488](http://arxiv.org/pdf/2508.13488)</td><td>该论文提出了一种在重复环境中进行鲁棒回环闭合验证的方法ROVER，其核心创新在于利用历史轨迹作为先验约束来提升验证可靠性。  
◆ 首次将机器人的时空运动轨迹作为先验知识引入回环验证过程，突破了传统方法仅依赖外观特征的局限性。  
◆ 提出通过位姿图优化生成候选回环对应的轨迹，并设计评分机制评估该轨迹与无回环先验轨迹的一致性。  
◆ 在存在高度相似结构的重复环境中能有效拒绝错误回环，显著降低SLAM系统的误检风险。  
实验证明，该方法在公开数据集和真实场景中均表现出优越性能，并可无缝集成至现有先进SLAM系统中。</td></tr>
<tr><td>2025-08-17</td><td>CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval</td><td>[2508.12290](http://arxiv.org/pdf/2508.12290)</td><td>◆ 提出CLAIR方法，利用CLIP生成的噪声伪标签进行弱监督零样本跨域图像检索（WSZS-CDIR），替代传统无监督方法。  
◆ 通过CLIP文本与图像特征的相似度计算置信分数，有效优化噪声伪标签的质量。  
◆ 设计类感知潜在空间编码机制，结合实例间和簇间对比损失，提升特征区分度。  
◆ 提出跨域对比损失减少域差异，并创新性地通过闭式解学习跨域映射函数，仅用CLIP文本嵌入实现特征对齐。  
◆ 引入可学习提示词增强零样本泛化能力，支持新类别检索。  
◆ 在TUBerlin、Sketchy等数据集上验证了CLAIR的优越性，性能超越现有最优方法。</td></tr>
<tr><td>2025-08-15</td><td>Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering</td><td>[2508.11272](http://arxiv.org/pdf/2508.11272)</td><td>◆ 提出了一种名为PMTFR的框架，结合金字塔匹配模型与无训练精炼机制，显著提升了监督式组合图像检索（CIR）的性能。  
◆ 设计了简单高效的金字塔修补模块（Pyramid Patcher），通过多粒度视觉信息理解增强模型对参考图像和修改指令的联合解析能力。  
◆ 首次将表示工程（Representation Engineering）引入CIR任务，利用思维链（CoT）数据提取表征并注入多模态大模型，无需额外训练排序模型即可优化检索结果。  
◆ 创新性地在监督式CIR中实现无训练精炼范式，摆脱传统方法对显式文本推理或复杂提示设计的依赖，仅通过表征注入即可提升分数。  
◆ 在主流CIR基准测试中超越现有最优方法，验证了框架的有效性，同时保持模型轻量化与可扩展性。</td></tr>
<tr><td>2025-08-12</td><td>Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications</td><td>[2508.10933](http://arxiv.org/pdf/2508.10933)</td><td>◆ 将相机位姿自编码器（PAE）从绝对位姿回归（APR）扩展到相对位姿回归（RPR），提出了一种新的基于PAE的RPR方法。  
◆ 设计了一种无需额外存储图像或位姿数据的重定位方案，通过PAE-based RPR对APR预测结果进行优化，显著提升了定位精度。  
◆ 在同等架构下，验证了PAE-based RPR相比传统图像基RPR模型的有效性，证明了其性能优势。  
◆ 在室内场景基准测试中，展示了该方法对APR定位精度的显著提升，尤其在数据有限的情况下表现突出。  
◆ 仅需30%的训练数据即可达到竞争性性能，大幅降低了零售场景部署中的数据收集负担，提升了数据效率。  
◆ 开源了代码与预训练模型，为后续研究与应用提供了便利。</td></tr>
<tr><td>2025-08-12</td><td>FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents</td><td>[2508.09241](http://arxiv.org/pdf/2508.09241)</td><td>◆ 提出了首个细粒度GUI代理控制评估标准FineState-Bench，填补了现有基准仅关注粗粒度任务完成的空白  
◆ 构建了跨平台（桌面/网页/移动端）的2257项任务测试集，包含四个组件模块和四阶段评估指标  
◆ 创新开发了即插即用的视觉诊断助手VDA，首次实现感知与定位能力的量化解耦分析  
◆ 通过实验揭示当前最先进模型在细粒度交互中仅达32.8%准确率，验证了基准的有效性  
◆ 首次证实视觉定位能力是当前GUI代理的主要瓶颈（理想视觉可使Gemini模型成功率提升14.9%）  
◆ 完整开源评估框架与数据集，为GUI代理研究提供标准化测试环境与诊断工具</td></tr>
<tr><td>2025-08-13</td><td>SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling</td><td>[2508.09105](http://arxiv.org/pdf/2508.09105)</td><td>◆ 提出首个面向半黑盒检索控制环境的来源感知成员审计框架(SMA)，实现生成内容细粒度来源追踪（预训练/外部检索/用户输入），解决传统成员推理方法在RAG系统中失效的问题。  
◆ 设计基于零阶优化的归因估计机制，通过大规模扰动采样和岭回归建模，在半黑盒约束下鲁棒近似输入令牌对输出的真实影响。  
◆ 首创跨模态归因技术，利用多模态大模型将图像输入投影为文本描述，实现文本模态的令牌级归因，首次支持MRAG系统中图像检索痕迹的成员推理。  
◆ 将成员推理的研究焦点从&quot;数据是否被记忆&quot;转向&quot;内容来源何处&quot;，为复杂生成系统的数据溯源审计提供新范式。  
◆ 突破现有方法在检索与多模态融合场景下的局限性，通过控制检索过程实现可验证的隐私泄露责任认定。</td></tr>
<tr><td>2025-08-12</td><td>A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition</td><td>[2508.08917](http://arxiv.org/pdf/2508.08917)</td><td>◆提出基于伪全局融合范式的跨视角网络，通过多模态分支协同学习统一语义空间特征，解决传统方法忽略特征空间内在结构的问题。  
◆创新性地引入伪全局信息引导机制，有效协调不同模态分支的特征表达，增强复杂环境下的场景识别能力。  
◆提出流形适应与成对方差-局部性学习度量方法，构建对称正定(SPD)矩阵计算马氏距离，取代传统欧氏距离度量。  
◆通过几何化建模准确刻画特征空间内数据本质分布，捕捉复杂的类间依赖关系，显著提升时变场景下的识别鲁棒性。  
◆实验证明该方法在复杂环境条件下具有竞争优势，尤其在GPS拒止环境中的定位和闭环检测任务表现突出。  
◆整体框架突破了欧式空间线性假设的局限性，为激光雷达地点识别任务提供了新的非线性特征学习范式。</td></tr>
<tr><td>2025-07-31</td><td>Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace</td><td>[2508.05661](http://arxiv.org/pdf/2508.05661)</td><td>◆ 提出了一种基于零样本检索的可扩展视觉搜索系统，适用于C2C电商平台，解决了非结构化商品列表的搜索难题。  
◆ 首次在Mercari平台中对比了多种视觉语言模型的零样本检索性能，发现多语言SigLIP模型表现最优，nDCG@5指标比原有微调基线提升13.3%。  
◆ 设计了实时推理与后台索引相结合的工作流，并通过降维优化统一嵌入管道，实现了高效部署。  
◆ 通过线上A/B测试验证了实际效果，实验组用户通过图像搜索的成交率提升高达40.9%，显著提升了用户参与度和转化率。  
◆ 证明了零样本模型可作为生产环境的强基线方案，既能快速部署，又保留了未来微调的灵活性，大幅降低开发成本。</td></tr>
<tr><td>2025-08-06</td><td>ACM Multimedia Grand Challenge on ENT Endoscopy Analysis</td><td>[2508.04801](http://arxiv.org/pdf/2508.04801)</td><td>◆ 提出了ENTRep挑战赛，首次将耳鼻喉内窥镜分析的细粒度解剖分类与跨模态检索（图像到图像、文本到图像）结合，填补了该领域公共基准的空白。  
◆ 构建了首个支持双语（越南语和英语）临床描述的专业数据集，包含专家标注的解剖区域、正常/异常状态及双语言叙事文本，增强了数据多样性。  
◆ 设计了三个标准化评测任务（分类、图像检索、文本检索），并建立服务器端评分机制，确保评估的公平性与可复现性。  
◆ 引入公开和私有测试集的双轨评估策略，兼顾模型泛化能力与临床实际需求。  
◆ 通过分析优胜团队方案，揭示了多模态融合和跨语言对齐在医疗影像分析中的关键作用，为后续研究提供方向。</td></tr>
<tr><td>2025-08-06</td><td>Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization</td><td>[2508.04790](http://arxiv.org/pdf/2508.04790)</td><td>◆ 提出首个针对BIRADS五分类乳腺图像检索的综合深度学习框架，解决了现有医学图像检索中样本不足、数据划分不当和统计验证不足的方法学局限。  
◆ 系统比较了DenseNet121、ResNet50和VGG16架构，结合差异化微调、度量学习和超集成优化等先进训练策略，其中差异化微调使DenseNet121和ResNet50的precision@10提升19.6%。  
◆ 采用严格分层数据划分（50%/20%/30%训练/验证/测试）和1000次bootstrap置信区间验证，测试集包含602例查询，确保结果临床可靠性。  
◆ 创新性提出超集成优化方法，整合互补架构实现36.33%的precision@10（95% CI: 34.78%-37.88%），比基线提升24.93%，每查询返回3.6个相关病例。  
◆ 通过统计验证显示不同优化策略间存在显著差异（p&lt;0.001，Cohen&#x27;s d&gt;0.8），同时保持2.8毫秒的实时检索效率，远超文献中5类医学检索20-25%的性能预期。  
◆ 建立临床部署的循证架构选择指南，为诊断支持和质量控制应用提供新性能基准。</td></tr>
<tr><td>2025-08-06</td><td>Metric Learning in an RKHS</td><td>[2508.04476](http://arxiv.org/pdf/2508.04476)</td><td>◆ 提出了一个在再生核希尔伯特空间（RKHS）中进行度量学习的通用框架，突破了以往仅限于欧几里得空间（ℝᵈ）的理论局限。  
◆ 首次为基于核方法和神经网络的非线性度量学习提供了理论保证，填补了该领域缺乏理论支撑的空白。  
◆ 推导了新的泛化误差界和样本复杂度上界，为实际应用中所需的数据量提供了理论指导。  
◆ 通过仿真实验和真实数据集验证了框架的有效性，代码已开源以促进后续研究。  
◆ 将三元组比较（如“h更接近i还是j？”）的弱监督信号与RKHS结合，扩展了度量学习在图像检索、推荐系统等场景的应用潜力。</td></tr>
<tr><td>2025-08-06</td><td>Composed Object Retrieval: Object-level Retrieval via Composed Expressions</td><td>[2508.04424](http://arxiv.org/pdf/2508.04424)</td><td>◆ 提出全新任务Composed Object Retrieval (COR)，突破现有图像级组合检索局限，实现基于参考对象+文本描述的对象级精确检索与分割。  
◆ 揭示COR任务的核心挑战：需在复杂场景中精准定位符合组合语义的任意对象，同时排除语义相似但无关的干扰对象。  
◆ 构建首个大规模COR基准数据集COR127K，包含408个类别、12.7万组检索三元组，覆盖多样化语义变换场景。  
◆ 设计统一端到端模型CORE，创新性整合参考区域编码、自适应视觉-文本交互和区域级对比学习三大关键技术。  
◆ 实验证明CORE在基类和新类上均显著优于现有模型，为细粒度多模态检索研究开辟新方向。  
◆ 首次实现从&quot;图像级匹配&quot;到&quot;对象级定位&quot;的跨越，推动多模态系统对用户意图的精细化理解。</td></tr>
<tr><td>2025-08-06</td><td>RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization</td><td>[2508.04335](http://arxiv.org/pdf/2508.04335)</td><td>◆ 提出RiemanLine，一种基于黎曼流形的3D直线统一最小表示法，可同时处理独立直线和平行线组，解决了人造环境中普遍存在的结构规律性问题。  
◆ 创新性地将直线地标解耦为全局和局部组件：在单位球面S²上优化的共享消失方向，以及在正交子空间上约束的缩放法向量，实现了结构规律的紧凑编码。  
◆ 对于n条平行线，将参数空间从4n（正交形式）减少到2n+2，无需显式约束即可自然嵌入平行性，显著降低了参数维度。  
◆ 将该参数化方法集成到因子图框架中，实现了全局方向对齐和局部重投影优化的统一基于流形的束调整。  
◆ 在ICL-NUIM、TartanAir和合成基准测试上的大量实验表明，该方法在姿态估计和直线重建方面显著提高了准确性，同时改善了收敛稳定性。</td></tr>
<tr><td>2025-08-05</td><td>Prototype-Enhanced Confidence Modeling for Cross-Modal Medical Image-Report Retrieval</td><td>[2508.03494](http://arxiv.org/pdf/2508.03494)</td><td>◆ 提出Prototype-Enhanced Confidence Modeling (PECM)框架，通过多级原型建模解决医学跨模态检索中语义模糊性问题。  
◆ 首次在医学图像-报告检索中引入双流置信度估计机制，结合原型相似度分布自适应调整高不确定性数据的影响权重。  
◆ 设计多模态原型学习模块，分别捕捉图像和文本的层次化语义特征，显著提升跨模态对齐的鲁棒性。  
◆ 开发自适应加权策略，动态平衡不同置信度样本在检索排序中的贡献，改善临床复杂场景下的结果可靠性。  
◆ 在完全监督和零样本检索任务上实现最高10.17%的性能提升，在多个放射学数据集上刷新当前最优水平。  
◆ 首次系统验证原型增强方法对医学数据固有歧义性的处理能力，为临床跨模态检索提供新范式。</td></tr>
<tr><td>2025-08-04</td><td>Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure</td><td>[2508.02034](http://arxiv.org/pdf/2508.02034)</td><td>◆提出Protego，一种用户中心化的隐私保护方法，通过3D面部签名生成姿态不变的2D表示，动态变形为自然3D面具，适配用户任意姿态表情的图像，在分享前进行保护。  
◆创新性地增强FR模型敏感性，使受保护图像无法相互匹配，突破现有方法仅防御外部查询的局限。  
◆实验证明在多种黑盒FR模型下显著降低检索准确率，性能至少优于现有方法2倍。  
◆首次实现视频场景下的视觉连贯性保护，满足动态内容对一致性和自然外观的高要求。  
◆为对抗FR技术滥用（如大规模监控与非自愿身份追踪）提供实用化解决方案，填补用户主动防护的技术空白。  
◆通过3D到2D的封装与动态适配机制，兼顾强隐私保护与视觉自然度，解决传统方法易被检测或影响用户体验的痛点。</td></tr>
<tr><td>2025-07-31</td><td>DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching</td><td>[2507.23629](http://arxiv.org/pdf/2507.23629)</td><td>◆ 提出DRACo-SLAM2框架，改进原有系统，专为配备多波束成像声纳的水下机器人团队设计，实现分布式SLAM。  
◆ 创新性地将声纳地图表示为对象图，通过对象图匹配实现高效跨机器人闭环检测，无需依赖先验几何信息。  
◆ 针对水下扫描匹配特点，提出增量式群组一致测量集最大化（GCM）方法，改进原有PCM算法，有效处理相邻跨机器人闭环共享相似配准误差的场景。  
◆ 通过模拟和真实数据集进行广泛对比分析，验证了所提方法的优越性和实用性。  
◆ 框架在通信效率和鲁棒性方面表现突出，特别适合水下机器人团队协作建图与定位需求。</td></tr>
<tr><td>2025-07-31</td><td>Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization</td><td>[2507.23569](http://arxiv.org/pdf/2507.23569)</td><td>◆ 提出高斯泼溅特征场（GSFFs），将显式几何模型（3DGS）与隐式特征场结合，用于视觉定位任务。  
◆ 利用3DGS的密集几何信息和可微分光栅化算法，学习基于3D空间的鲁棒特征表示。  
◆ 通过对比学习框架，将3D尺度感知特征场与2D特征编码器对齐到同一嵌入空间，提升特征一致性。  
◆ 引入3D结构感知的聚类方法，正则化表征学习并生成可用于隐私保护的场景分割结果。  
◆ 提出基于特征图或分割图对齐的位姿优化方法，支持隐私保护和非隐私保护两种定位流程。  
◆ 在多个真实数据集上验证了方法的先进性，实现了当前最优的定位性能。</td></tr>
<tr><td>2025-07-25</td><td>A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents</td><td>[2507.22938](http://arxiv.org/pdf/2507.22938)</td><td>◆ 提出基于图结构的创新方法，将流程图转化为图表示，解决了传统文本检索增强生成(RAG)系统难以处理图像问答的痛点。  
◆ 首次将视觉大语言模型(VLM)生成的流程图图表示与文本嵌入管道结合，实现电信领域多模态问答的端到端解决方案。  
◆ 开发了完整的处理流程，包括技术文档处理、图像类型分类、图表示构建等关键步骤，形成可落地的系统架构。  
◆ 实验证明微调后的VLM生成的图表示与真实值编辑距离更小，验证了该方法对流程图表示的鲁棒性。  
◆ 创新性地在推理阶段无需使用VLM，仅需文本嵌入模型即可实现高效检索，显著降低部署成本。  
◆ 在电信产品文档构建的QA数据集上验证了方法的有效性，特别是电信领域适配的文本嵌入模型表现优异。</td></tr>
<tr><td>2025-07-30</td><td>Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques</td><td>[2507.22791](http://arxiv.org/pdf/2507.22791)</td><td>◆ 全面综述了单模态与跨模态特征匹配技术，涵盖RGB图像、深度图像、3D点云、LiDAR扫描、医学图像及视觉-语言交互等多种模态，填补了该领域系统性总结的空白。  
◆ 对比分析了传统手工方法（如Harris角点、SIFT和ORB描述符）与深度学习方法（如SuperPoint和LoFTR）的优劣，指出后者在跨模态鲁棒性和适应性上的显著提升。  
◆ 重点介绍了模态感知技术进展，包括针对深度图像的几何与深度专用描述符、3D点云的稀疏与稠密学习方法、LiDAR扫描的注意力增强神经网络，以及医学图像匹配的MIND描述符等创新方案。  
◆ 深入探讨跨模态应用场景，如医学图像配准和视觉-语言任务，揭示了特征匹配技术在处理多样化数据交互中的关键作用与发展趋势。  
◆ 系统总结了当前挑战与未来方向，为跨模态特征匹配的研究提供了清晰的路线图，推动该领域向更复杂、更实用的场景拓展。</td></tr>
<tr><td>2025-07-29</td><td>Adversarial Reconstruction Feedback for Robust Fine-grained Generalization</td><td>[2507.21742](http://arxiv.org/pdf/2507.21742)</td><td>◆ 提出AdvRF框架，通过对抗性重建反馈机制学习与类别无关的差异表征，解决现有细粒度图像检索方法对预定义类别的语义依赖问题。  
◆ 将细粒度图像检索重新定义为视觉差异重建任务，结合检索模型的类别感知差异定位与重建模型的类别无关特征学习，实现双向优化。  
◆ 通过重建模型揭示检索模型忽略的残差异，迫使检索模型提升定位精度，同时检索模型的优化信号指导重建模型改进重建能力。  
◆ 采用知识蒸馏将重建模型生成的类别无关差异表征迁移到检索模型，实现高效部署。  
◆ 在广泛使用的细粒度和粗粒度数据集上验证了AdvRF的优越性能，定量和定性评估均显示其显著提升泛化能力。</td></tr>
<tr><td>2025-07-28</td><td>Exploring text-to-image generation for historical document image retrieval</td><td>[2507.20934](http://arxiv.org/pdf/2507.20934)</td><td>◆ 提出T2I-QBE新方法，首次将文本生成图像（T2I）技术应用于文档图像检索（DIR）领域，填补了基于属性检索（ABDIR）与基于示例检索（QBE）之间的技术鸿沟。  
◆ 利用生成式AI（Leonardo.Ai）将文本提示（包含文档类型描述和ABDIR风格属性列表）转化为查询图像，无需用户提供真实查询样本，解决了QBE依赖现有样本的局限性。  
◆ 针对历史文档的视觉多样性和独特性设计检索方案，通过CNN提取生成图像与数据集中文档的特征进行相似度匹配，验证了生成图像作为查询的有效性。  
◆ 在HisIR19历史文档数据集上的实验证明，该方法能够成功检索相关文档，为无样本场景下的文档检索提供了可行解决方案。  
◆ 首次探索了T2I生成技术与传统QBE范式的结合，为DIR领域开辟了基于生成模型的新研究方向。</td></tr>
<tr><td>2025-07-28</td><td>PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs</td><td>[2507.20892](http://arxiv.org/pdf/2507.20892)</td><td>◆ 提出了一种结合深度学习与经典模型规划算法的混合视觉导航方法，突破了纯端到端数据驱动模型的局限性。  
◆ 采用分层系统架构，整合了模型预测控制、可通行性估计、视觉地点识别和位姿估计等多项前沿技术。  
◆ 创新性地使用拓扑图作为环境表征，显著提升了系统的可扩展性和环境适应能力。  
◆ 相比端到端方案，该系统具有更高的可解释性，解决了黑箱模型在机器人应用中的关键瓶颈。  
◆ 通过大量真实场景实验验证了方法的有效性，为视觉导航提供了新的技术路径。  
◆ 在减少训练数据依赖的同时，保持了数据驱动方法的灵活性和适应性优势。</td></tr>
<tr><td>2025-07-28</td><td>ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning</td><td>[2507.20564](http://arxiv.org/pdf/2507.20564)</td><td>◆ 提出ZSE-Cap系统，在EVENTA竞赛中无需微调即获得第四名，展示了零样本学习的强大能力。  
◆ 创新性地集成CLIP、SigLIP和DINOv2三种模型的相似度分数，提升图像检索性能。  
◆ 通过精心设计的提示词引导Gemma 3模型，实现文章高层事件与图像视觉内容的关联生成描述。  
◆ 结合基础模型的集成和提示技术，在私有测试集上取得0.42002的高分，验证了方法的有效性。  
◆ 提供开源代码，促进零样本图像检索与描述生成领域的进一步研究。</td></tr>
<tr><td>2025-07-28</td><td>Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments</td><td>[2507.20538](http://arxiv.org/pdf/2507.20538)</td><td>◆ Uni-Mapper提出首个动态感知的多模态LiDAR地图统一框架，解决复杂动态环境中多传感器地图融合的难题。  
◆ 采用基于体素自由空间哈希的粗到细动态物体剔除方法，通过时序占用不一致性检测并移除动态对象，提升场景一致性。  
◆ 创新设计动态感知的闭环检测模块，结合保留的静态局部特征生成全局描述符，增强动态环境下的地点识别鲁棒性。  
◆ 提出集中式锚节点策略优化位姿图，有效解决地图合并时的会话内漂移误差和跨地图闭环问题。  
◆ 框架支持异构LiDAR（如机械式与固态雷达）的跨模态匹配，在公开数据集上显著优于现有方法。  
◆ 实现端到端的多地图对齐流程，包含动态处理、闭环检测与多阶段位姿图优化，适用于多机器人协作场景。</td></tr>
<tr><td>2025-07-24</td><td>DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition</td><td>[2507.18444](http://arxiv.org/pdf/2507.18444)</td><td>◆ 提出DSFormer双尺度交叉学习Transformer模块，通过双向信息传递整合CNN最后两层的双尺度特征，同时捕捉语义丰富性和空间细节。  
◆ 设计自注意力机制处理单尺度内的长程依赖关系，并引入共享交叉注意力实现跨尺度学习，增强特征表示能力。  
◆ 创新性地提出多视角块聚类策略，重构SF-XL训练数据集的分区方式，优化数据组织以提升对视角变化的鲁棒性。  
◆ 结合上述技术，生成适应环境变化的鲁棒全局嵌入表征，相比先前分区方法减少约30%训练数据需求。  
◆ 仅使用512维全局描述符即实现全局检索，在多数基准数据集上超越DELG、Patch-NetVLAD等先进方法，达到SOTA性能。  
◆ 显著提升计算效率，为视觉地点识别任务提供高效解决方案。</td></tr>
<tr><td>2025-07-23</td><td>VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization</td><td>[2507.17455](http://arxiv.org/pdf/2507.17455)</td><td>◆ 提出了一种新型混合地理定位框架，结合了视觉语言模型（VLM）和检索式视觉地点识别（VPR）方法的优势。  
◆ 利用VLM生成地理先验信息，有效缩小检索搜索空间，解决了传统检索方法在可扩展性和感知混淆上的不足。  
◆ 设计了重排序机制，结合特征相似度和初始坐标邻近性，筛选地理合理性最高的匹配结果。  
◆ 在多个地理定位基准测试中表现优异，尤其在街道级（提升4.51%）和城市级（提升13.52%）定位精度上显著超越现有方法。  
◆ 通过VLM与VPR的结合，实现了可扩展、鲁棒且高精度的行星级地理定位系统，解决了单一方法存在的幻觉和可解释性问题。</td></tr>
<tr><td>2025-07-23</td><td>Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging</td><td>[2507.17412](http://arxiv.org/pdf/2507.17412)</td><td>◆ 提出了一种不依赖预分割数据和器官特异性数据集的CBIR框架，适用于临床中大型非结构化图像归档系统（如PACS）。  
◆ 创新性地引入C-MIR方法，将ColBERT的上下文感知延迟交互机制适配于3D医学影像重排序，实现高效上下文感知检索。  
◆ 在四种肿瘤部位上进行了全面评估，结合三种特征提取器和三种数据库配置，验证了方法的普适性。  
◆ 研究发现C-MIR能自动定位感兴趣区域，无需数据预分割，显著降低了传统方法依赖昂贵数据增强的计算成本。  
◆ 实验证明C-MIR在肿瘤标记（尤其结肠和肺癌）中性能显著提升（p&lt;0.05），并在肿瘤分期任务中展现出潜在应用价值。  
◆ 该研究为先进检索技术在医疗实践中的落地提供了新思路，有望优化临床诊断流程。</td></tr>
<tr><td>2025-07-20</td><td>LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM</td><td>[2507.15109](http://arxiv.org/pdf/2507.15109)</td><td>◆ 提出LoopNet，一种基于多任务学习的少样本学习方法，专门针对大规模SLAM中的闭环检测问题，兼顾精度与实时性需求。  
◆ 采用改进的ResNet架构，支持动态视觉数据集的在线重训练，并针对嵌入式设备进行优化，适应实际部署场景。  
◆ 创新性结合少样本学习策略，使模型能够快速适应新环境，同时输出查询索引和预测质量评估，增强系统可靠性。  
◆ 利用DISK描述符替代传统手工特征或常规深度学习方法，显著提升光照、视角等变化条件下的闭环检测性能。  
◆ 开源了新型闭环检测基准数据集LoopDB，填补现有数据在动态场景和嵌入式硬件评估方面的不足。  
◆ 整体方案在精度和计算效率上均优于现有方法，代码与数据集均已公开，推动SLAM领域研究可复现性。</td></tr>
<tr><td>2025-07-20</td><td>Visual Place Recognition for Large-Scale UAV Applications</td><td>[2507.15089](http://arxiv.org/pdf/2507.15089)</td><td>◆ 提出了LASED大规模无人机视觉定位数据集，包含约100万张图片，覆盖爱沙尼亚17万个独特地点，具有广泛的地理和时间多样性，显著提升了模型在航空场景中的训练效果。  
◆ 数据集采用结构化设计，确保地点分离清晰，解决了现有数据集规模小、多样性不足导致的模型泛化能力差的问题。  
◆ 提出使用可转向卷积神经网络（steerable CNNs）处理无人机图像中的旋转模糊问题，利用其旋转等变性生成方向不变的特征表示。  
◆ 实验证明，基于LASED训练的模型召回率显著高于小规模数据集训练的模型，凸显了地理覆盖和时间多样性的重要性。  
◆ 可转向CNN在旋转模糊问题上表现优异，平均召回率比最佳非可转向网络提高12%，有效提升了航空视觉定位的鲁棒性。  
◆ 结合大规模结构化数据集和旋转等变网络，该方法显著增强了航空视觉定位模型的泛化能力和鲁棒性。</td></tr>
<tr><td>2025-07-20</td><td>U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs</td><td>[2507.14902](http://arxiv.org/pdf/2507.14902)</td><td>◆ 首次系统分析了基于MLLMs的通用多模态检索（UMR）中影响嵌入学习性能的关键因素，揭示了常被忽视的训练细节对模型性能的重要影响。  
◆ 提出了一种通用的MLLM嵌入学习框架U-MARVEL，通过渐进式过渡、困难负样本挖掘和重排序蒸馏等策略优化嵌入生成和训练过程。  
◆ 在监督学习场景下，U-MARVEL在M-BEIR基准测试中大幅超越现有最优方法，展示了显著的性能优势。  
◆ 框架在零样本场景下表现优异，在组合图像检索、文本-视频检索等任务中展现出强大的泛化能力。  
◆ 研究为多模态检索领域提供了可复现的代码实现和系统化的训练方案，推动了该领域的可解释性和可扩展性发展。</td></tr>
<tr><td>2025-07-19</td><td>OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition</td><td>[2507.14477](http://arxiv.org/pdf/2507.14477)</td><td>◆ OptiCorNet提出首个端到端可训练的序列建模框架，将空间特征提取与时序差分统一到单一模块中，突破了传统单帧嵌入方法的局限。  
◆ 创新设计可微分时序差分算子（DSD），通过固定权重差分核捕捉方向性序列差异，结合LSTM精修模块，有效建模短时空间上下文和长程时序关联。  
◆ 引入残差投影机制增强描述符判别力，生成的紧凑序列描述符对视角变化和外观差异具有显著鲁棒性。  
◆ 采用四元组损失函数同步优化批次内正样本对齐与多负样本分离，显著提升跨场景类间可分性。  
◆ 首次实现时序聚合的端到端联合学习，相比后处理方法直接优化序列级嵌入，在季节和视角变化场景下取得突破性性能提升。  
◆ 轻量级1D卷积编码器设计确保计算效率，在多个公开基准测试中全面超越现有最优方法。</td></tr>
<tr><td>2025-07-16</td><td>Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired</td><td>[2507.14215](http://arxiv.org/pdf/2507.14215)</td><td>◆ 开发了JerryNet，一种定制CNN架构，可实时精确定位9个方向的声源，方向识别准确率达91.1%，优于基线模型。  
◆ 基于CLAP模型进行微调，实现纯音频分类，在自定义数据集和AudioSet上分别达到98.5%和95%的准确率。  
◆ 提出多模态融合模型，结合音频、视觉和文本数据精确定位图像中的声源，采用Yolov9目标检测和音频-视觉定位模块，cIoU达0.892，AUC为0.658。  
◆ 设计了硬件系统，包括四麦克风矩形阵列和眼镜式摄像头，通过腕带显示方向等关键信息，提升聋哑人群的实时交互体验。  
◆ 填补了当前研究中针对弱势群体的技术空白，为新一代无障碍设备开发奠定基础。  
◆ 在自定义数据集上全面验证系统性能，各项指标均超越同类模型，展现出实际应用潜力。</td></tr>
<tr><td>2025-07-17</td><td>FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and Adaptive Reconciliation for Composed Image Retrieval</td><td>[2507.12823](http://arxiv.org/pdf/2507.12823)</td><td>◆ 提出FAR-Net多阶段融合框架，结合早期和晚期融合优势，解决现有方法在视觉-文本模态融合中的局限性。  
◆ 设计增强语义对齐模块（ESAM），通过跨注意力机制实现细粒度语义关联，弥补晚期融合对局部对齐的不足。  
◆ 引入自适应协调模块（ARM），利用不确定性嵌入的早期融合增强模型鲁棒性，平衡文本显式描述与视觉上下文。  
◆ 创新性地整合ESAM与ARM，形成互补机制，同时捕捉全局语义和局部细节，提升组合图像检索精度。  
◆ 在CIRR和FashionIQ数据集上显著超越现有方法，Recall@1最高提升2.4%，验证了框架的有效性和可扩展性。</td></tr>
<tr><td>2025-07-17</td><td>MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval</td><td>[2507.12819](http://arxiv.org/pdf/2507.12819)</td><td>◆ 提出MCoT-RE框架，首次在零样本训练自由的组合图像检索任务中引入多角度思维链（Chain-of-Thought）技术，解决现有方法模态交互不足的问题。  
◆ 通过双路径生成策略，分别生成侧重文本修改的 caption 和融合视觉上下文的 caption，平衡显式修改与隐式视觉线索的利用。  
◆ 设计两阶段检索流程：首阶段用修改导向 caption 粗筛候选图像，第二阶段结合双 caption 和参考图像进行多粒度重排序，提升精度。  
◆ 创新性地将思维链扩展至多模态场景，指导 MLLM 同时处理文本指令与视觉上下文，避免信息丢失。  
◆ 在主流数据集上实现显著提升，FashionIQ 的 Recall@10 提高 6.24%，CIRR 的 Recall@1 提升 8.58%，达到零样本方法最优性能。  
◆ 整个框架无需额外训练，仅依赖预训练模型，保持高效低成本优势的同时突破现有技术瓶颈。</td></tr>
<tr><td>2025-07-16</td><td>QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval</td><td>[2507.12416](http://arxiv.org/pdf/2507.12416)</td><td>◆ 提出QuRe方法，通过硬负样本采样解决组合图像检索（CIR）中假阴性问题，优化奖励模型目标以提升检索相关性。  
◆ 设计新颖的硬负样本采样策略，选择目标图像后相关性分数两次陡降之间的图像，有效过滤假阴性样本。  
◆ 创建HP-FashionIQ数据集，首次在CIR任务中明确捕获用户偏好，超越传统仅关注目标图像检索的评估方式。  
◆ 实验证明QuRe在FashionIQ和CIRR数据集上达到最优性能，并在HP-FashionIQ上展现出与人类偏好最强的对齐性。  
◆ 开源代码促进后续研究，为CIR领域提供可复现的基准方法和用户满意度导向的评估框架。</td></tr>
<tr><td>2025-07-16</td><td>CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning</td><td>[2507.11834](http://arxiv.org/pdf/2507.11834)</td><td>◆ 提出CorrMoE框架，首次针对跨场景和跨域对应点修剪任务设计，解决了现有方法在视觉域不一致和场景结构多样时的性能瓶颈。  
◆ 创新性地引入去风格化双分支结构，通过隐式和显式图特征的风格混合，有效减少域特异性表征的负面影响，提升跨域鲁棒性。  
◆ 设计双融合专家混合模块（Bi-Fusion MoE），结合线性复杂度注意力机制和动态专家路由，自适应整合多视角特征以应对场景多样性。  
◆ 在特征融合中实现计算效率优化，通过线性注意力降低传统Transformer的二次复杂度，同时保持多专家模型的动态适应性。  
◆ 在多个基准数据集上验证了方法的优越性，显著超越现有SOTA方法，尤其在跨域和跨场景任务中展现强泛化能力。  
◆ 开源代码与预训练模型，为后续研究提供可复现的基础。</td></tr>
<tr><td>2025-07-09</td><td>Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning</td><td>[2507.10571](http://arxiv.org/pdf/2507.10571)</td><td>◆ 提出了一种新型模块化Agentic AI视觉分类框架，将通用多模态智能体与非视觉推理协调器、RAG模块相结合，实现感知与元推理的分离。  
◆ 创新性地引入信任感知协调机制，通过置信度校准指标（ECE/OCR/CCC）动态调节对多智能体的信任度，在零样本场景下准确率提升77.94%。  
◆ 开发了基于CLIP图像检索和重评估循环的信任校准方法，利用视觉相似案例修正智能体的过度自信，增强预测可解释性。  
◆ 在苹果叶病害诊断任务中验证三种配置：零样本置信协调、微调智能体优化、以及RAG增强的信任校准协调，最高达85.63%准确率。  
◆ 发现GPT-4o具有更优校准性，而Qwen-2.5-VL存在过度自信现象，为多智能体行为分析提供实证依据。  
◆ 开源全部模型、提示词、软件代码及实验结果，为可信多智能体系统建立可复现基准，适用于生物诊断等高风险领域。</td></tr>
<tr><td>2025-07-14</td><td>GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space</td><td>[2507.10473](http://arxiv.org/pdf/2507.10473)</td><td>◆ GT-Loc首次提出联合学习图像拍摄时间和地理位置的统一嵌入空间，通过多编码器（图像、时间、地点）在共享特征空间中对齐三者表征，解决传统方法中时间预测依赖地理信息的局限性。  
◆ 创新性地采用环形时序度量学习目标，将时间差异建模为环面（toroidal）上的软目标，替代传统对比学习的硬正负样本，更贴合时间周期性的本质。  
◆ 实验证明联合优化显著优于现有时间预测方法，即使对比那些在推理阶段使用真实地理位置作为输入的方法，仍展现出更高精度。  
◆ 在标准地理定位任务中达到竞争性性能，同时统一嵌入空间支持组合检索（如&quot;夏季黄昏的巴黎&quot;）和文本引导的图像检索，扩展了应用场景。  
◆ 提出新基准验证方法有效性，揭示了时间与地理线索的深层关联，为跨模态检索研究提供新方向。</td></tr>
<tr><td>2025-07-14</td><td>Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources</td><td>[2507.10403](http://arxiv.org/pdf/2507.10403)</td><td>◆ 提出CrisisLandMark数据集，包含64.7万张Sentinel-1 SAR和Sentinel-2多光谱图像，并配对了结构化文本标注，覆盖土地覆盖、土地利用和危机事件，数据来源权威。  
◆ 开发CLOSP框架，通过文本作为桥梁，将未配对的光学和SAR图像对齐到统一的嵌入空间，实现跨模态检索。  
◆ CLOSP在检索性能上取得突破，nDGC指标比现有模型提升54%，显著提高了检索效果。  
◆ 提出统一训练策略，通过光学图像的丰富语义知识间接辅助SAR图像解译，克服SAR图像解译的固有困难。  
◆ 扩展GeoCLOSP模型，整合地理坐标信息，在通用语义任务和地理位置相关的危机事件检索之间取得平衡，成为特定领域的专家。  
◆ 强调多传感器数据和地理上下文整合的重要性，为遥感档案的全面利用提供了新思路。</td></tr>
<tr><td>2025-07-14</td><td>Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures</td><td>[2507.10265](http://arxiv.org/pdf/2507.10265)</td><td>◆提出了一种新颖的对抗攻击方法——万花筒背景攻击（KBA），通过多重复制对称纹理构造圆形背景图案，有效干扰相机位姿估计模型。  
◆首次利用自然纹理片段构建具有多重复制对称性的圆盘结构，这种设计在不同视角下保持高度相似性，显著提升了攻击的跨视角一致性。  
◆创新性地提出了投影方向一致性损失函数，通过优化万花筒纹理片段的空间分布，进一步增强了攻击效果。  
◆实验证明该方法能有效攻击多种主流相机位姿估计模型，揭示了稀疏输入场景下背景纹理对位姿估计的关键影响。  
◆为对抗攻击领域提供了新思路，将几何对称性与自然纹理相结合，实现了无需复杂对抗样本生成的高效攻击。</td></tr>
<tr><td>2025-07-11</td><td>RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features</td><td>[2507.08546](http://arxiv.org/pdf/2507.08546)</td><td>◆ 提出RadiomicsRetrieval框架，首次将手工设计的放射组学特征与深度学习嵌入结合，实现肿瘤级别的3D医学图像检索，突破现有2D方法的局限。  
◆ 采用可提示分割模型（如SAM）生成肿瘤特异性图像嵌入，并通过对比学习与放射组学特征对齐，增强特征表达能力。  
◆ 引入解剖位置嵌入（APE），为检索系统提供全局解剖上下文，支持基于位置的灵活查询。  
◆ 框架仅需最小用户交互（如单点标注），显著降低分割开销，适应多样临床场景。  
◆ 支持混合查询模式（图像嵌入或选定放射组学属性），提升诊断、治疗规划及医学研究的实用性。  
◆ 在肺部CT和脑部MRI公开数据集验证中，放射组学特征显著提高检索特异性，APE对基于位置的搜索至关重要。</td></tr>
<tr><td>2025-07-11</td><td>LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps</td><td>[2507.08420](http://arxiv.org/pdf/2507.08420)</td><td>◆ 提出了一种融合LiDAR、GNSS和IMU数据的统一框架，通过动态时间规整（DTW）进行速度对齐，解决城市规模3D建图时的累积漂移问题。  
◆ 采用扩展卡尔曼滤波优化GNSS和IMU信号，结合基于正态分布变换（NDT）的局部建图与位姿图优化，提升局部精度。  
◆ 引入GNSS约束锚点和重叠段精细配准技术，显著改善全局一致性，将平均全局对齐误差从3.32米降低至1.24米（提升61.4%）。  
◆ 发布了一个大规模多模态数据集，包含21条城市环线的12.8万帧128线LiDAR数据、同步RTK-GNSS轨迹及MEMS-IMU测量值，填补研究空白。  
◆ 提出基于道路中心线和交叉口的几何一致性评估指标，量化全局与局部精度，为后续研究建立新基准。  
◆ 所构建的高精度地图支持智慧城市规划、基础设施监测等应用，同时公开代码与数据集推动领域发展。</td></tr>
<tr><td>2025-07-11</td><td>Deep Hashing with Semantic Hash Centers for Image Retrieval</td><td>[2507.08404](http://arxiv.org/pdf/2507.08404)</td><td>◆ 提出语义哈希中心概念，通过数据依赖的相似性计算捕捉类别间的语义关系，取代传统数据无关的哈希中心生成方法。  
◆ 设计三阶段框架SHC：先训练分类网络识别语义相似性，再优化生成保留语义结构的哈希中心，最后训练深度哈希网络生成二进制码。  
◆ 开发新型优化算法，在保持语义相关性的同时强制最小中心间距，避免哈希码过度相似的问题。  
◆ 首次将类别语义关系建模为汉明空间的距离约束，使相似类别的哈希中心距离更近，不相似类别更远。  
◆ 在多个公开数据集上验证显著提升检索性能，MAP@100/1000/ALL指标平均提升7.26%/7.62%/11.71%，超越现有最佳方法。  
◆ 提出的数据依赖相似性计算方法能自适应不同数据分布，增强模型泛化能力。</td></tr>
<tr><td>2025-07-08</td><td>Unveiling Effective In-Context Configurations for Image Captioning: An External &amp; Internal Analysis</td><td>[2507.08021](http://arxiv.org/pdf/2507.08021)</td><td>◆ 首次对多模态上下文学习（ICL）在图像描述任务中的演示配置进行系统性外部研究，探索了示例数量、图像检索和描述分配三个维度的策略。  
◆ 通过内部注意力机制分析，揭示了典型大型多模态模型（LMM）的注意力特征，并开发了基于注意力的量化指标以评估模型行为。  
◆ 结合外部实验与内部机制分析的双重视角，提供了理解多模态ICL的新方法，揭示了示例配置如何通过注意力机制影响模型表现。  
◆ 提出注意力驱动的模型加速与压缩实验，验证了基于注意力分析的模型优化可行性。  
◆ 对比了相同架构与预训练策略的LMM性能差异，从预训练数据特征角度解释了模型表现差异的原因。  
◆ 开发了结合外部评估与内部指标的新方法论，可扩展至其他大模型研究领域。</td></tr>
<tr><td>2025-07-10</td><td>SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation</td><td>[2507.07467](http://arxiv.org/pdf/2507.07467)</td><td>◆ 提出了一种结合场景坐标回归（SCR）和证据学习的新型感知感知轨迹生成框架SCREP，用于GPS拒止环境下的自主飞行。  
◆ 通过证据学习方法优化SCR位姿估计器，能够预测像素不确定性并引导相机朝向可靠性高的场景坐标区域。  
◆ 采用滚动时域轨迹优化器，实时调整飞行轨迹以最大化定位精度，同时结合固定滞后平滑器融合低频SCR数据与高频IMU数据。  
◆ 在仿真实验中，相比固定偏航和前视基线方法，该框架将平移（旋转）平均误差降低了54%/15%（40%/31%）。  
◆ 通过硬件在环实验验证了框架的实时性和可行性，实现了感知-控制闭环的高效运行。</td></tr>
<tr><td>2025-07-10</td><td>VP-SelDoA: Visual-prompted Selective DoA Estimation of Target Sound via Semantic-Spatial Matching</td><td>[2507.07384](http://arxiv.org/pdf/2507.07384)</td><td>◆ 提出跨实例音频-视觉定位（CI-AVL）新任务，利用同类声音事件的不同实例图像定位目标声源，减少对配对数据的依赖并提升泛化能力。  
◆ 设计VP-SelDoA框架，通过语义级模态融合和Frequency-Temporal ConMamba架构生成目标选择性掩码，实现多声源场景下的目标声源隔离。  
◆ 提出语义-空间匹配机制，结合交叉注意力和自注意力对齐异构的语义与空间特征，解决传统方法中视觉语义与声学空间特征错位问题。  
◆ 构建大规模数据集VGG-SSL，包含296类声音事件的13,981条空间音频片段，为CI-AVL研究提供数据支持。  
◆ 实验表明，该方法在平均绝对误差（MAE）和准确率（ACC）上均优于现有音频-视觉定位方法，分别达到12.04和78.23%。</td></tr>
<tr><td>2025-07-08</td><td>FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval</td><td>[2507.07135](http://arxiv.org/pdf/2507.07135)</td><td>◆ 提出了FACap数据集，这是一个大规模自动构建的时尚领域组合图像检索数据集，解决了现有数据集缺乏专业细粒度标注的问题。  
◆ 设计了两阶段自动标注流程，结合视觉语言模型和大语言模型生成高质量修改文本，降低了人工标注成本。  
◆ 提出了FashionBLIP-2模型，通过在FACap上微调通用BLIP-2模型，并引入轻量级适配器和多头查询-候选匹配机制，提升了时尚细粒度信息的处理能力。  
◆ 在Fashion IQ基准和增强版enhFashionIQ数据集上验证了模型效果，实验表明该方法显著提升了时尚领域组合检索性能，尤其在细粒度文本修改场景。  
◆ 为电商等实际应用场景提供了高效的时尚图像检索解决方案，展示了自动构建领域专用数据集与模型适配相结合的有效性。</td></tr>
<tr><td>2025-07-09</td><td>Evaluating Attribute Confusion in Fashion Text-to-Image Generation</td><td>[2507.07079](http://arxiv.org/pdf/2507.07079)</td><td>◆ 针对时尚领域文本生成图像（T2I）任务中现有评估方法难以捕捉细粒度属性关联的问题，提出了一种基于视觉定位和视觉问答（VQA）的新型评估框架。  
◆ 通过单实体定位策略，在视觉和文本模态上同步分析属性混淆现象（如属性正确生成但归属错误实体），解决了传统方法对复杂组合语义评估的局限性。  
◆ 设计了局部化人工评估协议，并创新性地提出自动指标L-VQAScore，结合视觉定位与VQA技术，同时检测属性正确反映（reflection）和错误泄漏（leakage）情况。  
◆ 构建了包含挑战性组合对齐场景的新数据集，验证了L-VQAScore在细粒度实体-属性关联评估上的优越性，其与人类判断的相关性超越现有最优方法。  
◆ 该工作为时尚领域T2I模型提供了可扩展的客观评估方案，显著减少对主观评价的依赖，推动生成模型在复杂语义场景下的精准优化。</td></tr>
<tr><td>2025-07-09</td><td>MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval</td><td>[2507.06654](http://arxiv.org/pdf/2507.06654)</td><td>◆ 提出新任务CDR-CA（复合属性上下文多样性优化），针对文本到图像检索中不同应用场景对多样性需求的差异，实现多属性多样性按需调整。  
◆ 提出多源行列式点过程（MS-DPPs），将传统DPP扩展为多源形式，通过流形表示构建统一相似性矩阵，支持多属性联合优化。  
◆ 引入切线归一化（Tangent Normalization）技术，有效融合不同上下文信息，动态适应多样化应用场景的需求。  
◆ 实验验证了MS-DPPs在多样性优化任务中的优越性能，尤其在复合属性控制方面显著优于传统方法。  
◆ 公开代码促进后续研究，为文本到图像检索的实用化多样性优化提供新基线。</td></tr>
<tr><td>2025-07-08</td><td>Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval</td><td>[2507.05970](http://arxiv.org/pdf/2507.05970)</td><td>◆ 提出了一种可扩展的自动三元组生成流程，解决了传统CIR方法依赖人工标注数据导致的扩展性和零样本能力受限问题。  
◆ 构建了首个全合成数据集CIRHS，利用大语言模型生成多样化提示词，并通过文本到图像生成模型控制生成具有相同元素的图像对，经筛选重组形成高质量训练数据。  
◆ 创新性地提出混合上下文对齐框架（CoAlign），实现了全局对齐与局部推理的协同优化，能够学习更鲁棒且信息丰富的表征。  
◆ 首次验证了在全合成数据集上训练CIR模型的可行性，CoAlign在三个常用基准测试中展现出卓越的零样本性能。  
◆ 在监督训练场景下，该方法超越所有现有先进CIR模型，证明了检索框架的有效性。  
◆ 开源代码和CIRHS数据集将促进CIR领域的进一步研究。</td></tr>
<tr><td>2025-07-08</td><td>OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval</td><td>[2507.05631](http://arxiv.org/pdf/2507.05631)</td><td>◆ 提出基于分割的焦点映射特征提取器，通过主导区域分割和双重焦点映射模块，有效区分图像中的关键区域与噪声，提升查询特征质量。  
◆ 设计文本引导的焦点修正模块，利用修改文本的语义信息自适应调整参考图像的视觉焦点，解决传统方法中文本优先级被忽视的问题。  
◆ 首次在组合图像检索（CIR）中引入视觉主导区域分割技术，减少噪声干扰对多模态特征融合的负面影响。  
◆ 通过双焦点映射机制同步优化视觉与文本特征提取，增强模型对用户复杂修改意图的理解能力。  
◆ 构建完整网络OFFSET，在四个基准数据集上验证其优越性，为CIR领域提供新的解决方案。  
◆ 公开代码与数据，促进后续研究发展。</td></tr>
<tr><td>2025-07-07</td><td>Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model</td><td>[2507.05513](http://arxiv.org/pdf/2507.05513)</td><td>◆ 提出llama-nemoretriever-colembed模型，实现文本-图像跨模态检索的顶尖性能，在ViDoRe V1/V2基准上NDCG@5分别达到91.0和63.5，刷新榜单记录。  
◆ 基于NVIDIA Eagle2视觉语言模型进行架构改造，将因果注意力替换为双向注意力机制，增强多模态特征交互能力。  
◆ 创新性地引入ColBERT风格的延迟交互机制，在共享嵌入空间中实现细粒度跨模态检索，显著提升匹配精度。  
◆ 设计两阶段训练策略，先预训练再微调，有效增强模型检索能力。  
◆ 全面分析模型在存储效率与检索精度之间的权衡关系，为实际应用提供优化依据。  
◆ 发布1B和3B两种参数量变体，其中3B版本成为当前性能最优的跨模态检索模型。</td></tr>
<tr><td>2025-07-07</td><td>An analysis of vision-language models for fabric retrieval</td><td>[2507.04735](http://arxiv.org/pdf/2507.04735)</td><td>◆ 提出了一种自动化标注流程，利用多模态大语言模型（MLLMs）生成两种文本描述（自由自然语言和结构化属性描述），解决了织物领域公开数据集的缺失问题。  
◆ 首次系统评估了三种视觉语言模型（CLIP、LAION-CLIP和Meta Perception Encoder）在零样本织物图像检索任务中的性能。  
◆ 发现结构化属性描述能显著提升检索精度，尤其在视觉复杂的织物类别中，揭示了文本描述形式对跨模态检索的关键影响。  
◆ 验证了Meta Perception Encoder凭借更强的特征对齐能力，在织物检索任务中优于其他模型，为工业应用提供了模型选择依据。  
◆ 指出零样本检索在细粒度织物领域的局限性，强调领域自适应方法的必要性，为后续研究指明方向。  
◆ 结合技术性文本描述与先进视觉语言模型的策略，为制造业等专业领域的跨模态检索优化提供了实践指导。</td></tr>
<tr><td>2025-07-08</td><td>What&#x27;s Making That Sound Right Now? Video-centric Audio-Visual Localization</td><td>[2507.04667](http://arxiv.org/pdf/2507.04667)</td><td>◆ 提出AVATAR基准测试，首次引入视频中心化视角，解决现有音频-视觉定位（AVL）研究仅关注静态图像的问题。  
◆ 设计四种复杂场景（单声源、混合声源、多实体、屏幕外声源），突破传统方法假设声源可见且单一的局限性。  
◆ 开发TAVLO模型，首创高分辨率时序建模机制，有效捕捉声音与视觉对象的动态关联。  
◆ 实证发现传统方法因依赖全局音频特征和逐帧映射，难以追踪时序变化，而TAVLO通过时序建模实现精准对齐。  
◆ 建立视频中心化AVL新标准，首次系统论证时序动态对音频-视觉定位的关键影响。</td></tr>
<tr><td>2025-07-07</td><td>Simultaneous Localization and Mapping Using Active mmWave Sensing in 5G NR</td><td>[2507.04662](http://arxiv.org/pdf/2507.04662)</td><td>◆ 提出利用毫米波5G NR系统进行主动感知，实现类似激光雷达的点云生成，克服了传统被动SLAM技术依赖镜面反射假设的局限。  
◆ 采用二进制搜索方法从每个波束方向的功率延迟剖面中提取点云，提高了环境感知的精度和细节。  
◆ 通过多目标点校准硬件延迟，确保点云数据的准确性，为后续定位和建图提供可靠输入。  
◆ 利用点云配准算法从连续轨迹视角估计终端位姿变化，实现动态环境下的高精度定位。  
◆ 结合闭环检测与位姿图优化技术，进一步优化感知结果，完成精确的终端定位和无线电地图重建。  
◆ 通过仿真和实验验证了系统的有效性，为5G NR在SLAM领域的应用提供了实践依据。</td></tr>
<tr><td>2025-07-06</td><td>U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration</td><td>[2507.04503](http://arxiv.org/pdf/2507.04503)</td><td>◆ 提出U-ViLAR框架，首次将感知不确定性和定位不确定性同时纳入视觉定位系统，提升自动驾驶在复杂城市环境中的鲁棒性。  
◆ 创新性地将视觉特征映射到鸟瞰图（BEV）空间，增强与高精地图的空间一致性，解决视角差异问题。  
◆ 设计感知不确定性引导的特征关联模块（Perceptual Uncertainty-guided Association），有效降低感知误差对匹配的影响。  
◆ 开发定位不确定性引导的配准模块（Localization Uncertainty-guided Registration），通过量化定位置信度优化位姿估计精度。  
◆ 实现关联（大范围粗定位）与配准（精细定位）的协同优化，在保持大场景覆盖能力的同时提升厘米级定位准确性。  
◆ 通过大规模自动驾驶车队实测验证，在GNSS失效、动态障碍物等复杂场景下保持稳定性能，综合精度超越现有方法。</td></tr>
<tr><td>2025-07-04</td><td>Query-Based Adaptive Aggregation for Multi-Dataset Joint Training Toward Universal Visual Place Recognition</td><td>[2507.03831](http://arxiv.org/pdf/2507.03831)</td><td>◆提出基于查询的自适应聚合（QAA）方法，通过可学习的查询向量作为参考码本，有效提升多数据集联合训练中的信息容量，避免传统特征聚合层的信息饱和问题。  
◆创新性地引入跨查询相似度（CS）计算机制，利用查询级图像特征与参考码本的相似性生成鲁棒描述符，显著提升模型泛化能力。  
◆首次实现多数据集联合训练的通用视觉位置识别（VPR）模型，在保持单数据集峰值性能的同时，实现跨数据集的平衡泛化表现。  
◆通过可视化分析揭示学习到的查询向量具有跨数据集的多样化注意力模式，为多源数据融合提供可解释性依据。  
◆在计算效率和参数量控制方面表现优异，未显著增加模型复杂度的情况下实现性能突破。  
◆开源代码并辅以详尽的消融实验，验证了QAA机制的可扩展性和核心组件有效性。</td></tr>
<tr><td>2025-07-01</td><td>LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment</td><td>[2507.00659](http://arxiv.org/pdf/2507.00659)</td><td>◆ 提出LoD-Loc v2方法，首次实现基于低细节层次（LoD1）城市模型的无人机空中视觉定位，突破以往依赖高细节模型（LoD2/LoD3）的限制。  
◆ 采用粗到精的双阶段策略：通过显式轮廓对齐构建姿态代价体积筛选粗姿态，再结合粒子滤波与多光束跟踪进行精细优化。  
◆ 创新性设计姿态代价体积，通过均匀采样姿态假设并量化投影轮廓与预测轮廓的对齐度，实现高效概率分布建模。  
◆ 提出多光束跟踪的粒子滤波方法，显著扩大收敛域容错范围，可适应更大初始姿态误差。  
◆ 发布首个覆盖10.7平方公里的LoD1城市模型数据集，包含真实RGB查询图像与姿态真值，推动该领域研究。  
实验表明该方法在高/低LoD模型下均实现最优精度，甚至超越基于纹理模型的方法，为全球城市定位提供新范式。</td></tr>
<tr><td>2025-06-28</td><td>Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data</td><td>[2506.22939](http://arxiv.org/pdf/2506.22939)</td><td>这篇论文的核心贡献和创新点如下：  

◆ 提出了一种名为“Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN)”的新型深度学习方法，用于遥感数据的场景分类。  
◆ 通过结合双向循环神经网络和优化算法（Cuttlefish优化），显著提升了模型在复杂遥感数据中的特征提取能力。  
◆ 在实验中，CO-BRNN的准确率达到97%，优于现有的多种方法（如MLP-CNN、CNN-LSTM、LSTM-CRF等），展现了其优越性能。  
◆ 解决了传统深度学习方法对大规模、高噪声数据的依赖问题，提高了模型在有限数据条件下的鲁棒性。  
◆ 强调了物理验证在卫星数据应用中的重要性，确保模型结果的可靠性和实用性。  
◆ 为遥感场景分类提供了新的技术思路，可应用于灾害控制、生态监测、城市规划等多个领域。</td></tr>
<tr><td>2025-06-28</td><td>Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval</td><td>[2506.22864](http://arxiv.org/pdf/2506.22864)</td><td>◆ 提出Mask-aware TIR（MaTIR）新任务，首次将文本到图像检索（TIR）与指代表达分割（RES）统一，要求同时实现高效图像搜索和精确目标分割。  
◆ 设计两阶段框架：第一阶段利用SAM 2和Alpha-CLIP离线生成对象掩码和区域级嵌入，实现可扩展的分割感知检索；第二阶段通过多模态大语言模型（MLLM）重新排序并生成目标框，与掩码匹配提升精度。  
◆ 创新性结合分割模型（SAM 2）与跨模态检索技术（Alpha-CLIP），在离线阶段预计算掩码和嵌入，显著降低在线检索计算成本。  
◆ 引入MLLM进行结果重排和定位优化，利用其多模态理解能力提升检索准确率与分割质量。  
◆ 在COCO和D$^3$数据集上验证，检索精度和分割效果均显著优于现有方法，为跨模态任务提供新范式。</td></tr>
<tr><td>2025-06-27</td><td>MatChA: Cross-Algorithm Matching with Feature Augmentation</td><td>[2506.22336](http://arxiv.org/pdf/2506.22336)</td><td>◆ 提出了首个解决跨特征检测器视觉定位问题的方法MatChA，突破了现有方法必须使用相同检测器的限制。  
◆ 创新性地通过特征描述符增强技术提升跨检测器特征匹配性能，解决了关键点重复率低和描述符区分度不足的难题。  
◆ 设计了将特征转换到潜在空间的方案，有效实现了不同算法生成描述符的兼容匹配。  
◆ 在多个基准测试中验证了该方法显著提升了跨特征场景下的图像匹配和视觉定位精度。  
◆ 突破了传统方案依赖共同关键点的假设，更贴合实际应用中不同设备使用不同特征提取算法的复杂场景。</td></tr>
<tr><td>2025-06-26</td><td>OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography</td><td>[2506.21101](http://arxiv.org/pdf/2506.21101)</td><td>◆ 提出OracleFusion两阶段框架，首次将语义排版技术应用于甲骨文破译，通过结构约束生成语义增强的矢量字体。  
◆ 第一阶段采用多模态大语言模型（MLLM）结合空间感知推理（SAR），实现对甲骨文字形的结构分析与关键部件视觉定位。  
◆ 第二阶段创新性引入甲骨文结构向量融合（OSVF）技术，通过字形结构约束和字形保持约束，确保生成结果的结构完整性与语义准确性。  
◆ 在视觉呈现上突破传统方法，生成兼具美学质量与可读性的字形表达，为专家破译提供直观辅助。  
◆ 实验证明OracleFusion在语义相关性、视觉吸引力和字形保持方面均优于现有基线模型，显著提升破译效率。  
◆ 框架可对未解读甲骨文字符提供专家级见解，成为推动甲骨文研究的实用工具。</td></tr>
<tr><td>2025-06-25</td><td>Visualizing intercalation effects in 2D materials using AFM based techniques</td><td>[2506.20467](http://arxiv.org/pdf/2506.20467)</td><td>◆ 提出了一种基于原子力显微镜（AFM）的非侵入性方法，用于可视化二维材料（如MoS2/石墨烯/Ir(111)）中硫插层引起的局部结构和电子性质变化，避免了传统超高真空技术的耗时、高成本和空间限制问题。  
◆ 通过AFM形貌成像直接观察到插层导致的结构变化，并结合相位成像与力学测量，首次发现插层区域杨氏模量和粘附力的降低。  
◆ 利用开尔文探针力显微镜（KPFM）揭示了插层区域的表面电势和功函数变化，为插层效应提供了明确的电子学特征证据。  
◆ 创新性地采用光诱导力显微镜（PiFM）检测插层区域的光学响应增强，拓展了AFM技术在光学性质表征中的应用。  
◆ 综合多种AFM技术实现了插层效应的多维度映射（结构、力学、电子、光学），为二维材料性能调控提供了新工具和理论依据。  
◆ 证明了AFM技术在二维材料插层研究中的高效性和普适性，为未来材料设计和器件开发提供了低成本、高分辨率的表征方案。</td></tr>
<tr><td>2025-06-25</td><td>On the Burstiness of Faces in Set</td><td>[2506.20312](http://arxiv.org/pdf/2506.20312)</td><td>◆ 首次揭示了集合人脸识别(SFR)中普遍存在的&quot;突发性&quot;现象，即特定属性人脸在集合中高频出现，导致模型泛化能力下降和评估干扰。  
◆ 提出三种突发性人脸检测策略：基于Quickshift++的聚类方法、特征自相似性分析和广义最大池化(GMP)技术，有效识别集合中的高频人脸。  
◆ 在训练阶段通过调整采样比例抑制突发性影响，在评估阶段增强低频人脸的贡献度，显著提升模型在无约束场景下的表现。  
◆ 创新性提出质量感知GMP方法，使模型能够感知人脸质量并对低质量图像保持鲁棒性，解决了原始GMP的局限性。  
◆ 通过大量实验验证了突发性现象的广泛存在，证明抑制突发性能显著提升现有SFR基准测试的识别性能，为集合人脸识别提供了新思路。</td></tr>
<tr><td>2025-06-24</td><td>jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval</td><td>[2506.18902](http://arxiv.org/pdf/2506.18902)</td><td>◆ 提出jina-embeddings-v4模型，这是一个38亿参数的多模态嵌入模型，统一了文本和图像的表示。  
◆ 采用新颖的架构，支持单向量和多向量嵌入，并采用后期交互风格。  
◆ 引入任务特定的低秩适应（LoRA）适配器，优化了多种检索场景的性能，包括基于查询的信息检索、跨模态语义相似性和编程代码搜索。  
◆ 在单模态和跨模态检索任务中实现了最先进的性能，尤其在处理视觉丰富内容（如表格、图表、图表和混合媒体格式）方面表现突出。  
◆ 提出Jina-VDR基准，专门用于评估视觉丰富图像检索能力，填补了该领域的空白。</td></tr>
<tr><td>2025-06-26</td><td>Referring Expression Instance Retrieval and A Strong End-to-End Baseline</td><td>[2506.18246](http://arxiv.org/pdf/2506.18246)</td><td>◆ 提出新任务REIR（Referring Expression Instance Retrieval），填补了传统文本-图像检索（TIR）精度不足和指代表达理解（REC）扩展性差的空白，支持跨大规模图库的实例级检索与定位。  
◆ 构建首个大规模基准数据集REIRCOCO，通过视觉-语言模型生成细粒度指代表达，基于MSCOCO和RefCOCO实例增强数据多样性。  
◆ 提出端到端基线方法CLARE，采用双流架构设计，结合目标检测与REC预训练，实现跨模态特征对齐。  
◆ 创新性引入Mix of Relation Experts（MORE）模块，显式建模实例间关系，提升复杂场景下的检索精度。  
◆ 通过对比学习框架CLIA（Contrastive Language-Instance Alignment）优化语言-实例对齐，使模型在REIR、TIR和REC任务上均达到SOTA性能。  
◆ 验证了CLARE的强泛化能力，首次实现单一模型同时支持实例检索、粗粒度检索和细粒度定位三类任务。</td></tr>
<tr><td>2025-06-20</td><td>Class Agnostic Instance-level Descriptor for Visual Instance Search</td><td>[2506.16745](http://arxiv.org/pdf/2506.16745)</td><td>◆提出了一种基于自监督ViT的类无关实例级描述符，解决了视觉实例搜索中缺乏有效实例级特征表示的问题。  
◆通过层次化分解特征集，将实例区域发现建模为检测紧凑特征子集的过程，生成多层次的语义特征子集。  
◆构建的特征层次结构中，非叶节点和叶节点对应图像中不同语义尺度的实例区域，有效处理了物体嵌入和遮挡问题。  
◆生成的节点特征构成图像的全面实例表示，适用于已知和未知物体类别，具有强泛化能力。  
◆在三个实例搜索基准测试中表现显著优于现有方法，验证了其优越性。</td></tr>
<tr><td>2025-06-19</td><td>MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval</td><td>[2506.16353](http://arxiv.org/pdf/2506.16353)<br><a href=''>[代码]</a></td><td>◆ 首次将视觉状态空间模型（Mamba）引入大规模图像哈希检索任务，探索其在该领域的适用性和优势。  
◆ 提出分阶段的主干网络架构，通过分组Mamba操作实现多方向扫描，有效建模局部和全局信息。  
◆ 设计通道交互注意力模块，增强跨通道信息交流，提升特征表达能力。  
◆ 开发自适应特征增强模块，增加特征多样性并强化模型的视觉表示能力。  
◆ 在CIFAR-10、NUS-WIDE和IMAGENET等主流数据集上验证了方法的优越性，相比现有深度哈希方法具有更高效率和检索性能。  
◆ 开源代码促进后续研究，为线性复杂度模型在图像检索中的应用提供新思路。</td></tr>
<tr><td>2025-06-19</td><td>Fine-grained Image Retrieval via Dual-Vision Adaptation</td><td>[2506.16273](http://arxiv.org/pdf/2506.16273)</td><td>◆提出双视觉适应（DVA）方法，通过样本和特征协同适配解决细粒度图像检索（FGIR）中预训练模型易过拟合的问题，保留预训练知识的同时提升泛化能力。  
◆设计对象感知适配（Object-Perceptual Adaptation），通过修改输入样本引导冻结的预训练模型聚焦对类别预测关键的物体及局部特征。  
◆提出上下文内适配（In-Context Adaptation），仅引入少量可调参数进行特征适配，使调整后的特征更贴近预训练任务，避免修改原始预训练参数。  
◆结合知识蒸馏机制提出判别感知迁移（Discrimination Perception Transfer），将对象感知适配中的判别知识高效迁移至图像编码器，平衡检索效率与性能。  
◆实验表明DVA在3个分布内和3个分布外细粒度数据集上表现优异，且可学习参数量显著少于现有方法。</td></tr>
<tr><td>2025-06-19</td><td>Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation</td><td>[2506.15988](http://arxiv.org/pdf/2506.15988)<br><a href=''>[代码]</a></td><td>◆ 首次系统分析了四种常见对抗攻击和四种VPR专用攻击对视觉地点识别（VPR）定位性能的影响，揭示了现有系统的脆弱性。  
◆ 提出了一种闭环系统框架，将VPR、对抗攻击检测器（AAD）和主动导航决策相结合，并通过实验验证其性能优势。  
◆ 设计了新颖的实验范式，证明即使AAD的检测准确率有限（如真阳性率75%、假阳性率25%），也能显著降低平均沿轨定位误差约50%。  
◆ 首次研究了快速梯度符号法（FGSM）对抗攻击在VPR中的有效性，填补了该领域的研究空白。  
◆ 提出了多项关键评估指标（如沿轨误差、受攻击时间比例、不安全状态时间比例等），为系统设计提供了量化依据。  
◆ 强调了AAD在实际机器人导航系统中的必要性，为构建可信赖的导航系统提供了重要参考。</td></tr>
<tr><td>2025-06-18</td><td>Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles</td><td>[2506.15851](http://arxiv.org/pdf/2506.15851)</td><td>◆ 提出了一种结合图像特征和语义信息的轻量级传感器误差模型，用于预测视觉定位中的二维误差分布。  
◆ 通过条件化不确定性估计，隐含地捕捉了未标注的关键环境因素（如城市/高速、动态/静态场景、季节变化）。  
◆ 采用高斯混合模型（GMM）替代传统高斯分布，更准确地描述恶劣天气和光照条件下的测量误差特性。  
◆ 在Ithaca365多天气/光照数据集上验证了框架的准确性，涵盖晴天、夜间和雪天等复杂场景。  
◆ 提出独特的传感器门控方法，结合贝叶斯定位滤波器评估传感器与神经网络的联合不确定性量化性能。  
◆ 为自动驾驶安全关键系统提供了可解释的上下文相关不确定性量化工具。</td></tr>
<tr><td>2025-06-18</td><td>ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections</td><td>[2506.15180](http://arxiv.org/pdf/2506.15180)</td><td>◆ 提出ReSeDis任务，首次将大规模图像检索与像素级定位结合，要求模型根据文本描述在图像库中检索目标并精确定位其位置（边界框或分割掩码）。  
◆ 构建首个针对该任务的基准数据集，确保每个描述唯一对应分散在大规模多样化图像库中的目标实例，避免误匹配问题。  
◆ 设计联合评估指标，同时衡量检索召回率与定位精度，解决现有技术只能单独评估某一方面的局限。  
◆ 提供基于冻结视觉语言模型的零样本基线方法，揭示该任务未来研究的巨大提升空间。  
◆ 为构建下一代鲁棒、可扩展的多模态搜索系统提供真实端到端测试平台，弥补现有技术（视觉定位假设目标必然存在，文本检索缺乏细粒度定位）的不足。</td></tr>
<tr><td>2025-06-17</td><td>HARMONY: A Scalable Distributed Vector Database for High-Throughput Approximate Nearest Neighbor Search</td><td>[2506.14707](http://arxiv.org/pdf/2506.14707)</td><td>◆ 提出Harmony分布式向量数据库，解决单机处理高维向量时的内存和效率瓶颈。  
◆ 创新性地采用多粒度分区策略，结合基于维度和基于向量的分区方法，实现计算负载均衡。  
◆ 通过优化分区策略有效降低节点间通信开销，提升系统整体吞吐量。  
◆ 引入基于距离计算单调性的早期停止剪枝机制，大幅减少计算和通信开销。  
◆ 在真实数据集上的实验表明，Harmony在四节点配置下平均吞吐量达到现有方案的4.63倍。  
◆ 针对倾斜工作负载，性能比传统分布式方案提升58%，展现出优异的负载适应能力。</td></tr>
<tr><td>2025-06-17</td><td>TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Indoor Localization and Mapping</td><td>[2506.14178](http://arxiv.org/pdf/2506.14178)</td><td>◆ 提出TACS-Graphs框架，首次将地面机器人可通行性（traversability）与房间分割相结合，解决传统3D场景图中房间层分割不一致问题。  
◆ 通过可通行性约束重新定义房间边界，克服体素方法仅依赖几何邻近性导致的欠分割（开放空间误判）和过分割（复杂环境碎片化）缺陷。  
◆ 构建拓扑与语义更一致的场景图，在结构复杂室内环境中实现更准确的房间层语义分割。  
◆ 开发基于一致性场景图的闭环检测方法（CoSG-LCD），利用增强的分割一致性提升闭环检测效率，进而提高位姿估计精度。  
◆ 实验验证该方法在场景图一致性和位姿图优化性能上优于现有先进技术，为机器人定位与建图提供更可靠的环境表征。</td></tr>
<tr><td>2025-06-16</td><td>A Semantically-Aware Relevance Measure for Content-Based Medical Image Retrieval Evaluation</td><td>[2506.13509](http://arxiv.org/pdf/2506.13509)</td><td>◆ 提出了一种基于知识图谱的语义感知相关性度量方法，用于解决医学图像检索（CBIR）的性能评估难题。  
◆ 创新性地利用医学文本（如放射学报告或文献描述）中隐含的医学概念，避免了传统评估方法对人工标注数据的依赖。  
◆ 通过知识图谱量化医学概念间的语义距离，克服了现有方法将医学概念视为独立标签的局限性，能够捕捉概念间的细微关联。  
◆ 设计了基于近似匹配的相关性评分机制，通过计算两组医学概念的相似性间接衡量医学图像的相似度。  
◆ 在公开数据集上验证了所提方法的有效性和可行性，为医学CBIR评估提供了更符合临床语义的新标准。</td></tr>
<tr><td>2025-06-19</td><td>Hierarchical Multi-Positive Contrastive Learning for Patent Image Retrieval</td><td>[2506.13496](http://arxiv.org/pdf/2506.13496)</td><td>◆提出分层多正例对比学习损失函数，首次利用Locarno国际分类体系（LIC）的层级关系指导专利图像检索。  
◆通过层级分类树动态分配多组正样本对，根据专利图像在LIC中的层级距离赋予不同相似度权重。  
◆突破传统对比学习仅使用单一样本对的限制，能同时学习跨层级的细粒度语义关联。  
◆在DeepPatent2数据集上验证了方法的普适性，可适配多种视觉和多模态预训练模型。  
◆特别优化了小参数量模型的检索性能，在计算资源受限环境下具有显著部署优势。  
◆实验表明该方法能有效捕捉专利图像的技术细节和复杂语义，提升跨类别检索准确率。</td></tr>
<tr><td>2025-06-16</td><td>EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints for Visual Place Recognition</td><td>[2506.13133](http://arxiv.org/pdf/2506.13133)</td><td>◆ 提出了一种新颖的简单重排序方法，通过混合特征（MoF）方法在具身约束下优化全局特征，提升视觉地点识别（VPR）性能。  
◆ 首次系统分析了具身约束在VPR中的实际可行性，并根据现有数据集将其分类为GPS标签、时序戳、局部特征匹配和自相似矩阵等类型。  
◆ 设计了一种基于学习的MoF权重计算策略，采用多度量损失函数，有效融合多种特征信息。  
◆ 在公开数据集上实现了性能提升，仅需25 KB额外参数和每帧10微秒处理时间，显著优于现有方法。  
◆ 在Pitts-30k测试集上，基于DINOv2的基线性能提升0.9%，计算开销极低，适合实际机器人应用。</td></tr>
<tr><td>2025-06-16</td><td>SuperPlace: The Renaissance of Classical Feature Aggregation for Visual Place Recognition in the Era of Foundation Models</td><td>[2506.13073](http://arxiv.org/pdf/2506.13073)</td><td>◆ 提出SuperPlace框架，重新利用经典特征聚合方法（如GeM和NetVLAD），在基础模型时代优化视觉地点识别（VPR）性能。  
◆ 开发监督标签对齐方法，实现跨多个VPR数据集的统一训练框架，提升模型泛化能力。  
◆ 提出G²M特征聚合方法，通过双GeM结构学习特征图的主成分并校准输出，仅需十分之一特征维度即可达到优异效果。  
◆ 设计NetVLAD-Linear（NVL）的二次微调策略（FT²），先在高维空间学习特征向量，再通过单线性层压缩，显著提升性能。  
◆ 实验证明SuperPlace的优越性，G²M在低维度下表现突出，NVL-FT²在MSLS排行榜上排名第一。</td></tr>
<tr><td>2025-06-14</td><td>Feature Complementation Architecture for Visual Place Recognition</td><td>[2506.12401](http://arxiv.org/pdf/2506.12401)</td><td>◆ 提出局部-全局特征互补网络（LGCN），通过并行CNN-ViT混合架构解决视觉地点识别（VPR）中局部细节与全局上下文难以兼顾的问题。  
◆ 设计动态特征融合模块（DFM），通过联合建模空间和通道依赖关系实现自适应特征融合，提升特征表达的鲁棒性。  
◆ 在冻结的ViT主干中引入轻量级频域-空间融合适配器，以可控参数量实现任务特定适配，增强ViT分支对VPR任务的适应能力。  
◆ 实验证明LGCN在多个VPR基准数据集上均优于现有方法，定位精度和鲁棒性显著提升。  
◆ 整体架构兼顾计算效率与性能，为复杂环境下的机器人定位提供了新思路。</td></tr>
<tr><td>2025-06-11</td><td>Towards a general-purpose foundation model for fMRI analysis</td><td>[2506.11167](http://arxiv.org/pdf/2506.11167)</td><td>◆ 提出NeuroSTORM，首个面向fMRI分析的通用基础模型，直接从4D fMRI数据学习，解决传统方法因复杂预处理和任务专用模型导致的复现性和迁移性不足问题。  
◆ 采用Mamba架构和移位扫描策略，高效处理完整4D fMRI体积，突破传统时空建模效率瓶颈。  
◆ 设计空间-时间联合优化的预训练方法，结合任务特定提示微调（prompt tuning），显著提升跨任务迁移能力。  
◆ 基于超大规模数据集预训练（28.65百万帧fMRI，50,000+受试者，跨多中心及5-100岁年龄范围），建立迄今最全面的脑功能表征库。  
◆ 在五项任务（年龄/性别预测、表型预测、疾病诊断、fMRI-图像检索、任务态分类）中全面超越现有方法，并在美、韩、澳临床数据验证中展现卓越诊断性能。  
◆ 开源标准化模型框架，为fMRI临床研究提供可复现、可迁移的基础工具，推动脑疾病诊断的跨中心应用。</td></tr>
<tr><td>2025-06-11</td><td>Improving Personalized Search with Regularized Low-Rank Parameter Updates</td><td>[2506.10182](http://arxiv.org/pdf/2506.10182)<br><a href=''>[代码]</a></td><td>◆ 提出一种正则化低秩参数更新方法，仅需微调语言编码器最后一层的少量参数，即可有效适应个性化视觉-语言检索任务，避免传统文本反转方法的不足。  
◆ 发现参数相加策略能有效整合多个已学习个性化概念的参数，提升模型对多概念组合的识别能力。  
◆ 引入基于视觉语言模型生成描述的图像检索评估指标，量化微调后模型对通用知识的保留程度。  
◆ 在DeepFashion2和ConCon-Chi两个基准测试中实现最先进性能，个性化检索准确率较之前方法提升4%-22%。  
◆ 通过双编码器模型内部表征的针对性适配，解决了少样本场景下个性化概念与通用知识融合的难题。  
◆ 实验证明低秩参数更新在保留通用知识的同时，显著提升对&quot;我的狗Fido&quot;等个性化概念的跨上下文识别能力。</td></tr>
<tr><td>2025-06-10</td><td>Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment</td><td>[2506.10030](http://arxiv.org/pdf/2506.10030)</td><td>◆ 提出了首个针对多模态RAG系统中图像知识版权保护的水印框架AQUA，填补了该领域的空白。  
◆ 设计了两种互补的水印嵌入方法：基于首字母缩写的触发器和空间关系线索，确保水印信号在图像到文本的间接传播中保持有效。  
◆ 实现了水印的高效性、强鲁棒性和不可感知性，能够在不同模型和数据集上稳定工作。  
◆ 解决了现有RAG水印技术仅关注文本知识而忽略图像保护的局限性，扩展了版权保护范围。  
◆ 通过实验验证了AQUA在跨模态场景下的可靠性，支持对贡献数据的精准版权追踪。  
◆ 为RAG-as-a-Service环境中的多模态知识共享提供了实用的版权安全保障方案。</td></tr>
<tr><td>2025-06-11</td><td>Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints</td><td>[2506.09748](http://arxiv.org/pdf/2506.09748)</td><td>◆ 提出了一种分层跨源图像匹配方法，结合语义感知和结构约束的粗匹配模块与轻量级细粒度匹配模块，显著提升了无人机绝对视觉定位的精度。  
◆ 在粗匹配模块中，利用视觉基础模型提取的语义特征，在语义和结构约束下建立区域级对应关系，有效克服了跨源差异和时变因素带来的挑战。  
◆ 设计了轻量级细粒度匹配模块，通过提取精细特征建立像素级对应关系，进一步提升了定位的准确性。  
◆ 构建了不依赖相对定位技术的无人机绝对视觉定位流程，通过图像检索模块与分层匹配模块的结合，实现了完全基于视觉的全局定位。  
◆ 在公开基准数据集和新提出的CS-UAV数据集上验证了方法的优越性，展示了其在多种挑战性条件下的高精度和鲁棒性。</td></tr>
<tr><td>2025-06-10</td><td>Robust Visual Localization via Semantic-Guided Multi-Scale Transformer</td><td>[2506.08526](http://arxiv.org/pdf/2506.08526)</td><td>◆ 提出了一种结合多尺度特征学习与语义场景理解的视觉定位框架，通过层次化Transformer和跨尺度注意力机制融合几何细节与上下文信息，在保持空间精度的同时适应环境变化。  
◆ 创新性地引入神经场景表征提供的语义监督信号，指导网络学习视角不变特征，有效编码持久结构信息并抑制动态环境干扰。  
◆ 设计了多尺度Transformer架构，利用跨层级注意力机制整合不同尺度的视觉线索，显著提升了复杂场景下的定位鲁棒性。  
◆ 在TartanAir数据集上的实验表明，该方法在动态物体、光照变化和遮挡等挑战性场景中优于现有位姿回归方法。  
◆ 首次验证了语义引导与多尺度处理的协同策略对现实动态环境中视觉定位的有效性，为鲁棒定位提供了新思路。</td></tr>
<tr><td>2025-06-08</td><td>Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs</td><td>[2506.07045](http://arxiv.org/pdf/2506.07045)</td><td>◆ 提出了一种基于多模态大语言模型（MLLMs）的可解释AI生成图像检测方法，通过结合视觉定位和文本推理能力，不仅检测准确率高，还能提供人类可理解的解释。  
◆ 构建了一个包含边界框标注和描述性文本的数据集，突出AI生成图像的合成伪影，为模型提供视觉-文本对齐的推理基础。  
◆ 设计了多阶段优化策略，逐步平衡检测准确性、视觉定位能力和文本解释连贯性，解决了现有MLLMs在检测任务中的幻觉问题。  
◆ 通过微调MLLMs，使其能够同时定位图像中的视觉缺陷并生成合理的解释，显著提升了检测的可靠性和可解释性。  
◆ 实验表明，该方法在检测AI生成图像和定位视觉瑕疵方面均优于基线方法，为可解释的伪造检测提供了新思路。</td></tr>
<tr><td>2025-06-07</td><td>Zero Shot Composed Image Retrieval</td><td>[2506.06602](http://arxiv.org/pdf/2506.06602)</td><td>◆ 提出了一种基于BLIP-2的轻量级Q-Former模型，通过融合视觉和文本特征到单一嵌入，显著提升了零样本组合图像检索（Zero-shot CIR）的性能。  
◆ 在FashionIQ基准测试中，将Recall@10指标从原先的20-25%大幅提升至45.6%（衬衫）、40.1%（裙子）和50.4%（T恤），平均Recall@50达到67.6%。  
◆ 探索了Retrieval-DPO方法，尝试通过直接偏好优化（DPO）损失微调CLIP文本编码器，但发现其效果远低于基线（仅0.02% Recall@10）。  
◆ 分析了Retrieval-DPO失败的四大原因：缺乏图像-文本联合融合、目标函数与Top-K指标不匹配、负样本质量低，以及视觉和Transformer层冻结。  
◆ 研究表明，有效的基于偏好的CIR需要真正的多模态融合、与排名相关的目标函数，以及精心筛选的负样本。</td></tr>
<tr><td>2025-06-06</td><td>GenIR: Generative Visual Feedback for Mental Image Retrieval</td><td>[2506.06220](http://arxiv.org/pdf/2506.06220)</td><td>◆ 提出Mental Image Retrieval (MIR)任务，研究用户通过多轮交互从模糊心理图像中检索目标图像的真实场景，填补了现有文本-图像检索研究的空白。  
◆ 设计GenIR方法，首次利用扩散模型生成可视化反馈，将AI系统对用户意图的理解转化为直观的合成图像，克服传统抽象语言反馈的模糊性问题。  
◆ 构建自动化流水线生成高质量多轮MIR数据集，为后续研究提供基准支持。  
◆ 实验证明GenIR在多轮交互检索中显著优于现有方法，验证了生成式视觉反馈的有效性。  
◆ 开创性地将生成模型与交互式检索结合，为心理图像检索领域奠定新范式，推动人机协同搜索系统的发展。</td></tr>
<tr><td>2025-06-06</td><td>Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning</td><td>[2506.06205](http://arxiv.org/pdf/2506.06205)</td><td>◆提出Astra双模型架构（Astra-Global和Astra-Local），通过分层多模态学习实现通用移动机器人导航，突破传统模块化系统的局限性。  
◆Astra-Global首次将多模态大语言模型（LLM）与混合拓扑-语义图结合，显著提升视觉地点识别和全局定位能力，优于传统方法。  
◆Astra-Local采用自监督训练的4D时空编码器生成鲁棒特征，支持局部路径规划和里程计估计等多任务学习。  
◆创新性提出基于流匹配和掩码ESDF损失的规划头，有效降低碰撞风险，生成更安全的局部轨迹。  
◆里程计头通过Transformer编码器融合多传感器数据，实现高精度相对位姿预测。  
◆在真实室内场景的移动机器人上验证，端到端任务成功率显著提升，展现强泛化能力。</td></tr>
<tr><td>2025-06-05</td><td>HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular Visual Place Recognition</td><td>[2506.04764](http://arxiv.org/pdf/2506.04764)</td><td>◆ 提出HypeVPR框架，首次将双曲空间嵌入引入透视到环视（P2E）视觉位置识别任务，利用双曲空间更适合表示层次结构的特性。  
◆ 设计分层特征聚合机制，在双曲空间中组织局部到全局的特征表示，有效捕捉全景图像的固有层次关系。  
◆ 开发高效的粗到精搜索策略，显著提升匹配速度（最高达5倍）同时保持高精度，解决跨图像类型的鲁棒匹配问题。  
◆ 通过双曲空间的距离保持特性，优化特征空间中的距离度量，增强不同视角下描述符的区分能力。  
◆ 在多个基准数据集上验证了方法的优越性，性能超越现有最优方法，同时大幅降低检索时间。  
◆ 开源代码和模型，推动相关领域研究。</td></tr>
<tr><td>2025-06-05</td><td>Deep Learning Reforms Image Matching: A Survey and Outlook</td><td>[2506.04619](http://arxiv.org/pdf/2506.04619)</td><td>这篇论文系统综述了深度学习如何逐步革新传统图像匹配流程，并提出了分类框架。  
◆创新点一：首次从&quot;逐步替代传统模块&quot;和&quot;端到端合并多步骤&quot;两个维度，对深度学习方法进行系统分类（包括可学习检测-...</td></tr>
<tr><td>2025-06-02</td><td>Entity Image and Mixed-Modal Image Retrieval Datas...</td><td>[2506.02291](http://arxiv.org/pdf/2506.02291)</td><td>◆提出首个结合视觉与文本信息的混合模态图像检索基准MMIR，包含单实体图像和多实体图像两种复杂查询类型。  
◆发布Entity Image和MMIR两个高质量数据集，通过众包标注验证数据质量，...</td></tr>
<tr><td>2025-06-01</td><td>Quantization-based Bounds on the Wasserstein Metri...</td><td>[2506.00976](http://arxiv.org/pdf/2506.00976)</td><td>◆提出了一种基于量化网格的高效Wasserstein距离近似方法，通过粗网格上的Kantorovich问题精确求解结合升尺度校正步骤，在保持2%误差内实现10-100倍加速。◆创新性地在原始空间...</td></tr>
<tr><td>2025-05-30</td><td>SORCE: Small Object Retrieval in Com...</td><td>[2505.24441](http://arxiv.org/pdf/2505.24441)<br><a href=''>[代码]</a></td><td>◆提出新任务SORCE（复杂环境中的小物体检索），专注于通过文本查询检索复杂图像中的不显眼小物体。  
◆构建新基准SORCE-1K，包含复杂环境图像和描述小物体的文本查询，揭示现有T2IR方法...</td></tr>
<tr><td>**2025-05-29**</td><td>**Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch**</td><td>[2505.23763](http://arxiv.org/abs/2505.23763)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians**</td><td>[2505.22859](http://arxiv.org/abs/2505.22859)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images**</td><td>[2505.22098](http://arxiv.org/abs/2505.22098)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule**</td><td>[2505.22089](http://arxiv.org/abs/2505.22089)</td><td>摘要生成中...</td></tr>
<tr><td>2025-05-29</td><td>Visual Loop Closure Detection Through Deep Graph Consensus</td><td>[2505.21754](http://arxiv.org/pdf/2505.21754)</td><td>◆ Visual loop closure detection traditionally relies on place recognition methods to retrieve candidate loops that are validated using computationally expensive RANSAC-based geometric verification.
◆ As false positive loop closures significantly degrade downstream pose graph estimates, verifying a large number of candidates in online simultaneous localization and mapping scenarios is constrained by limited time and compute resources.
◆ While most deep loop closure detection approaches only operate on pairs of keyframes, we relax this constraint by considering neighborhoods of multiple keyframes when detecting loops.</td></tr>
<tr><td>**2025-05-27**</td><td>**QuARI: Query Adaptive Retrieval Improvement**</td><td>[2505.21647](http://arxiv.org/abs/2505.21647)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-27**</td><td>**ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval**</td><td>[2505.20764](http://arxiv.org/abs/2505.20764)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**Visualized Text-to-Image Retrieval**</td><td>[2505.20291](http://arxiv.org/abs/2505.20291)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval**</td><td>[2505.19952](http://arxiv.org/abs/2505.19952)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**Can Visual Encoder Learn to See Arrows?**</td><td>[2505.19944](http://arxiv.org/abs/2505.19944)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-22**</td><td>**TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition**</td><td>[2505.16447](http://arxiv.org/abs/2505.16447)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-21**</td><td>**Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval**</td><td>[2505.15877](http://arxiv.org/abs/2505.15877)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-21**</td><td>**SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval**</td><td>[2505.15867](http://arxiv.org/abs/2505.15867)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-20**</td><td>**Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models**</td><td>[2505.13828](http://arxiv.org/abs/2505.13828)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-18**</td><td>**MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark**</td><td>[2505.12254](http://arxiv.org/abs/2505.12254)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-16**</td><td>**Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization**</td><td>[2505.11620](http://arxiv.org/abs/2505.11620)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-16**</td><td>**Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing**</td><td>[2505.11121](http://arxiv.org/abs/2505.11121)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-04**</td><td>**OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery**</td><td>[2505.03836](http://arxiv.org/abs/2505.03836)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-06**</td><td>**Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and Low-Visibility Conditions**</td><td>[2505.03565](http://arxiv.org/abs/2505.03565)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-06**</td><td>**LiftFeat: 3D Geometry-Aware Local Feature Matching**</td><td>[2505.03422](http://arxiv.org/abs/2505.03422)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-06**</td><td>**Seeing the Abstract: Translating the Abstract Language for Vision Language Models**</td><td>[2505.03242](http://arxiv.org/abs/2505.03242)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-13**</td><td>**SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment**</td><td>[2505.01956](http://arxiv.org/abs/2505.01956)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-02**</td><td>**NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization**</td><td>[2505.01113](http://arxiv.org/abs/2505.01113)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-01**</td><td>**GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting**</td><td>[2504.20379](http://arxiv.org/abs/2504.20379)</td><td>摘要生成中...</td></tr>
</tbody>
</table>
</div>

<div align='right'><a href='#top'>↑ 返回顶部</a></div>

<h2 id='keypoint-detection'>Keypoint Detection</h2>

<div class="table-container">
<table>
<thead><tr><th>日期</th><th>标题</th><th>论文与代码</th><th>摘要</th></tr></thead>
<tbody>
<tr><td>2026-02-06</td><td>Perception-Control Coupled Visual Servoing for Textureless Objects Using Keypoint-Based EKF</td><td>[2602.06834](http://arxiv.org/pdf/2602.06834)</td><td>◆ Visual servoing is fundamental to robotic applications, enabling precise positioning and control.
◆ However, applying it to textureless objects remains a challenge due to the absence of reliable visual features.
◆ Moreover, adverse visual conditions, such as occlusions, often corrupt visual feedback, leading to reduced accuracy and instability in visual servoing.</td></tr>
<tr><td>2026-02-05</td><td>DroneKey++: A Size Prior-free Method and New Benchmark for Drone 3D Pose Estimation from Sequential Images</td><td>[2602.06211](http://arxiv.org/pdf/2602.06211)</td><td>◆ Accurate 3D pose estimation of drones is essential for security and surveillance systems.
◆ However, existing methods often rely on prior drone information such as physical sizes or 3D meshes.
◆ At the same time, current datasets are small-scale, limited to single models, and collected under constrained environments, which makes reliable validation of generalization difficult.</td></tr>
<tr><td>2026-01-22</td><td>Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures</td><td>[2601.16348](http://arxiv.org/pdf/2601.16348)</td><td>◆ Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography.
◆ For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually.
◆ Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision.</td></tr>
<tr><td>2026-01-21</td><td>ZENITH: Automated Gradient Norm Informed Stochastic Optimization</td><td>[2601.15212](http://arxiv.org/pdf/2601.15212)</td><td>◆ Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule.
◆ While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices.
◆ In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm.</td></tr>
<tr><td>2026-01-02</td><td>UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data</td><td>[2601.00991](http://arxiv.org/pdf/2601.00991)</td><td>◆ Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth.
◆ We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering.
◆ Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics.</td></tr>
<tr><td>2025-12-31</td><td>GenZ: Foundational models as latent variable generators within traditional statistical models</td><td>[2512.24834](http://arxiv.org/pdf/2512.24834)</td><td>◆ We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features.
◆ While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks.
◆ Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model&#x27;s domain understanding.</td></tr>
<tr><td>2026-01-11</td><td>BLANKET: Anonymizing Faces in Infant Video Recordings</td><td>[2512.15542](http://arxiv.org/pdf/2512.15542)</td><td>◆ Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods.
◆ We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes.
◆ Our method comprises two stages.</td></tr>
<tr><td>2025-12-17</td><td>Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</td><td>[2512.15508](http://arxiv.org/pdf/2512.15508)</td><td>◆ Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency.
◆ We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, &quot;Off The Grid&quot; distribution.
◆ Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches.</td></tr>
<tr><td>2025-12-15</td><td>JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</td><td>[2512.13677](http://arxiv.org/pdf/2512.13677)</td><td>◆ In this paper, we present JoVA, a unified framework for joint video-audio generation.
◆ Despite recent encouraging advances, existing methods face two critical limitations.
◆ First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements.</td></tr>
<tr><td>2025-12-16</td><td>UnCageNet: Tracking and Pose Estimation of Caged Animal</td><td>[2512.07712](http://arxiv.org/pdf/2512.07712)</td><td>◆ Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions.
◆ We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames.
◆ Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods.</td></tr>
<tr><td>2025-12-03</td><td>Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer</td><td>[2512.04282](http://arxiv.org/pdf/2512.04282)</td><td>◆ Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty.
◆ To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods.
◆ While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity.</td></tr>
<tr><td>2025-12-03</td><td>Leveraging topological data analysis to estimate bone strength from micro-CT as a surrogate for advanced imaging</td><td>[2512.03880](http://arxiv.org/pdf/2512.03880)</td><td>◆ Accurate bone strength prediction is essential for assessing fracture risk, particularly in aging populations and individuals with osteoporosis.
◆ Bone imaging has evolved from X-rays and DXA to clinical computed tomography (CT), and now to advanced modalities such as high-resolution peripheral quantitative CT and synchrotron radiation CT, which offer unprecedented resolution of bone microarchitecture.
◆ However, analytical methods have not kept pace with these imaging advances.</td></tr>
<tr><td>2025-12-03</td><td>DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction</td><td>[2512.03715](http://arxiv.org/pdf/2512.03715)</td><td>◆ This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.
◆ The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.
◆ DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.</td></tr>
<tr><td>2025-12-03</td><td>A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection</td><td>[2512.03684](http://arxiv.org/pdf/2512.03684)</td><td>◆ This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping.
◆ The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting.
◆ For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination.</td></tr>
<tr><td>2025-12-03</td><td>Linking Aneurysmal Geometry and Hemodynamics Using Computational Fluid Dynamics</td><td>[2512.03660](http://arxiv.org/pdf/2512.03660)</td><td>◆ The development and progression of abdominal aortic aneurysms (AAA) are related to complex flow patterns and wall-shear-driven mechanobiological stimuli, yet the quantitative relationship between aneurysmal geometry and hemodynamics remains poorly defined.
◆ In this study, we conducted a comprehensive hemodynamic analysis of 74 patient-specific abdominal aortas, representing one of the largest Computational Fluid Dynamics (CFD) cohorts reported to date.
◆ A multiscale framework coupling 0D-1D systemic circulation models with 3D stabilized finite-element simulations is used to generate physiologically consistent boundary conditions and high-fidelity flow fields.</td></tr>
<tr><td>2025-12-03</td><td>Memory-Guided Point Cloud Completion for Dental Reconstruction</td><td>[2512.03598](http://arxiv.org/pdf/2512.03598)</td><td>◆ Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures.
◆ We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines.
◆ After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding.</td></tr>
<tr><td>2025-12-02</td><td>Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces</td><td>[2512.02783](http://arxiv.org/pdf/2512.02783)</td><td>◆ Digital sound synthesis presents the opportunity to explore vast parameter spaces containing millions of configurations.
◆ Quality diversity (QD) evolutionary algorithms offer a promising approach to harness this potential, yet their success hinges on appropriate sonic feature representations.
◆ Existing QD methods predominantly employ handcrafted descriptors or supervised classifiers, potentially introducing unintended exploration biases and constraining discovery to familiar sonic regions.</td></tr>
<tr><td>2025-12-01</td><td>Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager</td><td>[2512.01611](http://arxiv.org/pdf/2512.01611)</td><td>◆ In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction.
◆ This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm.
◆ The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment.</td></tr>
<tr><td>2025-12-01</td><td>Non-Markovian dynamics in ice nucleation</td><td>[2512.01479](http://arxiv.org/pdf/2512.01479)</td><td>◆ In simulation studies of crystallisation, the size of the largest crystalline nucleus is often used as a reaction coordinate to monitor the progress of the nucleation process.
◆ Here, we investigate, for the case of homogeneous ice nucleation, whether the nucleus size exhibits Markovian dynamics, as assumed in classical nucleation theory.
◆ Using 300 independent nucleation trajectories generated by molecular dynamics, we evaluate the mean recurrence time required to reach selected values of the largest nucleus size.</td></tr>
<tr><td>2025-11-30</td><td>LAHNet: Local Attentive Hashing Network for Point Cloud Registration</td><td>[2512.00927](http://arxiv.org/pdf/2512.00927)</td><td>◆ Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features.
◆ However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness.
◆ In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors.</td></tr>
<tr><td>2025-11-29</td><td>Explainable Multi-Modal Deep Learning for Automatic Detection of Lung Diseases from Respiratory Audio Signals</td><td>[2512.00563](http://arxiv.org/pdf/2512.00563)</td><td>◆ Respiratory diseases remain major global health challenges, and traditional auscultation is often limited by subjectivity, environmental noise, and inter-clinician variability.
◆ This study presents an explainable multimodal deep learning framework for automatic lung-disease detection using respiratory audio signals.
◆ The proposed system integrates two complementary representations: a spectral-temporal encoder based on a CNN-BiLSTM Attention architecture, and a handcrafted acoustic-feature encoder capturing physiologically meaningful descriptors such as MFCCs, spectral centroid, spectral bandwidth, and zero-crossing rate.</td></tr>
<tr><td>2025-11-29</td><td>Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction</td><td>[2512.00521](http://arxiv.org/pdf/2512.00521)</td><td>◆ In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role.
◆ Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound.
◆ To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings.</td></tr>
<tr><td>2025-11-29</td><td>Interpretable Graph Neural Networks for Classifying Structure and Magnetism in Delafossite Compounds</td><td>[2512.00292](http://arxiv.org/pdf/2512.00292)</td><td>◆ Delafossites (ABC2, where A and B are metals and C is a chalcogen) are a versatile family of quantum materials and layered oxides/chalcogenides whose properties are highly sensitive to atomic composition and stacking geometry.
◆ Their broad chemical tunability makes them an ideal platform for large-scale combinatorial exploration and high-throughput computational screening with desirable quantum properties.
◆ In this work, we employ a Concept Whitening Graph Neural Network, a gray-box AI model, to classify delafossite structures by stacking sequence and magnetic states.</td></tr>
<tr><td>2025-11-28</td><td>Quantum spectroscopy of topological dynamics via a supersymmetric Hamiltonian</td><td>[2511.23169](http://arxiv.org/pdf/2511.23169)</td><td>◆ Topological data analysis (TDA) characterizes complex dynamics through global invariants, but classical computation becomes prohibitive for high-dimensional data.
◆ We reinterpret time-domain dynamics as the eigenvalue spectrum of a supersymmetric (SUSY) Hamiltonian and thereby estimate topological descriptors through quantum spectroscopy.
◆ While zero modes correspond to Betti numbers, we show that low-lying excited states quantify the stability of topological features.</td></tr>
<tr><td>2025-11-27</td><td>All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning</td><td>[2511.22739](http://arxiv.org/pdf/2511.22739)</td><td>◆ Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers.
◆ Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources.
◆ However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations.</td></tr>
<tr><td>2025-11-27</td><td>UMind-VL: A Generalist Ultrasound Vision-Language Model for Unified Grounded Perception and Comprehensive Interpretation</td><td>[2511.22256](http://arxiv.org/pdf/2511.22256)</td><td>◆ Despite significant strides in medical foundation models, the ultrasound domain lacks a comprehensive solution capable of bridging low-level Ultrasound Grounded Perception (e.g., segmentation, localization) and high-level Ultrasound Comprehensive Interpretation (e.g., diagnosis, reasoning).
◆ To bridge this gap, we propose UMind-VL, a unified foundation model designed to synergize pixel-level structural understanding with complex clinical reasoning.
◆ We first introduce UMind-DS, a large-scale multimodal dataset comprising 1.2 million ultrasound image-text pairs across 16 anatomical regions, enriching standard data with pixel-level annotations and clinician-validated rationales.</td></tr>
<tr><td>2025-11-27</td><td>3D Affordance Keypoint Detection for Robotic Manipulation</td><td>[2511.22195](http://arxiv.org/pdf/2511.22195)</td><td>◆ This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts&#x27; functionality.
◆ The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question.
◆ To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent.</td></tr>
<tr><td>2025-11-24</td><td>fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding</td><td>[2511.21760](http://arxiv.org/pdf/2511.21760)</td><td>◆ Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored.
◆ Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations.
◆ To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework.</td></tr>
<tr><td>2025-11-26</td><td>Uncertainty Quantification for Visual Object Pose Estimation</td><td>[2511.21666](http://arxiv.org/pdf/2511.21666)</td><td>◆ Quantifying the uncertainty of an object&#x27;s pose estimate is essential for robust control and planning.
◆ Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions.
◆ We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting.</td></tr>
<tr><td>2025-11-26</td><td>Semantic-Enhanced Feature Matching with Learnable Geometric Verification for Cross-Modal Neuron Registration</td><td>[2511.21452](http://arxiv.org/pdf/2511.21452)</td><td>◆ Accurately registering in-vivo two-photon and ex-vivo fluorescence micro-optical sectioning tomography images of individual neurons is critical for structure-function analysis in neuroscience.
◆ This task is profoundly challenging due to a significant cross-modality appearance gap, the scarcity of annotated data and severe tissue deformations.
◆ We propose a novel deep learning framework to address these issues.</td></tr>
<tr><td>2025-11-26</td><td>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model</td><td>[2511.20956](http://arxiv.org/pdf/2511.20956)</td><td>◆ Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models.
◆ We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision.
◆ BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations.</td></tr>
<tr><td>2025-11-25</td><td>Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features</td><td>[2511.20469](http://arxiv.org/pdf/2511.20469)</td><td>◆ Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories.
◆ Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns.
◆ This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos.</td></tr>
<tr><td>2025-11-25</td><td>Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation</td><td>[2511.20250](http://arxiv.org/pdf/2511.20250)</td><td>◆ Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world.
◆ This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video.
◆ To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task.</td></tr>
<tr><td>2025-11-21</td><td>Teager-Kaiser Energy Methods For EEG Feature Extraction In Biomedical Applications</td><td>[2511.17164](http://arxiv.org/pdf/2511.17164)</td><td>◆ Electroencephalography (EEG) signals are inherently non-linear, non-stationary, and vulnerable to noise sources, making the extraction of discriminative features a long-standing challenge.
◆ In this work, we investigate the non-linear Teager-Kaiser Energy Operator (TKEO) for modeling the underlying energy dynamics of EEG in three representative tasks: motor imagery, emotion recognition, and epilepsy detection.
◆ To accommodate the narrowband nature of the operator, we employ Gabor filterbanks to isolate canonical frequency bands, followed by the Energy Separation Algorithm to decompose the TKEO output into amplitude envelope and instantaneous frequency components.</td></tr>
<tr><td>2025-11-21</td><td>Exploring the added value of pretherapeutic MR descriptors in predicting breast cancer pathologic complete response to neoadjuvant chemotherapy</td><td>[2511.17158](http://arxiv.org/pdf/2511.17158)</td><td>◆ Objectives: To evaluate the association between pretreatment MRI descriptors and breast cancer (BC) pathological complete response (pCR) to neoadjuvant chemotherapy (NAC).
◆ Materials \&amp; Methods: Patients with BC treated by NAC with a breast MRI between 2016 and 2020 were included in this retrospective observational single-center study.
◆ MR studies were described using the standardized BI-RADS and breast edema score on T2-weighted MRI.</td></tr>
<tr><td>2025-11-20</td><td>PySERA: Open-Source Standardized Python Library for Automated, Scalable, and Reproducible Handcrafted and Deep Radiomics</td><td>[2511.15963](http://arxiv.org/pdf/2511.15963)</td><td>◆ Radiomics enables the extraction of quantitative biomarkers from medical images for precision modeling, but reproducibility and scalability remain limited due to heterogeneous software implementations and incomplete adherence to standards.
◆ Existing tools also lack unified support for deep learning based radiomics.
◆ To address these limitations, we introduce PySERA, an open source, Python native, standardized radiomics framework designed for automation, reproducibility, and seamless AI integration.</td></tr>
<tr><td>2025-11-23</td><td>Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors</td><td>[2511.14335](http://arxiv.org/pdf/2511.14335)</td><td>◆ Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera.
◆ Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive.
◆ Monocular SLAM also faces scale ambiguities, which affect its accuracy.</td></tr>
<tr><td>2025-11-19</td><td>Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution</td><td>[2511.14210](http://arxiv.org/pdf/2511.14210)</td><td>◆ We introduce Orion, a visual agent framework that can take in any modality and generate any modality.
◆ Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results.
◆ Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows.</td></tr>
<tr><td>2025-11-18</td><td>$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors</td><td>[2511.14109](http://arxiv.org/pdf/2511.14109)</td><td>◆ Visual Place Recognition (VPR) aims to match query images against a database using visual cues.
◆ State-of-the-art methods aggregate features from deep backbones to form global descriptors.
◆ Optimal transport-based aggregation methods reformulate feature-to-cluster assignment as a transport problem, but the standard Sinkhorn algorithm symmetrically treats source and target marginals, limiting effectiveness when image features and cluster centers exhibit substantially different distributions.</td></tr>
<tr><td>2025-11-17</td><td>Classification of Aortic Shape with Topographical Pair Correlation Functions</td><td>[2511.13960](http://arxiv.org/pdf/2511.13960)</td><td>◆ Quantitative descriptors convert high-dimensional medical images into low-dimensional features capable of differentiating organ shapes that correlate with injury or disease progression for diagnostic purposes.
◆ An important example is aortic dissections, which can be imaged using high-resolution CT scans and for which the shape of the true and false lumens of the aorta has long been used to predict disease state and the potential for positive surgical outcomes (namely thoracic endovascular repair or TEVAR).
◆ Here we present a method for calculating the topographical pair correlation function (TPCF), a descriptor of the spatial correlation of point estimates for Gaussian curvature, mean curvature, shape index, and bending ratio constrained to the surface of a meshed image.</td></tr>
<tr><td>2025-11-17</td><td>End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer</td><td>[2511.13208](http://arxiv.org/pdf/2511.13208)</td><td>◆ Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation.
◆ This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency.
◆ In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations.</td></tr>
<tr><td>2025-11-17</td><td>THIR: Topological Histopathological Image Retrieval</td><td>[2511.13170](http://arxiv.org/pdf/2511.13170)</td><td>◆ According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020.
◆ Early diagnosis and accurate clinical decision making are critical in reducing this global burden.
◆ In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns.</td></tr>
<tr><td>2025-11-17</td><td>SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration</td><td>[2511.13168](http://arxiv.org/pdf/2511.13168)</td><td>◆ Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics.
◆ Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory.
◆ Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences.</td></tr>
<tr><td>2025-11-15</td><td>Multimodal RGB-HSI Feature Fusion with Patient-Aware Incremental Heuristic Meta-Learning for Oral Lesion Classification</td><td>[2511.12268](http://arxiv.org/pdf/2511.12268)</td><td>◆ Early detection of oral cancer and potentially malignant disorders is challenging in low-resource settings due to limited annotated data.
◆ We present a unified four-class oral lesion classifier that integrates deep RGB embeddings, hyperspectral reconstruction, handcrafted spectral-textural descriptors, and demographic metadata.
◆ A pathologist-verified subset of oral cavity images was curated and processed using a fine-tuned ConvNeXt-v2 encoder, followed by RGB-to-HSI reconstruction into 31-band hyperspectral cubes.</td></tr>
<tr><td>2025-11-15</td><td>AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos</td><td>[2511.12241](http://arxiv.org/pdf/2511.12241)</td><td>◆ Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death.
◆ Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data.
◆ We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset.</td></tr>
<tr><td>2025-11-14</td><td>Interpretable descriptors enable prediction of hydrogen-based superconductors at moderate pressures</td><td>[2511.11284](http://arxiv.org/pdf/2511.11284)</td><td>◆ Room temperature superconductivity remains elusive, and hydrogen-base compounds despite remarkable transition temperatures(Tc) typically require extreme pressures that hinder application.
◆ To accelerate discovery under moderate pressures, an interpretable framework based on symbolic regression is developed to predict Tc in hydrogen-based superconductors.
◆ A key descriptor is an integrated density of states (IDOS) within 1 eV of the Fermi level (EF), which exhibits greater robustness than conventional single-point DOS features.</td></tr>
<tr><td>2025-11-12</td><td>A thermoinformational framework for the description of neuropsychological systems</td><td>[2511.09506](http://arxiv.org/pdf/2511.09506)</td><td>◆ This work presents a statistical thermodynamics-inspired framework that summarizes multichannel EEG and behavior using macroscopic state variables (entropy, internal energy, temperature, Helmholtz free energy) to quantify stability and reconfiguration in neuropsychological systems.
◆ Applied to mother-infant EEG dyads performing the A-not-B task, these variables dissociate neural reconfiguration from behavioral success across a large set of model and feature configurations.
◆ Informational heat increases during environmental switches and decision errors, consistent with increased information exchange with the task context.</td></tr>
<tr><td>2025-11-12</td><td>Hydrogen permeability prediction in palladium alloys and virtual screening of B2-phase stabilized Pd(100-x-y)CuxMy ternary alloys using machine learning</td><td>[2511.09245](http://arxiv.org/pdf/2511.09245)</td><td>◆ We present a forward prediction material screening framework designed to discover Pd-Cu alloys with improved B2 phase stability, thereby unlocking simultaneous $H_2$ generation and utilization.
◆ First, we trained CatBoost models with literature-derived Pd alloy data to predict $H_2$ permeability from composition and testing conditions.
◆ We evaluated fractional, composition-based, and physics-informed descriptors, individually and in combination, and showed that sequential Pearson filtering and fold-wise SHAP-based recursive feature elimination with cross-fold aggregation reduced errors while controlling complexity.</td></tr>
<tr><td>2025-11-11</td><td>Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms</td><td>[2511.08833](http://arxiv.org/pdf/2511.08833)</td><td>◆ Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations.
◆ However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures.
◆ We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries.</td></tr>
<tr><td>2025-11-10</td><td>Semi-distributed Cross-modal Air-Ground Relative Localization</td><td>[2511.06749](http://arxiv.org/pdf/2511.06749)</td><td>◆ Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks.
◆ However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy.
◆ To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework.</td></tr>
<tr><td>2025-11-09</td><td>DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization</td><td>[2511.06422](http://arxiv.org/pdf/2511.06422)</td><td>◆ With the rapid growth of the low-altitude economy, unmanned aerial vehicles (UAVs) have become key platforms for measurement and tracking in intelligent patrol systems.
◆ However, in GNSS-denied environments, localization schemes that rely solely on satellite signals are prone to failure.
◆ Cross-view image retrieval-based localization is a promising alternative, yet substantial geometric and appearance domain gaps exist between oblique UAV views and nadir satellite orthophotos.</td></tr>
<tr><td>2025-11-08</td><td>Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era</td><td>[2511.06024](http://arxiv.org/pdf/2511.06024)</td><td>◆ Visual place recognition (VPR) is typically regarded as a specific image retrieval task, whose core lies in representing images as global descriptors.
◆ Over the past decade, dominant VPR methods (e.g., NetVLAD) have followed a paradigm that first extracts the patch features/tokens of the input image using a backbone, and then aggregates these patch features into a global descriptor via an aggregator.
◆ This backbone-plus-aggregator paradigm has achieved overwhelming dominance in the CNN era and remains widely used in transformer-based models.</td></tr>
<tr><td>2025-11-08</td><td>Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition</td><td>[2511.05893](http://arxiv.org/pdf/2511.05893)</td><td>◆ Low-rank sparse regression models have been widely applied in the field of face recognition.
◆ To further address the challenges caused by complex occlusions and illumination variations, this paper proposes a Hybrid Second-Order Gradient Histogram based Global Low-Rank Sparse Regression (H2H-GLRSR) model.
◆ Specifically, a novel feature descriptor called the Hybrid Second-Order Gradient Histogram (H2H) is first designed to more effectively characterize the local structural features of facial images.</td></tr>
<tr><td>2025-11-07</td><td>From Quantum Annealing to Alloy Discovery: Towards Accelerated Design of High-Entropy Alloys</td><td>[2511.05750](http://arxiv.org/pdf/2511.05750)</td><td>◆ Data scarcity remains a central challenge in materials discovery, where finding meaningful descriptors and tuning models for generalization is critical but inherently a discrete optimization problem prone to multiple local minima confounding the true optimal state.
◆ Classical methods often get trapped in these minima, while quantum annealing can escape them via quantum fluctuations, including tunneling, that overcome narrow energy barriers.
◆ We present a quantum-assisted machine-learning (QaML) framework that employs quantum annealing to address these combinatorial optimization challenges through feature selection, support-vector training formulated in QUBO form for classification and regression, and a new QUBO-based neural-network pruning formulation.</td></tr>
<tr><td>2025-11-07</td><td>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</td><td>[2511.05404](http://arxiv.org/pdf/2511.05404)</td><td>◆ Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration.
◆ In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity.
◆ This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments.</td></tr>
<tr><td>2025-11-06</td><td>Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification</td><td>[2511.04190](http://arxiv.org/pdf/2511.04190)</td><td>◆ Covariance descriptors capture second-order statistics of image features.
◆ They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging.
◆ We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices.</td></tr>
<tr><td>2025-11-06</td><td>Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)</td><td>[2511.04126](http://arxiv.org/pdf/2511.04126)</td><td>◆ This study presents a complete pipeline for automated tennis match analysis.
◆ Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference.
◆ Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times.</td></tr>
<tr><td>2025-10-26</td><td>Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation</td><td>[2510.22618](http://arxiv.org/pdf/2510.22618)</td><td>◆ Pose estimation serves as a cornerstone of computer vision for understanding animal posture, behavior, and welfare.
◆ Yet, agricultural applications remain constrained by the scarcity of large, annotated datasets for livestock, especially dairy cattle.
◆ This study evaluates the potential and limitations of cross-species transfer learning by adapting ZebraPose - a vision transformer-based model trained on synthetic zebra imagery - for 27-keypoint detection in dairy cows under real barn conditions.</td></tr>
<tr><td>2025-10-25</td><td>SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET Tomography Using Semi-Supervised Learning</td><td>[2510.22454](http://arxiv.org/pdf/2510.22454)</td><td>◆ Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging (SVA) is the only imaging modality capable of resolving protein structures inside cells at molecular resolution.
◆ Particle picking, the task of localizing and classifying target proteins in 3D CryoET volumes, remains the main bottleneck.
◆ Due to the reliance on time-consuming manual labels, the vast reserve of unlabeled tomograms remains underutilized.</td></tr>
<tr><td>2025-10-21</td><td>GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization</td><td>[2510.18539](http://arxiv.org/pdf/2510.18539)</td><td>◆ This technical report outlines the top-ranking solution for RoboSense 2025: Track 3, achieving state-of-the-art performance on 3D object detection under various sensor placements.
◆ Our submission utilizes GBlobs, a local point cloud feature descriptor specifically designed to enhance model generalization across diverse LiDAR configurations.
◆ Current LiDAR-based 3D detectors often suffer from a \enquote{geometric shortcut} when trained on conventional global features (\ie, absolute Cartesian coordinates).</td></tr>
<tr><td>2025-10-21</td><td>DeepDetect: Learning All-in-One Dense Keypoints</td><td>[2510.17422](http://arxiv.org/pdf/2510.17422)</td><td>◆ Keypoint detection is the foundation of many computer vision tasks, including image registration, structure-from motion, 3D reconstruction, visual odometry, and SLAM.
◆ Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong performance yet suffer from key limitations: sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding, often failing to prioritize visually important regions.
◆ We present DeepDetect, an intelligent, all-in-one, dense keypoint detector that unifies the strengths of classical detectors using deep learning.</td></tr>
<tr><td>2025-09-30</td><td>Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks</td><td>[2510.00083](http://arxiv.org/pdf/2510.00083)</td><td>◆ Deep neural networks have been widely adopted in many vision and robotics applications with visual inputs.
◆ It is essential to verify its robustness against semantic transformation perturbations, such as brightness and contrast.
◆ However, current certified training and robustness certification methods face the challenge of over-parameterization, which hinders the tightness and scalability due to the over-complicated neural networks.</td></tr>
<tr><td>2025-09-26</td><td>PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning</td><td>[2509.21926](http://arxiv.org/pdf/2509.21926)</td><td>◆ Visual In-Context Learning (VICL) uses input-output image pairs, referred to as in-context pairs (or examples), as prompts alongside query images to guide models in performing diverse vision tasks.
◆ However, VICL often suffers from over-reliance on a single in-context pair, which can lead to biased and unstable predictions.
◆ We introduce PAtch-based $k$-Nearest neighbor visual In-Context Learning (PANICL), a general training-free framework that mitigates this issue by leveraging multiple in-context pairs.</td></tr>
<tr><td>2025-09-15</td><td>Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference</td><td>[2509.11731](http://arxiv.org/pdf/2509.11731)</td><td>该论文针对轨迹数据地图推断中稀疏区域道路断裂和密集区域冗余段的问题，提出了DGMap双解码框架。其核心创新点包括：
◆ 提出多尺度网格编码方法，整合全局语义上下文与局部几何特征，提升关键点检测精度。
◆ 设计掩码增强关键点提取机制，有效减少稀疏轨迹区的道路断裂问题。
◆ 引入全局上下文感知关系预测模块，通过建模长轨迹模式抑制密集区域的错误连接。
实验表明，DGMap在三个真实数据集上APLS指标优于现有方法5%，尤其在滴滴平台数据上表现突出。</td></tr>
<tr><td>2025-09-11</td><td>A Path Signature Framework for Detecting Creative Fatigue in Digital Advertising</td><td>[2509.09758](http://arxiv.org/pdf/2509.09758)</td><td>本文提出了一种基于路径签名分析的数字广告创意疲劳检测新框架。  
◆首次将随机分析中的路径签名方法应用于广告疲劳检测领域，开辟了营销分析中几何方法的新途径。  
◆将广告性能时间序列视为二维空间中的路径，利用其签名作为丰富的特征描述符以捕捉动态变化。  
◆通过计算连续时间窗口签名间的距离，识别性能动态中统计显著的变更点，灵敏度优于传统方法。  
◆将统计检测结果转化为直接财务指标，量化持续投资疲劳创意的机会成本，提升决策实用性。  
◆通过合成实验和案例验证了该数学原理框架的有效性，为广告疲劳分析提供了互补性新工具。</td></tr>
<tr><td>2025-09-10</td><td>Computational Imaging for Enhanced Computer Vision</td><td>[2509.08712](http://arxiv.org/pdf/2509.08712)</td><td>该论文系统综述了计算成像技术如何提升计算机视觉应用的性能与鲁棒性。  
◆ 系统梳理了计算成像技术（如光场成像、高动态范围成像、去模糊、高速成像和眩光抑制）在提升图像采集与重建质量方面的作用。  
◆ 深入分析了计算成像技术与核心计算机视觉任务（如目标检测、深度估计、光流估计、人脸识别和关键点检测）之间的协同关系。  
◆ 强调了针对特定任务的自适应成像流程的潜力，可显著提升在自主导航、监控、增强现实和机器人等实际应用中的准确性、鲁棒性和效率。  
◆ 指出了该领域新兴的研究机遇与挑战，并为未来研究方向提供了重要参考。  
论文通过跨领域整合，为复杂场景下的视觉系统优化提供了理论与实践基础。</td></tr>
<tr><td>2025-08-28</td><td>Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation</td><td>[2508.20830](http://arxiv.org/pdf/2508.20830)</td><td>本文提出了一种利用视觉语言模型（VLM）进行手术工具二维关键点检测的新方法。  
◆ 创新性地采用预训练视觉语言模型（VLM），并通过低秩自适应（LoRA）技术进行微调，有效缓解了在小型医疗数据集上容易过拟合的问题。  
◆ 设计了一套精心构建的提示模板，构建指令微调数据集，实现了视觉特征与语义关键点描述之间的对齐。  
◆ 仅需两个训练周期即可显著超越传统CNN或Transformer基线模型，显示出在低资源场景下的高效性能。  
◆ 该方法为后续三维手术器械及手部姿态估计奠定了基础，拓展了VLM在医疗视觉任务中的应用潜力。</td></tr>
<tr><td>2025-08-25</td><td>DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning</td><td>[2508.17746](http://arxiv.org/pdf/2508.17746)</td><td>该论文提出了一种名为DroneKey的新型框架，用于从图像序列中精确估计无人机的三维位姿。其核心贡献在于解决了无人机关键点检测中因螺旋桨视觉相似性高和姿态多样性大而导致的难题。  
◆ 提出了一种结合二维关键点检测器和三维位姿估计器的专用框架，针对无人机特性进行优化。  
◆ 在关键点检测阶段，创新性地从每个Transformer编码器层提取两种关键表示（中间表示和紧凑表示），并通过门控求和进行最优融合。  
◆ 引入了姿态自适应的马氏距离损失函数，有效提升了极端姿态下关键点预测的稳定性和准确性。  
◆ 构建并公开了新的无人机二维关键点及三维位姿数据集，为后续研究提供基础。  
实验结果表明，该方法在关键点检测中达到了99.68%的AP（OKS指标），三维位姿估计误差极低（角度MAE为10.62度，位置RMSE为0.221米），同时实现了44 FPS的实时处理速度。</td></tr>
<tr><td>2025-08-25</td><td>HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images</td><td>[2508.16465](http://arxiv.org/pdf/2508.16465)</td><td>该论文提出了名为HOSt3R的无需关键点检测的手-物体三维重建方法。其核心贡献在于摆脱了传统方法对关键点检测的依赖，显著提升了在复杂场景下的鲁棒性和泛化能力。
◆ 提出了一种无需关键点检测器的、直接从单目运动视频估计手-物体三维变换的鲁棒方法。
◆ 摆脱了对预扫描物体模板或已知相机内参的依赖，实现了更通用和无约束的应用。
◆ 将所提出的变换估计方法与多视图重建流程相结合，实现了精确的手-物体三维形状恢复。
◆ 在SHOWMe基准测试中，在手-物体三维变换和形状估计任务上达到了最先进的性能。
◆ 在HO3D数据集上验证了其对未见过的物体类别具有良好的泛化能力。</td></tr>
<tr><td>2025-08-21</td><td>Mag-Match: Magnetic Vector Field Features for Map Matching and Registration</td><td>[2508.15300](http://arxiv.org/pdf/2508.15300)</td><td>Mag-Match提出了一种利用三维磁场矢量特征进行地图匹配与注册的新方法。  
◆ 创新性地利用磁力计数据提取高阶导数特征，这些特征对视觉或激光传感器失效的烟雾、粉尘等恶劣环境具有鲁棒性。  
◆ 所提出的磁场特征描述子具有全局旋转不变性，无需依赖重力方向对齐，显著提升了不同地图或不同机器人数据之间的配准灵活性。  
◆ 采用物理信息高斯过程进行概率推理，能够从离散点云数据中高效、递归地推断整个地图的磁场及其高阶导数，实现了对磁场场的连续建模。  
◆ 在仿真和真实实验中验证了方法有效性，实现了地图-地图、机器人-地图和机器人-机器人之间的精确变换，性能优于基于SIFT的传统方法。</td></tr>
<tr><td>2025-08-17</td><td>Splat Feature Solver</td><td>[2508.12216](http://arxiv.org/pdf/2508.12216)</td><td>◆ 提出了一种统一且与核函数及特征无关的特征提升问题稀疏线性逆问题公式化方法，可通过闭式解高效求解。  
◆ 在凸损失函数下提供了全局最优误差的可证明上界，确保高质量的特征提升结果。  
◆ 引入两种互补的正则化策略（Tikhonov Guidance和Post-Lifting Aggregation）以解决多视角观测中的不一致性和噪声问题，提升语义保真度。  
◆ Tikhonov Guidance通过软对角优势确保数值稳定性，Post-Lifting Aggregation通过特征聚类过滤噪声输入。  
◆ 在开放词汇3D分割基准测试中达到最先进性能，显著优于基于训练、分组和启发式的前沿基线方法，且特征提升仅需数分钟完成。</td></tr>
<tr><td>2025-08-13</td><td>Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram</td><td>[2508.10942](http://arxiv.org/pdf/2508.10942)</td><td>◆ 提出了一种新型特征描述符——方向直方图形状（shape of orientation histogram），用于描述Artcode的通用拓扑结构。  
◆ 将Artcode识别问题重新定义为Artcode提案检测任务，将拓扑相似但几何和语义不同的对象归为同一类别。  
◆ 构建了专门的数据集并进行全面实验，验证了所提特征向量在表示拓扑结构方面的可行性。  
◆ 开发了基于该特征向量的Artcode检测系统，实验结果表明其检测效果显著。  
◆ 首次尝试开发基于特征的拓扑对象检测系统，为Artcode等拓扑对象的识别提供了新思路。  
◆ 该研究为虚实交互开辟了新机会，并展示了拓扑对象检测的潜在应用前景。</td></tr>
<tr><td>2025-08-13</td><td>Stable Diffusion Models are Secretly Good at Visual In-Context Learning</td><td>[2508.09949](http://arxiv.org/pdf/2508.09949)</td><td>◆ 揭示了现成Stable Diffusion模型具备视觉上下文学习潜力，无需专门训练即可适应多种视觉任务。  
◆ 提出原位注意力重计算机制，通过改造自注意力层显式融合查询与示例提示的上下文关系。  
◆ 首次实现单一预训练扩散模型在六大视觉任务（如前景分割、目标检测等）的零样本迁移，突破现有方法需定制化训练的局限。  
◆ 在Pascal-5i数据集上，前景分割任务mIoU指标分别超越Visual Prompting和IMProv方法8.9%和3.2%。  
◆ 通过集成多提示样本提升任务推理能力，证明模型能有效利用上下文示例提升性能。  
◆ 为视觉上下文学习提供轻量化新范式，仅需修改注意力机制且保持模型权重冻结，显著提升通用性。</td></tr>
<tr><td>2025-08-16</td><td>AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation</td><td>[2508.07112](http://arxiv.org/pdf/2508.07112)</td><td>◆ 提出AugLift方法，通过简单但有效的输入增强策略，显著提升了基于2D关键点提升的3D人体姿态估计模型的泛化能力，无需额外数据或传感器。  
◆ 创新性地在标准2D关键点坐标(x,y)基础上，稀疏地增加了关键点检测置信度c和对应深度估计d两个信号，利用预训练模型计算这些信号，继承了它们的强泛化能力。  
◆ 方法具有模块化特性，可以轻松集成到现有的各种提升架构中，无需修改模型结构。  
◆ 在四个数据集上的大量实验表明，AugLift将未见数据集的跨数据集性能平均提升10.1%，同时将分布内性能提升4.0%。  
◆ 分析表明，这些稀疏的关键点对齐线索提供了鲁棒的帧级上下文信息，为提升任何基于提升的姿态估计模型的泛化性能提供了实用方案。  
◆ 所有代码将公开，便于研究社区使用和复现。</td></tr>
<tr><td>2025-08-07</td><td>Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</td><td>[2508.05514](http://arxiv.org/pdf/2508.05514)</td><td>◆ 提出融合目标检测器回归分支和分类分支特征的双重特征增强策略，将空间位置信息直接嵌入特征表示，提升外观建模的鲁棒性。  
◆ 创新性引入头部关键点检测模块，利用头部不易被遮挡的特性，有效缓解密集场景中全身特征丢失导致的跟踪失效问题。  
◆ 设计迭代式卡尔曼滤波运动模型，突破传统线性匀速假设，结合3D场景先验知识实现复杂遮挡下的轨迹补全。  
◆ 首次将检测任务的多维度特征（分类/回归/关键点）与改进运动模型联合优化，形成外观-运动协同增强的跟踪框架。  
◆ 针对严重遮挡场景，通过头部定位与全身检测的异构特征互补，显著提升密集人群的轨迹连续性和ID保持能力。  
◆ 所提方法在保持实时性的前提下，对重叠率超过70%的极端遮挡情况展现出优于传统Re-ID方案的跟踪稳定性。</td></tr>
<tr><td>2025-07-31</td><td>Mitigating Resolution-Drift in Federated Learning: Case of Keypoint Detection</td><td>[2507.23461](http://arxiv.org/pdf/2507.23461)</td><td>◆ 首次在联邦学习中提出并系统研究了&quot;分辨率漂移&quot;问题，揭示了分辨率差异作为非独立同分布数据的新维度对关键点检测任务的重要影响。  
◆ 提出分辨率自适应联邦学习（RAF）方法，通过基于热图的多分辨率知识蒸馏机制，在高分辨率教师模型和低分辨率学生模型间传递知识，有效增强模型对分辨率变化的鲁棒性。  
◆ 创新性地采用高低分辨率双向蒸馏策略，既避免了低分辨率客户端过拟合，又保留了高分辨率空间细节特征，突破了传统联邦学习在非分类任务中的性能瓶颈。  
◆ 通过理论分析和大量实验验证，证明RAF不仅能显著缓解分辨率漂移（最高提升23.6%准确率），还能无缝集成到现有联邦学习框架中，具有强实用性。  
◆ 通过t-SNE可视化分析，首次揭示了分类任务与高分辨率表征任务在特征分布上的本质差异，为RAF方法扩展到其他需要保持空间细节的任务（如医疗影像分析）提供了理论基础。  
◆ 开辟了联邦学习在人体姿态估计等非分类任务中的新研究方向，为解决跨设备视觉任务中的分辨率异构性问题提供了标准化解决方案。</td></tr>
<tr><td>2025-07-25</td><td>Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching</td><td>[2507.19118](http://arxiv.org/pdf/2507.19118)</td><td>◆提出跨时空融合注意力机制(CSTF)，通过独立检测参考图像和查询图像中的尺度不变关键点来增强特征表示，解决多模态遥感图像间几何和辐射差异大的问题。  
◆创新性地构建对应关系图，同时利用多个图像区域的信息，有效捕捉跨模态相似性，克服传统全连接层特征提取的局限性。  
◆将相似性匹配重新定义为分类任务，结合SoftMax和全卷积网络(FCN)层，在保持局部特征敏感性的同时整合全局上下文信息。  
◆在HRSC2016和DOTA基准数据集上实现目标检测任务的最优性能，平均mAP分别达到90.99%和90.86%，显著超越现有模型。  
◆保持12.5 FPS的推理速度，证明该方法在提升精度的同时具备实际应用的高效性。  
◆验证了改进的跨模态特征匹配能直接提升遥感目标检测等下游任务性能，为多模态遥感分析提供新思路。</td></tr>
<tr><td>2025-07-24</td><td>A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration</td><td>[2507.18551](http://arxiv.org/pdf/2507.18551)</td><td>◆提出了一种新型3D跨模态关键点描述符，专门用于解决MRI与实时超声(iUS)之间的配准难题，克服了两种模态在外观、分辨率和视野上的显著差异。  
◆采用患者特异性的合成匹配方法，从术前MRI生成合成iUS体积，通过监督对比学习训练共享描述符空间，增强了跨模态匹配能力。  
◆设计了基于概率的关键点检测策略，能够识别解剖学显著且模态一致的位置，提高了关键点的可靠性和一致性。  
◆在训练阶段引入课程式三元组损失和动态难负样本挖掘，使描述符对iUS伪影（如斑点噪声和有限覆盖）具有鲁棒性，同时保持旋转不变性。  
◆在推理阶段，通过稀疏匹配实现刚性配准，无需人工初始化，且在ReMIND数据集上验证了其优越性，平均匹配精度达69.8%，配准误差低至2.39 mm。  
◆相比现有方法，该框架具有可解释性，对iUS视野变化表现出强鲁棒性，代码已开源，便于进一步研究和应用。</td></tr>
<tr><td>2025-07-23</td><td>CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits</td><td>[2507.17327](http://arxiv.org/pdf/2507.17327)</td><td>◆ 提出CartoonAlive方法，首次实现从单张肖像照片快速生成高质量Live2D卡通模型，耗时不足30秒。  
◆ 创新地将3D人脸建模中的形状基概念引入2D领域，构建适用于Live2D的面部混合形状系统。  
◆ 通过面部关键点检测自动推断混合形状权重，无需人工干预即可实现高精度表情驱动。  
◆ 采用分层分割技术模拟3D运动效果，在保持2D卡通风格的同时实现类似3D的实时动态操控。  
◆ 相比传统3D建模方案大幅降低制作成本，相比2D视频方案显著提升交互灵活性。  
◆ 为数字内容创作提供高效可扩展的解决方案，拓展了虚拟角色动画的应用场景。</td></tr>
<tr><td>2025-07-21</td><td>Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors</td><td>[2507.16850](http://arxiv.org/pdf/2507.16850)</td><td>◆ 提出了一种实时单目3D人体姿态估计框架，结合2D关键点检测与几何感知的2D到3D提升技术，显著提升了在无约束环境下的性能。  
◆ 显式利用相机内参和个性化解剖学先验知识，通过自校准和生物力学约束的反向运动学增强模型精度。  
◆ 创新性地从动作捕捉和合成数据集中生成大规模合理的2D-3D训练对，解决了标注数据不足的问题。  
◆ 框架无需专用硬件即可实现快速、个性化的高精度3D姿态估计，具有强部署适应性。  
◆ 融合数据驱动学习与模型先验知识，在提升准确性的同时增强了模型的可解释性和边缘设备部署能力。</td></tr>
<tr><td>2025-07-17</td><td>DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model</td><td>[2507.13145](http://arxiv.org/pdf/2507.13145)</td><td>◆ 提出DINO-VO系统，首次将视觉基础模型DINOv2的鲁棒语义特征应用于单目视觉里程计（VO），解决了传统学习型VO在泛化性和鲁棒性上的不足。  
◆ 针对DINOv2特征粒度粗糙的问题，设计了专用显著关键点检测器，有效提升稀疏特征匹配的精度。  
◆ 结合DINOv2的语义特征与细粒度几何特征，生成兼具鲁棒性和局部化能力的混合特征表示。  
◆ 采用基于Transformer的匹配器和可微分位姿估计层，通过端到端学习优化特征匹配与运动估计。  
◆ 在TartanAir、KITTI等数据集上超越传统帧间VO方法（如SuperPoint），并在室外驾驶场景中与视觉SLAM系统性能相当，同时保持72 FPS实时性。  
◆ 系统内存占用低于1GB，展现了高效部署潜力，为视觉基础模型在实时定位任务中的应用提供新范式。</td></tr>
<tr><td>2025-07-15</td><td>KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model</td><td>[2507.11102](http://arxiv.org/pdf/2507.11102)</td><td>◆ 提出KptLLM++，首个专用于通用关键点理解的多模态大语言模型，通过用户指令整合多样化输入模态，填补了MLLMs在细粒度语义捕捉上的空白。  
◆ 创新性地采用&quot;先识别后检测&quot;范式，通过结构化思维链推理机制，先解析关键点语义再精确定位，提升复杂场景下的理解能力。  
◆ 构建超50万样本的大规模训练数据集，覆盖多样物体、关键点类别、图像风格及遮挡场景，显著增强模型泛化性。  
◆ 实现跨场景关键点检测的统一框架，建立高效人机协作接口，支持细粒度图像分析、物体检索和行为识别等应用。  
◆ 在多个关键点检测基准测试中达到SOTA性能，验证了其作为统一细粒度图像理解解决方案的潜力。  
◆ 为AI理解结构化像素级语义信息提供新思路，对推动人机交互变革具有重要启示意义。</td></tr>
<tr><td>2025-07-15</td><td>GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft</td><td>[2507.11077](http://arxiv.org/pdf/2507.11077)</td><td>◆ 提出基于图结构的关键点网络GKNet，利用关键点间的几何约束关系提升非合作航天器单目姿态估计的精度。  
◆ 针对航天器结构对称性和局部遮挡问题，通过图网络建模关键点拓扑关系，增强检测鲁棒性。  
◆ 构建中等规模航天器关键点检测数据集SKD，包含3种航天器目标、9万张仿真图像及高精度标注，填补领域数据空白。  
◆ 实验证明GKNet在关键点检测精度上显著优于现有先进方法，尤其适用于复杂空间场景。  
◆ 开源代码和数据集促进后续研究，为在轨服务任务（如卫星维护、太空碎片清理）提供可靠技术支撑。</td></tr>
<tr><td>2025-07-14</td><td>FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching</td><td>[2507.10770](http://arxiv.org/pdf/2507.10770)</td><td>◆提出FPC-Net，通过特征金字塔和基于一致性的隐式匹配实现无描述符的关键点检测，重新改进了SuperPoint方法。  
◆创新性地在关键点检测阶段直接建立关联性，省去了传统方法中描述符的计算、存储、传输和匹配步骤。  
◆尽管匹配精度略低于传统方法，但完全消除了描述符需求，大幅降低了定位系统的内存占用。  
◆通过特征金字塔和一致性匹配机制，实现了高效且轻量化的关键点检测与匹配流程。  
◆在实验中对比了传统手工方法和现代学习方法，验证了该方法的有效性和实用性。  
◆为几何计算机视觉任务提供了一种更简洁、更高效的解决方案，尤其适合资源受限的应用场景。</td></tr>
<tr><td>2025-07-27</td><td>Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection</td><td>[2507.07994](http://arxiv.org/pdf/2507.07994)</td><td>◆ 提出首个基于草图的小样本关键点检测框架，利用人类手绘草图作为无源数据替代方案，解决传统方法在查询数据分布不一致时的困境。  
◆ 设计跨模态嵌入学习机制，有效桥接草图与真实图像之间的模态差异，实现草图到关键点的精准映射。  
◆ 引入网格化定位器（grid-based locator）增强空间感知能力，结合原型网络优化关键点定位精度。  
◆ 创新性采用原型域适应技术（prototypical domain adaptation），自适应消除用户手绘风格的个体差异，提升模型泛化性。  
◆ 通过大量实验验证框架在跨类别、跨关键点任务中的小样本快速收敛能力，扩展了关键点检测的应用边界。</td></tr>
<tr><td>2025-07-09</td><td>Reading a Ruler in the Wild</td><td>[2507.07077](http://arxiv.org/pdf/2507.07077)</td><td>◆ 提出RulerNet深度学习框架，将标尺读数重新定义为统一的关键点检测问题，通过几何级数参数表示标尺刻度，实现透视变换下的鲁棒性测量。  
◆ 采用抗畸变标注和训练策略直接定位厘米刻度，摆脱传统方法对手工阈值或固定流程的依赖，显著提升对不同标尺类型和成像条件的泛化能力。  
◆ 开发可扩展的合成数据生成流程，结合图形化标尺生成与ControlNet技术添加逼真背景，有效缓解数据稀缺问题并增强训练多样性。  
◆ 提出轻量级DeepGP网络，直接从噪声标记回归几何级数参数，替代传统迭代优化方法，实现移动或边缘设备上的实时尺度估计。  
◆ 实验证明RulerNet在复杂真实场景中能提供高精度、一致且高效的尺度估计，为生物医学、法医等领域的自动化测量提供通用解决方案。</td></tr>
<tr><td>2025-07-09</td><td>MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning</td><td>[2507.06662](http://arxiv.org/pdf/2507.06662)</td><td>◆ 提出MK-Pose框架，首次融合RGB图像、点云数据和类别级文本描述，通过多模态输入提升类别级物体姿态估计的鲁棒性。  
◆ 设计自监督关键点检测模块，结合注意力机制生成查询、软热图匹配和图关系建模，有效解决遮挡和跨实例泛化问题。  
◆ 创新性引入图增强特征融合模块，整合局部几何信息与全局上下文，增强对复杂场景的建模能力。  
◆ 在CAMERA25和REAL275数据集上验证性能，无需形状先验即超越现有最优方法（IoU和平均精度指标）。  
◆ 额外测试跨数据集能力（HouseCat6D），证明模型具备强泛化性，适用于实际工业场景。  
◆ 开源代码并提供完整实现，推动领域研究与应用落地。</td></tr>
<tr><td>2025-06-27</td><td>MatChA: Cross-Algorithm Matching with Feature Augmentation</td><td>[2506.22336](http://arxiv.org/pdf/2506.22336)</td><td>◆ 提出了首个解决跨特征检测器场景下视觉定位问题的方法MatChA，突破了现有方法必须使用相同检测器的限制。  
◆ 通过特征描述符增强技术提升跨检测器特征匹配性能，解决了关键点重复率低和描述符区分度不足的难题。  
◆ 创新性地将增强后的特征转换到潜在空间，实现了不同算法生成描述符的有效匹配。  
◆ 在多个基准测试中验证了该方法显著提升了跨特征场景下的图像匹配和视觉定位精度。  
◆ 突破了传统方案依赖共同关键点的假设，更贴合实际应用中不同设备使用不同特征提取算法的复杂场景。</td></tr>
<tr><td>2025-06-27</td><td>SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images</td><td>[2506.21945](http://arxiv.org/pdf/2506.21945)</td><td>◆ 提出堆叠式深度残差网络（SDRNet），通过双编码器-解码器结构同时捕获长程语义并保留空间细节，解决高分辨率遥感图像分割中空间信息丢失问题。  
◆ 在编码器与解码器之间引入膨胀残差块（DRB），增强全局依赖关系建模能力，有效应对地物类别差异和遮挡导致的特征提取挑战。  
◆ 通过多上下文特征学习机制，覆盖不同尺寸地物目标，缓解因物体尺寸变化导致的细分不准问题。  
◆ 结合全局与局部上下文信息，显著提升对细小地物和复杂边界的识别精度，克服传统深度网络下采样导致的边界模糊缺陷。  
◆ 在ISPRS Vaihingen和Potsdam数据集上验证了模型优越性，性能超越现有深度卷积网络，为高精度地物分类提供新解决方案。</td></tr>
<tr><td>2025-05-29</td><td>TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning</td><td>[2505.23475](http://arxiv.org/pdf/2505.23475)<br><a href=''>[代码]</a></td><td>◆提出TimePoint方法，通过自监督学习从合成数据中提取关键点和描述符，显著加速动态时间规整（DTW）的对齐过程，同时提高对齐精度。  
◆创新性地将2D关键点检测思想适配到1D信号，设计高效的一维微分同胚模型生成逼真训练数据，有效模拟非线性时间扭曲。  
◆采用全卷积和小波卷积架构提取信息丰富的稀疏表示，使DTW在稀疏数据上运行时获得数量级加速，且精度通常优于原始信号上的标准DTW。  
◆仅使用合成数据训练即可在真实时间序列上展现强泛化能力，结合真实数据微调后性能进一步提升。  
◆通过大量实验验证，TimePoint在速度和精度上均优于标准DTW，为大规模时间序列分析提供可扩展解决方案。</td></tr>
<tr><td>2025-05-24</td><td>Why Not Replace? Sustaining Long-Term Visual Localization via Handcrafted-Learned Feature Collaboration on CPU</td><td>[2505.18652](http://arxiv.org/pdf/2505.18652)<br><a href=''>[代码]</a></td><td>◆ 提出手工-学习特征协作机制：首次系统论证手工特征（适合连续跟踪）与学习特征（擅长宽基线匹配）的功能互补性，打破传统&quot;替代&quot;思维，建立协同框架。  
◆ 设计CPU友好的分层定位架构：实时层采用手工特征进行相对位姿估计，异步层选择性调用学习特征进行绝对定位，实现仅需CPU的长期稳定运行。  
◆ 创新关键帧优化策略：通过动态筛选机制平衡学习特征的计算开销与定位精度，使系统在光照变化下保持47%的平均误差降低。  
◆ 实现全时段环境适应性：通过特征协作有效应对工业场景中的季节更替、昼夜光照变化等挑战，定位一致性显著提升。  
◆ 提供完整开源实现：公开代码包含特征互补性分析、计算延迟剖析到系统级验证的全套实验数据，推动工业应用落地。  
◆ 建立三阶段验证体系：从特征特性对比、CPU平台算力剖析到真实光照变化测试，形成严谨的技术验证链路。</td></tr>
<tr><td>2025-05-18</td><td>SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving</td><td>[2505.12246](http://arxiv.org/pdf/2505.12246)</td><td>◆ 提出SEPT框架，利用标准定义地图（SD地图）作为先验知识，增强自动驾驶场景感知与拓扑推理能力，减少对高精地图的依赖。  
◆ 设计混合特征融合策略，结合SD地图与鸟瞰图（BEV）特征，同时处理栅格化和矢量化表示，解决两者空间对齐问题。  
◆ 创新性引入基于SD地图的辅助任务——交叉路口感知关键点检测，提升长距离和遮挡场景下的理解性能。  
◆ 通过实验验证，在OpenLane-V2数据集上显著超越现有方法，证明SD地图先验的有效性。  
◆ 整体框架兼顾感知与推理，为无高精地图自动驾驶系统提供更鲁棒的在线环境理解方案。</td></tr>
<tr><td>2025-05-17</td><td>Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation</td><td>[2505.12130](http://arxiv.org/pdf/2505.12130)</td><td>◆ 提出Keypoints as Dynamic Centroid (KDC)方法，通过动态质心表示统一解决人体姿态估计和实例分割任务，克服传统方法在关节重叠或快速运动时的局限性。  
◆ 采用自底向上范式生成关键点热图，并引入KeyCentroids（基于关键点磁盘）提升关键点检测精度和置信度得分。  
◆ 利用高置信度关键点作为嵌入空间中的动态质心（MaskCentroids），实现快速运动下像素到人体实例的高效聚类。  
◆ 在CrowdPose、OCHuman和COCO等基准测试中验证了KDC的优越性，尤其在复杂场景下的准确性和实时性能表现突出。  
◆ 通过动态质心机制有效处理实例级分割中的遮挡和姿态快速变化问题，增强了模型的泛化能力。</td></tr>
<tr><td>2025-05-16</td><td>Deepfake Forensic Analysis: Source Dataset Attribution and Legal Implications of Synthetic Media Manipulation</td><td>[2505.11110](http://arxiv.org/pdf/2505.11110)</td><td>◆提出了一种新型的GAN生成图像溯源框架，通过可解释特征分析准确识别训练数据集（如CelebA或FFHQ）。  
◆创新性地融合频域变换（傅里叶/DCT）、色彩分布度量和局部特征描述符（SIFT），提取合成图像中的 discriminative 统计特征。  
◆监督分类器（随机森林、SVM、XGBoost）在二元分类（真实vs合成）和多类数据集溯源任务中达到98-99%准确率，覆盖多种主流GAN架构（如StyleGAN系列）。  
◆实验证明频域特征（DCT/FFT）对捕捉数据集特异性伪影（如上采样模式、频谱异常）具有显著优势，色彩直方图则能揭示GAN训练的隐式正则化策略。  
◆首次系统探讨了合成媒体数据集溯源的法律应用场景，包括版权侵权、隐私数据滥用（如GDPR合规）及加州AB 602法案等监管应对方案。  
◆该框架为生成模型的问责制治理提供了技术支撑，可应用于数字取证、内容审核和知识产权诉讼等实际领域。</td></tr>
<tr><td>2025-06-19</td><td>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</td><td>[2505.08013](http://arxiv.org/pdf/2505.08013)</td><td>◆ 提出RDD（Robust Deformable Detector），一种基于可变形Transformer的新型关键点检测与描述方法，通过可变形自注意力机制捕获全局上下文和几何不变性。  
◆ 利用可变形注意力机制聚焦关键位置，显著降低搜索空间复杂度并有效建模几何变换，解决了传统方法难以学习长程视觉关系的问题。  
◆ 结合标准MegaDepth数据集与自建的Air-to-Ground（空对地）数据集进行训练，增强模型在跨视角和跨尺度场景下的鲁棒性。  
◆ 在稀疏匹配任务中性能超越现有最优方法，并具备半稠密匹配能力，扩展了应用场景。  
◆ 引入两个新基准测试：一个针对大视角与尺度变化，另一个为空对地场景，填补了跨高度3D重建评估的空白。</td></tr>
<tr><td>2025-05-12</td><td>Enabling Privacy-Aware AI-Based Ergonomic Analysis</td><td>[2505.07306](http://arxiv.org/pdf/2505.07306)</td><td>◆ 提出了一种隐私感知的AI工效学分析框架，通过对抗训练开发轻量级神经网络，在视频数据中模糊隐私信息，仅保留人体姿态估计所需关键特征。  
◆ 采用数据混淆技术确保与标准姿态估计算法兼容，在保护隐私的同时维持高精度分析能力，解决了传统摄像头系统的隐私泄露问题。  
◆ 创新性地将混淆后的数据传输至中央服务器处理，结合多视角融合技术重建3D关键点，实现远程高精度工效学评估。  
◆ 整合REBA（快速全身评估）方法对3D姿态进行工效学风险量化，形成从数据采集到风险评估的完整闭环系统。  
◆ 在工业场景中首次实现隐私保护与工效学监测的平衡，为制造业提供兼顾安全性与合规性的解决方案。  
◆ 系统设计轻量化且可扩展，适用于资源受限的工业环境，具有实际部署的可行性优势。</td></tr>
<tr><td>2025-05-09</td><td>My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing</td><td>[2505.06436](http://arxiv.org/pdf/2505.06436)</td><td>◆ 提出了一种结合面部关键点检测模型的新损失函数（HFLD损失），用于解决StyleGAN/2潜在空间编辑中的表情纠缠问题。  
◆ 通过在现有模型损失函数中增加HFLD损失，有效限制了编辑过程中对面部表情的干扰，实验显示情绪变化减少高达49%。  
◆ 首次将面部关键点检测技术与GAN潜在空间编辑结合，定量和定性验证了该方法在保持表情一致性上的优越性。  
◆ 相比现有方法，显著提升了生成图像在固定表情下变换外貌特征的能力，为手势和表情研究提供了可靠的数据增强手段。  
◆ 通过对比实验证明，该方法在保持面部表情的同时编辑其他属性（如性别、年龄）的效果优于当前最先进模型。  
◆ 为面部生成任务提供了一种可解释的技术路径，通过关键点约束直接解决特征解耦问题，而非依赖隐式学习。</td></tr>
<tr><td>2025-05-05</td><td>Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration</td><td>[2505.02787](http://arxiv.org/pdf/2505.02787)</td><td>◆提出首个不依赖关键点检测的无监督描述符学习方法，突破视网膜图像配准领域对标注数据的依赖。  
◆创新性地实现描述符网络与关键点检测器的解耦，使模型能适配任意检测器，提升临床应用灵活性。  
◆在标准视网膜配准数据集上进行了全面验证，证明无监督方法性能媲美有监督方法。  
◆设计并测试了多种新型关键点检测器，验证了方法对不同检测器的强鲁棒性。  
◆为医学领域无监督学习应用提供了重要范例，解决了医学图像标注稀缺的核心痛点。  
◆通过端到端无监督训练框架，显著降低了视网膜图像配准的技术门槛和实现成本。</td></tr>
<tr><td>2025-05-05</td><td>Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance</td><td>[2505.02779](http://arxiv.org/pdf/2505.02779)</td><td>◆提出首个完全无监督的视网膜图像配准流程，无需任何标注数据，解决了医学领域标注稀缺的难题。  
◆创新性地颠覆传统思路，通过描述子性能反推关键点检测（描述子驱动检测器），而非传统的关键点驱动描述子学习。  
◆开发了无需关键点检测或标签的描述子学习方法，可直接为视网膜图像任意位置生成高质量描述符。  
◆设计了新型无标签关键点检测网络，能够直接从输入图像预测描述子匹配性能来定位关键点。  
◆在四个独立数据集上验证表明，无监督描述子超越有监督SOTA方法，无监督检测器显著优于现有无监督检测方法。  
◆整个配准流程性能媲美主流有监督方法，且无需标注数据的特性使其可直接迁移到其他领域和模态。</td></tr>
<tr><td>**2025-05-04**</td><td>**Focus What Matters: Matchability-Based Reweighting for Local Feature Matching**</td><td>[2505.02161](http://arxiv.org/abs/2505.02161)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-04**</td><td>**Enhancing Lidar Point Cloud Sampling via Colorization and Super-Resolution of Lidar Imagery**</td><td>[2505.02049](http://arxiv.org/abs/2505.02049)</td><td>摘要生成中...</td></tr>
<tr><td>2025-10-01</td><td>SuperEvent: Cross-Modal Learning of Event-based Keypoint Detection for SLAM</td><td>[2504.00139](http://arxiv.org/pdf/2504.00139)</td><td>◆ Event-based keypoint detection and matching holds significant potential, enabling the integration of event sensors into highly optimized Visual SLAM systems developed for frame cameras over decades of research.
◆ Unfortunately, existing approaches struggle with the motion-dependent appearance of keypoints and the complex noise prevalent in event streams, resulting in severely limited feature matching capabilities and poor performance on downstream tasks.
◆ To mitigate this problem, we propose SuperEvent, a data-driven approach to predict stable keypoints with expressive descriptors.</td></tr>
</tbody>
</table>
</div>

<div align='right'><a href='#top'>↑ 返回顶部</a></div>

<h2 id='image-matching'>Image Matching</h2>

<div class="table-container">
<table>
<thead><tr><th>日期</th><th>标题</th><th>论文与代码</th><th>摘要</th></tr></thead>
<tbody>
<tr><td>2026-02-09</td><td>Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features</td><td>[2602.08430](http://arxiv.org/pdf/2602.08430)</td><td>该论文的核心贡献在于对基于注意力的稀疏图像匹配模型进行了深入分析与优化，并提出了一个通用的匹配模型。其创新点可总结如下：

◆ 首次指出了一个先前被忽视但对LightGlue模型性能有重大影响的关键设计选择，为模型优化提供了新方向。

◆ 系统研究了在基于Transformer的匹配框架中检测器与描述子的作用，发现性能差异的主要根源通常在于检测器而非描述子，这一发现挑战了传统认知。

◆ 提出了一种新颖的微调方法，能够利用来自多种不同检测器的关键点数据来训练现有图像匹配模型。

◆ 最终训练出了一个通用的、与检测器无关的匹配模型。该模型在作为新检测器的零样本匹配器时，其精度能够达到甚至超过为特定特征专门训练的模型，显著提升了部署的灵活性和性能。</td></tr>
<tr><td>2026-01-31</td><td>Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency</td><td>[2602.07016](http://arxiv.org/pdf/2602.07016)</td><td>本文针对多场景无序图像集合的无监督三维重建难题，提出了一种基于高斯约束表示的新方法。其核心贡献与创新点可总结如下：

◆ 首次将LeJEPA架构的思想引入无监督场景发现与相机姿态估计任务，利用其联合嵌入预测特性学习图像表示。
◆ 创新性地在学习的图像嵌入上施加各向同性高斯约束，旨在提升表示的规整性与判别力。
◆ 通过三个逐步优化的流程进行实证研究，系统评估了高斯约束对聚类一致性和姿态估计鲁棒性的实际影响，而非侧重理论证明。
◆ 在IMC2025挑战赛的真实复杂数据上验证了方法的有效性，证明该方法相比启发式基线能更好地分离不同场景并提升姿态合理性，尤其在视觉模糊场景中优势明显。
◆ 研究表明，将自监督学习原则与经典运动恢复结构流程相结合，采用理论驱动的表示约束是一条富有前景的技术路径。</td></tr>
<tr><td>2026-02-05</td><td>SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing</td><td>[2602.05480](http://arxiv.org/pdf/2602.05480)</td><td>◆ Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation.
◆ However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models.
◆ To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels.</td></tr>
<tr><td>2026-02-04</td><td>Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition</td><td>[2602.04401](http://arxiv.org/pdf/2602.04401)</td><td>◆ Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall.
◆ Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change.
◆ We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall.</td></tr>
<tr><td>2026-01-19</td><td>A Streamlined Attention-Based Network for Descriptor Extraction</td><td>[2601.13126](http://arxiv.org/pdf/2601.13126)</td><td>◆ We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.
◆ Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector.
◆ We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency.</td></tr>
<tr><td>2026-01-18</td><td>XRefine: Attention-Guided Keypoint Match Refinement</td><td>[2601.12530](http://arxiv.org/pdf/2601.12530)</td><td>◆ Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches.
◆ Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector.
◆ We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints.</td></tr>
<tr><td>2026-02-03</td><td>Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data</td><td>[2601.12090](http://arxiv.org/pdf/2601.12090)</td><td>◆ The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation.
◆ Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary.
◆ We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings.</td></tr>
<tr><td>2026-01-17</td><td>SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM</td><td>[2601.11930](http://arxiv.org/pdf/2601.11930)</td><td>◆ Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM).
◆ However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs.
◆ non-overlapping pairs) fail to capture.</td></tr>
<tr><td>2026-01-14</td><td>CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation</td><td>[2601.09230](http://arxiv.org/pdf/2601.09230)</td><td>◆ Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality.
◆ Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency.
◆ To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies.</td></tr>
<tr><td>2026-01-13</td><td>Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching</td><td>[2601.08798](http://arxiv.org/pdf/2601.08798)</td><td>◆ Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species.
◆ We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys.
◆ We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models.</td></tr>
<tr><td>2026-01-13</td><td>Second-order Gaussian directional derivative representations for image high-resolution corner detection</td><td>[2601.08182](http://arxiv.org/pdf/2601.08182)</td><td>◆ Corner detection is widely used in various computer vision tasks, such as image matching and 3D reconstruction.
◆ Our research indicates that there are theoretical flaws in Zhang et al.&#x27;s use of a simple corner model to obtain a series of corner characteristics, as the grayscale information of two adjacent corners can affect each other.
◆ In order to address the above issues, a second-order Gaussian directional derivative (SOGDD) filter is used in this work to smooth two typical high-resolution angle models (i.e.</td></tr>
<tr><td>2026-01-09</td><td>Stationaere Kurven auf endlichdimensionalen Mannigfaltigkeiten</td><td>[2601.05695](http://arxiv.org/pdf/2601.05695)</td><td>◆ In this work we discuss the notion of stationary curves of the length functional, the so-called (weak) geodesics, on a Riemannian manifold.
◆ The motivation behind this work is to give a detailed description of many key concepts from differential geometry that one needs in order to understand the important notion of a (weak) geodesic.
◆ For this, we mainly focus on finite-dimensional smooth manifolds, so that we can develop an intuitive and geometric understanding of the concepts that we want to discuss.</td></tr>
<tr><td>2026-01-05</td><td>Exact Clique Number Manipulation via Edge Interdiction</td><td>[2601.01869](http://arxiv.org/pdf/2601.01869)</td><td>◆ The Edge Interdiction Clique Problem (EICP) aims to remove at most $k$ edges from a graph so as to minimize the size of the largest clique in the remaining graph.
◆ This problem captures a fundamental question in graph manipulation: which edges are structurally critical for preserving large cliques?
◆ Such a problem is also motivated by practical applications including protein function maintenance and image matching.</td></tr>
<tr><td>2025-12-31</td><td>Quantum Visual Word Sense Disambiguation: Unraveling Ambiguities Through Quantum Inference Model</td><td>[2512.24687](http://arxiv.org/pdf/2512.24687)</td><td>◆ Visual word sense disambiguation focuses on polysemous words, where candidate images can be easily confused.
◆ Traditional methods use classical probability to calculate the likelihood of an image matching each gloss of the target word, summing these to form a posterior probability.
◆ However, due to the challenge of semantic uncertainty, glosses from different sources inevitably carry semantic biases, which can lead to biased disambiguation results.</td></tr>
<tr><td>2025-12-24</td><td>VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs</td><td>[2512.21194](http://arxiv.org/pdf/2512.21194)</td><td>◆ Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning.
◆ Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear.
◆ To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision.</td></tr>
<tr><td>2025-12-20</td><td>Analog Quantum Image Representation with Qubit-Frugal Encoding</td><td>[2512.18451](http://arxiv.org/pdf/2512.18451)</td><td>◆ In this work, we introduce a fundamentally new paradigm for quantum image representation tailored for neutral-atom quantum devices.
◆ The proposed method constructs a qubit-efficient image representation by first applying a cartographic generalization algorithm to a classical edge-extracted input image, yielding a highly optimized sparse-dot based geometric description.
◆ While ensuring the structural integrity of the image, this sparse representation is then embedded into the atomic configuration of Aquila (QuEra Computing Inc.), modeled through the Bloqade simulation software stack.</td></tr>
<tr><td>2025-12-17</td><td>The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs</td><td>[2512.15949](http://arxiv.org/pdf/2512.15949)</td><td>◆ Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized.
◆ In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge.
◆ Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations.</td></tr>
<tr><td>2025-12-11</td><td>Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching</td><td>[2512.10379](http://arxiv.org/pdf/2512.10379)</td><td>◆ Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness.
◆ In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation.
◆ However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques.</td></tr>
<tr><td>2025-12-14</td><td>MotionEdit: Benchmarking and Learning Motion-Centric Image Editing</td><td>[2512.10284](http://arxiv.org/pdf/2512.10284)</td><td>◆ We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility.
◆ Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos.
◆ This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.</td></tr>
<tr><td>2025-12-05</td><td>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</td><td>[2512.05663](http://arxiv.org/pdf/2512.05663)</td><td>◆ Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning.
◆ Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy.
◆ We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities.</td></tr>
<tr><td>2025-12-05</td><td>DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model</td><td>[2512.05613](http://arxiv.org/pdf/2512.05613)</td><td>◆ Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples.
◆ This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time.
◆ To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model&#x27;s parameters through a teacher--student distillation process.</td></tr>
<tr><td>2025-12-05</td><td>An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation</td><td>[2512.05599](http://arxiv.org/pdf/2512.05599)</td><td>◆ Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources.
◆ Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities.
◆ Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets).</td></tr>
<tr><td>2025-12-05</td><td>MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging</td><td>[2512.05571](http://arxiv.org/pdf/2512.05571)</td><td>◆ Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions.
◆ Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions.
◆ Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information.</td></tr>
<tr><td>2025-12-05</td><td>Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images</td><td>[2512.05410](http://arxiv.org/pdf/2512.05410)</td><td>◆ Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame.
◆ However, these algorithms require meticulous parameter tuning.
◆ We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency.</td></tr>
<tr><td>2025-12-05</td><td>On-Orbit Calibration of Danuri/PolCam. I. Geometric Calibration</td><td>[2512.05330](http://arxiv.org/pdf/2512.05330)</td><td>◆ The wide-angle Polarimetric Camera (PolCam) onboard South Korea&#x27;s first lunar orbiter, Danuri, is a pioneering instrument designed to conduct the first global polarimetric and high-phase-angle survey of the Moon.
◆ Precise geometric calibration is critical for this mission, particularly due to PolCam&#x27;s highly oblique viewing geometry, which introduces significant topographic distortion.
◆ We present a comprehensive on-orbit geometric calibration that relies on 160,256 tie points derived from matching features between PolCam images and the well-orthorectified global map of the Kaguya Multiband Imager (MI).</td></tr>
<tr><td>2025-12-04</td><td>Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models</td><td>[2512.05198](http://arxiv.org/pdf/2512.05198)</td><td>◆ Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask.
◆ We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC).
◆ An equivalent latent compositor should be the same as compositing in pixel space.</td></tr>
<tr><td>2025-12-04</td><td>Value Gradient Guidance for Flow Matching Alignment</td><td>[2512.05116](http://arxiv.org/pdf/2512.05116)</td><td>◆ While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation.
◆ In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models.
◆ The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function.</td></tr>
<tr><td>2025-12-04</td><td>Deep infant brain segmentation from multi-contrast MRI</td><td>[2512.05114](http://arxiv.org/pdf/2512.05114)</td><td>◆ Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures.
◆ However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints.
◆ Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts.</td></tr>
<tr><td>2025-12-04</td><td>Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression</td><td>[2512.05081](http://arxiv.org/pdf/2512.05081)</td><td>◆ Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration.
◆ We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation.
◆ To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning.</td></tr>
<tr><td>2025-12-04</td><td>Improving Posterior Inference of Galaxy Properties with Image-Based Conditional Flow Matching</td><td>[2512.05078](http://arxiv.org/pdf/2512.05078)</td><td>◆ Estimating physical properties of galaxies from wide-field surveys remains a central challenge in astrophysics.
◆ While spectroscopy provides precise measurements, it is observationally expensive, and photometry discards morphological information that correlates with mass, star formation history, metallicity, and dust.
◆ We present a conditional flow matching (CFM) framework that leverages pixel-level imaging alongside photometry to improve posterior inference of galaxy properties.</td></tr>
<tr><td>2025-12-04</td><td>Generative Neural Video Compression via Video Diffusion Prior</td><td>[2512.05016](http://arxiv.org/pdf/2512.05016)</td><td>◆ We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec.
◆ Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering.
◆ To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details.</td></tr>
<tr><td>2025-12-04</td><td>Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels</td><td>[2512.04966](http://arxiv.org/pdf/2512.04966)</td><td>◆ Accurate channel state information (CSI) underpins reliable and efficient wireless communication.
◆ However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments.
◆ By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates.</td></tr>
<tr><td>2025-12-04</td><td>LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation</td><td>[2512.04821](http://arxiv.org/pdf/2512.04821)</td><td>◆ Generative models have achieved remarkable progress with the emergence of flow matching (FM).
◆ It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities.
◆ Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation.</td></tr>
<tr><td>2025-12-04</td><td>Unveiling gravitational waves from core-collapse supernovae with MUSE</td><td>[2512.04804](http://arxiv.org/pdf/2512.04804)</td><td>◆ The core collapse of a massive star at the end of its life can give rise to one of the most powerful phenomena in the Universe.
◆ Because of violent mass motions that take place during the explosion, core-collapse supernovae have been considered a potential source of detectable gravitational waveforms for decades.
◆ However, their intrinsic stochasticity makes ineffective the use of modelled techniques such as matched filtering, forcing us to develop model independent technique to unveil their nature.</td></tr>
<tr><td>2025-12-04</td><td>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</td><td>[2512.04677](http://arxiv.org/pdf/2512.04677)</td><td>◆ Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis.
◆ We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model.
◆ Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming.</td></tr>
<tr><td>2025-12-04</td><td>Spectral micro-CT for quantitative analysis of calcification in fibrocartilage</td><td>[2512.04662](http://arxiv.org/pdf/2512.04662)</td><td>◆ This work introduces a quantitative method for assessing calcification in fibrocartilage using spectral micro-computed tomography ($μ$CT).
◆ Tissue samples of hip acetabular labrum from patients with osteoarthritis and femoroacetabular impingement were imaged with a laboratory-based spectral $μ$CT system equipped with a small-pixel photon-counting detector.
◆ The detector operated with two energy thresholds, allowing the simultaneous acquisition of two CT datasets at different X-ray energies.</td></tr>
<tr><td>2025-12-03</td><td>DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction</td><td>[2512.03715](http://arxiv.org/pdf/2512.03715)</td><td>◆ This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images.
◆ The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching.
◆ DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue.</td></tr>
<tr><td>2025-12-03</td><td>Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus</td><td>[2512.03346](http://arxiv.org/pdf/2512.03346)</td><td>◆ The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge.
◆ The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention.
◆ This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved.</td></tr>
<tr><td>2025-12-02</td><td>The Convex Matching Distance in Multiparameter Persistence</td><td>[2512.02944](http://arxiv.org/pdf/2512.02944)</td><td>◆ We introduce the convex matching distance, a novel metric for comparing functions with values in the real plane.
◆ This metric measures the maximal bottleneck distance between the persistence diagrams associated with the convex combinations of the two function components.
◆ Similarly to the traditional matching distance, the convex matching distance aggregates the information provided by two real-valued components.</td></tr>
<tr><td>2025-12-02</td><td>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</td><td>[2512.02920](http://arxiv.org/pdf/2512.02920)</td><td>◆ We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes.
◆ Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings.
◆ In this work, we construct a large multimodal dataset across six U.S.</td></tr>
<tr><td>2025-12-02</td><td>Terahertz Emission from Spintronic Stack Nanodecorated with Drop-Cast Core-Shell Plasmonic Nanoparticles</td><td>[2512.02889](http://arxiv.org/pdf/2512.02889)</td><td>◆ Spintronic emitters promise to revolutionise terahertz (THz) sources by converting ultrafast optical pulses into broadband THz radiation without phase-matching constraints.
◆ Because the conversion relies on spin-current injection across a nanometre-thin magnetic layer, its efficiency is ordinarily limited by weak optical coupling.
◆ Here, we present a demonstration of a drop-casting based approach to introduce ultrafast plasmonic-mediated coupling: a sparse-layer of silica-gold core-shell nanoparticles is deposited directly onto a W/Fe/Pt spintronic trilayer.</td></tr>
<tr><td>2025-12-02</td><td>A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models</td><td>[2512.02833](http://arxiv.org/pdf/2512.02833)</td><td>◆ We investigate input normalization methods for Time-Series Foundation Models (TSFMs).
◆ While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical.
◆ Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity.</td></tr>
<tr><td>2025-12-02</td><td>From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity</td><td>[2512.02826](http://arxiv.org/pdf/2512.02826)</td><td>◆ Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos.
◆ However, their memorization-generalization behavior remains poorly understood.
◆ In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target.</td></tr>
<tr><td>2025-12-02</td><td>Direct observational evidence that higher-luminosity type 1 active galactic nuclei are most commonly triggered by galaxy mergers</td><td>[2512.02805](http://arxiv.org/pdf/2512.02805)</td><td>◆ We examine the connection between galaxy mergers and the triggering of active galactic nuclei (AGNs) using a sample of 614 type 1 AGNs at $z&lt;0.07$, along with a control sample of inactive galaxies matched to the AGNs for comparison.
◆ We used tidal features, detected in deep images from the DESI Legacy Imaging Survey, as direct evidence of recent mergers.
◆ We find that the fraction of type 1 AGN hosts with tidal features ($f_T$) is higher for AGNs with higher luminosities and (to a lesser extent) more massive black holes.</td></tr>
<tr><td>2025-12-02</td><td>Diffusion-Prior Split Gibbs Sampling for Synthetic Aperture Radar Imaging under Incomplete Measurements</td><td>[2512.02768](http://arxiv.org/pdf/2512.02768)</td><td>◆ Synthetic aperture radar (SAR) imaging plays a critical role in all-weather, day-and-night remote sensing, yet reconstruction is often challenged by noise, undersampling, and complex scattering scenarios.
◆ Conventional methods, including matched filtering and sparsity-based compressed sensing, are limited in capturing intricate scene structures and frequently suffer from artifacts, elevated sidelobes, and loss of fine details.
◆ Recent diffusion models have demonstrated superior capability in representing high-order priors; however, existing diffusion-based SAR methods still yield degraded reconstructions due to oversimplified likelihood approximations in guided sampling.</td></tr>
<tr><td>2025-12-02</td><td>Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone</td><td>[2512.02737](http://arxiv.org/pdf/2512.02737)</td><td>◆ Image-based localization in GNSS-denied environments is critical for UAV autonomy.
◆ Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training.
◆ Such data are costly to acquire and often unavailable, limiting their applicability.</td></tr>
<tr><td>2025-12-02</td><td>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</td><td>[2512.02697](http://arxiv.org/pdf/2512.02697)</td><td>◆ Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image.
◆ However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable.
◆ It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image).</td></tr>
<tr><td>2025-12-02</td><td>WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting</td><td>[2512.02359](http://arxiv.org/pdf/2512.02359)</td><td>◆ Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting.
◆ Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations.
◆ Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations.</td></tr>
<tr><td>2025-12-01</td><td>SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception</td><td>[2512.01908](http://arxiv.org/pdf/2512.01908)</td><td>◆ Contact-rich robotic manipulation requires representations that encode local geometry.
◆ Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues.
◆ Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information.</td></tr>
<tr><td>2025-12-01</td><td>Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching</td><td>[2512.01850](http://arxiv.org/pdf/2512.01850)</td><td>◆ Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.
◆ In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.
◆ Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.</td></tr>
<tr><td>2025-12-01</td><td>Envision: Benchmarking Unified Understanding &amp; Generation for Causal World Process Insights</td><td>[2512.01816](http://arxiv.org/pdf/2512.01816)</td><td>◆ Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency.
◆ However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time.
◆ To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation.</td></tr>
<tr><td>2025-12-01</td><td>ViT$^3$: Unlocking Test-Time Training in Vision</td><td>[2512.01643](http://arxiv.org/pdf/2512.01643)</td><td>◆ Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling.
◆ TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time.
◆ This reformulation opens a rich and flexible design space while achieving linear computational complexity.</td></tr>
<tr><td>2025-12-01</td><td>Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager</td><td>[2512.01611](http://arxiv.org/pdf/2512.01611)</td><td>◆ In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction.
◆ This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm.
◆ The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment.</td></tr>
<tr><td>2025-12-01</td><td>Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation</td><td>[2512.01510](http://arxiv.org/pdf/2512.01510)</td><td>◆ We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation.
◆ To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training.
◆ We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM.</td></tr>
<tr><td>2025-12-01</td><td>nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis</td><td>[2512.01273](http://arxiv.org/pdf/2512.01273)</td><td>◆ Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases.
◆ Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation.
◆ As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient.</td></tr>
<tr><td>2025-11-30</td><td>Accelerating Inference of Masked Image Generators via Reinforcement Learning</td><td>[2512.01094](http://arxiv.org/pdf/2512.01094)</td><td>◆ Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images.
◆ However, they need many sampling steps to create high-quality generations, resulting in slow inference speed.
◆ In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps.</td></tr>
<tr><td>2025-11-30</td><td>Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</td><td>[2512.01030](http://arxiv.org/pdf/2512.01030)</td><td>◆ Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures.
◆ While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning.
◆ Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions.</td></tr>
<tr><td>2025-11-30</td><td>LAHNet: Local Attentive Hashing Network for Point Cloud Registration</td><td>[2512.00927](http://arxiv.org/pdf/2512.00927)</td><td>◆ Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features.
◆ However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness.
◆ In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors.</td></tr>
<tr><td>2025-11-28</td><td>Deep Learning for Restoring MPI System Matrices Using Simulated Training Data</td><td>[2511.23251](http://arxiv.org/pdf/2511.23251)</td><td>◆ Magnetic particle imaging reconstructs tracer distributions using a system matrix obtained through time-consuming, noise-prone calibration measurements.
◆ Methods for addressing imperfections in measured system matrices increasingly rely on deep neural networks, yet curated training data remain scarce.
◆ This study evaluates whether physics-based simulated system matrices can be used to train deep learning models for different system matrix restoration tasks, i.e., denoising, accelerated calibration, upsampling, and inpainting, that generalize to measured data.</td></tr>
<tr><td>2025-11-28</td><td>Vision Bridge Transformer at Scale</td><td>[2511.23199](http://arxiv.org/pdf/2511.23199)</td><td>◆ We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation.
◆ Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm.
◆ By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks.</td></tr>
<tr><td>2025-11-28</td><td>REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection</td><td>[2511.23158](http://arxiv.org/pdf/2511.23158)</td><td>◆ With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity.
◆ Consequently, there is an urgent need for efficient and truly explainable image forensic methods.
◆ Recent detection paradigms have shifted towards explainable forensics.</td></tr>
<tr><td>2025-11-28</td><td>On Computational Aspects of Ordered Matching Problems</td><td>[2511.23093](http://arxiv.org/pdf/2511.23093)</td><td>◆ Ordered matchings, defined as graphs with linearly ordered vertices, where each vertex is connected to exactly one edge, play a crucial role in the area of ordered graphs and their homomorphisms.
◆ Therefore, we consider related problems from the complexity point of view and determine their corresponding computational and parameterized complexities.
◆ We show that the subgraph of ordered matchings problem is NP-complete and we prove that the problem of finding ordered homomorphisms between ordered matchings is NP-complete as well, implying NP-completeness of more generic problems.</td></tr>
<tr><td>2025-11-28</td><td>Geodiffussr: Generative Terrain Texturing with Elevation Fidelity</td><td>[2511.23029](http://arxiv.org/pdf/2511.23029)</td><td>◆ Large-scale terrain generation remains a labor-intensive task in computer graphics.
◆ We introduce Geodiffussr, a flow-matching pipeline that synthesizes text-guided texture maps while strictly adhering to a supplied Digital Elevation Map (DEM).
◆ The core mechanism is multi-scale content aggregation (MCA): DEM features from a pretrained encoder are injected into UNet blocks at multiple resolutions to enforce global-to-local elevation consistency.</td></tr>
<tr><td>2025-11-28</td><td>A Trainable Centrality Framework for Modern Data</td><td>[2511.22959](http://arxiv.org/pdf/2511.22959)</td><td>◆ Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data.
◆ We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations.
◆ FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential.</td></tr>
<tr><td>2025-11-28</td><td>One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfe</td><td>[2511.22940](http://arxiv.org/pdf/2511.22940)</td><td>◆ Recent advances in diffusion models have greatly improved pose-driven character animation.
◆ However, existing methods are limited to spatially aligned reference-pose pairs with matched skeletal structures.
◆ Handling reference-pose misalignment remains unsolved.</td></tr>
<tr><td>2025-11-28</td><td>ViGG: Robust RGB-D Point Cloud Registration using Visual-Geometric Mutual Guidance</td><td>[2511.22908](http://arxiv.org/pdf/2511.22908)</td><td>◆ Point cloud registration is a fundamental task in 3D vision.
◆ Most existing methods only use geometric information for registration.
◆ Recently proposed RGB-D registration methods primarily focus on feature fusion or improving feature learning, which limits their ability to exploit image information and hinders their practical applicability.</td></tr>
<tr><td>2025-11-28</td><td>Robust Indexing for Challenging Serial X-ray Diffraction Patterns</td><td>[2511.22875](http://arxiv.org/pdf/2511.22875)</td><td>◆ Serial crystallography experiments routinely produce thousands of diffraction patterns from crystals in random orientations.
◆ To turn this stream of images into a usable dataset, each pattern must be indexed before integration and merging can proceed.
◆ In practice, diffraction patterns may contain only a small number of reliable peaks, be contaminated by background or spuriously detected reflections, or arise from crystals with highly skewed unit cells.</td></tr>
<tr><td>2025-11-28</td><td>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</td><td>[2511.22860](http://arxiv.org/pdf/2511.22860)</td><td>◆ Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise.
◆ We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization.
◆ At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity.</td></tr>
<tr><td>2025-11-26</td><td>Fast 3D Ultrasound Localization Microscopy via Projection-based Processing Framework</td><td>[2511.21647](http://arxiv.org/pdf/2511.21647)</td><td>◆ Three-dimensional ultrasound localization microscopy (ULM) enables comprehensive visualization of the vasculature, thereby improving diagnostic reliability.
◆ Nevertheless, its clinical translation remains challenging, as the exponential growth in voxel count for full 3D reconstruction imposes heavy computational demands and extensive post-processing time.
◆ In this row-column array (RCA)-based 3D in vivo pig kidney ULM study, we reformulate each step of the full 3D ULM pipeline, including beamforming, clutter filtering, motion estimation, microbubble separation and localization into a series of computational-efficient 2D operations, substantially reducing the number of voxels to be processed while maintaining comparable accuracy.</td></tr>
<tr><td>2025-11-26</td><td>MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</td><td>[2511.21592](http://arxiv.org/pdf/2511.21592)</td><td>◆ Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics.
◆ A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion.
◆ We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data.</td></tr>
<tr><td>2025-11-26</td><td>Semantic-Enhanced Feature Matching with Learnable Geometric Verification for Cross-Modal Neuron Registration</td><td>[2511.21452](http://arxiv.org/pdf/2511.21452)</td><td>◆ Accurately registering in-vivo two-photon and ex-vivo fluorescence micro-optical sectioning tomography images of individual neurons is critical for structure-function analysis in neuroscience.
◆ This task is profoundly challenging due to a significant cross-modality appearance gap, the scarcity of annotated data and severe tissue deformations.
◆ We propose a novel deep learning framework to address these issues.</td></tr>
<tr><td>2025-11-26</td><td>TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models</td><td>[2511.21335](http://arxiv.org/pdf/2511.21335)</td><td>◆ Score-based generative models (SGMs) have demonstrated unparalleled sampling quality and diversity in numerous fields, such as image generation, voice synthesis, and tabular data synthesis, etc.
◆ Inspired by those outstanding results, we apply SGMs to synthesize time-series by learning its conditional score function.
◆ To this end, we present a conditional score network for time-series synthesis, deriving a denoising score matching loss tailored for our purposes.</td></tr>
<tr><td>2025-11-26</td><td>Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting</td><td>[2511.21265](http://arxiv.org/pdf/2511.21265)</td><td>◆ Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data.
◆ 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation.
◆ However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling.</td></tr>
<tr><td>2025-11-26</td><td>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</td><td>[2511.21215](http://arxiv.org/pdf/2511.21215)</td><td>◆ We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow.
◆ While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals.
◆ We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98).</td></tr>
<tr><td>2025-11-26</td><td>Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching</td><td>[2511.21203](http://arxiv.org/pdf/2511.21203)</td><td>◆ In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched.
◆ We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control.
◆ This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat.</td></tr>
<tr><td>2025-11-26</td><td>Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval</td><td>[2511.21121](http://arxiv.org/pdf/2511.21121)</td><td>◆ Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction.
◆ These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer.
◆ Vision first retrieval has emerged as a strong alternative.</td></tr>
<tr><td>2025-11-26</td><td>CLRecogEye : Curriculum Learning towards exploiting convolution features for Dynamic Iris Recognition</td><td>[2511.21097](http://arxiv.org/pdf/2511.21097)</td><td>◆ Iris authentication algorithms have achieved impressive recognition performance, making them highly promising for real-world applications such as border control, citizen identification, and both criminal investigations and commercial systems.
◆ However, their robustness is still challenged by variations in rotation, scale, specular reflections, and defocus blur.
◆ In addition, most existing approaches rely on straightforward point-to-point comparisons, typically using cosine or L2 distance, without effectively leveraging the spatio-spatial-temporal structure of iris patterns.</td></tr>
<tr><td>2025-11-26</td><td>Deep Parameter Interpolation for Scalar Conditioning</td><td>[2511.21028](http://arxiv.org/pdf/2511.21028)</td><td>◆ We propose deep parameter interpolation (DPI), a general-purpose method for transforming an existing deep neural network architecture into one that accepts an additional scalar input.
◆ Recent deep generative models, including diffusion models and flow matching, employ a single neural network to learn a time- or noise level-dependent vector field.
◆ Designing a network architecture to accurately represent this vector field is challenging because the network must integrate information from two different sources: a high-dimensional vector (usually an image) and a scalar.</td></tr>
<tr><td>2025-11-25</td><td>LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight</td><td>[2511.20648](http://arxiv.org/pdf/2511.20648)</td><td>◆ To act in the world, a model must name what it sees and know where it is in 3D.
◆ Today&#x27;s vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox.
◆ We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem.</td></tr>
<tr><td>2025-11-25</td><td>Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition</td><td>[2511.20641](http://arxiv.org/pdf/2511.20641)</td><td>◆ Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes.
◆ Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance.
◆ However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity.</td></tr>
<tr><td>2025-11-25</td><td>Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning</td><td>[2511.20549](http://arxiv.org/pdf/2511.20549)</td><td>◆ Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive.
◆ Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation.
◆ Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking.</td></tr>
<tr><td>2025-11-25</td><td>STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow</td><td>[2511.20462](http://arxiv.org/pdf/2511.20462)</td><td>◆ Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation.
◆ Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models.
◆ In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation.</td></tr>
<tr><td>2025-11-25</td><td>Planar Josephson junctions for sensors and electronics:Different geometry, new functionality</td><td>[2511.20424](http://arxiv.org/pdf/2511.20424)</td><td>◆ Josephson junctions are key elements in superconducting electronics.
◆ The most common type is the overlap (sandwich-type) junction, formed by vertically stacking two superconducting layers.
◆ In contrast, planar junctions are fabricated without overlap, at the edge of two superconducting films within a single plane.</td></tr>
<tr><td>2025-11-25</td><td>Restora-Flow: Mask-Guided Image Restoration with Flow Matching</td><td>[2511.20152](http://arxiv.org/pdf/2511.20152)</td><td>◆ Flow matching has emerged as a promising generative approach that addresses the lengthy sampling times associated with state-of-the-art diffusion models and enables a more flexible trajectory design, while maintaining high-quality image generation.
◆ This capability makes it suitable as a generative prior for image restoration tasks.
◆ Although current methods leveraging flow models have shown promising results in restoration, some still suffer from long processing times or produce over-smoothed results.</td></tr>
<tr><td>2025-11-25</td><td>LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening</td><td>[2511.20116](http://arxiv.org/pdf/2511.20116)</td><td>◆ Lung cancer risk estimation is gaining increasing importance as more countries introduce population-wide screening programs using low-dose CT (LDCT).
◆ As imaging volumes grow, scalable methods that can process entire lung volumes efficiently are essential to tap into the full potential of these large screening datasets.
◆ Existing approaches either over-rely on pixel-level annotations, limiting scalability, or analyze the lung in fragments, weakening performance.</td></tr>
<tr><td>2025-11-25</td><td>Unusual Thermally Induced Blueshift and Emission Amplification of Mn2+ ions Enable Filter-Free Luminescent Thermal Imaging</td><td>[2511.19993](http://arxiv.org/pdf/2511.19993)</td><td>◆ The shift from point-based thermal sensing to filter-free thermal imaging requires luminescent thermometers that exhibit pronounced and thermally driven spectral changes within spectral regions matching the sensitivity profiles of the R, G, and B channels of a digital camera.
◆ In this work, we introduce such a system, enabled by the synergistic interplay between (i) thermal redistribution among the vibronic components of the 4T1 excited state of Mn2+ ions and (ii) thermally assisted population of this state via optical trap sites.
◆ These combined processes result in a simultaneous thermal enhancement and blueshift of the Mn2+ emission band associated with the 4T1 -&gt; 6A1 electronic transition.</td></tr>
<tr><td>2025-11-25</td><td>SONIC: Spectral Optimization of Noise for Inpainting with Consistency</td><td>[2511.19985](http://arxiv.org/pdf/2511.19985)</td><td>◆ We propose a novel training-free method for inpainting with off-the-shelf text-to-image models.
◆ While guidance-based methods in theory allow generic models to be used for inverse problems such as inpainting, in practice, their effectiveness is limited, leading to the necessity of specialized inpainting-specific models.
◆ In this work, we argue that the missing ingredient for training-free inpainting is the optimization (guidance) of the initial seed noise.</td></tr>
<tr><td>2025-11-25</td><td>Learning Degenerate Manifolds of Frustrated Magnets with Boltzmann Machines</td><td>[2511.19879](http://arxiv.org/pdf/2511.19879)</td><td>◆ We show that Restricted Boltzmann Machines (RBMs) provide a flexible generative framework for modeling spin configurations in disordered yet strongly correlated phases of frustrated magnets.
◆ As a benchmark, we first demonstrate that an RBM can learn the zero-temperature ground-state manifold of the one-dimensional ANNNI model at its multiphase point, accurately reproducing its characteristic oscillatory and exponentially decaying correlations.
◆ We then apply RBMs to kagome spin ice and show that they successfully learn the local ice rules and short-range correlations of the extensively degenerate ice-I manifold.</td></tr>
<tr><td>2025-11-24</td><td>Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts</td><td>[2511.19434](http://arxiv.org/pdf/2511.19434)</td><td>◆ Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity.
◆ We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory.
◆ Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics.</td></tr>
<tr><td>2025-11-24</td><td>Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware</td><td>[2511.19379](http://arxiv.org/pdf/2511.19379)</td><td>◆ Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps.
◆ This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware.
◆ By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency.</td></tr>
<tr><td>2025-11-24</td><td>DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation</td><td>[2511.19365](http://arxiv.org/pdf/2511.19365)</td><td>◆ Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion.
◆ This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity.
◆ Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT).</td></tr>
<tr><td>2025-11-24</td><td>BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment</td><td>[2511.19268](http://arxiv.org/pdf/2511.19268)</td><td>◆ Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources.
◆ These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text.
◆ Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide.</td></tr>
<tr><td>2025-11-24</td><td>DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation</td><td>[2511.19071](http://arxiv.org/pdf/2511.19071)</td><td>◆ The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation.
◆ Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation.
◆ However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance.</td></tr>
<tr><td>2025-11-24</td><td>Machine Learning Based Identification of Solar Disk and Plages in Kodaikanal Solar Observatory Historical Suncharts</td><td>[2511.19040](http://arxiv.org/pdf/2511.19040)</td><td>◆ Kodaikanal Solar Observatory (KoSO) is one of the oldest solar observatories, possessing an archive of multi-wavelength solar observations, including white light, Ca II K, and H-alpha images spanning over a century.
◆ In addition to these observations, KoSO has preserved hand-drawn suncharts (1904-2022), on which various solar features such as sunspots, plages, filaments, and prominences are marked on the Stonyhurst grid with distinct colour coding.
◆ In this study, we present the first comprehensive result that includes the entire data set from these suncharts using a supervised Machine Learning model called &quot;Convolutional Neural Networks (CNNs)&quot;, firstly to identify the solar disks from the charts (1909-2007), secondly to identify the plages, spanning 9 solar cycles (1916-2007).</td></tr>
<tr><td>2025-11-24</td><td>VeCoR - Velocity Contrastive Regularization for Flow Matching</td><td>[2511.18942](http://arxiv.org/pdf/2511.18942)</td><td>◆ Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models.
◆ Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.
◆ To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both &quot;where to go&quot; and &quot;where not to go.&quot; To be formal, we propose \textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision.</td></tr>
<tr><td>2025-11-24</td><td>FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories</td><td>[2511.18834](http://arxiv.org/pdf/2511.18834)</td><td>◆ With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application.
◆ Among flow models&#x27; accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching.
◆ This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation.</td></tr>
<tr><td>2025-11-24</td><td>VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction</td><td>[2511.18831](http://arxiv.org/pdf/2511.18831)</td><td>◆ The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets.
◆ While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics.
◆ In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy.</td></tr>
<tr><td>2025-11-24</td><td>NI-Tex: Non-isometric Image-based Garment Texture Generation</td><td>[2511.18765](http://arxiv.org/pdf/2511.18765)</td><td>◆ Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited.
◆ To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes.
◆ However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility.</td></tr>
<tr><td>2025-12-26</td><td>SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration</td><td>[2511.17750](http://arxiv.org/pdf/2511.17750)</td><td>◆ Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses.
◆ However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint.
◆ Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry.</td></tr>
<tr><td>2025-11-21</td><td>SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation</td><td>[2511.17432](http://arxiv.org/pdf/2511.17432)</td><td>◆ Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment.
◆ While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important.
◆ Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations.</td></tr>
<tr><td>2025-11-21</td><td>SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding</td><td>[2511.17411](http://arxiv.org/pdf/2511.17411)</td><td>◆ Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control.
◆ Yet their ability to generalize across new environments, tasks, and embodiments remains limited.
◆ We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs).</td></tr>
<tr><td>2025-11-21</td><td>ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP</td><td>[2511.17362](http://arxiv.org/pdf/2511.17362)</td><td>◆ Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images.
◆ As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness.
◆ In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC).</td></tr>
<tr><td>2025-11-21</td><td>NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior</td><td>[2511.17322](http://arxiv.org/pdf/2511.17322)</td><td>◆ In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.
◆ Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios.
◆ To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation.</td></tr>
<tr><td>2025-11-21</td><td>Angular clustering and bias of photometric quasars in the Kilo-Degree Survey Data Release 4</td><td>[2511.17311](http://arxiv.org/pdf/2511.17311)</td><td>◆ We investigate the angular clustering and effective bias of photometrically selected quasars in the Kilo-Degree Survey Data Release 4 (KiDS DR4).
◆ We update the previous photometric redshifts (photo-$z$s) of the KiDS quasars using Hybrid-z, a deep learning framework combining four-band KiDS images and nine-band KiDS+VIKING magnitudes.
◆ Hybrid-z is trained on the latest Dark Energy Spectroscopic Instrument (DESI) DR1 and Sloan Digital Sky Survey (SDSS) DR17 quasars matching with KiDS, and achieves average bias $\langle δz \rangle &lt; 0.01$ and scatter $\sim 0.04(1 + z)$ on a test sample.</td></tr>
<tr><td>2025-11-21</td><td>MuM: Multi-View Masked Image Modeling for 3D Vision</td><td>[2511.17309](http://arxiv.org/pdf/2511.17309)</td><td>◆ Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data.
◆ When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption.
◆ However, most prior efforts are optimized for semantic understanding rather than geometric reasoning.</td></tr>
<tr><td>2025-11-21</td><td>UI-Styler: Ultrasound Image Style Transfer with Class-Aware Prompts for Cross-Device Diagnosis Using a Frozen Black-Box Inference Network</td><td>[2511.17155](http://arxiv.org/pdf/2511.17155)</td><td>◆ The appearance of ultrasound images varies across acquisition devices, causing domain shifts that degrade the performance of fixed black-box downstream inference models when reused.
◆ To mitigate this issue, it is practical to develop unpaired image translation (UIT) methods that effectively align the statistical distributions between source and target domains, particularly under the constraint of a reused inference-blackbox setting.
◆ However, existing UIT approaches often overlook class-specific semantic alignment during domain adaptation, resulting in misaligned content-class mappings that can impair diagnostic accuracy.</td></tr>
<tr><td>2025-11-21</td><td>Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation</td><td>[2511.17111](http://arxiv.org/pdf/2511.17111)</td><td>◆ Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs.
◆ Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped.
◆ To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes.</td></tr>
<tr><td>2025-11-21</td><td>Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models</td><td>[2511.16955](http://arxiv.org/pdf/2511.16955)</td><td>◆ Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences.
◆ However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm.
◆ Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity.</td></tr>
<tr><td>2025-11-20</td><td>Stable diffusion models reveal a persisting human and AI gap in visual creativity</td><td>[2511.16814](http://arxiv.org/pdf/2511.16814)</td><td>◆ While recent research suggests Large Language Models match human creative performance in divergent thinking tasks, visual creativity remains underexplored.
◆ This study compared image generation in human participants (Visual Artists and Non Artists) and using an image generation AI model (two prompting conditions with varying human input: high for Human Inspired, low for Self Guided).
◆ Human raters (N=255) and GPT4o evaluated the creativity of the resulting images.</td></tr>
<tr><td>2025-11-20</td><td>Dataset Distillation for Pre-Trained Self-Supervised Vision Models</td><td>[2511.16674](http://arxiv.org/pdf/2511.16674)</td><td>◆ The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples.
◆ Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models.
◆ In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch.</td></tr>
<tr><td>2025-11-20</td><td>NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening</td><td>[2511.16566](http://arxiv.org/pdf/2511.16566)</td><td>◆ Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention.
◆ In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children&#x27;s images, simultaneously addressing generalizability and class imbalance.
◆ In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings.</td></tr>
<tr><td>2025-11-20</td><td>Saving Foundation Flow-Matching Priors for Inverse Problems</td><td>[2511.16520](http://arxiv.org/pdf/2511.16520)</td><td>◆ Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors.
◆ How can we unlock their potential?
◆ We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs.</td></tr>
<tr><td>2025-11-20</td><td>Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation</td><td>[2511.16435](http://arxiv.org/pdf/2511.16435)</td><td>◆ Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm.
◆ Existing methods mainly mine references from support images as meta guidance.
◆ However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes.</td></tr>
<tr><td>2025-11-20</td><td>DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration</td><td>[2511.16364](http://arxiv.org/pdf/2511.16364)</td><td>◆ Offline signature verification (OSV) is a frequently utilized technology in forensics.
◆ This paper proposes a new model, DetailSemNet, for OSV.
◆ Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV.</td></tr>
<tr><td>2025-11-20</td><td>CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering</td><td>[2511.16349](http://arxiv.org/pdf/2511.16349)</td><td>◆ Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content.
◆ Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure.
◆ This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud.</td></tr>
<tr><td>2025-11-20</td><td>A pilot VLBI study of the SQUAB quasar sample featuring multiple Gaia detections</td><td>[2511.16206](http://arxiv.org/pdf/2511.16206)</td><td>◆ Our previous work identified a class of SDSS quasars exhibiting multiple Gaia detections, classifying them as candidates for various astrophysical systems such as quasar-star pairs, dual quasars, and gravitationally lensed quasars.
◆ In this paper, we present a pilot VLBI study targeting a radio-bright subsample and report the first high-resolution imaging results.
◆ By leveraging the milliarcsecond-scale resolution of VLBI and its precise astrometric coordination incorporating with Gaia, we aim to refine the classification of these multiple matched sources, search for potential dual AGNs, and assess the efficacy of the combined Gaia-VLBI approach in resolving ambiguous quasar systems.</td></tr>
<tr><td>2025-11-20</td><td>PySERA: Open-Source Standardized Python Library for Automated, Scalable, and Reproducible Handcrafted and Deep Radiomics</td><td>[2511.15963](http://arxiv.org/pdf/2511.15963)</td><td>◆ Radiomics enables the extraction of quantitative biomarkers from medical images for precision modeling, but reproducibility and scalability remain limited due to heterogeneous software implementations and incomplete adherence to standards.
◆ Existing tools also lack unified support for deep learning based radiomics.
◆ To address these limitations, we introduce PySERA, an open source, Python native, standardized radiomics framework designed for automation, reproducibility, and seamless AI integration.</td></tr>
<tr><td>2025-11-19</td><td>The MeerKAT Fornax Survey VI. The collapse of the galaxy HI Mass Function in Fornax</td><td>[2511.15795](http://arxiv.org/pdf/2511.15795)</td><td>◆ We present the deepest HI mass Function (HIMF) ever measured, outside the Local Group.
◆ The observations are part of the MeerKAT Fornax Survey and cover a 4 x 4 deg^2 field, corresponding to ~ Rvir.
◆ The 3$σ$ detection limit is log(MHI/Msun) = 5.7 for a 50 km/s-wide point source.</td></tr>
<tr><td>2025-11-20</td><td>RoMa v2: Harder Better Faster Denser Feature Matching</td><td>[2511.15706](http://arxiv.org/pdf/2511.15706)</td><td>◆ Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness.
◆ However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability.
◆ In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model.</td></tr>
<tr><td>2025-11-19</td><td>What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs</td><td>[2511.15316](http://arxiv.org/pdf/2511.15316)</td><td>◆ Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA).
◆ Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage.
◆ To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features.</td></tr>
<tr><td>2025-11-19</td><td>Magnetic signal scan imaging system based on giant magnetoimpedance (GMI) differential sensor</td><td>[2511.15209](http://arxiv.org/pdf/2511.15209)</td><td>◆ This paper presents the design and implementation of a magnetic signal scanning and imaging system based on the giant magnetoimpedance (GMI) effect.
◆ The system employs a pair of performance-matched GMI sensing elements configured as a differential probe structure.
◆ Through co-optimized low-noise electronic and probe design, the system effectively suppresses both intrinsic sensor common-mode drift and external environmental magnetic noise, enabling high signal-to-noise ratio detection of nono-tesla to micro-tesla-level magnetic signals without magnetic shielding.</td></tr>
<tr><td>2025-11-19</td><td>BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching</td><td>[2511.15066](http://arxiv.org/pdf/2511.15066)</td><td>◆ Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest.
◆ Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge.
◆ Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency.</td></tr>
<tr><td>2025-11-19</td><td>Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans</td><td>[2511.15029](http://arxiv.org/pdf/2511.15029)</td><td>◆ Mathematical thinking is a fundamental aspect of human cognition.
◆ Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan.
◆ Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults.</td></tr>
<tr><td>2025-11-18</td><td>LFreeDA: Label-Free Drift Adaptation for Windows Malware Detection</td><td>[2511.14963](http://arxiv.org/pdf/2511.14963)</td><td>◆ Machine learning (ML)-based malware detectors degrade over time as concept drift introduces new and evolving families unseen during training.
◆ Retraining is limited by the cost and time of manual labeling or sandbox analysis.
◆ Existing approaches mitigate this via drift detection and selective labeling, but fully label-free adaptation remains largely unexplored.</td></tr>
<tr><td>2025-11-18</td><td>FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis</td><td>[2511.14419](http://arxiv.org/pdf/2511.14419)</td><td>◆ Autonomous migration is essential for the function of immune cells such as neutrophils and plays a pivotal role in diverse diseases.
◆ Recently, we introduced ComplexEye, a multi-lens array microscope comprising 16 independent aberration-corrected glass lenses arranged at the pitch of a 96-well plate, capable of capturing high-resolution movies of migrating cells.
◆ This architecture enables high-throughput live-cell video microscopy for migration analysis, supporting routine quantification of autonomous motility with strong potential for clinical translation.</td></tr>
<tr><td>2025-11-18</td><td>Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays</td><td>[2511.14411](http://arxiv.org/pdf/2511.14411)</td><td>◆ In forensic craniofacial identification and in many biomedical applications, craniometric landmarks are important.
◆ Traditional methods for locating landmarks are time-consuming and require specialized knowledge and expertise.
◆ Current methods utilize superimposition and deep learning-based methods that employ automatic annotation of landmarks.</td></tr>
<tr><td>2025-11-18</td><td>Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs</td><td>[2511.14315](http://arxiv.org/pdf/2511.14315)</td><td>◆ Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery.
◆ While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging.
◆ The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation.</td></tr>
<tr><td>2025-11-18</td><td>NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration</td><td>[2511.14286](http://arxiv.org/pdf/2511.14286)</td><td>◆ In computer- and robot-assisted orthopedic surgery (CAOS), patient-specific surgical plans derived from preoperative imaging define target locations and implant trajectories.
◆ During surgery, these plans must be accurately transferred, relying on precise cross-registration between preoperative and intraoperative data.
◆ However, substantial modality heterogeneity across imaging modalities makes this registration challenging and error-prone.</td></tr>
<tr><td>2025-11-18</td><td>EBind: a practical approach to space binding</td><td>[2511.14229](http://arxiv.org/pdf/2511.14229)</td><td>◆ We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days.
◆ We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models.
◆ We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size.</td></tr>
<tr><td>2025-11-18</td><td>InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior</td><td>[2511.14208](http://arxiv.org/pdf/2511.14208)</td><td>◆ Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints.
◆ Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use.
◆ We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior.</td></tr>
<tr><td>2025-11-18</td><td>iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion</td><td>[2511.14149](http://arxiv.org/pdf/2511.14149)</td><td>◆ Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model.
◆ However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose.
◆ This multi-round process incurs significant computational overhead, hindering real-time performance in robotics.</td></tr>
<tr><td>2025-11-18</td><td>$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors</td><td>[2511.14109](http://arxiv.org/pdf/2511.14109)</td><td>◆ Visual Place Recognition (VPR) aims to match query images against a database using visual cues.
◆ State-of-the-art methods aggregate features from deep backbones to form global descriptors.
◆ Optimal transport-based aggregation methods reformulate feature-to-cluster assignment as a transport problem, but the standard Sinkhorn algorithm symmetrically treats source and target marginals, limiting effectiveness when image features and cluster centers exhibit substantially different distributions.</td></tr>
<tr><td>2025-11-18</td><td>SMGeo: Cross-View Object Geo-Localization with Grid-Level Mixture-of-Experts</td><td>[2511.14093](http://arxiv.org/pdf/2511.14093)</td><td>◆ Cross-view object Geo-localization aims to precisely pinpoint the same object across large-scale satellite imagery based on drone images.
◆ Due to significant differences in viewpoint and scale, coupled with complex background interference, traditional multi-stage &quot;retrieval-matching&quot; pipelines are prone to cumulative errors.
◆ To address this, we present SMGeo, a promptable end-to-end transformer-based model for object Geo-localization.</td></tr>
<tr><td>2025-11-18</td><td>The CHASM-SWPC Dataset for Coronal Hole Detection &amp; Analysis</td><td>[2511.14044](http://arxiv.org/pdf/2511.14044)</td><td>◆ Coronal holes (CHs) are low-activity, low-density solar coronal regions with open magnetic field lines (Cranmer 2009).
◆ In the extreme ultraviolet (EUV) spectrum, CHs appear as dark patches.
◆ Using daily hand-drawn maps from the Space Weather Prediction Center (SWPC), we developed a semi-automated pipeline to digitize the SWPC maps into binary segmentation masks.</td></tr>
<tr><td>2025-11-17</td><td>Language-Guided Invariance Probing of Vision-Language Models</td><td>[2511.13494](http://arxiv.org/pdf/2511.13494)</td><td>◆ Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations.
◆ We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching.
◆ Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.</td></tr>
<tr><td>2025-11-17</td><td>PDRs4All XX. Haute Couture: Spectral stitching of JWST MIRI-IFU cubes with matrix completion</td><td>[2511.13377](http://arxiv.org/pdf/2511.13377)</td><td>◆ MIRI is the imager and spectrograph covering wavelengths from $4.9$ to $27.9$ $μ$m onboard the James Webb Space Telescope (JWST).
◆ The Medium-Resolution Spectrometer (MRS) consists of four integral field units (IFU), each of which has three sub-channels.
◆ The twelve resulting spectral data cubes have different fields of view, spatial, and spectral resolutions.</td></tr>
<tr><td>2025-11-17</td><td>Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images</td><td>[2511.13353](http://arxiv.org/pdf/2511.13353)</td><td>◆ Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases.
◆ However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture.
◆ This gap is mainly due to the high cost of detailed annotations.</td></tr>
<tr><td>2025-11-17</td><td>Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention</td><td>[2511.13249](http://arxiv.org/pdf/2511.13249)</td><td>◆ Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions.
◆ Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results.
◆ We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features.</td></tr>
<tr><td>2025-11-17</td><td>GenTract: Generative Global Tractography</td><td>[2511.13183](http://arxiv.org/pdf/2511.13183)</td><td>◆ Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI).
◆ Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data.
◆ In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive.</td></tr>
<tr><td>2025-11-17</td><td>THIR: Topological Histopathological Image Retrieval</td><td>[2511.13170](http://arxiv.org/pdf/2511.13170)</td><td>◆ According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020.
◆ Early diagnosis and accurate clinical decision making are critical in reducing this global burden.
◆ In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns.</td></tr>
<tr><td>2025-11-17</td><td>SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration</td><td>[2511.13168](http://arxiv.org/pdf/2511.13168)</td><td>◆ Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics.
◆ Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory.
◆ Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences.</td></tr>
<tr><td>2025-11-17</td><td>CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation</td><td>[2511.13102](http://arxiv.org/pdf/2511.13102)</td><td>◆ Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks.
◆ While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept &quot;leg&quot; exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat).
◆ To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images.</td></tr>
<tr><td>2025-11-17</td><td>SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models</td><td>[2511.12972](http://arxiv.org/pdf/2511.12972)</td><td>◆ The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target.
◆ This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions.
◆ In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions.</td></tr>
<tr><td>2025-11-17</td><td>Functional Mean Flow in Hilbert Space</td><td>[2511.12898](http://arxiv.org/pdf/2511.12898)</td><td>◆ We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space.
◆ FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling.
◆ We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form.</td></tr>
<tr><td>2025-11-14</td><td>Bridging Hidden States in Vision-Language Models</td><td>[2511.11526](http://arxiv.org/pdf/2511.11526)</td><td>◆ Vision-Language Models (VLMs) are a new family of models that align image content with natural language.
◆ Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings.
◆ Many methods also tie fusion to an autoregressive decoder.</td></tr>
<tr><td>2025-11-14</td><td>CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation</td><td>[2511.11522](http://arxiv.org/pdf/2511.11522)</td><td>◆ Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms.
◆ However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences.
◆ This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move.</td></tr>
<tr><td>2025-11-14</td><td>Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping</td><td>[2511.11437](http://arxiv.org/pdf/2511.11437)</td><td>◆ Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex.
◆ This overlooks the brain&#x27;s hierarchical processing and blurs the roles of early, middle, and late visual areas.
◆ We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit.</td></tr>
<tr><td>2025-11-14</td><td>The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models</td><td>[2511.11435](http://arxiv.org/pdf/2511.11435)</td><td>◆ Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity.
◆ This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene.
◆ While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them.</td></tr>
<tr><td>2025-11-14</td><td>Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment</td><td>[2511.11422](http://arxiv.org/pdf/2511.11422)</td><td>◆ Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach.
◆ We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG&#x27;s inherent noise and signal degradation, vs.
◆ vision&#x27;s high-fidelity features) and a Semantic Gap (arising from EEG&#x27;s shallow conceptual representation, vs.</td></tr>
<tr><td>2025-11-14</td><td>Robust inverse material design with physical guarantees using the Voigt-Reuss Net</td><td>[2511.11388](http://arxiv.org/pdf/2511.11388)</td><td>◆ We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees.
◆ Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense.
◆ In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $&gt;\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs.</td></tr>
<tr><td>2025-11-14</td><td>Arcee: Differentiable Recurrent State Chain for Generative Vision Modeling with Mamba SSMs</td><td>[2511.11243](http://arxiv.org/pdf/2511.11243)</td><td>◆ State-space models (SSMs), Mamba in particular, are increasingly adopted for long-context sequence modeling, providing linear-time aggregation via an input-dependent, causal selective-scan operation.
◆ Along this line, recent &quot;Mamba-for-vision&quot; variants largely explore multiple scan orders to relax strict causality for non-sequential signals (e.g., images).
◆ Rather than preserving cross-block memory, the conventional formulation of the selective-scan operation in Mamba reinitializes each block&#x27;s state-space dynamics from zero, discarding the terminal state-space representation (SSR) from the previous block.</td></tr>
<tr><td>2025-11-14</td><td>Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing</td><td>[2511.11236](http://arxiv.org/pdf/2511.11236)</td><td>◆ In recent years, image editing has garnered growing attention.
◆ However, general image editing models often fail to produce satisfactory results when confronted with new styles.
◆ The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data.</td></tr>
<tr><td>2025-11-14</td><td>Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image</td><td>[2511.11074](http://arxiv.org/pdf/2511.11074)</td><td>◆ While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached.
◆ Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated.
◆ In this work, we compare two of the most promising generative models--Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers--which we adapt for the tasks of generative shape modeling and completion.</td></tr>
<tr><td>2025-11-14</td><td>Hyperbolic Hierarchical Alignment Reasoning Network for Text-3D Retrieval</td><td>[2511.11045](http://arxiv.org/pdf/2511.11045)</td><td>◆ With the daily influx of 3D data on the internet, text-3D retrieval has gained increasing attention.
◆ However, current methods face two major challenges: Hierarchy Representation Collapse (HRC) and Redundancy-Induced Saliency Dilution (RISD).
◆ HRC compresses abstract-to-specific and whole-to-part hierarchies in Euclidean embeddings, while RISD averages noisy fragments, obscuring critical semantic cues and diminishing the model&#x27;s ability to distinguish hard negatives.</td></tr>
<tr><td>2025-11-13</td><td>One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models</td><td>[2511.10629](http://arxiv.org/pdf/2511.10629)</td><td>◆ Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding.
◆ We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator&#x27;s latent code before the final VAE decoding step.
◆ LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space.</td></tr>
<tr><td>2025-11-13</td><td>From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis</td><td>[2511.10597](http://arxiv.org/pdf/2511.10597)</td><td>◆ Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT.
◆ To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information.
◆ Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data.</td></tr>
<tr><td>2025-11-13</td><td>Revealing the Connection Between the Filamentary Hierarchy and Star Cluster Formation in a Simulated NGC 628 Galaxy</td><td>[2511.10486](http://arxiv.org/pdf/2511.10486)</td><td>◆ There is abundant observational evidence for the hierarchical, interconnected nature of filaments in the interstellar medium (ISM) extending from galactic down to sub-parsec scales.
◆ New JWST images of NGC 628 in particular, show clusters forming along the two spiral arms of this galaxy.
◆ In this paper we investigate filament and cluster properties in an NGC 628-like multi-scale high-resolution magnetohydrodynamic simulation.</td></tr>
<tr><td>2025-11-13</td><td>Domain Adaptation for Camera-Specific Image Characteristics using Shallow Discriminators</td><td>[2511.10424](http://arxiv.org/pdf/2511.10424)</td><td>◆ Each image acquisition setup leads to its own camera-specific image characteristics degrading the image quality.
◆ In learning-based perception algorithms, characteristics occurring during the application phase, but absent in the training data, lead to a domain gap impeding the performance.
◆ Previously, pixel-level domain adaptation through unpaired learning of the pristine-to-distorted mapping function has been proposed.</td></tr>
<tr><td>2025-11-13</td><td>Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision</td><td>[2511.10316](http://arxiv.org/pdf/2511.10316)</td><td>◆ Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions.
◆ Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions.
◆ This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting.</td></tr>
<tr><td>2025-11-13</td><td>DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection</td><td>[2511.10035](http://arxiv.org/pdf/2511.10035)</td><td>◆ As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians.
◆ However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems.
◆ We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities.</td></tr>
<tr><td>2025-11-13</td><td>New ASKAP radio-continuum surveys of the Small Magellanic Cloud</td><td>[2511.09954](http://arxiv.org/pdf/2511.09954)</td><td>◆ We present two new radio continuum images from the ASKAP POSSUM survey in the direction of the Small Magellanic Cloud.
◆ The two new source lists contain 36,571 radio continuum sources detected at 944 MHz and 15,227 sources at 1367 MHz, with beam sizes of approximately 14.5 by 12.2 arcsec and 8.7 by 8.2 arcsec, respectively.
◆ We used the Aegean software package to generate these point source catalogues, and together with the previously published MeerKAT catalogue, we estimated spectral indices for the full set of matched radio point sources.</td></tr>
<tr><td>2025-11-13</td><td>Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment</td><td>[2511.09948](http://arxiv.org/pdf/2511.09948)</td><td>◆ Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as &quot;a good photo&quot; or &quot;a bad photo.&quot; However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality.
◆ In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue.
◆ Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity.</td></tr>
<tr><td>2025-11-12</td><td>From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance</td><td>[2511.09820](http://arxiv.org/pdf/2511.09820)</td><td>◆ Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments.
◆ However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment.
◆ In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training.</td></tr>
<tr><td>2025-11-12</td><td>STORM: Segment, Track, and Object Re-Localization from a Single 3D Model</td><td>[2511.09771](http://arxiv.org/pdf/2511.09771)</td><td>◆ Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots.
◆ However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement.
◆ To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation.</td></tr>
<tr><td>2025-11-08</td><td>U(PM)$^2$:Unsupervised polygon matching with pre-trained models for challenging stereo images</td><td>[2511.05949](http://arxiv.org/pdf/2511.05949)</td><td>◆ Stereo image matching is a fundamental task in computer vision, photogrammetry and remote sensing, but there is an almost unexplored field, i.e., polygon matching, which faces the following challenges: disparity discontinuity, scale variation, training requirement, and generalization.
◆ To address the above-mentioned issues, this paper proposes a novel U(PM)$^2$: low-cost unsupervised polygon matching with pre-trained models by uniting automatically learned and handcrafted features, of which pipeline is as follows: firstly, the detector leverages the pre-trained segment anything model to obtain masks; then, the vectorizer converts the masks to polygons and graphic structure; secondly, the global matcher addresses challenges from global viewpoint changes and scale variation based on bidirectional-pyramid strategy with pre-trained LoFTR; finally, the local matcher further overcomes local disparity discontinuity and topology inconsistency of polygon matching by local-joint geometry and multi-feature matching strategy with Hungarian algorithm.
◆ We benchmark our U(PM)$^2$ on the ScanNet and SceneFlow datasets using our proposed new metric, which achieved state-of-the-art accuracy at a competitive speed and satisfactory generalization performance at low cost without any training requirement.</td></tr>
<tr><td>2025-11-05</td><td>Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort</td><td>[2511.03416](http://arxiv.org/pdf/2511.03416)</td><td>◆ Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans.
◆ In this work, we propose an automated method for standardizing this alignment.
◆ Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo&#x27;s principal axes, from which four candidate orientations are derived.</td></tr>
<tr><td>2025-11-04</td><td>Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization</td><td>[2511.02489](http://arxiv.org/pdf/2511.02489)</td><td>◆ With the rapid growth of the low-altitude economy, UAVs have become crucial for measurement and tracking in patrol systems.
◆ However, in GNSS-denied areas, satellite-based localization methods are prone to failure.
◆ This paper presents a cross-view UAV localization framework that performs map matching via object detection, aimed at effectively addressing cross-temporal, cross-view, heterogeneous aerial image matching.</td></tr>
<tr><td>2025-10-27</td><td>The MDW Hα Sky Survey: Data Release 1</td><td>[2510.22900](http://arxiv.org/pdf/2510.22900)</td><td>◆ The Mittelman-di Cicco-Walker (MDW) H$\alpha$ Sky Survey is an autonomously-operated all-sky narrow-band (3nm) H$\alpha$ imaging survey.
◆ The survey was founded by amateur astronomers and the northern sky (Decl.
◆ $\geq$ 0$^\circ$) is presented here in its second stage of refinement for academic use.</td></tr>
<tr><td>2025-10-26</td><td>FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment</td><td>[2510.22827](http://arxiv.org/pdf/2510.22827)</td><td>◆ Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how well images match prompts and how models treat social attributes.
◆ Common proxies -- face classifiers and contrastive similarity -- reward surface cues, lack calibrated abstention, and miss attributes only weakly visible (for example, religion, culture, disability).
◆ We present FairJudge, a lightweight protocol that treats instruction-following multimodal LLMs as fair judges.</td></tr>
<tr><td>2025-10-08</td><td>Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera</td><td>[2510.06829](http://arxiv.org/pdf/2510.06829)</td><td>◆ Line segment extraction is effective for capturing geometric features of human-made environments.
◆ Event-based cameras, which asynchronously respond to contrast changes along edges, enable efficient extraction by reducing redundant data.
◆ However, recent methods often rely on additional frame cameras or struggle with high event rates.</td></tr>
<tr><td>2025-10-08</td><td>StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</td><td>[2510.06827](http://arxiv.org/pdf/2510.06827)</td><td>◆ In the domain of text-to-image generation, diffusion models have emerged as powerful tools.
◆ Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content.
◆ However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style.</td></tr>
<tr><td>2025-10-08</td><td>Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</td><td>[2510.06820](http://arxiv.org/pdf/2510.06820)</td><td>◆ Multimodal retrieval still leans on embedding-based models like CLIP for fast vector search over pre-computed image embeddings.
◆ Yet, unlike text retrieval, where joint-encoder rerankers are standard, comparable vision--language rerankers are largely absent.
◆ We find that seminal joint encoders such as BLIP are severely bottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale.</td></tr>
<tr><td>2025-10-24</td><td>SegMASt3R: Geometry Grounded Segment Matching</td><td>[2510.05051](http://arxiv.org/pdf/2510.05051)</td><td>◆ Segment matching is an important intermediate task in computer vision that establishes correspondences between semantically or geometrically coherent regions across images.
◆ Unlike keypoint matching, which focuses on localized features, segment matching captures structured regions, offering greater robustness to occlusions, lighting variations, and viewpoint changes.
◆ In this paper, we leverage the spatial understanding of 3D foundation models to tackle wide-baseline segment matching, a challenging setting involving extreme viewpoint shifts.</td></tr>
<tr><td>2025-09-23</td><td>Hierarchical Neural Semantic Representation for 3D Semantic Correspondence</td><td>[2509.17431](http://arxiv.org/pdf/2509.17431)</td><td>◆ This paper presents a new approach to estimate accurate and robust 3D semantic correspondence with the hierarchical neural semantic representation.
◆ Our work has three key contributions.
◆ First, we design the hierarchical neural semantic representation (HNSR), which consists of a global semantic feature to capture high-level structure and multi-resolution local geometric features to preserve fine details, by carefully harnessing 3D priors from pre-trained 3D generative models.</td></tr>
<tr><td>2025-09-20</td><td>PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality</td><td>[2509.16519](http://arxiv.org/pdf/2509.16519)</td><td>◆ We introduce PM25Vision (PM25V), the largest and most comprehensive dataset to date for estimating air quality - specifically PM2.5 concentrations - from street-level images.
◆ The dataset contains over 11,114 images matched with timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations and 11 years, significantly exceeding the scale of previous benchmarks.
◆ The spatial accuracy of this dataset has reached 5 kilometers, far exceeding the city-level accuracy of many datasets.</td></tr>
<tr><td>2025-09-19</td><td>DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching</td><td>[2509.16017](http://arxiv.org/pdf/2509.16017)</td><td>◆ Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis.
◆ However, the significant appearance differences between modalities make this task challenging.
◆ Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios.</td></tr>
<tr><td>2025-09-18</td><td>RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</td><td>[2509.14966](http://arxiv.org/pdf/2509.14966)</td><td>◆ The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult.
◆ As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features.
◆ Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps.</td></tr>
<tr><td>2025-09-14</td><td>A Geometrically Consistent Matching Framework for Side-Scan Sonar Mapping</td><td>[2509.11255](http://arxiv.org/pdf/2509.11255)</td><td>该论文针对侧扫声纳图像因视角依赖、阴影和几何畸变导致的匹配难题，提出了一种结合物理解耦与几何一致性的创新匹配框架。  
◆ 提出自监督多分支网络，基于朗伯反射模型将原始声纳图像分解为海底反射率、地形高程和声学路径损耗，增强物理可解释性。  
◆ 利用反射率图谱作为稳定匹配域，结合无训练匹配流程（SuperPoint与MINIMA LightGlue），提升跨视角对应关系准确性。  
◆ 引入几何感知异常点剔除机制，联合地形高程与物理衍生的阴影图谱，有效抑制声学遮挡和地形不一致区域的误匹配。  
实验表明，该方法在匹配误差、几何一致性和视角鲁棒性上均优于传统及基于CNN与Transformer的先进方法，为复杂海底环境提供了高精度、数据高效且物理可解释的匹配解决方案。</td></tr>
<tr><td>2025-09-29</td><td>Loc$^2$: Interpretable Cross-View Localization via Depth-Lifted Local Feature Matching</td><td>[2509.09792](http://arxiv.org/pdf/2509.09792)</td><td>本文提出了一种精细粒度跨视角定位方法，通过局部特征匹配与单目深度先验实现地面图像的三自由度位姿估计。  
◆ 直接建立地面与航空图像间的局部特征对应关系，避免传统方法中因视角转换造成的信息损失。  
◆ 利用单目深度先验仅将匹配关键点提升至鸟瞰图空间，支持度量深度与相对深度两种模式。  
◆ 提出尺度感知的普氏对齐算法，能够从对应关系中估计相机位姿，并在使用相对深度时恢复尺度。  
◆ 仅需弱监督位姿标注即可学习精确特征对应，在跨区域泛化与未知朝向等挑战性场景中表现优异。  
◆ 兼容多种相对深度模型且无需针对每个模型微调，具备较强的实用性与部署灵活性。</td></tr>
<tr><td>2025-09-11</td><td>ObjectReact: Learning Object-Relative Control for Visual Navigation</td><td>[2509.09594](http://arxiv.org/pdf/2509.09594)</td><td>该论文提出了一种基于物体相对控制的视觉导航新范式ObjectReact，以解决传统图像相对控制方法的局限性。  
◆ 创新性地采用“物体相对”控制替代主流的“图像相对”控制，利用物体作为地图固有属性，摆脱对智能体位姿和具体形态的严格依赖。  
◆ 设计了基于“相对3D场景图”的拓扑-度量混合地图表示，能够生成更高效的对象级全局路径规划代价。  
◆ 开发了直接以高层“WayObject Costmap”为输入条件的本地控制器，无需显式RGB输入，实现了控制预测与图像匹配问题的解耦。  
◆ 该方法在跨形态部署（如传感器高度变化）和挑战性任务（如反向轨迹导航）中表现出显著优势，且仅使用仿真训练的策略能很好地泛化到真实室内环境。</td></tr>
<tr><td>2025-09-23</td><td>Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching</td><td>[2509.08805](http://arxiv.org/pdf/2509.08805)</td><td>本文提出了一种在稠密图像匹配中处理多重假设的新方法BEAMER，其核心贡献在于显著提升了在挑战性场景下的匹配鲁棒性。  
◆ 摒弃了传统方法在每个尺度上仅为每个源位置预测单一对应点的做法，创新性地提出在每个尺度上预测并保留多个对应假设。  
◆ 采用束搜索（beam search）策略，在由粗到细的匹配过程中逐尺度地传播和保留这些多重假设。  
◆ 设计了一种新颖的架构，将多重假设集成到交叉注意力（cross-attention）层中，使网络能够学习如何有效地在不同尺度间筛选和传播最优假设。  
该方法在处理深度不连续或目标图像是源图像的极大缩放场景时，性能显著优于现有先进技术，有效减少了错误匹配。</td></tr>
<tr><td>2025-09-08</td><td>Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval</td><td>[2509.06566](http://arxiv.org/pdf/2509.06566)</td><td>本文针对场景级草图检索任务，提出了一种强调草图固有模糊性和噪声的鲁棒性训练方法。  
◆ 指出以往研究忽略真实草图的歧义与噪声问题，转而关注训练目标的鲁棒性设计。  
◆ 提出结合预训练策略、编码器架构和损失函数的优化方案，无需增加模型复杂度即可提升性能。  
◆ 在FS-COCO和SketchyCOCO数据集上实现最先进效果，验证了训练设计对跨模态检索的关键作用。  
◆ 强调需改进场景级草图检索的评估场景，推动任务向更实用化发展。</td></tr>
<tr><td>2025-09-04</td><td>Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation</td><td>[2509.04273](http://arxiv.org/pdf/2509.04273)</td><td>本文提出了一种用于半监督医学图像分割的新框架，其核心贡献在于将变分模型中的先验知识与深度学习网络有效结合。  
◆ 创新性地引入了双尺度体积先验，即在图像尺度和数据集尺度上分别利用强显式先验和弱隐式先验来约束分割网络。  
◆ 设计了一个回归网络来估计未标注图像的目标区域体积，并通过图像尺度的Wasserstein距离损失，强制分割结果与回归预测的类别比例一致。  
◆ 提出了一个数据集尺度的Wasserstein距离损失函数，使得未标注数据集预测的体积分布与已标注数据集的分布相似，从而利用数据集层面的统计信息。  
◆ 将Threshold Dynamics空间正则化方法融入分割网络主干，增强了特征提取的几何约束能力。  
实验在多个公开数据集上验证了该方法的优越性，显著提升了半监督分割性能。</td></tr>
<tr><td>2025-09-09</td><td>POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection</td><td>[2508.19742](http://arxiv.org/pdf/2508.19742)</td><td>该论文提出了一个名为POEv2的通用且鲁棒的线检测框架，其核心贡献是统一了通用线段检测和结构化线段检测两大任务。  
◆ 提出了一个灵活的框架，能够同时胜任通用线检测和结构化线检测，解决了以往两类检测器因设计目标不同而无法互相替代的问题。  
◆ 作为Pixel Orientation Estimation (POE)方法的改进版，新框架能从边缘强度图中检测线段，并可兼容任何边缘检测器。  
◆ 通过结合高效的边缘检测器，该方法在三个公开数据集上实现了最先进的性能，证明了其有效性和优越性。  
◆ 该框架兼具鲁棒性和灵活性，为不同应用场景下的线检测需求提供了一个统一的解决方案。</td></tr>
<tr><td>2025-08-14</td><td>Revisiting Cross-View Localization from Image Matching</td><td>[2508.10716](http://arxiv.org/pdf/2508.10716)</td><td>◆ 提出基于跨视角图像匹配的新框架，将定位问题转化为匹配问题，突破传统直接位姿回归或BEV特征对齐的局限。  
◆ 引入Surface Model精确建模地面视角可见区域，实现更准确的鸟瞰图投影，解决几何不一致问题。  
◆ 设计SimRefiner模块通过局部-全局残差校正优化相似度矩阵，无需RANSAC后处理即可获得精细匹配。  
◆ 构建首个像素级标注的跨视角匹配基准CVFM（含32,509对图像），填补领域数据空白。  
◆ 在极端视角差异下实现定位精度和匹配质量双重提升，实验验证其显著优于现有方法。  
核心贡献在于通过建模、匹配、数据三方面的创新，首次系统解决了跨视角图像严格对应难题，推动GNSS拒止环境下的高精度定位发展。</td></tr>
<tr><td>2025-08-14</td><td>A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method</td><td>[2508.10294](http://arxiv.org/pdf/2508.10294)</td><td>◆ 提出了一种基于相位一致性加权最小绝对偏差（PCWLAD）的亚像素模板匹配方法，显著提高了多模态光学图像的匹配精度。  
◆ 采用两阶段匹配策略：先通过结构相似性指数（SSIM）进行粗匹配，再利用WLAD进行精细匹配，兼顾效率与精度。  
◆ 在粗匹配阶段保留原始结构细节（无噪声滤波），通过SSIM增强对非线性辐射差异的鲁棒性。  
◆ 在精细匹配阶段引入辐射和几何变换模型，结合互结构滤波抑制噪声对结构一致性的影响，提升跨模态匹配稳定性。  
◆ 在可见光-红外（Landsat、无人机）和可见光-近红外（近景）三类数据集上验证，平均匹配精度达0.4像素，优于现有8种先进方法。  
◆ 公开了软件和数据集，促进多模态遥感图像匹配研究的发展。</td></tr>
<tr><td>2025-08-13</td><td>Episodic Memory Representation for Long-form Video Understanding</td><td>[2508.09486](http://arxiv.org/pdf/2508.09486)</td><td>◆ 提出Video-EM框架，解决现有Video-LLMs因上下文窗口限制难以处理长视频的问题，无需额外训练即可实现高效视频理解。  
◆ 突破传统关键帧检索方法的静态图像匹配局限，通过模拟人类情景记忆机制，将关键帧建模为时序化情景事件，保留时空动态关系。  
◆ 创新性地结合思维链（CoT）技术，利用大语言模型迭代筛选信息量最大化的最小情景记忆子集，避免冗余帧干扰。  
◆ 首次在关键帧表示中同时捕捉空间关联与时间动态性，精准还原视频叙事逻辑，提升场景转换和上下文连续性的理解能力。  
◆ 在四大主流评测基准（Video-MME等）上验证有效性，性能显著优于基线4-9%，且使用更少帧数，兼顾效率与准确性。</td></tr>
<tr><td>2025-08-11</td><td>VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models</td><td>[2508.08521](http://arxiv.org/pdf/2508.08521)</td><td>◆提出VISOR方法，仅通过优化视觉输入即可实现精准的行为控制，无需修改模型内部参数或文本指令。  
◆首创&quot;通用引导图像&quot;概念，通过视觉刺激诱导目标激活模式，在保持隐蔽性的同时实现双向行为调控。  
◆在LLaVA-1.5-7B模型上验证了三大关键对齐任务（拒绝、谄媚、生存本能），单张150KB图像即可达到与激活向量相当的调控效果。  
◆相比系统提示词（3-4%改变）和激活向量（微弱负向调控），VISOR实现高达25%的行为偏移，同时保持99.9%的MMLU基准性能。  
◆揭示了视觉通道的新型安全威胁：攻击者仅通过图像即可绕过文本防御机制，实现复杂行为操控。  
◆为多模态模型控制提供了无需运行时开销、兼容API服务的解决方案，同时警示了视觉引导攻击的防御紧迫性。</td></tr>
<tr><td>2025-08-11</td><td>Semi-supervised Multiscale Matching for SAR-Optical Image</td><td>[2508.07812](http://arxiv.org/pdf/2508.07812)</td><td>◆提出半监督多尺度匹配框架S2M2-SAR，利用少量标注数据和大量无标注SAR-光学图像对进行训练，解决标注成本高的问题。  
◆通过结合深层和浅层匹配结果生成伪标签相似性热图，为无标注数据提供监督信号，提升模型泛化能力。  
◆设计跨模态特征增强模块，采用无监督的跨模态互独立性损失，分离模态共享和模态特定特征，增强特征解耦能力。  
◆无需人工标注即可优化跨模态特征表示，降低对标注数据的依赖，提升模型实用性。  
◆实验表明，S2M2-SAR性能优于现有半监督方法，并与全监督SOTA方法相当，验证了其高效性和应用潜力。</td></tr>
<tr><td>2025-08-07</td><td>Refining Gaussian Splatting: A Volumetric Densification Approach</td><td>[2508.05187](http://arxiv.org/pdf/2508.05187)</td><td>◆ 提出基于惯性体积的新型密度控制方法，利用高斯函数的惯性体积指导3D高斯分布的精细化过程，克服了原始3DGS密度策略的缺陷。  
◆ 创新性地研究了传统SfM与深度图像匹配(DIM)两种点云初始化方法对重建质量的影响，为初始化选择提供了新见解。  
◆ 通过自适应密度控制(ADC)自动化实现了高斯基元的动态增删，显著提升了点基元管理效率。  
◆ 在Mip-NeRF 360数据集上的实验表明，该方法在重建质量上全面超越原始3DGS，且在不同场景中均表现优异。  
◆ 将体积信息与密度控制相结合，为3D高斯泼溅技术的几何优化开辟了新思路。</td></tr>
<tr><td>2025-08-09</td><td>SGAD: Semantic and Geometric-aware Descriptor for Local Feature Matching</td><td>[2508.02278](http://arxiv.org/pdf/2508.02278)</td><td>◆ 提出SGAD网络，通过生成高区分度的区域描述符，直接实现区域匹配，避免传统低效的像素级比较和复杂图优化，显著提升匹配精度和效率。  
◆ 设计新颖的监督策略，将区域匹配任务分解为分类和排序子任务，进一步提升匹配性能。  
◆ 引入层次包容冗余过滤器（HCRF），通过分析包容图消除重叠区域，优化匹配结果。  
◆ 在效率上实现重大突破，相比MESA方法运行时减少60倍（0.82秒 vs 60.23秒），同时保持更高精度。  
◆ 在多个基准测试中验证有效性：SGAD+LoFTR在室外姿态估计中比DKM更快（0.82秒 vs 1.51秒）且更准确（65.98 vs 61.11）；SGAD+ROMA在室内姿态估计中AUC@5°提升7.39%，达到新SOTA。</td></tr>
<tr><td>2025-07-31</td><td>VMatcher: State-Space Semi-Dense Local Feature Matching</td><td>[2507.23371](http://arxiv.org/pdf/2507.23371)</td><td>◆ 提出VMatcher，一种结合Mamba和Transformer的混合网络，用于图像对的半稠密特征匹配。  
◆ 首次将选择性状态空间模型（SSM）引入特征匹配任务，利用Mamba的线性计算复杂度显著降低传统Transformer的二次方计算开销。  
◆ 设计多层级混合架构，同时保留Transformer注意力机制的优势和Mamba高效长序列处理能力，兼顾性能与效率。  
◆ 在保持或超越当前最优方法精度的前提下，大幅提升计算效率，适合实时性要求高的应用场景。  
◆ 开源代码并提供多种配置方案，为后续研究提供灵活可扩展的基准框架。</td></tr>
<tr><td>2025-07-30</td><td>Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques</td><td>[2507.22791](http://arxiv.org/pdf/2507.22791)</td><td>◆ 全面综述了单模态与跨模态特征匹配技术，涵盖RGB图像、深度图像、3D点云、LiDAR扫描、医学图像及视觉-语言交互等多种模态，填补了该领域系统性总结的空白。  
◆ 对比分析了传统手工方法（如Harris角点、SIFT和ORB描述子）与深度学习方法（如SuperPoint和LoFTR）的优劣，指出后者在跨模态鲁棒性和适应性上的显著提升。  
◆ 重点介绍了模态感知的创新技术，例如针对深度图像的几何与深度专用描述子、针对3D点云的稀疏与稠密学习方法，以及LiDAR扫描中基于注意力增强的神经网络。  
◆ 强调了跨模态应用的突破，如医学图像配准中的MIND描述子和视觉-语言任务中的交互匹配技术，展示了特征匹配在多样化数据交互中的扩展潜力。  
◆ 系统总结了当前挑战与未来方向，为跨模态特征匹配的研究提供了清晰的路线图，推动该领域向更复杂场景发展。</td></tr>
<tr><td>2025-07-25</td><td>Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching</td><td>[2507.19118](http://arxiv.org/pdf/2507.19118)</td><td>◆提出跨时空融合注意力机制(CSTF)，通过独立检测参考图和查询图中的尺度不变关键点来增强特征表示，解决多模态遥感图像间几何和辐射差异大的问题。  
◆创新性地构建对应图，同时利用多图像区域信息，提升跨模态特征匹配能力。  
◆将相似性匹配重新定义为分类任务，结合SoftMax和全卷积网络(FCN)层，兼顾局部特征敏感性和全局上下文信息。  
◆在HRSC2016和DOTA基准数据集上实现目标检测任务的最优性能，平均mAP分别达到90.99%和90.86%。  
◆保持12.5 FPS的推理速度，验证了算法的高效性和实用性。  
◆首次证明改进的跨模态特征匹配能直接提升遥感下游任务（如目标检测）的性能。</td></tr>
<tr><td>2025-07-24</td><td>A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration</td><td>[2507.18551](http://arxiv.org/pdf/2507.18551)</td><td>◆提出了一种新型3D跨模态关键点描述符，用于解决MRI与实时超声(iUS)之间的配准难题，克服了模态间外观、分辨率和视野差异大的问题。  
◆采用患者特异性的&quot;合成匹配&quot;方法，从术前MRI生成合成iUS体积，实现了有监督对比学习的共享描述符空间训练。  
◆开发了概率关键点检测策略，能够识别解剖学显著且模态一致的特征位置，提高了匹配的准确性。  
◆在训练阶段使用基于课程的三元组损失函数和动态难负样本挖掘技术，使描述符具有抗iUS伪影(如斑点噪声)和旋转不变性的特点。  
◆整个框架具有可解释性，无需人工初始化，对iUS视野变化表现出强鲁棒性，在ReMIND数据集上达到69.8%的平均匹配精度和2.39mm的配准误差。  
◆相比现有方法，该方案首次实现了从关键点匹配到刚性配准的完整流程，在临床实际应用中更具实用价值。</td></tr>
<tr><td>2025-07-22</td><td>A Single-step Accurate Fingerprint Registration Method Based on Local Feature Matching</td><td>[2507.16201](http://arxiv.org/pdf/2507.16201)</td><td>◆ 提出了一种端到端的单步指纹配准算法，直接通过预测两幅指纹图像之间的半密集匹配点对应关系来实现对齐，避免了传统两步法的复杂性。  
◆ 解决了低质量指纹图像因特征点数量不足导致的初始配准失败问题，提高了配准的鲁棒性和成功率。  
◆ 创新性地结合全局-局部注意力机制，实现了两幅指纹图像之间的端到端像素级对齐，提升了配准精度。  
◆ 实验证明该方法仅需单步配准即可达到最先进的匹配性能，同时还能与密集配准算法结合以进一步提升性能。  
◆ 为指纹识别中的图像失真问题提供了一种高效且可靠的解决方案，具有实际应用价值。</td></tr>
<tr><td>2025-07-09</td><td>Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching</td><td>[2507.06744](http://arxiv.org/pdf/2507.06744)</td><td>◆ 提出局部-全局双粒度身份关联机制，通过批内跨模态显式关联强化身份约束，提升模型对细微差异的捕捉能力。  
◆ 构建以视觉模态为锚点的动态跨模态关联网络，引入基于置信度的动态调整机制，有效增强弱关联样本的识别能力。  
◆ 设计信息不对称样本对构建方法，结合一致性学习解决困难样本挖掘问题，提升模型鲁棒性。  
◆ 首次在弱监督文本-行人图像匹配任务中实现复杂一对多身份关系的建模，突破性能瓶颈。  
◆ 实验证明该方法显著提升跨模态匹配准确率，为实际应用提供高效解决方案。</td></tr>
<tr><td>2025-07-05</td><td>From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM</td><td>[2507.03868](http://arxiv.org/pdf/2507.03868)</td><td>◆ 提出Uni-Retrieval模块，通过提取查询风格原型并动态匹配Prompt Bank中的标记，解决现有检索系统无法处理教育场景多样性和模糊性的问题。  
◆ 引入Prompt Bank，结合MoE-LoRA模块编码领域知识，支持测试时适应未见查询类型，增强检索灵活性。  
◆ 将Uni-Retrieval与轻量级指令调优语言模型结合，构建完整的Uni-RAG流程，实现从检索到自然语言生成的教育内容输出。  
◆ 采用风格条件查询机制，生成符合学习目标的可读解释、反馈或教学内容，提升个性化教学效果。  
◆ 在SER等多模态基准测试中，Uni-RAG在检索精度和生成质量上均优于基线系统，同时保持低计算成本。  
◆ 为STEM教育提供可扩展的智能解决方案，弥合检索与生成的鸿沟，支持高效、可解释的学习辅助。</td></tr>
<tr><td>2025-07-02</td><td>What does really matter in image goal navigation?</td><td>[2507.01667](http://arxiv.org/pdf/2507.01667)</td><td>◆ 研究了端到端强化学习在图像目标导航任务中的有效性，挑战了传统依赖专用图像匹配或预训练视觉模块的方法。  
◆ 通过大规模实验分析了多种架构设计（如延迟融合、通道堆叠、空间到深度投影和交叉注意力）对导航性能的影响。  
◆ 揭示了仿真环境设置对现有方法性能的影响，指出仿真中存在的捷径问题，同时证明部分能力可迁移到更真实场景。  
◆ 首次发现导航性能与相对位姿估计能力之间存在相关性，表明后者是导航任务中自然涌现的重要子技能。  
◆ 为仅通过导航奖励信号训练相对位姿估计器提供了可能性，对具身AI及其他领域具有潜在影响。  
◆ 通过系统实验验证了端到端训练智能体的潜力，同时指出了仿真与现实场景间的性能差距问题。</td></tr>
<tr><td>2025-06-30</td><td>Efficient and Accurate Image Provenance Analysis: A Scalable Pipeline for Large-scale Images</td><td>[2506.23707](http://arxiv.org/pdf/2506.23707)</td><td>这篇论文的核心贡献是提出了一种高效且准确的图像溯源分析管道，解决了现有方法在精度和可扩展性上的两大瓶颈。  

◆ 创新性地引入修改关系追踪技术，显著提升了图像变体的过滤效果，能够全面发现与查询图像视觉相似度低的变体，解决了传统方法因低相似度而遗漏严重修改图像的问题。  

◆ 通过结合局部特征匹配和压缩伪影捕捉技术，增强了方法对多样化修改的鲁棒性，能够更准确地分析图像间的关联性和修改方向。  

◆ 提出了一种优化的相似度计算策略，并在构建有向溯源图时消除了冗余的成对分析，将时间复杂度从二次降低到线性，实现了大规模场景下的高效处理。  

◆ 实验证明，该方法在精度上比现有技术提升了16.7%-56.1%，并在1000万规模图像上平均响应时间仅3秒，远优于现有方法的12分钟，展现了卓越的可扩展性。  

◆ 最终生成的溯源图能够精确刻画图像的演化历史，为数字治理提供了可靠的取证工具。</td></tr>
<tr><td>2025-06-29</td><td>Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization</td><td>[2506.23077](http://arxiv.org/pdf/2506.23077)</td><td>◆ 提出了Distance-Aware Cross-View Geo-Localization (DACVGL)新问题，强调模型需综合捕捉目标周围上下文信息并降低定位误差成本。  
◆ 构建首个多视角图像与精确距离标注的基准数据集DA-Campus，涵盖三种空间分辨率，支持系统性研究。  
◆ 将DACVGL问题形式化为跨域分层检索任务，揭示传统度量学习无法解决建筑间复杂空间关系的问题。  
◆ 提出动态对比学习框架DyCL，通过分层空间间隔逐步对齐特征表示，解决跨视角层次化检索难题。  
◆ 实验证明DyCL与现有多尺度度量学习方法高度互补，显著提升分层检索性能和跨视角地理定位精度。  
◆ 公开代码和基准数据集，推动后续研究。</td></tr>
<tr><td>2025-06-27</td><td>MatChA: Cross-Algorithm Matching with Feature Augmentation</td><td>[2506.22336](http://arxiv.org/pdf/2506.22336)</td><td>◆ 首次提出跨特征检测器的特征匹配方法，解决了不同设备使用不同稀疏特征提取算法时视觉定位失效的问题。  
◆ 通过特征描述符增强技术提升跨检测器场景下的特征匹配性能，突破了现有方法依赖相同关键点的限制。  
◆ 引入特征转换到潜在空间的策略，有效应对关键点低重复性和描述符区分度不足的挑战。  
◆ 实验证明该方法在跨特征场景下显著提升了图像匹配和视觉定位的准确率。  
◆ 在多个基准数据集上验证了方法的有效性，为实际应用中不同描述符混合使用的场景提供了可行解决方案。</td></tr>
<tr><td>2025-07-22</td><td>Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs</td><td>[2506.22139](http://arxiv.org/pdf/2506.22139)</td><td>◆提出Q-Frame方法，通过查询自适应的帧选择策略解决视频-大语言模型中关键时空信息丢失的问题，突破传统均匀采样的局限性。  
◆创新性地结合CLIP等文本-图像匹配网络，实现无需训练的即插即用式帧选择，利用Gumbel-Max技巧提升选择效率。  
◆引入多分辨率缩放机制，根据视频内容和查询需求动态调整帧的时空分辨率，优化计算资源分配。  
◆在保持计算负载不变的前提下，显著增加可处理的帧数，同时保留对任务至关重要的时空细节。  
◆在MLVU、LongVideoBench等基准测试中验证了方法的优越性，涵盖多种视频理解任务，性能超越现有技术。  
◆为视频-大模型的实际应用提供轻量化解决方案，平衡了计算效率与语义理解深度的矛盾。</td></tr>
<tr><td>2025-06-27</td><td>ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction</td><td>[2506.21923](http://arxiv.org/pdf/2506.21923)</td><td>◆ 提出ZeroReg3D，首个针对连续组织病理切片3D重建的零样本配准框架，无需训练或微调即可直接应用。  
◆ 创新结合零样本深度学习关键点匹配与基于优化的仿射/非刚性配准技术，解决传统方法在精度与泛化性上的矛盾。  
◆ 首次系统解决组织变形、切片伪影、染色差异和光照不一致四大挑战，显著提升3D重建的解剖结构保真度。  
◆ 突破现有深度学习方法依赖大规模标注数据的限制，通过零样本策略实现跨数据集的高适应性。  
◆ 公开完整代码库，为病理学研究和临床诊断提供可直接部署的开源工具。</td></tr>
<tr><td>2025-06-25</td><td>Fast entropy-regularized SDP relaxations for permutation synchronization</td><td>[2506.20191](http://arxiv.org/pdf/2506.20191)</td><td>◆ 提出了一种快速随机算法，用于解决部分排列同步问题（PPS）的半定规划（SDP）松弛，显著提升了多图像匹配的效率。  
◆ 利用熵正则化技术解决了标准松弛中优化解非唯一性的问题，增强了算法的稳定性和可靠性。  
◆ 开发了一种随机求解器，其计算复杂度在观测到的对应关系数量上接近最优，大幅提升了计算效率。  
◆ 设计了多种舍入程序，能够从隐式表示的原问题解变量中恢复组合解，同时支持保持循环一致性而不影响计算效率。  
◆ 在合成和真实数据集上验证了算法的优越性，在速度和精度方面均达到了当前最优水平。  
◆ 展示了熵正则化SDP在PPS问题中的理论和实践优势，为传统低秩或谱技术提供了新的替代方案。</td></tr>
<tr><td>2025-06-18</td><td>ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections</td><td>[2506.15180](http://arxiv.org/pdf/2506.15180)</td><td>◆ 提出ReSeDis任务，首次将大规模图像检索与像素级定位统一，要求模型根据文本描述在图像库中同时判断对象是否存在并精确定位。  
◆ 构建首个针对该任务的基准数据集，确保每个描述唯一对应分散在大规模多样图像库中的对象实例，避免误匹配。  
◆ 设计联合评估指标，同时衡量检索召回率与定位精度，为端到端性能提供量化标准。  
◆ 提出基于冻结视觉语言模型的零样本基线方法，揭示该任务未来研究的巨大提升空间。  
◆ 为构建下一代鲁棒、可扩展的多模态搜索系统提供真实场景下的测试平台，弥补现有技术仅侧重检索或定位单一能力的缺陷。</td></tr>
<tr><td>2025-06-16</td><td>EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints for Visual Place Recognition</td><td>[2506.13133](http://arxiv.org/pdf/2506.13133)</td><td>◆ 提出了一种新颖的简单重排序方法，通过混合特征（MoF）方法在具身约束下优化全局特征，提升视觉地点识别（VPR）性能。  
◆ 首次系统分析了具身约束在VPR中的实际可行性，并根据现有数据集将其分类为GPS标签、时序戳、局部特征匹配和自相似矩阵等类型。  
◆ 设计了一种基于学习的MoF权重计算策略，采用多度量损失函数，有效融合多种特征信息。  
◆ 在公开数据集上实现了性能提升，仅需25 KB额外参数和每帧10微秒处理时间，计算开销极低。  
◆ 在Pitts-30k测试集上，基于DINOv2的基线性能提升0.9%，显著优于现有方法。</td></tr>
<tr><td>2025-06-12</td><td>RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration</td><td>[2506.10344](http://arxiv.org/pdf/2506.10344)</td><td>◆ 提出RealKeyMorph（RKM），首个无需固定分辨率重采样的医学图像配准方法，直接处理原始分辨率数据，避免插值伪影。  
◆ 创新性地将关键点输出为扫描仪真实世界坐标（而非体素坐标），通过利用扫描仪生成的仿射矩阵实现跨分辨率配准。  
◆ 扩展KeyMorph框架，在训练过程中融入真实世界坐标转换，使关键点提取与图像分辨率完全解耦。  
◆ 在腹部MRI正交2D堆栈和不同分辨率3D脑数据集上验证了方法的优越性，证明其对分辨率差异的鲁棒性。  
◆ 通过闭式关键点匹配计算变换参数，保持了KeyMorph原有的可解释性优势，同时突破分辨率限制。</td></tr>
<tr><td>2025-06-11</td><td>Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints</td><td>[2506.09748](http://arxiv.org/pdf/2506.09748)</td><td>◆ 提出了一种分层跨源图像匹配方法，结合语义感知和结构约束的粗匹配模块与轻量级细粒度匹配模块，显著提升了无人机绝对视觉定位的精度。  
◆ 利用视觉基础模型提取语义特征，在语义和结构约束下建立区域级对应关系，有效解决了跨源差异和时变因素导致的匹配难题。  
◆ 设计了轻量级细粒度匹配模块，通过提取精细特征建立像素级对应关系，进一步提升了定位的准确性。  
◆ 构建了不依赖相对定位技术的无人机绝对视觉定位流程，通过图像检索模块与分层匹配模块的结合，实现了独立定位。  
◆ 在公开基准数据集和新提出的CS-UAV数据集上验证了方法的优越性，展示了其在多种挑战性条件下的高精度和鲁棒性。</td></tr>
<tr><td>2025-06-11</td><td>ScaleLSD: Scalable Deep Line Segment Detection Streamlined</td><td>[2506.09369](http://arxiv.org/pdf/2506.09369)<br><a href=''>[代码]</a></td><td>◆ 提出ScaleLSD，首个通过大规模自监督学习（超过1000万无标签图像）训练的领域无关鲁棒线检测模型，显著提升自然图像的线段检测能力。  
◆ 重新设计并简化了传统（深度与非深度）线段检测方法的核心架构，实现高效高性能的线段检测，检测数量远超经典非深度方法。  
◆ 在线段几何表征上更完整且准确，首次实现深度方法在所有测试场景（检测性能、单视图3D几何估计等）全面超越经典非深度LSD。  
◆ 通过零样本协议验证模型泛化性，在单视图3D重建、双视图线段匹配、多视图3D线段映射等任务中均表现优异。  
◆ 开源代码与模型，为图像线几何的广泛应用（如三维重建、匹配）提供更强通用性支持，强化线段几何在多任务中的实用性。</td></tr>
<tr><td>2025-05-21</td><td>Anti-interrupted sampling repeater jamming via linear canonical Wigner distribution lightweight LFM detection</td><td>[2506.06302](http://arxiv.org/pdf/2506.06302)</td><td>◆ 提出基于广义线性正则维格纳分布（GLWD）的抗干扰方法，通过合理设置参数获得高时频分辨率和能量集中性，显著提升信号分离能力和信噪比。  
◆ 改进现有移动线段检测（M-LSD）算法，提出移动长线段检测（M-LSD）算法，增强对目标线性调频信号的检测能力，降低对干扰信号的敏感性。  
◆ 利用GLWD与短时傅里叶变换（STFT）的映射关系构建时频滤波器，在STFT域进行滤波以高效抑制干扰。  
◆ 方法在低信噪比条件下仍能有效区分能量接近真实目标的干扰采样转发干扰（ISRJ），解决传统时频域方法在多分量信号场景中的时频混叠问题。  
◆ 仿真与实验验证了该方法对难区分干扰的有效抑制能力，兼具实时性和鲁棒性，适用于实际雷达抗干扰场景。</td></tr>
<tr><td>2025-06-05</td><td>Vanishing arcs for isolated plane curve singularities</td><td>[2506.04917](http://arxiv.org/pdf/2506.04917)</td><td>◆ 提出&quot;消失弧集&quot;新概念，作为传统消失循环的几何对应物，通过几何变分算子将嵌入弧与闭曲线联系起来。  
◆ 建立几何变分算子的拓扑框架，用几何弧和闭曲线替代同调循环，拓展了经典超曲面奇点理论的工具集。  
◆ 给出判定嵌入弧被几何变分算子映射为消失循环的充要条件，基于弧与几何单值化映像的交点数特征。  
◆ 证明对任意由A&#x27;Campo剖分产生的消失循环集，存在拓扑例外弧集使其变分映像与该消失循环集完全匹配。  
◆ 将几何单值化与交点数理论相结合，为平面曲线奇点的拓扑研究提供了新的几何化方法。</td></tr>
<tr><td>2025-06-05</td><td>Deep Learning Reforms Image Matching: A Survey and Outlook</td><td>[2506.04619](http://arxiv.org/pdf/2506.04619)</td><td>◆ 该论文首次从深度学习逐步改造传统图像匹配流程的视角，系统梳理了该领域的革命性进展，突破了传统综述按技术分类的框架。  
◆ 提出与经典流水线高度对齐的新型分类体系：一方面拆解各环节的可学习替代方案（如可学习检测-描述子、离群点过滤器），另一方面整合多环节的端到端模块（如中端稀疏匹配器、稠密匹配器）。  
◆ 深度剖析了可学习组件与端到端模块的设计哲学及优劣，首次明确揭示两类技术路线的互补性与适用边界。  
◆ 在相对位姿恢复、单应估计等核心任务上建立统一评测基准，定量比较了代表性方法的性能突破与现存缺陷。  
◆ 前瞻性指出自监督学习、跨模态匹配、动态场景适应等未来方向，为领域发展绘制了清晰的技术演进地图。  
◆ 通过揭示传统流程被深度学习&quot;解构-重构&quot;的完整路径，为计算机视觉基础问题研究提供了方法论层面的新范式。</td></tr>
<tr><td>2025-06-20</td><td>SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping</td><td>[2505.24305](http://arxiv.org/pdf/2505.24305)</td><td>◆提出SR3D框架，首次实现无需训练的基于单视角的透明与镜面物体3D重建与抓取，突破传统深度感知限制。  
◆利用外部视觉模型直接从RGB图像生成物体网格，结合深度图实现3D场景融合，避免复杂多视角采集系统。  
◆创新性提出视图匹配与关键点匹配双机制，联合2D语义与3D几何信息精准定位物体位姿与尺度。  
◆通过将重建物体逆向映射回原始深度缺失场景，生成高精度深度图，显著提升抓取检测效果。  
◆在仿真与真实场景中验证有效性，为透明/镜面物体抓取提供实用化解决方案，简化硬件依赖。</td></tr>
<tr><td>2025-06-05</td><td>Universal Domain Adaptation for Semantic Segmentation</td><td>[2505.22458](http://arxiv.org/pdf/2505.22458)</td><td>这篇论文提出了通用领域自适应语义分割（UniDA-SS）方法，解决了传统方法因忽略类别设置差异导致的性能下降问题。其核心贡献和创新点如下：

◆ 提出UniDA-SS框架，首次在语义分割任务中实现无需预知源域与目标域类别设置的通用领域自适应。  
◆ 设计Domain-Specific Prototype-based Distinction（DSPD）模块，通过将每类划分为两个域特定原型，增强跨域共有类别的特征区分能力。  
◆ 开发Target-based Image Matching（TIM）策略，基于目标域伪标签匹配最佳源域图像进行批量训练，有效提升共有类别的学习效果。  
◆ 构建新的UniDA-SS基准数据集，为后续研究提供标准化评估平台。  
◆ 实验证明UniMAP方法显著优于基线模型，代码已开源。</td></tr>
<tr><td>2025-05-23</td><td>To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models</td><td>[2505.17973](http://arxiv.org/pdf/2505.17973)<br><a href=''>[代码]</a></td><td>◆ 首次系统比较了传统手工特征匹配（如SIFT+RANSAC）与深度学习特征匹配方法在语义3D建筑模型相机定位任务中的性能差异。  
◆ 针对移动测绘相机与纹理化CityGML LoD2模型的匹配场景，提出定制化评估框架，填补了该领域的研究空白。  
◆ 结合标准数据集（HPatches、MegaDepth-1500）和自建数据集（含地面/无人机拍摄的立面纹理与对应影像），验证方法普适性。  
◆ 通过PnP算法量化绝对位姿估计精度，利用地理参考轨迹数据生成几何真值，建立客观评估基准。  
◆ 实验证明学习式特征匹配在挑战性场景（RANSAC内点数0-12、AUC 0-0.16）中显著优于传统方法，准确率和鲁棒性提升明显。  
◆ 公开代码库促进模型化视觉定位技术发展，为后续研究提供可复现基础。</td></tr>
<tr><td>2025-05-16</td><td>Multi-view dense image matching with similarity learning and geometry priors</td><td>[2505.11264](http://arxiv.org/pdf/2505.11264)</td><td>◆提出MV-DeepSimNets深度学习框架，首次将多视图相似性学习与极线几何先验结合，无需繁琐的多视图训练数据构建。  
◆创新性地引入在线几何先验，通过极线约束或单应性校正动态建模像素关系，生成几何感知的特征表示。  
◆采用平面扫描法将几何特征投影到候选深度假设空间，实现端到端的几何条件化特征适配，提升多视图重建精度。  
◆通过聚合学习到的相似性构建并正则化代价体，相比传统稠密匹配方法显著改善了表面重建质量。  
◆在泛化能力上表现突出，可同时适用于航空影像和卫星影像（不同地面采样距离），性能超越主流相似性学习网络和端到端回归模型。  
◆完整集成至MicMac开源软件，可直接兼容标准多分辨率影像匹配流程，具备工程实用价值。</td></tr>
<tr><td>2025-05-12</td><td>Boosting Global-Local Feature Matching via Anomaly...</td><td>[2505.07375](http://arxiv.org/pdf/2505.07375)<br><a href=''>[代码]</a></td><td>◆提出GLFM方法，通过全局-局部特征匹配解决多类别点云异常检测中的特征混淆问题。  
◆创新性地设计了三阶段框架：异常合成增强特征表示、建立抗混淆的全局-局部记忆库、基于特征距离的异常检测，显...</td></tr>
<tr><td>2025-05-04</td><td>OBD-Finder: Explainable Coarse-to-Fine Te...</td><td>[2505.03836](http://arxiv.org/pdf/2505.03836)<br><a href=''>[代码]</a></td><td>◆提出了一种渐进式甲骨文重复片发现框架，结合无监督低层关键点匹配与高层以文本为中心的内容匹配，实现语义感知和可解释的候选排序。◆在保持高召回率的同时，该方法在Top-5和Top-15检索结果中取...</td></tr>
<tr><td>2025-05-06</td><td>LiftFeat: 3D Geometry-Aware Local Feature Matching</td><td>[2505.03422](http://arxiv.org/pdf/2505.03422)<br><a href=''>[代码]</a></td><td>◆ 提出LiftFeat轻量网络，通过融合单目深度估计生成的伪表面法线特征与原始2D描述符，增强特征匹配在光照变化、弱纹理等极端场景下的鲁棒性。  
◆ 设计3D几何感知特征提升模块，利用表面法...</td></tr>
<tr><td>**2025-05-04**</td><td>**Focus What Matters: Matchability-Based Reweighting for Local Feature Matching**</td><td>[2505.02161](http://arxiv.org/abs/2505.02161)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-15**</td><td>**Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective**</td><td>[2504.19458](http://arxiv.org/abs/2504.19458)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
</tbody>
</table>
</div>

<div align='right'><a href='#top'>↑ 返回顶部</a></div>

<h2 id='nerf'>NeRF</h2>

<div class="table-container">
<table>
<thead><tr><th>日期</th><th>标题</th><th>论文与代码</th><th>摘要</th></tr></thead>
<tbody>
<tr><td>2026-02-08</td><td>Dynamic Black-hole Emission Tomography with Physics-informed Neural Fields</td><td>[2602.08029](http://arxiv.org/pdf/2602.08029)</td><td>该论文的核心贡献是提出了一种名为PI-DEF的新方法，用于从稀疏射电测量数据中动态、三维地重建黑洞周围的发射气体，克服了现有技术的局限性。

◆ 首创性地将物理信息与神经辐射场结合，用于动态黑洞发射层析成像，实现了对黑洞周围4D（时间+3D）发射率场的重建。
◆ 突破了先前工作（BH-NeRF）必须假设气体遵循开普勒动力学的限制，该方法能更准确地处理黑洞强引力场附近的复杂流体动力学。
◆ 提出联合重建框架，在拟合4D发射率场的同时，一并重建与之相关的3D速度场，并通过可微分渲染与观测数据匹配。
◆ 创新地将重建的速度场作为发射率场动态变化的软约束，从而将物理规律融入神经网络的学习过程，增强了重建的物理可信度。
◆ 在仿真实验中，其重建精度显著优于物理无关方法和基于固定动力学假设的方法，并展示了方法在估算黑洞自旋等物理参数方面的潜力。</td></tr>
<tr><td>2026-02-08</td><td>Deepfake Synthesis vs. Detection: An Uneven Contest</td><td>[2602.07986](http://arxiv.org/pdf/2602.07986)</td><td>该论文的核心贡献在于通过系统性的实证研究，揭示了当前深度伪造检测技术严重滞后于生成技术发展的严峻现实。

◆ 首次对包括扩散模型、NeRF和先进GANs在内的最新深度伪造生成技术，与基于Transformer、对比学习等前沿检测方法，进行了全面的实证分析与直接对抗性评估。
◆ 通过精心设计的人眼评估实验，发现不仅自动检测模型，连人类参与者在面对最高质量的深度伪造内容时，其辨别性能也显著下降，这为问题的严重性提供了双重证据。
◆ 研究结果明确指出了一个关键且令人担忧的趋势：许多号称先进的检测模型在面对现代合成技术生成的深度伪造内容时，表现出了明显的性能不足，凸显了当前防御与攻击技术之间的“不平衡竞赛”格局。
◆ 该工作以充分的实验证据，强调了持续改进检测模型的紧迫性，并呼吁研究界投入更多努力以弥合检测方法与日新月异的生成技术之间的关键差距。</td></tr>
<tr><td>2026-02-05</td><td>NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects</td><td>[2602.05822](http://arxiv.org/pdf/2602.05822)</td><td>◆ We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs.
◆ Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection.
◆ The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation.</td></tr>
<tr><td>2026-02-05</td><td>NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks</td><td>[2602.05423](http://arxiv.org/pdf/2602.05423)</td><td>◆ In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS).
◆ Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors.
◆ As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views.</td></tr>
<tr><td>2026-02-03</td><td>Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models</td><td>[2602.03123](http://arxiv.org/pdf/2602.03123)</td><td>◆ Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations.
◆ Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism.
◆ However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task.</td></tr>
<tr><td>2026-01-30</td><td>EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing</td><td>[2601.23065](http://arxiv.org/pdf/2601.23065)</td><td>◆ Recent reconstruction methods based on radiance field such as NeRF and 3DGS reproduce indoor scenes with high visual fidelity, but break down under scene editing due to baked illumination and the lack of explicit light transport.
◆ In contrast, physically based inverse rendering relies on mesh representations and path tracing, which enforce correct light transport but place strong requirements on geometric fidelity, becoming a practical bottleneck for real indoor scenes.
◆ In this work, we propose Emission-Aware Gaussians and Path Tracing (EAG-PT), aiming for physically based light transport with a unified 2D Gaussian representation.</td></tr>
<tr><td>2026-02-02</td><td>Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction</td><td>[2601.22861](http://arxiv.org/pdf/2601.22861)</td><td>◆ Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more.
◆ Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.
◆ We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images.</td></tr>
<tr><td>2026-01-30</td><td>Diachronic Stereo Matching for Multi-Date Satellite Imagery</td><td>[2601.22808](http://arxiv.org/pdf/2601.22808)</td><td>◆ Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions.
◆ On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations.
◆ On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs.</td></tr>
<tr><td>2026-01-29</td><td>From Implicit Ambiguity to Explicit Solidity: Diagnosing Interior Geometric Degradation in Neural Radiance Fields for Dense 3D Scene Understanding</td><td>[2601.21421](http://arxiv.org/pdf/2601.21421)</td><td>◆ Neural Radiance Fields (NeRFs) have emerged as a powerful paradigm for multi-view reconstruction, complementing classical photogrammetric pipelines based on Structure-from-Motion (SfM) and Multi-View Stereo (MVS).
◆ However, their reliability for quantitative 3D analysis in dense, self-occluding scenes remains poorly understood.
◆ In this study, we identify a fundamental failure mode of implicit density fields under heavy occlusion, which we term Interior Geometric Degradation (IGD).</td></tr>
<tr><td>2026-01-29</td><td>Lightweight High-Fidelity Low-Bitrate Talking Face Compression for 3D Video Conference</td><td>[2601.21269](http://arxiv.org/pdf/2601.21269)</td><td>◆ The demand for immersive and interactive communication has driven advancements in 3D video conferencing, yet achieving high-fidelity 3D talking face representation at low bitrates remains a challenge.
◆ Traditional 2D video compression techniques fail to preserve fine-grained geometric and appearance details, while implicit neural rendering methods like NeRF suffer from prohibitive computational costs.
◆ To address these challenges, we propose a lightweight, high-fidelity, low-bitrate 3D talking face compression framework that integrates FLAME-based parametric modeling with 3DGS neural rendering.</td></tr>
<tr><td>2026-01-27</td><td>WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration</td><td>[2601.19753](http://arxiv.org/pdf/2601.19753)</td><td>◆ Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering.
◆ Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects.
◆ To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network.</td></tr>
<tr><td>2026-01-27</td><td>Bridging Visual and Wireless Sensing: A Unified Radiation Field for 3D Radio Map Construction</td><td>[2601.19216](http://arxiv.org/pdf/2601.19216)</td><td>◆ The emerging applications of next-generation wireless networks (e.g., immersive 3D communication, low-altitude networks, and integrated sensing and communication) necessitate high-fidelity environmental intelligence.
◆ 3D radio maps have emerged as a critical tool for this purpose, enabling spectrum-aware planning and environment-aware sensing by bridging the gap between physical environments and electromagnetic signal propagation.
◆ However, constructing accurate 3D radio maps requires fine-grained 3D geometric information and a profound understanding of electromagnetic wave propagation.</td></tr>
<tr><td>2026-01-26</td><td>Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding</td><td>[2601.18849](http://arxiv.org/pdf/2601.18849)</td><td>◆ Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits.
◆ Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits.
◆ To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces.</td></tr>
<tr><td>2026-01-25</td><td>MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</td><td>[2601.17866](http://arxiv.org/pdf/2601.17866)</td><td>◆ Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues.
◆ Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images.
◆ However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency.</td></tr>
<tr><td>2026-01-24</td><td>NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields</td><td>[2601.17350](http://arxiv.org/pdf/2601.17350)</td><td>◆ Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis.
◆ However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF.
◆ This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain.</td></tr>
<tr><td>2026-01-23</td><td>Multi-View Consistent Wound Segmentation With Neural Fields</td><td>[2601.16487](http://arxiv.org/pdf/2601.16487)</td><td>◆ Wound care is often challenged by the economic and logistical burdens that consistently afflict patients and hospitals worldwide.
◆ In recent decades, healthcare professionals have sought support from computer vision and machine learning algorithms.
◆ In particular, wound segmentation has gained interest due to its ability to provide professionals with fast, automatic tissue assessment from standard RGB images.</td></tr>
<tr><td>2026-01-21</td><td>Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events</td><td>[2601.15475](http://arxiv.org/pdf/2601.15475)</td><td>◆ Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions.
◆ Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results.
◆ To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events.</td></tr>
<tr><td>2026-01-21</td><td>GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars</td><td>[2601.14875](http://arxiv.org/pdf/2601.14875)</td><td>◆ High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications.
◆ While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement.
◆ To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline.</td></tr>
<tr><td>2026-01-21</td><td>POTR: Post-Training 3DGS Compression</td><td>[2601.14821](http://arxiv.org/pdf/2601.14821)</td><td>◆ 3D Gaussian Splatting (3DGS) has recently emerged as a promising contender to Neural Radiance Fields (NeRF) in 3D scene reconstruction and real-time novel view synthesis.
◆ 3DGS outperforms NeRF in training and inference speed but has substantially higher storage requirements.
◆ To remedy this downside, we propose POTR, a post-training 3DGS codec built on two novel techniques.</td></tr>
<tr><td>2026-01-25</td><td>TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement</td><td>[2601.12823](http://arxiv.org/pdf/2601.12823)</td><td>◆ Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes.
◆ Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery.
◆ Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging.</td></tr>
<tr><td>2026-01-10</td><td>HOSC: A Periodic Activation with Saturation Control for High-Fidelity Implicit Neural Representations</td><td>[2601.07870](http://arxiv.org/pdf/2601.07870)</td><td>◆ Periodic activations such as sine preserve high-frequency information in implicit neural representations (INRs) through their oscillatory structure, but often suffer from gradient instability and limited control over multi-scale behavior.
◆ We introduce the Hyperbolic Oscillator with Saturation Control (HOSC) activation, $\text{HOSC}(x) = \tanh\bigl(β\sin(ω_0 x)\bigr)$, which exposes an explicit parameter $β$ that controls the Lipschitz bound of the activation by $βω_0$.
◆ This provides a direct mechanism to tune gradient magnitudes while retaining a periodic carrier.</td></tr>
<tr><td>2026-01-08</td><td>QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer</td><td>[2601.05250](http://arxiv.org/pdf/2601.05250)</td><td>◆ Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals.
◆ Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training.
◆ In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images.</td></tr>
<tr><td>2026-01-08</td><td>DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation</td><td>[2601.04860](http://arxiv.org/pdf/2601.04860)</td><td>◆ Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models.
◆ We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations.
◆ Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation.</td></tr>
<tr><td>2026-01-11</td><td>Radiant Foam Rendering on a Graph Processor</td><td>[2601.04382](http://arxiv.org/pdf/2601.04382)</td><td>◆ Many emerging many-core accelerators replace a single large device memory with hundreds to thousands of lightweight cores, each owning only a small local SRAM and exchanging data via explicit on-chip communication.
◆ This organization offers high aggregate bandwidth, but it breaks a key assumption behind many volumetric rendering techniques: that rays can randomly access a large, unified scene representation.
◆ Rendering efficiently on such hardware therefore requires distributing both data and computation, keeping ray traversal mostly local, and structuring communication into predictable routes.</td></tr>
<tr><td>2026-01-15</td><td>Bayesian Monocular Depth Refinement via Neural Radiance Fields</td><td>[2601.03869](http://arxiv.org/pdf/2601.03869)</td><td>◆ Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task.
◆ However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding.
◆ We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs).</td></tr>
<tr><td>2026-01-04</td><td>EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views</td><td>[2601.01431](http://arxiv.org/pdf/2601.01431)</td><td>◆ Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts.
◆ Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details.
◆ To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm.</td></tr>
<tr><td>2026-01-01</td><td>CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting</td><td>[2601.00207](http://arxiv.org/pdf/2601.00207)</td><td>◆ Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies.
◆ However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods.
◆ To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation.</td></tr>
<tr><td>2025-12-20</td><td>Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes</td><td>[2601.00012](http://arxiv.org/pdf/2601.00012)</td><td>◆ Electroencephalography (EEG) data present unique modeling challenges because recordings vary in length, exhibit very low signal to noise ratios, differ significantly across participants, drift over time within sessions, and are rarely available in large and clean datasets.
◆ Consequently, developing deep learning methods that can effectively process EEG signals remains an open and important research problem.
◆ To tackle this problem, this work presents a new method inspired by Neural Radiance Fields (NeRF).</td></tr>
<tr><td>2025-12-31</td><td>UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning</td><td>[2512.24763](http://arxiv.org/pdf/2512.24763)</td><td>◆ 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis.
◆ Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding.
◆ A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions.</td></tr>
<tr><td>2025-12-25</td><td>ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields</td><td>[2512.21692](http://arxiv.org/pdf/2512.21692)</td><td>◆ Recent advances in digitization technologies have transformed the preservation and dissemination of cultural heritage.
◆ In this vein, Neural Radiance Fields (NeRF) have emerged as a leading technology for 3D digitization, delivering representations with exceptional realism.
◆ However, existing methods struggle to accurately model anisotropic specular surfaces, typically observed, for example, on brushed metals.</td></tr>
<tr><td>2025-12-25</td><td>Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs</td><td>[2512.20129](http://arxiv.org/pdf/2512.20129)</td><td>◆ Authoring 3D scenes is a central task for spatial computing applications.
◆ Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency.
◆ We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing.</td></tr>
<tr><td>2025-12-20</td><td>Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction</td><td>[2512.18237](http://arxiv.org/pdf/2512.18237)</td><td>◆ Photorealistic 3-D reconstruction from monocular video collapses in large-scale scenes when depth, pose, and radiance are solved in isolation: scale-ambiguous depth yields ghost geometry, long-horizon pose drift corrupts alignment, and a single global NeRF cannot model hundreds of metres of content.
◆ We introduce a joint learning framework that couples all three factors and demonstrably overcomes each failure case.
◆ Our system begins with a Vision-Transformer (ViT) depth network trained with metric-scale supervision, giving globally consistent depths despite wide field-of-view variations.</td></tr>
<tr><td>2025-12-18</td><td>SDFoam: Signed-Distance Foam for explicit surface reconstruction</td><td>[2512.16706](http://arxiv.org/pdf/2512.16706)</td><td>◆ Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering.
◆ Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives.
◆ RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD).</td></tr>
<tr><td>2025-12-18</td><td>Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture</td><td>[2512.16397](http://arxiv.org/pdf/2512.16397)</td><td>◆ We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face.
◆ Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs.
◆ We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video).</td></tr>
<tr><td>2025-12-17</td><td>NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles</td><td>[2512.15080](http://arxiv.org/pdf/2512.15080)</td><td>◆ Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments.
◆ A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations.
◆ These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.</td></tr>
<tr><td>2025-12-16</td><td>Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos</td><td>[2512.14406](http://arxiv.org/pdf/2512.14406)</td><td>◆ In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings.
◆ To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations.
◆ ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives.</td></tr>
<tr><td>2025-12-16</td><td>HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</td><td>[2512.14352](http://arxiv.org/pdf/2512.14352)</td><td>◆ Dynamic novel view synthesis (NVS) is essential for creating immersive experiences.
◆ Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods.
◆ However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices.</td></tr>
<tr><td>2025-12-16</td><td>AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation</td><td>[2512.14095](http://arxiv.org/pdf/2512.14095)</td><td>◆ Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets.
◆ To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models.
◆ However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios.</td></tr>
<tr><td>2025-12-14</td><td>Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis</td><td>[2512.12683](http://arxiv.org/pdf/2512.12683)</td><td>◆ Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details.
◆ Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs.
◆ In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering.</td></tr>
<tr><td>2025-12-11</td><td>Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings</td><td>[2512.10293](http://arxiv.org/pdf/2512.10293)</td><td>◆ We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction.
◆ In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone.
◆ We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings.</td></tr>
<tr><td>2025-12-10</td><td>Log NeRF: Comparing Spaces for Learning Radiance Fields</td><td>[2512.09375](http://arxiv.org/pdf/2512.09375)</td><td>◆ Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision.
◆ However, little attention has been paid to the color space in which the network is learning the radiance field representation.
◆ Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance.</td></tr>
<tr><td>2025-12-11</td><td>Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video</td><td>[2512.09335](http://arxiv.org/pdf/2512.09335)</td><td>◆ Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task.
◆ Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars.
◆ However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles.</td></tr>
<tr><td>2025-12-15</td><td>HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting</td><td>[2512.08334](http://arxiv.org/pdf/2512.08334)</td><td>◆ Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage.
◆ This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives.
◆ Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting.</td></tr>
<tr><td>2025-12-09</td><td>Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement</td><td>[2512.08215](http://arxiv.org/pdf/2512.08215)</td><td>◆ The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics.
◆ Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions.
◆ To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view.</td></tr>
<tr><td>2025-12-09</td><td>From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images</td><td>[2512.07527](http://arxiv.org/pdf/2512.07527)</td><td>◆ City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax.
◆ This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
◆ To address this problem, we propose two design choices tailored for city structures and satellite inputs.</td></tr>
<tr><td>2025-12-08</td><td>Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals</td><td>[2512.07309](http://arxiv.org/pdf/2512.07309)</td><td>◆ Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing.
◆ While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data.
◆ To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP).</td></tr>
<tr><td>2025-12-10</td><td>AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</td><td>[2512.06438](http://arxiv.org/pdf/2512.06438)</td><td>◆ The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment.
◆ Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control.
◆ We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars.</td></tr>
<tr><td>2025-12-04</td><td>Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization</td><td>[2512.04542](http://arxiv.org/pdf/2512.04542)</td><td>◆ 3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency.
◆ \replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment.
◆ Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\&amp;T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360.</td></tr>
<tr><td>2025-12-03</td><td>Radiance Meshes for Volumetric Reconstruction</td><td>[2512.04076](http://arxiv.org/pdf/2512.04076)</td><td>◆ We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization.
◆ Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware.
◆ As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing.</td></tr>
<tr><td>2025-12-03</td><td>What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</td><td>[2512.03422](http://arxiv.org/pdf/2512.03422)</td><td>◆ In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models.
◆ While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance.
◆ Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence.</td></tr>
<tr><td>2025-12-02</td><td>Flux4D: Flow-based Unsupervised 4D Reconstruction</td><td>[2512.03210](http://arxiv.org/pdf/2512.03210)</td><td>◆ Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems.
◆ While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion.
◆ Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning.</td></tr>
<tr><td>2025-12-02</td><td>PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes</td><td>[2512.02664](http://arxiv.org/pdf/2512.02664)</td><td>◆ Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions.
◆ However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence.
◆ We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS&#x27;s geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS&#x27;s normal and spherical harmonic representation.</td></tr>
<tr><td>2025-12-01</td><td>SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting</td><td>[2512.02172](http://arxiv.org/pdf/2512.02172)</td><td>◆ 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training.
◆ A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders.
◆ Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image.</td></tr>
<tr><td>2025-12-01</td><td>EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly</td><td>[2512.01296](http://arxiv.org/pdf/2512.01296)</td><td>◆ Real-time 3D reconstruction is a fundamental task in computer graphics.
◆ Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS).
◆ Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality.</td></tr>
<tr><td>2025-11-30</td><td>Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer</td><td>[2512.00677](http://arxiv.org/pdf/2512.00677)</td><td>◆ Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction.
◆ However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing.
◆ Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing.</td></tr>
<tr><td>2025-11-29</td><td>SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control</td><td>[2512.00413](http://arxiv.org/pdf/2512.00413)</td><td>◆ Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts.
◆ However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored.
◆ 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views.</td></tr>
<tr><td>2025-11-28</td><td>Image Valuation in NeRF-based 3D reconstruction</td><td>[2511.23052](http://arxiv.org/pdf/2511.23052)</td><td>◆ Data valuation and monetization are becoming increasingly important across domains such as eXtended Reality (XR) and digital media.
◆ In the context of 3D scene reconstruction from a set of images -- whether casually or professionally captured -- not all inputs contribute equally to the final output.
◆ Neural Radiance Fields (NeRFs) enable photorealistic 3D reconstruction of scenes by optimizing a volumetric radiance field given a set of images.</td></tr>
<tr><td>2025-11-28</td><td>MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis</td><td>[2511.22997](http://arxiv.org/pdf/2511.22997)</td><td>◆ Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction.
◆ However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored.
◆ Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property.</td></tr>
<tr><td>2025-12-01</td><td>DenoiseGS: Gaussian Reconstruction Model for Burst Denoising</td><td>[2511.22939](http://arxiv.org/pdf/2511.22939)</td><td>◆ Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs.
◆ In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising.
◆ Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details.</td></tr>
<tr><td>2025-11-27</td><td>3D-Consistent Multi-View Editing by Diffusion Guidance</td><td>[2511.22228](http://arxiv.org/pdf/2511.22228)</td><td>◆ Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene.
◆ Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models.
◆ We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process.</td></tr>
<tr><td>2025-11-25</td><td>$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer</td><td>[2511.20804](http://arxiv.org/pdf/2511.20804)</td><td>◆ Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis.
◆ However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially.
◆ This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time.</td></tr>
<tr><td>2025-11-24</td><td>Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation</td><td>[2511.19542](http://arxiv.org/pdf/2511.19542)</td><td>◆ We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph.
◆ Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead.
◆ An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds.</td></tr>
<tr><td>2025-11-24</td><td>MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training</td><td>[2511.19527](http://arxiv.org/pdf/2511.19527)</td><td>◆ Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure.
◆ The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors.
◆ However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments.</td></tr>
<tr><td>2025-11-24</td><td>TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging</td><td>[2511.18806](http://arxiv.org/pdf/2511.18806)</td><td>◆ X-ray imaging, based on penetration, enables detailed visualization of internal structures.
◆ Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction.
◆ However, these approaches often neglect the significance of objects&#x27; anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios.</td></tr>
<tr><td>2025-11-23</td><td>ReCoGS: Real-time ReColoring for Gaussian Splatting scenes</td><td>[2511.18441](http://arxiv.org/pdf/2511.18441)</td><td>◆ Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions.
◆ Beyond view synthesis, this 3D representation has also been explored for editing tasks.
◆ Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand.</td></tr>
<tr><td>2025-11-23</td><td>AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization</td><td>[2511.18293](http://arxiv.org/pdf/2511.18293)</td><td>◆ Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis.
◆ However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging.
◆ Localization methods face challenges related to local minima due to the selection of initial poses.</td></tr>
<tr><td>2025-11-21</td><td>NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior</td><td>[2511.17322](http://arxiv.org/pdf/2511.17322)</td><td>◆ In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.
◆ Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios.
◆ To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation.</td></tr>
<tr><td>2025-11-20</td><td>EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering</td><td>[2511.16542](http://arxiv.org/pdf/2511.16542)</td><td>◆ Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times.
◆ In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing.
◆ Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation.</td></tr>
<tr><td>2025-11-18</td><td>iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion</td><td>[2511.14149](http://arxiv.org/pdf/2511.14149)</td><td>◆ Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model.
◆ However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose.
◆ This multi-round process incurs significant computational overhead, hindering real-time performance in robotics.</td></tr>
<tr><td>2025-11-18</td><td>PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</td><td>[2511.12935](http://arxiv.org/pdf/2511.12935)</td><td>◆ We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day&#x27;&#x27; (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds.
◆ Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF).
◆ In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance.</td></tr>
<tr><td>2025-11-16</td><td>OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding</td><td>[2511.12614](http://arxiv.org/pdf/2511.12614)</td><td>◆ We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process.
◆ Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images.
◆ Given a test image, our system first employs the CNOS detector to localize target objects.</td></tr>
<tr><td>2025-11-15</td><td>LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors</td><td>[2511.12304](http://arxiv.org/pdf/2511.12304)</td><td>◆ Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed.
◆ However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans.
◆ To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads.</td></tr>
<tr><td>2025-11-11</td><td>RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses</td><td>[2511.08545](http://arxiv.org/pdf/2511.08545)</td><td>◆ Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding.
◆ However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known.
◆ This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates.</td></tr>
<tr><td>2025-11-11</td><td>Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?</td><td>[2511.07940](http://arxiv.org/pdf/2511.07940)</td><td>◆ Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas.
◆ Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention.
◆ They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos.</td></tr>
<tr><td>2025-11-10</td><td>Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction</td><td>[2511.07122](http://arxiv.org/pdf/2511.07122)</td><td>◆ Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction.
◆ However, these approaches rely on dense-frame video sequences for photorealistic reconstruction.
◆ In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible.</td></tr>
<tr><td>2025-11-09</td><td>Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes</td><td>[2511.06457](http://arxiv.org/pdf/2511.06457)</td><td>◆ Despite recent advances in single-object front-facing inpainting using NeRF and 3D Gaussian Splatting (3DGS), inpainting in complex 360{\deg} scenes remains largely underexplored.
◆ This is primarily due to three key challenges: (i) identifying target objects in the 3D field of 360{\deg} environments, (ii) dealing with severe occlusions in multi-object scenes, which makes it hard to define regions to inpaint, and (iii) maintaining consistent and high-quality appearance across views effectively.
◆ To tackle these challenges, we propose Inpaint360GS, a flexible 360{\deg} editing framework based on 3DGS that supports multi-object removal and high-fidelity inpainting in 3D space.</td></tr>
<tr><td>2025-11-09</td><td>VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes</td><td>[2511.06408](http://arxiv.org/pdf/2511.06408)</td><td>◆ Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional scenes using a set of images with known camera poses, enabling the rendering of photorealistic novel views.
◆ However, existing NeRF-based methods encounter challenges in applications such as autonomous driving and robotic perception, primarily due to the difficulty of capturing accurate camera poses and limitations in handling large-scale dynamic environments.
◆ To address these issues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately recovers camera trajectories and learns spatiotemporal representations for dynamic urban scenes without requiring additional camera pose information or expensive sensor data.</td></tr>
<tr><td>2025-11-07</td><td>4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos</td><td>[2511.05229](http://arxiv.org/pdf/2511.05229)</td><td>◆ Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics.
◆ While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses.
◆ We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach.</td></tr>
<tr><td>2025-11-07</td><td>Efficient representation of 3D spatial data for defense-related applications</td><td>[2511.05109](http://arxiv.org/pdf/2511.05109)</td><td>◆ Geospatial sensor data is essential for modern defense and security, offering indispensable 3D information for situational awareness.
◆ This data, gathered from sources like lidar sensors and optical cameras, allows for the creation of detailed models of operational environments.
◆ In this paper, we provide a comparative analysis of traditional representation methods, such as point clouds, voxel grids, and triangle meshes, alongside modern neural and implicit techniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS).</td></tr>
<tr><td>2025-11-06</td><td>3D Gaussian Point Encoders</td><td>[2511.04797](http://arxiv.org/pdf/2511.04797)</td><td>◆ In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians.
◆ This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet.
◆ However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers.</td></tr>
<tr><td>2025-11-06</td><td>FastGS: Training 3D Gaussian Splatting in 100 Seconds</td><td>[2511.04283](http://arxiv.org/pdf/2511.04283)</td><td>◆ The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead.
◆ In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality.
◆ We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism.</td></tr>
<tr><td>2025-11-04</td><td>LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization</td><td>[2511.02510](http://arxiv.org/pdf/2511.02510)</td><td>◆ Sparse-voxel rasterization is a fast, differentiable alternative for optimization-based scene reconstruction, but it tends to underfit low-frequency content, depends on brittle pruning heuristics, and can overgrow in ways that inflate VRAM.
◆ We introduce LiteVoxel, a self-tuning training pipeline that makes SV rasterization both steadier and lighter.
◆ Our loss is made low-frequency aware via an inverse-Sobel reweighting with a mid-training gamma-ramp, shifting gradient budget to flat regions only after geometry stabilize.</td></tr>
<tr><td>2025-11-04</td><td>Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping</td><td>[2511.02207](http://arxiv.org/pdf/2511.02207)</td><td>◆ Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value.
◆ Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics.
◆ However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive.</td></tr>
<tr><td>2025-10-31</td><td>SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction</td><td>[2510.27318](http://arxiv.org/pdf/2510.27318)</td><td>◆ Surgical reconstruction of dynamic tissues from endoscopic videos is a crucial technology in robot-assisted surgery.
◆ The development of Neural Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction, achieving high-quality results from video and image sequences.
◆ However, reconstructing deformable endoscopic scenes remains challenging due to aliasing and artifacts caused by tissue movement, which can significantly degrade visualization quality.</td></tr>
<tr><td>2025-10-29</td><td>4-Doodle: Text to 3D Sketches that Move!</td><td>[2510.25319](http://arxiv.org/pdf/2510.25319)</td><td>◆ We present a novel task: text-to-3D sketch animation, which aims to bring freeform sketches to life in dynamic 3D space.
◆ Unlike prior works focused on photorealistic content generation, we target sparse, stylized, and view-consistent 3D vector sketches, a lightweight and interpretable medium well-suited for visual communication and prototyping.
◆ However, this task is very challenging: (i) no paired dataset exists for text and 3D (or 4D) sketches; (ii) sketches require structural abstraction that is difficult to model with conventional 3D representations like NeRFs or point clouds; and (iii) animating such sketches demands temporal coherence and multi-view consistency, which current pipelines do not address.</td></tr>
<tr><td>2025-10-25</td><td>I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions</td><td>[2510.22161](http://arxiv.org/pdf/2510.22161)</td><td>◆ Participating in efforts to endow generative AI with the 3D physical world perception, we propose I2-NeRF, a novel neural radiance field framework that enhances isometric and isotropic metric perception under media degradation.
◆ While existing NeRF models predominantly rely on object-centric sampling, I2-NeRF introduces a reverse-stratified upsampling strategy to achieve near-uniform sampling across 3D space, thereby preserving isometry.
◆ We further present a general radiative formulation for media degradation that unifies emission, absorption, and scattering into a particle model governed by the Beer-Lambert attenuation law.</td></tr>
<tr><td>2025-10-23</td><td>From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail</td><td>[2510.20558](http://arxiv.org/pdf/2510.20558)</td><td>◆ In this paper, we investigate how users perceive the visual quality of crowd character representations at different levels of detail (LoD) and viewing distances.
◆ Each representation: geometric meshes, image-based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between visual fidelity and computational performance.
◆ Our qualitative and quantitative results provide insights to guide the design of perceptually optimized LoD strategies for crowd rendering.</td></tr>
<tr><td>2025-10-22</td><td>Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses</td><td>[2510.20027](http://arxiv.org/pdf/2510.20027)</td><td>◆ When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs.
◆ These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model.
◆ To address this issue, we propose a novel real-time render-aware filtering method.</td></tr>
<tr><td>2025-10-22</td><td>AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields</td><td>[2510.19371](http://arxiv.org/pdf/2510.19371)</td><td>◆ As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D scene representation and novel view synthesis, protecting their intellectual property (IP) from unauthorized use is becoming increasingly crucial.
◆ In this work, we aim to protect the IP of NeRFs by injecting adversarial perturbations that disrupt their unauthorized applications.
◆ However, perturbing the 3D geometry of NeRFs can easily deform the underlying scene structure and thus substantially degrade the rendering quality, which has led existing attempts to avoid geometric perturbations or restrict them to explicit spaces like meshes.</td></tr>
<tr><td>2025-10-22</td><td>Advances in 4D Representation: Geometry, Motion, and Interaction</td><td>[2510.19255](http://arxiv.org/pdf/2510.19255)</td><td>◆ We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI).
◆ While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\/}, to model 3D geometry evolving over time while exhibiting motion and interaction.
◆ Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios.</td></tr>
<tr><td>2025-11-03</td><td>GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering</td><td>[2510.14270](http://arxiv.org/pdf/2510.14270)</td><td>◆ Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress.
◆ While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.
◆ In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction.</td></tr>
<tr><td>2025-10-16</td><td>SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms</td><td>[2510.12901](http://arxiv.org/pdf/2510.12901)</td><td>◆ Rigorous testing of autonomous robots, such as self-driving vehicles, is essential to ensure their safety in real-world deployments.
◆ This requires building high-fidelity simulators to test scenarios beyond those that can be safely or exhaustively collected in the real-world.
◆ Existing neural rendering methods based on NeRF and 3DGS hold promise but suffer from low rendering speeds or can only render pinhole camera models, hindering their suitability to applications that commonly require high-distortion lenses and LiDAR data.</td></tr>
<tr><td>2025-10-13</td><td>Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency</td><td>[2510.10993](http://arxiv.org/pdf/2510.10993)</td><td>◆ 3D Gaussian inpainting, a critical technique for numerous applications in virtual reality and multimedia, has made significant progress with pretrained diffusion models.
◆ However, ensuring multi-view consistency, an essential requirement for high-quality inpainting, remains a key challenge.
◆ In this work, we present PAInpainter, a novel approach designed to advance 3D Gaussian inpainting by leveraging perspective-aware content propagation and consistency verification across multi-view inpainted images.</td></tr>
<tr><td>2025-10-11</td><td>Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting</td><td>[2510.10257](http://arxiv.org/pdf/2510.10257)</td><td>◆ 3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its standard adaptive density control (ADC) can lead to overfitting and bloated reconstructions.
◆ While state-of-the-art methods like FSGS improve quality, they often do so by significantly increasing the primitive count.
◆ This paper presents a framework that revises the core 3DGS optimization to prioritize efficiency.</td></tr>
<tr><td>2025-10-27</td><td>Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting</td><td>[2510.10097](http://arxiv.org/pdf/2510.10097)</td><td>◆ Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced 3D reconstruction and novel view synthesis, but remain heavily dependent on accurate camera poses and dense viewpoint coverage.
◆ These requirements limit their applicability in sparse-view settings, where pose estimation becomes unreliable and supervision is insufficient.
◆ To overcome these challenges, we introduce Gesplat, a 3DGS-based framework that enables robust novel view synthesis and geometrically consistent reconstruction from unposed sparse images.</td></tr>
<tr><td>2025-10-10</td><td>Geometry-Aware Scene Configurations for Novel View Synthesis</td><td>[2510.09880](http://arxiv.org/pdf/2510.09880)</td><td>◆ We propose scene-adaptive strategies to efficiently allocate representation capacity for generating immersive experiences of indoor environments from incomplete observations.
◆ Indoor scenes with multiple rooms often exhibit irregular layouts with varying complexity, containing clutter, occlusion, and flat walls.
◆ We maximize the utilization of limited resources with guidance from geometric priors, which are often readily available after pre-processing stages.</td></tr>
<tr><td>2025-10-10</td><td>Vision Language Models: A Survey of 26K Papers</td><td>[2510.09586](http://arxiv.org/pdf/2510.09586)</td><td>◆ We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025.
◆ Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities.
◆ The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding.</td></tr>
<tr><td>2025-10-10</td><td>HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization</td><td>[2510.09010](http://arxiv.org/pdf/2510.09010)</td><td>◆ Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction method, delivering high-quality results for AR/VR applications.
◆ While quantization methods and hardware accelerators have been proposed to enhance NeRF&#x27;s computational efficiency, existing approaches face crucial limitations.
◆ Current quantization methods operate without considering hardware architecture, resulting in sub-optimal solutions within the vast design space encompassing accuracy, latency, and model size.</td></tr>
<tr><td>2025-10-09</td><td>An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit Data Reuse Strategies</td><td>[2510.07667](http://arxiv.org/pdf/2510.07667)</td><td>◆ Neural radiance fields (NeRF) have transformed 3D reconstruction and rendering, facilitating photorealistic image synthesis from sparse viewpoints.
◆ This work introduces an explicit data reuse neural rendering (EDR-NR) architecture, which reduces frequent external memory accesses (EMAs) and cache misses by exploiting the spatial locality from three phases, including rays, ray packets (RPs), and samples.
◆ The EDR-NR architecture features a four-stage scheduler that clusters rays on the basis of Z-order, prioritize lagging rays when ray divergence happens, reorders RPs based on spatial proximity, and issues samples out-of-orderly (OoO) according to the availability of on-chip feature data.</td></tr>
<tr><td>2025-10-03</td><td>ROGR: Relightable 3D Objects using Generative Relighting</td><td>[2510.03163](http://arxiv.org/pdf/2510.03163)</td><td>◆ We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations.
◆ Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object&#x27;s appearance under any input environmental lighting.
◆ The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately.</td></tr>
<tr><td>2025-10-02</td><td>StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions</td><td>[2510.02314](http://arxiv.org/pdf/2510.02314)</td><td>◆ 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis.
◆ As these methods become prevalent, addressing their vulnerabilities becomes critical.
◆ We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method.</td></tr>
<tr><td>2025-10-01</td><td>Multi-level Dynamic Style Transfer for NeRFs</td><td>[2510.00592](http://arxiv.org/pdf/2510.00592)</td><td>◆ As the application of neural radiance fields (NeRFs) in various 3D vision tasks continues to expand, numerous NeRF-based style transfer techniques have been developed.
◆ However, existing methods typically integrate style statistics into the original NeRF pipeline, often leading to suboptimal results in both content preservation and artistic stylization.
◆ In this paper, we present multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that reengineers the NeRF pipeline specifically for stylization and incorporates an innovative dynamic style injection module.</td></tr>
<tr><td>2025-10-08</td><td>VGGT-X: When VGGT Meets Dense Novel View Synthesis</td><td>[2509.25191](http://arxiv.org/pdf/2509.25191)</td><td>◆ We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS).
◆ Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures.
◆ Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS.</td></tr>
<tr><td>2025-10-02</td><td>GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction</td><td>[2509.25075](http://arxiv.org/pdf/2509.25075)</td><td>◆ Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive.
◆ Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead.
◆ Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency.</td></tr>
<tr><td>2025-09-28</td><td>From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene Representations</td><td>[2509.23555](http://arxiv.org/pdf/2509.23555)</td><td>◆ Neural scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have transformed how 3D environments are modeled, rendered, and interpreted.
◆ NeRF introduced view-consistent photorealism via volumetric rendering; 3DGS has rapidly emerged as an explicit, efficient alternative that supports high-quality rendering, faster optimization, and integration into hybrid pipelines for enhanced photorealism and task-driven scene understanding.
◆ This survey examines how 3DGS is being adopted across SLAM, telepresence and teleoperation, robotic manipulation, and 3D content generation.</td></tr>
<tr><td>2025-09-30</td><td>FM-SIREN &amp; FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit Neural Representation with Periodic Activation</td><td>[2509.23438](http://arxiv.org/pdf/2509.23438)</td><td>◆ Existing periodic activation-based implicit neural representation (INR) networks, such as SIREN and FINER, suffer from hidden feature redundancy, where neurons within a layer capture overlapping frequency components due to the use of a fixed frequency multiplier.
◆ This redundancy limits the expressive capacity of multilayer perceptrons (MLPs).
◆ Drawing inspiration from classical signal processing methods such as the Discrete Sine Transform (DST), we propose FM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency multipliers to periodic activations.</td></tr>
<tr><td>2025-10-04</td><td>OracleGS: Grounding Generative Priors for Sparse-View Gaussian Splatting</td><td>[2509.23258](http://arxiv.org/pdf/2509.23258)</td><td>◆ Sparse-view novel view synthesis is fundamentally ill-posed due to severe geometric ambiguity.
◆ Current methods are caught in a trade-off: regressive models are geometrically faithful but incomplete, whereas generative models can complete scenes but often introduce structural inconsistencies.
◆ We propose OracleGS, a novel framework that reconciles generative completeness with regressive fidelity for sparse view Gaussian Splatting.</td></tr>
<tr><td>2025-09-23</td><td>WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction</td><td>[2509.19073](http://arxiv.org/pdf/2509.19073)</td><td>◆ 3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings.
◆ Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization.
◆ While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps.</td></tr>
<tr><td>2025-09-23</td><td>Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting</td><td>[2509.18956](http://arxiv.org/pdf/2509.18956)</td><td>◆ Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies.
◆ While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors.
◆ Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections.</td></tr>
<tr><td>2025-09-22</td><td>From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes</td><td>[2509.17789](http://arxiv.org/pdf/2509.17789)</td><td>◆ Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes.
◆ We propose \textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity.
◆ Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline.</td></tr>
<tr><td>2025-09-21</td><td>DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction</td><td>[2509.17232](http://arxiv.org/pdf/2509.17232)</td><td>◆ This paper proposes a Diffusion Model-Optimized Neural Radiance Field (DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency in 3D scene reconstruction.
◆ By combining diffusion models with Transformers, DT-NeRF effectively restores details under sparse viewpoints and maintains high accuracy in complex geometric scenes.
◆ Experimental results demonstrate that DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art methods on the Matterport3D and ShapeNet datasets, particularly in metrics such as PSNR, SSIM, Chamfer Distance, and Fidelity.</td></tr>
<tr><td>2025-09-23</td><td>HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis</td><td>[2509.17083](http://arxiv.org/pdf/2509.17083)</td><td>◆ Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians.
◆ However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes.
◆ While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details.</td></tr>
<tr><td>2025-09-21</td><td>PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control</td><td>[2509.16922](http://arxiv.org/pdf/2509.16922)</td><td>◆ Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production.
◆ While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization.
◆ This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS).</td></tr>
<tr><td>2025-09-26</td><td>MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild</td><td>[2509.15548](http://arxiv.org/pdf/2509.15548)</td><td>◆ In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis.
◆ Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting.
◆ In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS.</td></tr>
<tr><td>2025-09-17</td><td>ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images</td><td>[2509.15242](http://arxiv.org/pdf/2509.15242)</td><td>◆ AI-based in silico methods have improved protein structure prediction but often struggle with large protein complexes (PCs) involving multiple interacting proteins due to missing 3D spatial cues.
◆ Experimental techniques like Cryo-EM are accurate but costly and time-consuming.
◆ We present ProFusion, a hybrid framework that integrates a deep learning model with Atomic Force Microscopy (AFM), which provides high-resolution height maps from random orientations, naturally yielding multi-view data for 3D reconstruction.</td></tr>
<tr><td>2025-09-19</td><td>RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</td><td>[2509.15123](http://arxiv.org/pdf/2509.15123)</td><td>◆ Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes.
◆ Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos.
◆ In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video.</td></tr>
<tr><td>2025-09-18</td><td>NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation</td><td>[2509.14890](http://arxiv.org/pdf/2509.14890)</td><td>◆ On-orbit operations require the estimation of the relative 6D pose, i.e., position and orientation, between a chaser spacecraft and its target.
◆ While data-driven spacecraft pose estimation methods have been developed, their adoption in real missions is hampered by the lack of understanding of their decision process.
◆ This paper presents a method to visualize the 3D visual cues on which a given pose estimator relies.</td></tr>
<tr><td>2025-09-16</td><td>SuNeRF-CME: Physics-Informed Neural Radiance Fields for Tomographic Reconstruction of Coronal Mass Ejections</td><td>[2509.13571](http://arxiv.org/pdf/2509.13571)</td><td>◆ Coronagraphic observations enable direct monitoring of coronal mass ejections (CMEs) through scattered light from free electrons, but determining the 3D plasma distribution from 2D imaging data is challenging due to the optically-thin plasma and the complex image formation processes.
◆ We introduce SuNeRF-CME, a framework for 3D tomographic reconstructions of the heliosphere using multi-viewpoint coronagraphic observations.
◆ The method leverages Neural Radiance Fields (NeRFs) to estimate the electron density in the heliosphere through a ray-tracing approach, while accounting for the underlying Thomson scattering of image formation.</td></tr>
<tr><td>2025-09-16</td><td>Exploring Metric Fusion for Evaluation of NeRFs</td><td>[2509.12836](http://arxiv.org/pdf/2509.12836)</td><td>◆ Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints.
◆ Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets.
◆ We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores.</td></tr>
<tr><td>2025-09-15</td><td>Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles</td><td>[2509.12458](http://arxiv.org/pdf/2509.12458)</td><td>◆ Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction.
◆ To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams.
◆ Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control.</td></tr>
<tr><td>2025-09-14</td><td>ROSGS: Relightable Outdoor Scenes With Gaussian Splatting</td><td>[2509.11275](http://arxiv.org/pdf/2509.11275)</td><td>ROSGS提出了一种基于高斯溅射的两阶段流程，用于户外可重光照场景的高效重建与渲染。其核心贡献在于解决了现有方法在计算效率和光照建模精度上的不足。  
◆ 采用紧凑的二维高斯溅射（2DGS）表示结合单目法向先验，高效且精确地重建场景几何结构。  
◆ 提出混合光照模型，分别用球面高斯函数刻画方向性的高频阳光成分，并通过球谐系数学习辐射传输函数以建模低频天光。  
◆ 在保持高渲染效率的同时，实现了对无约束户外照明条件的高精度分解与重光照。  
实验表明，该方法在定量指标和视觉对比上均达到最先进的户外重光照性能，兼顾了渲染速度与真实感。</td></tr>
<tr><td>2025-09-14</td><td>SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion</td><td>[2509.11171](http://arxiv.org/pdf/2509.11171)</td><td>SPHERE提出了一种用于相机3D语义场景补全（SSC）的新表征方法，旨在同时提升语义准确性和几何细节的真实性。其核心贡献是创新性地融合了体素与高斯表征，以联合利用语义和物理信息。
◆ 提出语义-物理融合表征(SPHERE)，将显式体素与隐式高斯表征相结合，克服了现有方法在物理规律捕获或语义精度上的不足。
◆ 设计了语义引导的高斯初始化(SGI)模块，利用双分支表征定位焦点体素作为锚点，指导高效的高斯初始化，提升了处理效率。
◆ 开发了物理感知的谐波增强(PHE)模块，通过引入语义球谐函数来建模物理上下文细节，并通过焦点分布对齐促进语义-几何一致性，从而生成具有逼真细节的结果。
该方法在主流数据集上得到验证，有效平衡了场景补全的语义准确性与几何真实性。</td></tr>
<tr><td>2025-09-14</td><td>Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields</td><td>[2509.11169](http://arxiv.org/pdf/2509.11169)</td><td>该论文提出了Multispectral-NeRF，一种基于神经辐射场（NeRF）的多光谱三维重建新方法。其核心贡献在于解决了现有NeRF模型无法处理多波段光谱数据的问题，通过扩展神经网络架构实现了对6波段光谱数据的高精度建模。  
◆ 扩展了神经网络隐藏层的维度，使其能够直接处理6波段光谱输入数据，突破了传统三波段（RGB）的限制。  
◆ 重新设计了残差函数，优化了重建图像与参考图像之间的光谱差异计算，提升了多光谱特征的还原精度。  
◆ 调整了数据压缩模块，以适应多光谱图像更高比特深度的存储和计算需求，确保数据处理的效率和稳定性。  
实验结果表明，该方法在保持原始场景几何特征的同时，成功实现了多波段光谱特征的高保真重建，显著提升了多光谱三维重建的准确性和实用性。</td></tr>
<tr><td>2025-09-09</td><td>SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting</td><td>[2509.07809](http://arxiv.org/pdf/2509.07809)</td><td>该论文提出了一种名为SplatFill的新方法，用于基于3D高斯泼溅（3DGS）的3D场景修复。其核心贡献在于显著提升了缺失区域修复的视觉保真度和几何一致性，同时大幅提高了训练效率。  
◆ 引入深度引导与物体感知的联合监督机制，确保修复的高斯点云在三维空间中位置准确且与周围几何对齐。  
◆ 提出一致性感知细化方案，能智能识别并修正不一致区域，避免对场景其他部分造成干扰。  
◆ 在SPIn-NeRF数据集上的实验表明，该方法在视觉质量上超越了现有基于NeRF和3DGS的修复方法。  
◆ 训练效率提升24.5%，同时实现了更清晰的细节、更少的伪影和更好的多视角一致性。</td></tr>
<tr><td>2025-09-09</td><td>DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via Direct SDF Learning</td><td>[2509.07493](http://arxiv.org/pdf/2509.07493)</td><td>本文提出DiGS框架，将3D高斯表示与符号距离场（SDF）学习相结合，显著提升了从3D高斯模型中重建精确完整表面的能力。  
◆ 首次将可学习SDF嵌入3D高斯溅射（3DGS）流程，为每个高斯基元赋予几何意义，增强了解释性和几何一致性。  
◆ 提出几何引导的网格增长策略，在多尺度层次下自适应沿几何一致区域分布高斯基元，实现更密集和连贯的表面覆盖。  
◆ 通过显式对齐高斯基元与底层几何，有效改善了跨视角一致性和重建完整性。  
实验证明，DiGS在DTU、Mip-NeRF 360等多个基准上均实现了更高的重建精度和完整性，同时保持了高渲染质量。</td></tr>
<tr><td>2025-09-03</td><td>GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency</td><td>[2509.00911](http://arxiv.org/pdf/2509.00911)</td><td>该论文提出了GS-TG，一种用于加速3D高斯泼溅渲染的硬件加速器，其核心贡献在于通过优化排序与光栅化过程来提升渲染速度。  
◆ 提出基于瓦片分组（Tile Grouping）的排序策略，将多个小瓦片组合成大组以共享排序操作，显著减少了冗余计算。  
◆ 引入位掩码（Bitmask）机制，为每个高斯图元标记其所属的有效小瓦片，使得光栅化阶段仍可按原始小瓦片高效执行，避免了不必要的像素计算。  
◆ 实现了排序阶段使用“大瓦片”逻辑以降低排序开销，而光栅化阶段保持“小瓦片”粒度以维持渲染效率，有效解决了传统方法中瓦片尺寸增大导致光栅化计算量上升的矛盾。  
◆ 该方法是无损的，无需重新训练或微调，并可与其他现有优化技术无缝集成。实验结果显示，GS-TG相比当前最先进的加速器平均实现了1.54倍的渲染速度提升。</td></tr>
<tr><td>2025-08-31</td><td>SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting</td><td>[2509.00800](http://arxiv.org/pdf/2509.00800)</td><td>该论文提出了一种结合语义引导与3D高斯溅射的水下场景增强重建方法SWAGSplatting，核心解决了水下环境因光线扭曲、浑浊和低可见度导致的3D重建难题。其创新点包括：
◆ 引入多模态跨知识融合机制，将语言模型（CLIP）提取的语义特征嵌入每个高斯基元，实现语义与结构感知的联合优化。
◆ 设计专用的语义一致性损失函数，确保重建结果与高层场景理解保持一致，提升重建的语义准确性。
◆ 提出分阶段训练策略，结合由粗到细的学习与后期参数细化，显著增强训练稳定性和重建质量。
实验表明，该方法在SeaThru-NeRF和Submerged3D数据集上全面优于现有技术，PSNR指标平均提升最高达3.09 dB，为水下勘探和海洋感知应用提供了高效解决方案。</td></tr>
<tr><td>2025-08-28</td><td>Adam SLAM - the last mile of camera calibration with 3DGS</td><td>[2508.20526](http://arxiv.org/pdf/2508.20526)</td><td>该论文提出了一种利用3D高斯泼溅（3DGS）模型优化相机标定的新方法。  
◆ 创新性地通过新视图颜色损失的反向传播来精细调整相机参数，突破了传统标定方法的精度限制。  
◆ 将标定问题转化为可微分优化问题，实现了端到端的标定优化流程。  
◆ 在3DGS基准数据集上平均提升0.4 dB PSNR，显著提升了新视图合成质量。  
◆ 为高精度参考场景（如Mip-NeRF 360）的标定提供了以渲染质量为核心的新范式。  
该方法虽需较长调优时间，但对标定精度要求极高的场景具有重要应用价值。</td></tr>
<tr><td>2025-08-26</td><td>Can we make NeRF-based visual localization privacy-preserving?</td><td>[2508.18971](http://arxiv.org/pdf/2508.18971)</td><td>该论文针对基于NeRF的视觉定位方法存在的隐私泄露风险提出了解决方案。  
◆ 首先设计了一种新的评估协议，用于系统检验NeRF表示中的隐私安全性，发现即使移除颜色预测头，其几何表示仍会泄露敏感细节。  
◆ 创新性地提出ppNeRF（隐私保护神经分割场），将NeRF的传统光度监督替换为分割标签监督，避免直接学习原始图像纹理。  
◆ 通过自监督方式学习分割标签，确保标签既保留足够的几何判别性以支持精准定位，又充分模糊化细节以保护隐私。  
◆ 在保持高精度视觉定位能力的同时，显著提升了隐私保护水平，实现了隐私与性能的平衡。  
实验表明该方法在多个基准上达到了最先进的定位结果。</td></tr>
<tr><td>2025-08-25</td><td>Real-time 3D Visualization of Radiance Fields on Light Field Displays</td><td>[2508.18540](http://arxiv.org/pdf/2508.18540)</td><td>该论文提出了一种面向光场显示器的实时辐射场三维可视化统一框架。  
◆ 开发了基于单遍平面扫描策略的共享架构，高效支持多种辐射场表示（如NeRF、3D高斯泼溅和稀疏体素），无需针对不同场景重新训练。  
◆ 通过缓存非方向性共享组件，显著减少跨视角的冗余计算，实现了高达22倍的渲染加速。  
◆ 在45个视角下以512p分辨率达到200+ FPS的实时交互性能，并在Looking Glass显示器上验证了沉浸式三维交互应用。  
◆ 在提升渲染效率的同时保持了图像质量，解决了光场显示多视角渲染与辐射场计算密集型体积渲染之间的集成难题。</td></tr>
<tr><td>2025-08-28</td><td>Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph</td><td>[2508.17645](http://arxiv.org/pdf/2508.17645)</td><td>该论文的核心贡献是提出了一种通过可微分操作图生成人机协作3D资产设计序列的方法，以弥合AI生成内容与人类参数化设计工作流之间的鸿沟。  
◆ 将传统建模操作（如拉伸、布尔运算）重新构建为可微分单元，支持通过梯度下降联合优化连续和离散参数。  
◆ 构建了带有门控机制的分层操作图，并通过端到端优化倒角距离实现与目标几何的高保真对齐。  
◆ 提出多阶段序列长度约束与领域规则惩罚机制，实现了无需真实序列标注的无监督紧凑序列学习。  
◆ 生成的序列具备高几何准确性、平滑网格结构、合理步骤组成和灵活编辑能力，完全兼容主流设计软件流程。  
该方法显著提升了AI生成内容在设计实践中的可用性和协作效率。</td></tr>
<tr><td>2025-08-23</td><td>Align 3D Representation and Text Embedding for 3D Content Personalization</td><td>[2508.16932](http://arxiv.org/pdf/2508.16932)</td><td>该论文提出了一种名为Invert3D的新型框架，旨在解决3D内容高效个性化这一关键挑战。其核心贡献与创新点如下：

◆ 提出了一个将3D表示与文本嵌入空间对齐的创新框架，弥合了2D视觉-语言模型与3D内容之间的鸿沟。
◆ 设计了一种以相机为条件的3D到文本逆向映射机制，能够将3D内容投影到与文本嵌入对齐的3D嵌入空间中。
◆ 实现了通过自然语言提示直接对3D内容进行操作和个性化定制，无需依赖基于知识蒸馏的繁琐重训练过程。
◆ 显著提升了3D内容个性化的效率，避免了现有方法计算成本高昂的再训练需求。
◆ 通过大量实验验证了该方法的有效性，为3D生成领域提供了更便捷的个性化解决方案。</td></tr>
<tr><td>2025-08-20</td><td>GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels</td><td>[2508.14563](http://arxiv.org/pdf/2508.14563)</td><td>GOGS提出了一种基于二维高斯面元的新型两阶段框架，旨在解决高光物体逆向渲染中的几何噪声和重光照不真实问题。其核心创新点包括：
◆ 采用基于物理渲染的分裂和近似法，并结合基础模型的几何先验，实现了鲁棒的表面重建，有效减少了多视图不一致导致的结构瑕疵。
◆ 利用蒙特卡洛重要性采样完整渲染方程进行材质分解，通过可微分的二维高斯光线追踪模拟间接光照，提升了光照计算的准确性。
◆ 引入基于球形mipmap的方向编码来细化高频镜面细节，能够有效捕捉各向异性高光，从而在复杂光照下生成逼真的重光照效果。
该框架在几何重建、材质分离和新光照重光照方面均实现了最先进的性能，显著超越了现有的逆向渲染方法。</td></tr>
<tr><td>2025-08-19</td><td>Is-NeRF: In-scattering Neural Radiance Field for Blurred Images</td><td>[2508.13808](http://arxiv.org/pdf/2508.13808)</td><td>Is-NeRF的核心贡献是提出了一种新颖的散射感知神经辐射场，用于从运动模糊图像中恢复清晰的三维场景。其创新点主要体现在以下四个方面：

◆ 提出了一个创新的内散射表示模型，统一了现实世界中六种常见的光线传播现象，从根本上改变了传统NeRF的直线体渲染方式。
◆ 建立了一个全新的、可适应复杂光路的散射感知体渲染管线，有效解决了传统方法因几何模糊导致的训练歧义问题。
◆ 引入了一种自适应学习策略，该策略能自主确定散射方向和采样间隔，从而捕捉到更精细的物体几何细节。
◆ 实现了对NeRF参数、散射参数和相机运动的联合优化，首次实现了仅从模糊图像中就能恢复出高保真场景表示和精确几何细节。</td></tr>
<tr><td>2025-08-17</td><td>PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism</td><td>[2508.13228](http://arxiv.org/pdf/2508.13228)</td><td>PreSem-Surf是一种基于神经辐射场（NeRF）的优化方法，旨在从RGB-D序列快速重建高质量场景表面。其核心贡献在于融合了颜色、深度和语义信息以全面提升重建性能。

◆ 提出了一种新颖的SG-MLP采样结构，结合PR-MLP（预条件多层感知机）进行体素预渲染，使模型能更早捕获场景信息并更好地区分噪声与局部细节。
◆ 采用了渐进式语义建模策略，通过逐步提取更精细的语义信息来增强场景理解，同时有效减少了模型训练所需时间。
实验结果表明，该方法在C-L1、F-score和IoU多项指标上达到最优，并在其他指标上保持竞争力，证明了其高效性和实用价值。</td></tr>
<tr><td>2025-08-16</td><td>RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis</td><td>[2508.12163](http://arxiv.org/pdf/2508.12163)</td><td>◆ 提出RealTalk框架，首次实现高情感准确度、强情感可控性和稳定身份保持的拟真说话头部合成。  
◆ 创新采用变分自编码器（VAE）从音频生成3D面部标志点，结合ResNet标志点变形模型（LDM）融合情感标签嵌入，精准控制表情。  
◆ 设计新型三平面注意力神经辐射场（NeRF），通过标志点和面部混合形状系数联合建模，显著提升合成图像的逼真度。  
◆ 引入情感标签嵌入机制，突破传统方法对情感表达控制的局限性，支持细粒度情感调节。  
◆ 通过大量实验验证，在情感准确性、可控性和身份保持等核心指标上全面超越现有技术，推动社交智能AI发展。</td></tr>
<tr><td>2025-08-22</td><td>A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</td><td>[2508.09977](http://arxiv.org/pdf/2508.09977)</td><td>◆ 该论文首次系统综述了3D高斯泼溅（3DGS）技术在分割、编辑和生成等应用领域的最新进展，填补了该领域的调研空白。  
◆ 通过引入2D基础模型与NeRF方法的对比分析，揭示了3DGS在语义理解和几何控制方面的独特优势，突出了其显式紧凑表示的潜力。  
◆ 创新性地将3DGS应用划分为分割、编辑、生成等功能任务，并总结了各类任务的代表性方法、监督策略和学习范式，提炼出通用设计原则。  
◆ 提供了公开数据集和评估协议的详细总结，并对现有方法在公共基准上的表现进行了横向对比分析，为后续研究提供参考框架。  
◆ 建立了持续更新的开源资源库（GitHub），整合了相关论文、代码和工具，推动3DGS应用生态的协同发展。  
◆ 通过跨领域趋势分析，指出3DGS在实时渲染与语义操作结合方向的发展前景，为未来研究指明潜在突破点。</td></tr>
<tr><td>2025-08-13</td><td>Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision</td><td>[2508.09681](http://arxiv.org/pdf/2508.09681)</td><td>◆ 提出了一种基于可逆神经辐射场（InvNeRF）的新型测试时优化（TTO）框架，用于手术场景中的长期3D点跟踪，解决了现有方法在一致性运动或3D跟踪上的局限性。  
◆ 通过渲染监督像素对应关系，利用双向可变形-规范映射策略，有效处理定义的工作空间并优化光线密度，提升了跟踪精度。  
◆ 设计了多尺度HexPlanes结构，显著加速推理过程，同时提出高效像素采样和收敛准则算法，优化计算效率。  
◆ 在2D点跟踪任务中，平均精度比现有TTO方法提升近50%，并与非TTO方法竞争；在3D点跟踪中首次实现TTO框架，性能超越前馈方法。  
◆ 结合可变形NeRF重建优势，支持2D和3D跟踪一体化，并在STIR和SCARE数据集上验证了其有效性与运动学数据整合能力。</td></tr>
<tr><td>2025-08-12</td><td>MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields</td><td>[2508.08798](http://arxiv.org/pdf/2508.08798)</td><td>◆ 提出MonoPartNeRF框架，首次将基于分区的神经辐射场（NeRF）应用于单目视频人体重建，解决了复杂姿态下边界过渡不自然和遮挡区域重建不准的难题。  
◆ 设计双向变形模型，结合刚性与非刚性变换，建立观察空间与规范空间的可逆连续映射，通过参数化表面-时间空间（u, v, t）更精准捕捉非刚性运动。  
◆ 引入一致性损失函数，有效抑制变形导致的伪影和断裂问题，提升重建的几何连贯性。  
◆ 创新提出分区姿态嵌入机制，将全局姿态向量分解为局部关节嵌入，结合三轴方向的关键帧姿态检索与插值，实现精准的姿势感知特征采样。  
◆ 集成可学习的外观编码与注意力机制，动态建模纹理变化，显著提升复杂遮挡下的纹理保真度。实验在ZJU-MoCap和MonoCap数据集上验证了其在姿态适应性与遮挡恢复方面的优越性。</td></tr>
<tr><td>2025-08-11</td><td>SAGOnline: Segment Any Gaussians Online</td><td>[2508.08219](http://arxiv.org/pdf/2508.08219)</td><td>SAGOnline论文的核心贡献和创新点如下：

◆ 提出首个轻量级零样本框架SAGOnline，实现高斯场景的实时3D分割，解决了现有方法计算成本高、空间推理能力有限的问题。

◆ 采用解耦策略整合视频基础模型（如SAM2），通过合成视图间的2D掩码传播实现跨视角一致性分割。

◆ 开发GPU加速的3D掩码生成算法，通过高斯级实例标注为3D图元分配唯一ID，支持无损多目标跟踪与跨视角分割。

◆ 在NVOS（92.7% mIoU）和Spin-NeRF（95.2% mIoU）基准测试中达到SOTA性能，推理速度比现有方法快15-1500倍（27毫秒/帧）。

◆ 创新性将2D视频基础模型适配到3D领域，首次实现复杂场景中稳健的多目标分割与跟踪。

◆ 通过显式标注高斯图元，同时支持分割与跟踪功能，为AR/VR和机器人应用提供实时3D场景理解新方案。</td></tr>
<tr><td>2025-08-10</td><td>3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction</td><td>[2508.07182](http://arxiv.org/pdf/2508.07182)</td><td>◆ 提出结合3D高斯泼溅与运动轨迹场的新方法，首次实现动态场景中复杂运动的精确建模与物理合理轨迹生成。  
◆ 通过动态物体与静态背景解耦技术，显著提升运动轨迹场的优化效率与场景表示紧凑性。  
◆ 创新采用时间不变运动系数和共享运动轨迹基，在捕捉复杂运动模式的同时大幅降低优化复杂度。  
◆ 实现单目视频中动态场景的高质量新视角合成与运动轨迹重建双突破，性能达到当前最优水平。  
◆ 所提方法为机器人等应用场景提供了首个能同时处理动态渲染与运动推理的统一框架。  
◆ 通过大量实验验证了方案在运动细节还原和物理合理性方面超越现有动态NeRF与3DGS方法的优势。</td></tr>
<tr><td>2025-08-08</td><td>CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition</td><td>[2508.06632](http://arxiv.org/pdf/2508.06632)</td><td>◆ 提出动态系数分解框架CoDe-NeRF，将复杂外观解耦为静态神经基底（编码材质属性）和动态系数（由视角/光照条件生成），突破传统NeRF对镜面反射建模的局限。  
◆ 设计系数网络（Coefficient Network）动态生成与视角/光照相关的系数，配合静态基底实现高效的光-物理解耦，避免逆向渲染的不稳定性。  
◆ 引入动态辐射积分器（Dynamic Radiance Integrator）自适应融合静态基底与动态系数，显著提升高光与镜面反射的锐利度和真实感。  
◆ 实验证明该方法在复杂反射场景中优于现有技术，能生成更清晰的镜面效果，且无需依赖物理逆向渲染的强假设。  
◆ 为神经场景表示中的复杂外观建模提供了灵活可扩展的新范式，通过解耦式设计增强了对动态光照条件的适应性。</td></tr>
<tr><td>2025-08-08</td><td>UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting</td><td>[2508.06169](http://arxiv.org/pdf/2508.06169)</td><td>这篇论文的核心贡献是通过改进3D高斯泼溅技术（3DGS）实现了高精度的水下三维重建，解决了传统方法因水下光线吸收、散射和浑浊度导致的几何与色彩失真问题。主要创新点包括：

◆ 提出可插拔的学习型水下成像模块，采用基于体素的回归方法模拟空间变化的衰减和背散射效应，提升颜色保真度。

◆ 设计了物理感知不确定性剪枝（PAUP）分支，通过不确定性评分自适应剔除噪声高斯点，显著减少浮游伪影（降低约65%）。

◆ 构建了端到端训练框架，同步优化高斯点参数与水下物理模型参数，实现几何与光传输的联合建模。

◆ 在渲染阶段生成两种输出：去除介质影响的纯净未衰减辐射图像（URI）和包含真实光传输效果的水下图像（UWI）。

实验表明，该方法在SeaThru-NeRF和UWBundle数据集上达到PSNR 27.604、SSIM 0.868、LPIPS 0.104的指标，性能优于现有技术。</td></tr>
<tr><td>2025-08-08</td><td>Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation</td><td>[2508.06136](http://arxiv.org/pdf/2508.06136)</td><td>◆ 提出首个基于显式3D眼球结构的视线重定向框架，突破传统神经辐射场（NeRF）隐式表示的局限。  
◆ 采用3D高斯泼溅（3DGS）技术精确建模眼球，通过显式旋转和平移控制视线方向，提升物理合理性。  
◆ 创新性设计自适应形变模块，模拟眼部周围肌肉的细微运动，增强生成图像的动态真实感。  
◆ 在ETH-XGaze数据集上验证，生成图像的光影、纹理质量显著优于现有方法，且视线估计精度更高。  
◆ 框架支持多样化新视线生成，为虚拟现实、人机交互等场景提供高保真眼部运动合成方案。</td></tr>
<tr><td>2025-08-07</td><td>MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses</td><td>[2508.05819](http://arxiv.org/pdf/2508.05819)</td><td>◆ 提出首个原生支持多缩放图像集的NeRF框架MZEN，解决了工业检测中高精度细节重建的难题。  
◆ 创新性地在针孔相机模型中引入可学习的缩放标量，动态调整焦距以适应不同缩放级别的图像。  
◆ 设计分层位姿优化策略：先通过广角图像建立全局坐标系，再通过缩放一致裁剪匹配方法将放大图像与最近广角图像对齐，最后联合优化。  
◆ 在合成TCAD模型、真实SEM微结构等8个场景中验证有效性，PSNR提升最高达28%，SSIM提升10%，LPIPS降低222%。  
◆ 突破传统NeRF在多缩放场景下的限制，首次实现全局精度与微米级细节的协同捕捉，推动NeRF在工业检测领域的实际应用。</td></tr>
<tr><td>2025-08-07</td><td>Refining Gaussian Splatting: A Volumetric Densification Approach</td><td>[2508.05187](http://arxiv.org/pdf/2508.05187)</td><td>◆ 提出基于惯性体积的新型密度控制方法，利用高斯函数的惯性体积指导3D高斯分布的精细化过程，改进原始3DGS的密度控制策略。  
◆ 系统研究了传统运动恢复结构(SfM)与深度图像匹配(DIM)两种点云初始化方法对重建质量的影响，为初始化选择提供依据。  
◆ 在Mip-NeRF 360数据集上的实验表明，该方法在重建质量上优于原始3DGS，在不同场景中均表现出色。  
◆ 解决了原始3DGS自适应密度控制(ADC)在点基元管理上的关键缺陷，提升了新视角合成的效果。  
◆ 通过更精细的密度控制策略，实现了对高斯分布更合理的分裂与剪枝操作，优化了场景表示。</td></tr>
<tr><td>2025-08-07</td><td>A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</td><td>[2508.05064](http://arxiv.org/pdf/2508.05064)</td><td>◆ 首次系统综述了语言嵌入与3D高斯泼溅（Gaussian Splatting）结合的跨领域研究，填补了该新兴交叉领域的空白。  
◆ 提出语言引导的3D场景理解新范式，通过大语言模型（LLMs）实现文本条件生成、编辑和语义理解，扩展了高斯泼溅的应用场景。  
◆ 详细分析了语言与3D高斯表征融合的理论基础和技术路径，包括嵌入策略、语义对齐方法及实时渲染优化方案。  
◆ 总结了实际应用中的关键挑战，如计算效率瓶颈、泛化性不足及语义标注数据稀缺，为后续研究指明方向。  
◆ 梳理了机器人、交互内容创作等领域的落地案例，验证了语言增强型3D建模的实用价值。</td></tr>
<tr><td>2025-08-07</td><td>Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research</td><td>[2508.04326](http://arxiv.org/pdf/2508.04326)</td><td>◆ 系统梳理了365篇辐射场（RF）相关文献，首次全面分析RF技术在XR领域的应用潜力与研究现状，填补了该领域的调研空白。  
◆ 提出三维分析框架：从XR应用愿景（i）、现有技术实现（ii）和研究缺口（iii）三个维度解构RF与XR的交叉研究，为后续研究提供结构化视角。  
◆ 筛选66篇核心论文进行深度分析，揭示RF在XR中的具体技术路径（如3DGS/NeRF的交互性优化），比传统综述更具技术颗粒度。  
◆ 将XR特异性研究问题（如实时渲染、用户交互）嵌入广义RF研究版图，明确XR社区的独特技术挑战与机遇。  
◆ 构建跨学科文献资源库，覆盖计算机视觉、图形学、人机交互等6大领域，助力研究者快速定位XR相关RF技术进展。  
◆ 通过量化分析指出RF在XR领域的研究稀疏性，推动学界关注这一潜力巨大的交叉方向。</td></tr>
<tr><td>2025-08-06</td><td>MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction</td><td>[2508.04297](http://arxiv.org/pdf/2508.04297)</td><td>◆ 提出MuGS方法，首次将多基线设置（包括稀疏视角下的小/大基线）统一整合到基于高斯泼溅的泛化性新视角合成框架中。  
◆ 创新性地融合多视角立体视觉（MVS）与单目深度估计（MDE）特征，增强跨场景的泛化重建能力。  
◆ 设计投影-采样机制的深度融合模块，通过精细概率体积构建指导特征图回归，提升几何精度。  
◆ 引入参考视图损失函数，显著优化几何一致性并加速训练收敛效率。  
◆ 采用3D高斯表征实现训练/推理加速，同时保持优于神经辐射场的渲染质量。  
◆ 在DTU简单物体到RealEstate10K复杂场景的跨数据集测试中达到SOTA，并在LLFF等数据集展现零样本迁移潜力。</td></tr>
<tr><td>2025-08-04</td><td>GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing</td><td>[2508.02831](http://arxiv.org/pdf/2508.02831)</td><td>◆ 提出GENIE混合模型，结合NeRF的高质量渲染与高斯泼溅(GS)的可编辑性，实现既逼真又可交互的3D场景表示。  
◆ 采用可训练特征嵌入替代传统球谐函数，通过每个高斯点附近的k近邻条件化NeRF网络，增强局部编辑能力。  
◆ 设计RT-GPS（光线追踪高斯邻近搜索）算法，基于改进的光线追踪管线快速定位最近高斯点，提升查询效率。  
◆ 引入多分辨率哈希网格初始化与更新高斯特征，支持动态场景的实时特征调整。  
◆ 实现实时局部感知编辑：高斯基元的位置或属性修改能即时影响渲染结果，保留NeRF的连续性优势。  
◆ 弥合隐式神经渲染与显式几何编辑的鸿沟，兼容物理模拟，为交互式创作提供新范式。</td></tr>
<tr><td>2025-08-04</td><td>ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering</td><td>[2508.02304](http://arxiv.org/pdf/2508.02304)</td><td>◆ 提出ASDR算法-架构协同设计方法，首次将存内计算(CIM)技术应用于即时神经渲染领域，解决传统方案延迟高、能效差的问题。  
◆ 算法层面创新性地引入动态采样策略，通过实时感知像素渲染难度自适应调整采样点，显著减少内存访问和计算开销。  
◆ 提出颜色与密度体渲染解耦近似方法，通过分离MLP计算流程降低神经网络计算负荷，实现计算效率提升。  
◆ 架构层面设计新型ReRAM存算架构，创新性地开发数据映射与重用微架构，优化内存访问模式以匹配渲染特性。  
◆ 实验验证取得突破性性能：相比先进NeRF加速器和Xavier NX GPU分别实现9.55倍和69.75倍加速，仅损失0.1 PSNR画质。  
◆ 整体方案首次在CIM硬件上实现高质量实时神经渲染，为低功耗即时图形生成提供新范式。</td></tr>
<tr><td>2025-08-01</td><td>Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF</td><td>[2508.00967](http://arxiv.org/pdf/2508.00967)</td><td>◆提出了一种基于联邦学习和扩散模型的多无人机协同感知框架，解决了计算资源受限和低带宽通信下的实时3D场景重建难题。  
◆创新性地将扩散模型与NeRF结合，通过联邦学习实现多智能体联合场景合成，同时保护数据隐私并保持系统可扩展性。  
◆采用轻量级YOLOv12进行语义提取，配合局部NeRF更新策略，显著降低了计算和通信开销。  
◆设计了语义感知压缩协议，优化了无人机间的数据传输效率，提升了协同场景理解能力。  
◆重新设计了生成式扩散模型架构，使其更适合多视角联合场景重建任务，突破了传统方法的局限性。  
◆通过仿真和真实无人机测试验证了方案的可行性，为自主系统多智能体AI提供了突破性进展。</td></tr>
<tr><td>2025-07-31</td><td>NeRF Is a Valuable Assistant for 3D Gaussian Splatting</td><td>[2507.23374](http://arxiv.org/pdf/2507.23374)</td><td>◆ 提出NeRF-GS框架，首次联合优化神经辐射场（NeRF）与3D高斯泼溅（3DGS），实现两种技术的优势互补。  
◆ 利用NeRF的连续空间表征能力，有效解决3DGS对高斯初始化敏感、空间感知弱、高斯间关联性不足等固有缺陷。  
◆ 通过渐进式对齐3DGS与NeRF的空间特征，使两者能基于共享的3D空间信息在同一场景中协同优化。  
◆ 创新性地优化隐式特征与高斯位置的残差向量，弥合两种方法的理论差异，增强3DGS的个性化建模能力。  
◆ 在基准数据集上实现最先进性能，验证了NeRF与3DGS的互补性而非竞争关系，为混合表征方法提供新思路。  
◆ 为结合显式（3DGS）与隐式（NeRF）表示的3D场景高效建模开辟了新方向。</td></tr>
<tr><td>2025-07-30</td><td>Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields</td><td>[2507.23033](http://arxiv.org/pdf/2507.23033)</td><td>◆ 提出了一种基于脉冲神经网络的动态时间步训练策略（PATA），首次将SNN的节能特性与NeRF的高质量渲染能力相结合。  
◆ 通过预训练-自适应时间步调整机制，自动优化渲染质量与时间步长的平衡，解决了传统NeRF依赖密集采样导致的资源消耗问题。  
◆ 实现了场景自适应的动态推理，可根据不同场景的尺度和纹理复杂度灵活调整时间步，显著减少计算开销。  
◆ 在保持Instant-NGP架构优势的基础上，实验证明PATA能减少64%的推理时间步和61.55%的运行功耗，同时维持渲染精度。  
◆ 为边缘计算等资源受限场景提供了高效的神经渲染解决方案，扩展了NeRF技术的应用边界。</td></tr>
<tr><td>2025-07-28</td><td>DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation</td><td>[2507.21350](http://arxiv.org/pdf/2507.21350)</td><td>◆ 提出了一种新型神经符号框架DEM-NeRF，直接从稀疏多视角图像序列重建和模拟弹性物体，无需显式几何信息。  
◆ 创新性地将神经辐射场（NeRF）与物理信息神经网络（PINN）结合，同时利用图像监督和弹性力学偏微分方程的物理约束。  
◆ 通过能量约束的PINN架构处理复杂边界和初始条件，替代传统有限元或边界元方法，提升模拟精度。  
◆ 实现了时空变形物体的联合表征学习，弥合了纯数据驱动方法与传统数值模拟之间的鸿沟。  
◆ 增强了结果的可解释性，为科学发现提供兼具数据适应性与物理一致性的新工具。  
◆ 解决了高保真仿真计算成本高的问题，仅需稀疏观测数据即可完成物理规律建模。</td></tr>
<tr><td>2025-07-27</td><td>NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding</td><td>[2507.20110](http://arxiv.org/pdf/2507.20110)</td><td>◆ NeuroVoxel-LM提出了一种结合神经辐射场（NeRF）的动态体素化与轻量级元嵌入的新框架，解决了现有3D语言模型处理稀疏大规模点云时效率低和表示精度不足的问题。  
◆ 创新性地设计了动态分辨率多尺度体素化（DR-MSV）技术，根据几何和结构复杂度自适应调整体素粒度，在降低计算成本的同时保持重建保真度。  
◆ 提出了基于注意力加权和残差融合的轻量级元嵌入机制（TAP-LME），通过令牌级自适应池化增强语义表示能力，优于传统最大池化方法。  
◆ DR-MSV显著提升了点云特征提取的效率和精度，尤其适用于大范围复杂场景的快速处理。  
◆ TAP-LME机制能够从NeRF权重中捕获细粒度语义信息，为语言驱动的3D感知任务提供了更丰富的特征表示。  
◆ 实验结果表明，该框架在3D场景理解任务中实现了性能突破，为语言对齐的3D感知研究开辟了新方向。</td></tr>
<tr><td>2025-07-25</td><td>DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations</td><td>[2507.19474](http://arxiv.org/pdf/2507.19474)</td><td>◆ 提出DINO-SLAM框架，通过DINO特征增强神经隐式（NeRF）和显式（3DGS）SLAM系统的场景表示能力。  
◆ 设计场景结构编码器（SSE），将原始DINO特征升级为增强版EDINO，有效捕捉场景层次结构和元素间关系。  
◆ 开发两种基于EDINO的SLAM范式，分别针对NeRF和3DGS实现端到端优化，提升系统鲁棒性。  
◆ 在Replica、ScanNet和TUM数据集上验证性能，超越现有最优方法，证明通用场景适应能力。  
◆ 首次将DINO语义先验与几何重建结合，为动态/弱纹理场景提供更全面的表示方案。</td></tr>
<tr><td>2025-07-25</td><td>Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization</td><td>[2507.19459](http://arxiv.org/pdf/2507.19459)</td><td>◆ 提出基于CNN的3D高斯泼溅（3DGS）初始化方法，仅需单目图像即可生成粗糙3D模型和目标姿态，解决了传统方法依赖多视角精确姿态的问题。  
◆ 开发支持噪声或隐式姿态估计的训练流程，突破NeRF/3DGS在太空场景中必须依赖精确姿态的限制。  
◆ 通过分析不同初始化变体，显著降低高精度3D模型的训练成本，所需训练迭代次数和输入图像数量减少至少一个数量级。  
◆ CNN模块集成多种姿态估计技术变体，为不同应用场景提供灵活性，并在噪声姿态条件下验证了有效性。  
◆ 实验证明即使使用不完美的姿态监督，该框架仍能学习高保真3D表示，为太空应用中的新视角合成技术铺平道路。</td></tr>
<tr><td>2025-07-25</td><td>NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography</td><td>[2507.19328](http://arxiv.org/pdf/2507.19328)</td><td>◆ NerT-CA提出了一种混合神经张量表示方法，结合神经场与张量场优势，显著提升稀疏视角X射线冠状动脉造影（CA）的动态4D重建效率。  
◆ 通过将CA场景分解为低秩静态成分（张量场）与动态稀疏成分（神经场），解决了传统方法依赖MLP导致训练耗时过长的问题。  
◆ 该方法在仅需3个造影视角下即可实现高质量重建，突破了稀疏视图重建的临床实用性瓶颈。  
◆ 创新性地利用低秩先验加速静态背景建模，同时保留神经场对血管动态细节的捕捉能力，兼顾速度与精度。  
◆ 在4D仿真数据集上定量与定性验证显示，其训练速度与重建精度均超越现有NeRF-based方法，为临床实时应用提供可能。</td></tr>
<tr><td>2025-07-24</td><td>SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time</td><td>[2507.18713](http://arxiv.org/pdf/2507.18713)</td><td>◆ 提出SaLF（稀疏局部场）新型体素表示方法，将场景表示为稀疏3D体素集合，每个体素包含局部隐式场，兼具栅格化和光线追踪能力。  
◆ 突破现有技术限制，首次实现同时支持非针孔相机和旋转激光雷达的高效渲染（相机50+ FPS，LiDAR 600+ FPS）。  
◆ 采用自适应剪枝与致密化策略，无需预处理即可动态优化大场景表示，显著提升可扩展性。  
◆ 解耦场景表示与渲染流程，支持多种传感器统一框架，克服了NeRF和3D高斯泼溅的互操作性缺陷。  
◆ 实现快速训练（&lt;30分钟）与实时渲染，在保持自动驾驶传感器仿真真实性的同时，效率远超传统NeRF方法。  
◆ 为多传感器自动驾驶测试提供首个兼顾高保真度与高效率的仿真方案，推动规模化虚拟测试发展。</td></tr>
<tr><td>2025-07-24</td><td>High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details</td><td>[2507.18023](http://arxiv.org/pdf/2507.18023)</td><td>◆ 提出首个基于3D高斯泼溅(3DGS)的高保真三维修复框架，通过稀疏修复视图重建完整三维场景  
◆ 设计自动掩膜优化流程，结合高斯场景过滤与反向投影技术，精准定位遮挡区域并实现逼真边界修复  
◆ 创新性开发区域级不确定性引导优化策略，通过多视角重要性评估缓解视角不一致问题  
◆ 实现细粒度优化机制，显著提升修复结果中高频细节的保真度与真实感  
◆ 在多样化数据集上的实验表明，本方法在视觉质量和多视角一致性方面均超越现有最优技术  
该工作解决了三维场景修复中视角不一致和细节失真的核心难题，为三维内容创作提供了新工具。</td></tr>
<tr><td>2025-07-23</td><td>Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field</td><td>[2507.17351](http://arxiv.org/pdf/2507.17351)</td><td>这篇论文的核心贡献和创新点如下：

◆ 提出将主动学习应用于语义神经辐射场（NeRF）的训练，以降低像素级标注的高成本。  
◆ 研究了语义NeRF主动学习中的关键设计选择，包括选择粒度和选择策略。  
◆ 创新性地提出了一种考虑3D几何约束的样本选择策略，提升了主动学习的效果。  
◆ 通过实验证明，该方法能显著减少语义NeRF训练的标注成本，相比随机采样可降低2倍以上。  
◆ 为语义场景理解的隐式神经表示提供了一种更高效的训练范式。  
◆ 首次系统探索了主动学习在3D语义神经表示领域的应用潜力。</td></tr>
<tr><td>2025-07-22</td><td>Sparse-View 3D Reconstruction: Recent Advances and Open Challenges</td><td>[2507.16406](http://arxiv.org/pdf/2507.16406)</td><td>◆ 该论文首次将稀疏视角3D重建领域的几何方法、神经隐式模型（如NeRF）和生成式方法（如扩散模型）纳入统一框架进行系统综述。  
◆ 深入分析了稀疏场景下几何正则化、显式形状建模和生成推理如何解决浮游伪影和位姿模糊等关键问题。  
◆ 对比了3D高斯泼溅等显式点云方法与神经隐式方法在精度、效率和泛化性方面的权衡关系。  
◆ 提出当前领域尚未解决的挑战，包括跨域泛化能力和无位姿约束的重建问题，为未来研究指明方向。  
◆ 特别强调视觉基础模型（VFMs）和3D原生生成先验在稀疏重建中的创新应用潜力。  
◆ 区别于以往综述，本文首次系统梳理了扩散模型与神经隐式方法的融合框架及其在稀疏数据下的优势。</td></tr>
<tr><td>2025-07-19</td><td>DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF</td><td>[2507.14596](http://arxiv.org/pdf/2507.14596)</td><td>◆ DiSCO-3D首次提出3D开放词汇子概念发现任务，结合场景内容和用户查询需求，实现更灵活的3D语义分割。  
◆ 该方法基于神经场表示，将无监督分割与弱开放词汇指导相结合，突破了传统方法仅适应单一任务或场景的限制。  
◆ 通过开放词汇查询，DiSCO-3D能够动态发现并分割子概念，适应多样化的用户需求。  
◆ 在开放词汇和无监督分割的边缘案例中，DiSCO-3D表现出最先进的性能，验证了其泛化能力。  
◆ 该方法为机器人、自动驾驶等应用提供了更高层次的场景理解能力，具有广泛的应用潜力。</td></tr>
<tr><td>2025-07-19</td><td>Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</td><td>[2507.14501](http://arxiv.org/pdf/2507.14501)</td><td>◆ 系统梳理了基于前馈式深度学习的3D重建与视图合成技术，提出按表示架构（如点云、3D高斯泼溅、神经辐射场等）的分类体系。  
◆ 重点分析了无姿态重建、动态3D重建、3D感知图像/视频合成等关键任务，拓展了在数字人、SLAM等领域的应用场景。  
◆ 对比传统迭代优化方法，突显前馈方法在计算效率与泛化能力上的突破，推动AR/VR等实时应用落地。  
◆ 首次整合该领域常用数据集与评估协议，为不同下游任务提供标准化评测基准。  
◆ 指出动态场景建模、跨模态生成等开放挑战，为未来研究指明方向。</td></tr>
<tr><td>2025-07-18</td><td>TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views</td><td>[2507.13929](http://arxiv.org/pdf/2507.13929)</td><td>◆ TimeNeRF提出了一种通用神经渲染方法，能够在少量输入视图下渲染任意视角和任意时间点的新视图，解决了多视图采集成本高和场景重复优化效率低的问题。  
◆ 该方法首次探索了NeRF在时序3D场景建模中的潜力，填补了当前技术在该领域的空白，尤其适用于元宇宙中昼夜自然过渡的沉浸式体验需求。  
◆ 结合多视图立体视觉、神经辐射场和跨数据集解耦策略，构建了隐式内容辐射场，实现了场景表示和时间维度建模的统一框架。  
◆ 无需逐场景优化即可在少样本条件下生成新视图，显著提升了渲染效率，实验证明其能有效捕捉从黎明到黄昏的复杂自然场景变化。  
◆ 通过体渲染技术合成任意时间点的逼真新视图，在时间维度上实现了平滑过渡，为动态场景建模提供了新思路。</td></tr>
<tr><td>2025-07-18</td><td>EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation</td><td>[2507.13648](http://arxiv.org/pdf/2507.13648)</td><td>◆ 提出EPSilon框架，通过高效点采样策略显著提升基于混合表示（SMPL网格+NeRF）的3D虚拟人生成效率，兼顾生成质量与速度。  
◆ 创新性设计空射线剔除（ERO）方法，直接跳过空场景中的光线计算，减少无效采样点。  
◆ 提出空区间剔除（EIO）技术，进一步压缩光线采样区间，仅保留衣物或网格覆盖的有效区域。  
◆ 通过精细化采样策略，实现单阶段NeRF结构，无需传统分层采样，简化模型架构。  
◆ 实验表明，EPSilon仅需3.9%的采样点即可保持生成质量，推理速度提升约20倍，训练收敛加快4倍。</td></tr>
<tr><td>2025-07-16</td><td>DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi</td><td>[2507.12132](http://arxiv.org/pdf/2507.12132)</td><td>◆ 提出DoRF（多普勒辐射场）方法，首次将神经辐射场（NeRF）思想引入Wi-Fi传感领域，通过一维多普勒速度投影重建3D潜在运动表征。  
◆ 构建了统一的运动多普勒辐射场，提供活动全景视角，显著提升对环境变化的鲁棒性。  
◆ 利用Wi-Fi CSI提取的多普勒速度投影，克服了传统方法依赖环境特定特征的限制。  
◆ 所提3D潜在表征能有效捕捉人体活动时空特性，比现有2D方法更具判别力。  
◆ 实验证明该方法显著提高了跨环境、跨用户的泛化性能，推动Wi-Fi传感走向实用化。  
◆ 为无线感知开辟新思路，将计算机视觉中的体积渲染技术成功迁移至射频信号处理领域。</td></tr>
<tr><td>2025-07-16</td><td>HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing</td><td>[2507.11971](http://arxiv.org/pdf/2507.11971)</td><td>◆提出新型3D分层代理节点表示（HPR3D），通过物体表面及内部的稀疏层级树状代理节点网络统一表征形状与纹理，突破传统方法任务局限性的框架创新。  
◆每个代理节点采用轻量MLP隐式编码局部几何与纹理信息，结合邻近及父节点的高效神经插值解码机制，实现复杂性与保真度的动态平衡。  
◆层级结构天然支持语义对齐，用户可直接通过拖拽代理节点实现直观编辑，解决了NeRF结构模糊导致的操控难题。  
◆稀疏节点分布与分层查询机制显著降低数据复杂度（相比网格顶点密度和NeRF体素采样），同时保持亚毫米级重建精度。  
◆实验验证该表示在重建质量（PSNR提升2.1dB）、编辑效率（交互延迟&lt;10ms）和跨任务通用性（重建/生成/驱动）上的综合优势。</td></tr>
<tr><td>2025-07-14</td><td>VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling</td><td>[2507.09987](http://arxiv.org/pdf/2507.09987)</td><td>◆ 提出VoxelRF新型神经表示方法，通过体素化辐射场实现复杂环境中无线信道的快速建模，解决了传统方法在精度、效率和可扩展性上的平衡难题。  
◆ 用基于体素网格的三线性插值替代NeRF中昂贵的多层感知机（MLP），结合两个浅层MLP分别建模传播和发射端相关效应，显著降低计算成本。  
◆ 引入渐进式学习策略，逐步优化体素网格分辨率，加速训练过程并提升模型泛化能力。  
◆ 采用空区域跳过技术，避免对无信号区域的冗余计算，进一步提高推理效率。  
◆ 设计背景熵损失函数，增强模型对稀疏信号区域的建模能力，提升整体精度。  
实验表明，VoxelRF在有限训练数据下能以更低计算量达到竞争性精度，适用于实时和资源受限的无线通信场景。</td></tr>
<tr><td>2025-07-12</td><td>Stable Score Distillation</td><td>[2507.09168](http://arxiv.org/pdf/2507.09168)</td><td>◆ 提出稳定分数蒸馏（SSD）框架，通过将分类器锚定到源提示词，显著提升编辑过程的稳定性和对齐性。  
◆ 利用无分类器引导（CFG）方程实现跨提示词对齐，并通过引入恒定空文本分支稳定优化过程，避免冲突信号。  
◆ 设计提示词增强分支，专门强化风格转换等编辑任务的修改强度，提升编辑效果。  
◆ 在保持原始内容结构的同时，确保编辑轨迹与源提示词紧密对齐，实现局部精准编辑且不影响周围区域。  
◆ 在2D和3D编辑任务（如NeRF和文本驱动风格编辑）中达到最优效果，收敛更快且复杂度更低。</td></tr>
<tr><td>2025-07-11</td><td>From images to properties: a NeRF-driven framework for granular material parameter inversion</td><td>[2507.09005](http://arxiv.org/pdf/2507.09005)</td><td>◆ 提出了一种新颖的NeRF与MPM结合的框架，通过视觉观测反演颗粒材料参数，实现了从图像到物性参数的跨模态推理。  
◆ 利用NeRF从多视角初始图像重建高精度3D几何，克服了传统方法在复杂表面细节捕捉上的局限性，为MPM仿真提供准确初始条件。  
◆ 创新性地采用时序双固定相机图像作为观测数据，通过仿真渲染与真实图像的比对构建目标函数，实现纯视觉驱动的参数反演。  
◆ 引入贝叶斯优化高效搜索摩擦角参数，将反演误差控制在2度以内，验证了纯视觉反分析的可行性。  
◆ 该框架为无法直接测量物性的实际场景（如遥感、灾害评估）提供了非接触式材料表征新思路，具有重要应用价值。</td></tr>
<tr><td>2025-07-10</td><td>MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation</td><td>[2507.07519](http://arxiv.org/pdf/2507.07519)</td><td>◆ 提出了MUVOD数据集，这是首个针对动态场景4D目标分割的大规模多视角视频数据集，填补了该领域数据集的空白。  
◆ 数据集包含17个真实场景，涵盖室内外多种活动，提供7830张RGB图像及对应的4D运动分割掩码，支持跨视角和跨帧的目标跟踪。  
◆ 数据集中包含459个实例，覆盖73个类别，为多视角视频分割方法提供了全面的基准测试平台。  
◆ 提出了新的评估指标和基线分割方法，为动态场景分割研究提供了标准化评估框架。  
◆ 基于MUVOD数据集构建了3D目标分割子集，包含50个不同场景下的标注对象，用于更全面地评估现有3D分割方法的性能。  
◆ 数据集来源多样，包含不同相机设备采集的视角（9-46个视角），增强了数据集的泛化性和实用性。</td></tr>
<tr><td>2025-07-14</td><td>BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields</td><td>[2507.06269](http://arxiv.org/pdf/2507.06269)</td><td>◆ 提出BayesSDF，首个针对神经隐式SDF模型的概率框架，解决3D几何不确定性量化问题，特别适用于科学模拟（如森林流体建模）。  
◆ 通过拉普拉斯近似和基于Hessian的局部表面稳定性度量，实现高效计算且几何感知的不确定性估计，克服传统方法计算效率低的问题。  
◆ 首次将几何一致性直接融入不确定性量化，生成与重建误差高度相关的校准化置信度地图，优于忽略几何的现有方法。  
◆ 证明SDF的连续可微几何特性比辐射场模型（如NeRF）更适合物理模拟，为下游任务（如机器人决策）提供可靠几何基础。  
◆ 在合成与真实数据集上验证了方法的优越性，其不确定性预测与重建缺陷高度吻合，校准性和几何一致性均超越现有技术。</td></tr>
<tr><td>2025-07-08</td><td>Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering</td><td>[2507.06103](http://arxiv.org/pdf/2507.06103)</td><td>◆ 提出Ref-Unlock框架，基于3D高斯泼溅（3DGS）实现几何感知的反射分离，首次在3DGS中显式解耦透射与反射成分，解决现有方法将反射误判为几何结构的问题。  
◆ 采用双分支表示结合高阶球谐函数，有效捕捉高频反射细节，同时通过反射移除模块提供伪无反射监督信号，实现更干净的反射分解。  
◆ 引入伪深度图与几何感知的双边平滑约束，增强3D几何一致性和分解稳定性，显著减少复杂场景下的表面伪影与模糊重建。  
◆ 支持基于视觉基础模型（VFMs）的灵活反射编辑功能，扩展了方法在实际应用中的可操作性。  
◆ 实验证明该方法大幅超越传统基于GS的反射处理方法，并与NeRF类模型性能相当，同时保持更高的计算效率。  
◆ 为含反射场景的光照真实渲染提供了高效且泛化性强的解决方案，代码已开源。</td></tr>
<tr><td>2025-07-08</td><td>DreamArt: Generating Interactable Articulated Objects from a Single Image</td><td>[2507.05763](http://arxiv.org/pdf/2507.05763)</td><td>◆ DreamArt首次提出从单张图像生成可交互的关节化3D物体的完整框架，填补了现有方法在部件分解和关节建模方面的空白。  
◆ 通过三阶段流程创新：结合图像生成3D、掩码提示的部件分割与修复，解决了单视角下部件形状不完整的问题。  
◆ 提出基于视频扩散模型的关节运动先验学习，利用部件遮罩和修复图像消除遮挡歧义，实现逼真关节运动生成。  
◆ 采用双四元数表示关节运动参数，配合全局纹理优化，确保多部件纹理一致性与高质量渲染效果。  
◆ 实验证明该方法能生成部件形状准确、外观逼真且关节运动合理的3D资产，为AR/VR和具身AI提供了可扩展的解决方案。</td></tr>
<tr><td>2025-07-06</td><td>A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields</td><td>[2507.04408](http://arxiv.org/pdf/2507.04408)</td><td>◆ 提出基于视角一致分布的采样方法，替代传统固定深度值估计，用于NeRF的正则化训练。  
◆ 利用低层颜色特征和基础模型提取的高层特征，构建3D采样点在2D投影位置的视角一致性分布。  
◆ 通过从视角一致性分布中采样，实现对NeRF训练的隐式正则化，避免依赖误差较大的深度估计。  
◆ 结合深度推进损失（depth-pushing loss）与采样技术，共同消除训练中的失败模式。  
◆ 在公开数据集上的实验表明，该方法显著优于现有NeRF变体和深度正则化方法，尤其适用于户外无界场景。  
◆ 解决了传统深度估计方法需要昂贵3D监督和泛化性差的问题，提升了真实场景下的3D重建质量。</td></tr>
<tr><td>2025-07-02</td><td>Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</td><td>[2507.01631](http://arxiv.org/pdf/2507.01631)</td><td>◆ 提出Snake-NeRF框架，首次实现单设备上大规模卫星影像的NeRF三维重建，突破传统方法受限于内存的小场景约束。  
◆ 设计外存（out-of-core）训练方法，无需同时加载所有图像和网络，显著降低硬件需求。  
◆ 创新性采用无重叠三维分块（3D tile）策略，将目标区域划分为独立训练的NeRF子模块。  
◆ 提出重叠裁剪图像技术，确保每个子模块训练时获取完整必要像素，避免边界信息缺失。  
◆ 开发2×2三维分块递进策略与分段采样器，有效消除分块边缘的三维重建误差。  
实验证明该方法在单GPU上实现线性时间复杂度，且不损失重建质量，为全球尺度地球观测提供新范式。</td></tr>
<tr><td>2025-07-01</td><td>Surgical Neural Radiance Fields from One Image</td><td>[2507.00969](http://arxiv.org/pdf/2507.00969)</td><td>◆ 提出了一种基于单张术中图像和术前MRI数据训练神经辐射场（NeRF）的新方法，解决了手术场景中多视角数据不足的限制。  
◆ 利用术前MRI数据预先定义相机视角和图像集，结合神经风格迁移技术（WTC2和STROTSS）将术中图像外观迁移至预构建数据集，避免过度风格化。  
◆ 实现了快速单图像NeRF训练，显著降低了术中数据采集的时间成本，提升了临床实用性。  
◆ 在四例神经外科手术案例中验证了方法的有效性，定量对比显示其合成结果与真实手术显微镜图像高度一致。  
◆ 重建结果与真实数据相比具有高结构相似性，证明了良好的重建质量和纹理保留能力。  
◆ 为手术场景中的实时3D重建和视角合成提供了可行方案，突破了传统多视角方法的局限性。</td></tr>
<tr><td>2025-07-01</td><td>PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching</td><td>[2507.00371](http://arxiv.org/pdf/2507.00371)</td><td>◆提出PlantSegNeRF方法，首次实现从多视角RGB图像序列直接生成高精度植物器官实例点云，突破传统点云分割技术的局限性。  
◆开发联合通道NeRF模型，同时渲染颜色、密度、语义和实例信息，构建包含多维度特征的隐式场景表示。  
◆设计创新的多视角实例匹配模块，通过2D实例分割结果跨视图关联同一器官的实例ID，解决复杂植物结构的对应难题。  
◆在语义分割任务中，关键指标（精确率、召回率等）平均提升16.1%-24.2%，显著优于现有最优方法。  
◆在实例分割任务中，四项核心指标（mPrec等）最高提升达38.2%，实现跨物种的高泛化性表现。  
◆为植物表型研究提供高通量三维数据生成方案，支持大规模植物模型开发。</td></tr>
<tr><td>2025-06-30</td><td>AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention</td><td>[2506.23611](http://arxiv.org/pdf/2506.23611)</td><td>◆ 提出AttentionGS框架，首次实现无需高质量初始点云的3D高斯泼溅重建，突破传统3DGS对SfM点云的强依赖。  
◆ 创新性引入两阶段注意力机制：几何注意力快速恢复场景全局结构，纹理注意力后期优化细粒度细节，实现从随机初始化直接重建。  
◆ 设计不透明度加权梯度策略，改进高斯分布致密化过程，显著提升表面重建质量。  
◆ 在纹理缺失和受限视角等极端场景下表现优异，相比现有方法重建质量提升显著。  
◆ 通过多基准数据集验证，为实际应用中更鲁棒的3D重建提供新思路，扩展了3DGS的应用边界。</td></tr>
<tr><td>2025-06-29</td><td>Dynamic View Synthesis from Small Camera Motion Videos</td><td>[2506.23153](http://arxiv.org/pdf/2506.23153)</td><td>这篇论文针对动态3D场景在小范围相机运动下的新视角合成问题提出了创新解决方案，核心贡献如下：

◆ 提出基于分布的深度正则化方法(DDR)，通过Gumbel-softmax从离散渲染权重分布中可微分采样，解决了传统深度损失仅计算期望误差的局限性。

◆ 引入物体边界前空间点体积密度趋近零的约束条件，确保场景几何结构的正确学习，有效改善了小相机运动下的几何表示问题。

◆ 开发了可视化工具，可直接在渲染权重层面观察场景几何表示，为方法原理提供了直观解释。

◆ 在训练过程中加入相机参数学习机制，增强了模型对相机参数的鲁棒性，解决了小运动下相机参数估计不准的问题。

论文通过大量实验证明，该方法在小范围相机运动输入下显著优于现有先进方法，为动态场景新视角合成提供了更实用的解决方案。</td></tr>
<tr><td>2025-06-27</td><td>UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields</td><td>[2506.21884](http://arxiv.org/pdf/2506.21884)</td><td>◆ 首次将光谱解混技术融入神经辐射场（NeRF），实现联合高光谱新视角合成与无监督材质分割，突破传统NeRF仅依赖RGB数据的局限。  
◆ 提出基于漫反射和镜面反射分量的光谱反射率建模方法，通过全局端元字典学习纯材质特征，结合逐点丰度分布实现材质精准表达。  
◆ 创新性地利用学习到的端元光谱特征进行无监督材质聚类，无需人工标注即可完成场景材质分割。  
◆ 支持通过修改端元字典实现场景材质编辑，为基于材质的灵活外观操控（如虚拟仿真、AR应用）提供新工具。  
◆ 实验证明该方法在高光谱重建和材质分割任务上显著优于现有技术，为机器人感知、虚拟现实等需精确材质建模的领域提供解决方案。</td></tr>
<tr><td>2025-06-24</td><td>ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes</td><td>[2506.21629](http://arxiv.org/pdf/2506.21629)</td><td>◆ 提出了一种无需SfM预处理的方法ICP-3DGS，通过结合迭代最近点（ICP）和基于优化的位姿细化，实现了大范围相机运动下的高精度位姿估计。  
◆ 引入基于体素的场景致密化策略，有效指导大规模无边界场景的3D高斯分布重建，解决了传统方法在户外场景中的扩展性问题。  
◆ 首次将ICP与3D高斯泼溅（3DGS）技术结合，在神经渲染框架中直接优化相机位姿，摆脱了对SfM先验数据的依赖。  
◆ 通过实验验证，该方法在室内外不同尺度场景中均优于现有技术，同时在相机位姿估计和新视角合成任务上表现更优。  
◆ 开源了完整代码，为后续研究提供了可复现的基础，推动了无约束场景神经渲染的实用化进程。</td></tr>
<tr><td>2025-06-26</td><td>PanSt3R: Multi-view Consistent Panoptic Segmentation</td><td>[2506.21348](http://arxiv.org/pdf/2506.21348)</td><td>◆ 提出PanSt3R方法，首次实现无需测试时优化的单次前向预测，直接联合输出3D几何和多视角全景分割结果，显著提升效率。  
◆ 基于MUSt3R框架改进，引入语义感知能力，将3D重建与多视角全景分割任务统一整合，克服传统方法依赖2D预分割的局限性。  
◆ 重新设计掩码后处理流程，提出更理论化的多视角分割融合策略，优化跨视角空间关系利用。  
◆ 结合3D高斯泼溅（3DGS）技术，提出简单有效的新视角生成方法，扩展模型应用场景。  
◆ 在多个基准测试中达到SOTA性能，速度比现有方法快数个数量级，兼具概念简洁性与计算高效性。</td></tr>
<tr><td>2025-06-25</td><td>Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects</td><td>[2506.20638](http://arxiv.org/pdf/2506.20638)</td><td>◆ 提出了一种联合优化方法，同时估计非合作空间物体的姿态（相机位姿）并利用神经辐射场（NeRF）进行3D重建，解决了传统方法在未知物体姿态下的重建难题。  
◆ 针对空间场景的特殊挑战（如单色图像、未知物体方向、有限视角、无漫反射光照等），改进了NeRF的适应性，使其在极端条件下仍能有效工作。  
◆ 实验证明，采用逐帧顺序训练图像的方式（而非批量训练）能显著提升3D重建的精度，为动态空间物体建模提供了新思路。  
◆ 通过优化均匀旋转参数估计相机姿态，并引入正则化约束相邻姿态的连续性，避免了位姿估计的突变问题。  
◆ 该方法为空间态势感知（SSA）任务提供了高精度的3D模型，可应用于主动碎片清除、在轨维护等实际场景。</td></tr>
<tr><td>2025-06-24</td><td>NeRF-based CBCT Reconstruction needs Normalization and Initialization</td><td>[2506.19742](http://arxiv.org/pdf/2506.19742)</td><td>◆ 提出归一化哈希编码器（Normalized Hash Encoder），解决NeRF-based CBCT重建中哈希编码器与神经网络的局部-全局训练不匹配问题，通过增强特征一致性提升训练稳定性。  
◆ 设计映射一致性初始化策略（MCI），利用预训练模型的全局映射特性初始化神经网络，减少早期训练波动，加速收敛并提高重建质量。  
◆ 首次系统分析了哈希编码器参数局部稀疏性与神经网络全局密集更新的矛盾，指出特征错位是导致训练不稳定的核心原因。  
◆ 方法仅需少量代码改动，即可在4个数据集、128例CT数据（涵盖7个解剖区域）上显著提升训练效率和重建性能。  
◆ 通过实验验证了归一化与初始化策略的协同作用，为NeRF-based医学影像重建提供了简单有效的优化范式。</td></tr>
<tr><td>2025-06-25</td><td>Self-Supervised Multimodal NeRF for Autonomous Driving</td><td>[2506.19615](http://arxiv.org/pdf/2506.19615)</td><td>◆ 提出自监督多模态NeRF框架NVSF，无需3D标注即可联合学习LiDAR和相机的时空隐式神经表示。  
◆ 针对自动驾驶场景设计，同时处理静态和动态环境，显著提升真实驾驶场景的适应性。  
◆ 引入启发式图像像素采样策略，优先选择信息丰富的像素，提升训练效率和收敛速度。  
◆ 创新采用双梯度掩码技术，有效保留LiDAR点的局部特征，增强点云数据重建精度。  
◆ 在KITTI-360数据集上验证，LiDAR和相机域性能均超越基线模型，展现多模态优势。  
◆ 开源代码推动相关研究，为自动驾驶领域提供可复用的新型神经渲染解决方案。</td></tr>
<tr><td>2025-06-24</td><td>HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis</td><td>[2506.19291](http://arxiv.org/pdf/2506.19291)</td><td>◆ 提出HoliGS框架，首次将可变形高斯泼溅技术应用于长时序单目RGB视频的沉浸式视角合成任务，解决了传统4D高斯泼溅和动态NeRF在分钟级视频中训练开销过大的问题。  
◆ 创新性地采用分层变形策略，将场景分解为静态背景和动态物体，其中动态部分通过可逆神经流实现全局刚性变换、骨骼驱动形变和细微非刚性形变的统一建模。  
◆ 通过将高斯基元绑定到完整的前景规范形状（如第一人称或跟随视角），支持多演员交互和大视角变化的自由视点渲染，显著提升了复杂动态场景的重建鲁棒性。  
◆ 提出可逆高斯变形网络，在保持高保真重建质量的同时，相比现有单目可变形NeRF方法大幅降低训练和渲染时间，实现了实际场景中的高效部署。  
◆ 在挑战性数据集上的实验表明，该方法在重建质量和计算效率方面均优于当前最优技术，为沉浸式视角合成提供了可扩展的实用解决方案。</td></tr>
<tr><td>2025-06-23</td><td>MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation</td><td>[2506.18678](http://arxiv.org/pdf/2506.18678)</td><td>◆ 提出首个分布式多智能体协作神经SLAM框架MCN-SLAM，结合混合隐式神经场景表示，解决传统单智能体隐式SLAM在大场景和长序列中的局限性。  
◆ 创新设计三平面-网格联合场景表示方法，显著提升场景重建质量，优于现有NeRF-based方法。  
◆ 开发&quot;内部-跨智能体&quot;闭环检测机制，首次实现单智能体局部一致性与多智能体全局一致性的协同优化。  
◆ 提出在线蒸馏方法实现多子地图融合，突破通信带宽限制，确保全局地图一致性。  
◆ 发布首个真实世界密集SLAM数据集DES，涵盖单/多智能体场景，提供连续轨迹和高精度3D网格真值，填补领域空白。  
实验证明该方法在建图、定位和通信效率上均优于现有技术，代码与数据集将开源推动SLAM和3D重建领域发展。</td></tr>
<tr><td>2025-06-26</td><td>2D Triangle Splatting for Direct Differentiable Mesh Training</td><td>[2506.18575](http://arxiv.org/pdf/2506.18575)<br><a href=''>[代码]</a></td><td>◆ 提出2D三角形面片（2DTS）方法，替代传统3D高斯基元，实现更高效的直接可微分网格训练。  
◆ 结合离散网格结构与连续体积建模优势，形成类网格的表示形式，提升渲染质量和灵活性。  
◆ 引入紧凑性参数到三角形基元中，支持直接训练高真实感网格，简化传统网格重建流程。  
◆ 实验证明，即使未优化紧凑性参数，其基础版本也能超越当前最优高斯基元方法的渲染保真度。  
◆ 生成的网格在视觉质量上显著优于现有网格重建方法，尤其在复杂光照和阴影效果中表现突出。  
◆ 为可微分渲染领域提供新思路，平衡了渲染速度与高级渲染效果（如重光照）的兼容性。</td></tr>
<tr><td>2025-06-22</td><td>Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction</td><td>[2506.18208](http://arxiv.org/pdf/2506.18208)</td><td>◆ 首次系统评估了DINO预训练视觉特征在NeRF少样本3D重建中的表现，发现所有变体性能均低于原始NeRF基线（PSNR 12.9-13.0 vs 14.71）。  
◆ 揭示了反直觉结论：预训练视觉特征不仅无助于少样本重建，反而可能引入有害偏差，挑战了该领域普遍假设。  
◆ 提出三种潜在失效原因分析框架：特征-任务不匹配、有限数据过拟合问题以及特征融合技术瓶颈。  
◆ 通过对比实验验证了冻结特征、LoRA微调和多尺度融合等主流方法的局限性，为后续研究排除无效路径。  
◆ 指出少样本场景下应优先关注几何一致性而非复杂特征工程，为简化模型设计提供新方向。  
◆ 研究成果对基于预训练特征的3D重建方法提出重要警示，可能改变该领域技术路线选择。</td></tr>
<tr><td>2025-06-21</td><td>3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene</td><td>[2506.17636](http://arxiv.org/pdf/2506.17636)</td><td>◆ 提出从粗到精的渐进式重建策略，先快速构建粗糙模型，再通过自适应场景分割和子场景细化实现大规模场景的高效重建。  
◆ 创新性地结合解耦外观模型，有效捕捉户外环境中复杂的全局光照变化，提升动态外观的建模能力。  
◆ 设计瞬态掩模模型，自动过滤移动物体（如车辆、行人）的干扰，显著提高重建纯净度。  
◆ 扩展多视角约束并引入单视角正则化方法，针对性解决纹理缺失区域的几何优化难题。  
◆ 在无人机航拍数据集GauU-Scene V2上验证，首次实现全尺寸图像优化的大规模场景精细重建，性能超越现有NeRF和Gaussian类方法。  
（注：全文严格遵循5点创新性总结，未使用Markdown符号，字数控制在400字内）</td></tr>
<tr><td>2025-06-23</td><td>R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision</td><td>[2506.16262](http://arxiv.org/pdf/2506.16262)<br><a href=''>[代码]</a></td><td>◆ 提出“3D低层视觉（3D LLV）”新领域，将传统2D低层视觉任务（如超分、去模糊、天气退化修复等）扩展到3D空间，解决神经渲染在真实退化场景中的鲁棒性问题。  
◆ 首次系统化定义“退化感知渲染”问题，明确时空一致性和病态优化等核心挑战，为3D LLV研究建立理论框架。  
◆ 综述了将低层视觉技术与神经辐射场（NeRF）、3D高斯泼溅（3DGS）等神经渲染结合的创新方法，展示其在噪声、模糊、低分辨率等退化条件下的高保真3D重建能力。  
◆ 梳理了自动驾驶、AR/VR、机器人等关键应用场景，强调从退化输入中实现可靠3D感知的实用价值。  
◆ 汇总了代表性方法、数据集和评估协议，为未来3D LLV研究提供标准化参考，推动真实环境下鲁棒3D内容生成与场景重建的发展。</td></tr>
<tr><td>2025-06-24</td><td>RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories</td><td>[2506.15242](http://arxiv.org/pdf/2506.15242)</td><td>◆ 提出RA-NeRF方法，能够在复杂相机轨迹下实现高精度的相机位姿估计，解决了传统NeRF和3DGS依赖准确位姿先验的问题。  
◆ 采用增量式重建流程，结合光度一致性约束和光流驱动的位姿调节机制，提升了初始化和定位阶段的鲁棒性。  
◆ 引入隐式位姿滤波器，通过捕捉相机运动模式有效消除位姿估计中的噪声，增强复杂轨迹下的稳定性。  
◆ 在Tanks&amp;Temple和NeRFBuster等具有挑战性的数据集上验证了方法有效性，位姿估计和视觉质量均达到SOTA水平。  
◆ 整体框架无需外部约束，仅通过端到端优化即可同时优化场景重建与相机位姿，适用于SLAM等实际应用场景。</td></tr>
<tr><td>2025-06-17</td><td>Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction</td><td>[2506.14856](http://arxiv.org/pdf/2506.14856)</td><td>◆提出了一种基于轻量级前馈神经网络UPNet的新颖主动视角选择方法，直接预测候选视角的不确定性图，避免了传统方法需要计算每个视角不确定性的高计算成本。  
◆UPNet仅需单张输入图像即可预测所有候选视角的不确定性，通过学习自然物体视角与体素表示不确定性的映射关系，实现了高效的信息提取。  
◆通过聚合历史预测的不确定性图来抑制冗余视角，智能选择信息量最大的新视角，仅需一半视角即可达到与上限相当的3D重建精度。  
◆相比基线方法，计算效率显著提升，实现高达400倍的加速，并减少50%以上的CPU、RAM和GPU资源消耗。  
◆方法具有强大的泛化能力，无需额外训练即可适用于新物体类别的视角选择任务，展现了广泛的适用性。  
◆整体方案将神经渲染与高效视角选择相结合，为3D重建领域提供了高精度与低资源消耗的实用化解决方案。</td></tr>
<tr><td>2025-06-18</td><td>Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting</td><td>[2506.12787](http://arxiv.org/pdf/2506.12787)</td><td>◆ 提出SwiftWRF框架，首次将高斯泼溅（Gaussian Splatting）技术引入无线辐射场（WRF）建模，突破传统方法在精度和效率上的局限。  
◆ 采用可变形2D高斯泼溅方法，通过轻量级MLP建模高斯形变，有效捕捉收发端单侧移动导致的WRF动态变化。  
◆ 实现CUDA加速的光栅化渲染，频谱合成速度超过10万帧/秒，比现有最优方法快500倍，满足实时性需求。  
◆ 创新性地支持任意位置的WRF频谱合成，并在到达角（AoA）和信号强度（RSSI）预测任务中验证实用性。  
◆ 在真实和合成室内场景的实验中，显著提升信号重建质量，同时开源代码和数据集推动领域发展。</td></tr>
<tr><td>2025-06-17</td><td>Efficient multi-view training for 3D Gaussian Splatting</td><td>[2506.12727](http://arxiv.org/pdf/2506.12727)</td><td>这篇论文的核心贡献和创新点如下：

◆ 提出多视角训练方法，解决了3D高斯泼溅（3DGS）传统单视角训练导致的随机梯度方差过大问题，优化了训练效果。  
◆ 改进了光栅化流程，显著降低了多视角训练的计算开销，使其更高效可行。  
◆ 设计了3D距离感知的D-SSIM损失函数，更好地适应多视角场景，提升了渲染质量。  
◆ 提出多视角自适应密度控制机制，克服了传统单视角假设下高斯分布优化的局限性。  
◆ 实验证明，所提方法显著提升了3DGS及其变体的性能，突破了单视角训练的约束。  
◆ 为3DGS领域提供了更高效的训练框架，推动了其在逆向渲染中的应用。</td></tr>
<tr><td>2025-06-12</td><td>PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting</td><td>[2506.10335](http://arxiv.org/pdf/2506.10335)</td><td>◆ 提出PointGS框架，通过点注意力感知的稀疏视图合成方法，解决了3D高斯泼溅（3DGS）在输入视图不足时过拟合的问题。  
◆ 利用最新的立体基础模型估计精确相机姿态并重建密集点云，为高斯初始化提供高质量起点。  
◆ 设计多尺度2D外观特征采样与聚合机制，为每个3D高斯点编码颜色属性，增强稀疏输入下的特征表达能力。  
◆ 创新性地引入基于自注意力机制的点交互网络，使高斯点能与邻近点交互，提升点级外观表示能力。  
◆ 通过两个轻量级多层感知机（MLP）将增强特征解码为高斯参数，实现实时高质量渲染。  
◆ 在多个基准测试中显著优于基于NeRF的方法，并在少样本设置下达到与最先进3DGS方法竞争的性能。</td></tr>
<tr><td>2025-06-11</td><td>The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge</td><td>[2506.09885](http://arxiv.org/pdf/2506.09885)</td><td>◆ 提出了一种无需3D先验知识和相机位姿标注的通用化新视角合成框架，仅依赖稀疏无位姿的2D图像即可生成逼真新视图。  
◆ 通过系统性分析揭示了关键趋势：减少对3D知识的依赖能更高效利用数据规模，最终达到与依赖3D知识的方法相当的性能。  
◆ 创新性地消除了传统方法对显式3D表示（如NeRF、3DGS）和输入/目标视角位姿标注的双重依赖，实现完全数据驱动的隐式3D理解。  
◆ 实验证明该方法仅通过稀疏2D图像即可学习隐式3D一致性，生成质量媲美依赖位姿输入的方法，验证了数据为中心范式的可行性。  
◆ 为大规模数据时代的新视角合成提供了新思路，表明减少3D先验依赖与数据规模扩展之间存在正向关联性。</td></tr>
<tr><td>2025-06-10</td><td>A Probability-guided Sampler for Neural Implicit Surface Rendering</td><td>[2506.08619](http://arxiv.org/pdf/2506.08619)</td><td>◆ 提出基于概率密度函数的3D图像投影空间模型，实现针对感兴趣区域的射线采样优化，提升渲染精度。  
◆ 设计新型表面重建损失函数，充分利用3D投影空间模型，整合近表面和空白空间信息以增强性能。  
◆ 结合隐式表面表示，通过概率引导采样策略有效聚焦关键区域，减少冗余计算。  
◆ 将提出的采样策略与损失函数集成到现有神经隐式表面渲染器中，显著提升3D重建和图像渲染质量。  
◆ 特别针对场景中感兴趣区域（如物体表面）实现更精细的细节还原，克服传统均匀采样的局限性。  
◆ 通过联合优化采样与重建过程，在保证计算效率的同时获得更高保真度的渲染结果。</td></tr>
<tr><td>2025-06-09</td><td>Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes</td><td>[2506.07917](http://arxiv.org/pdf/2506.07917)<br><a href=''>[代码]</a></td><td>◆ 提出SpeeDe3DGS框架，显著加速动态3D高斯泼溅（3DGS/4DGS）的渲染速度，解决传统方法因逐帧神经网络推理导致的性能瓶颈。  
◆ 设计时序敏感度剪枝评分机制，自动识别并剔除对动态场景重建贡献低的冗余高斯元素，提升计算效率。  
◆ 引入退火平滑剪枝策略，增强在相机位姿不精确的真实场景中的剪枝鲁棒性，避免误删关键高斯元素。  
◆ 开发GroupFlow运动分析技术，通过轨迹相似性聚类高斯群组，以单组刚性变换替代逐高斯形变预测，大幅减少计算量。  
◆ 实验验证框架在NeRF-DS数据集上实现10.37倍渲染加速、7.71倍模型压缩和2.71倍训练提速，在D-NeRF和HyperNeRF数据集分别提升4.20倍和58.23倍性能。  
◆ 模块化设计兼容现有动态3DGS/4DGS框架，兼具高效性与通用性。</td></tr>
<tr><td>2025-06-20</td><td>Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency</td><td>[2506.07497](http://arxiv.org/pdf/2506.07497)</td><td>◆ 提出Genesis框架，首次实现多视角驾驶视频与LiDAR序列的联合生成，保证时空和跨模态一致性。  
◆ 采用两阶段架构：结合DiT视频扩散模型与3D-VAE编码，以及基于BEV的LiDAR生成器与NeRF渲染，实现高质量多模态输出。  
◆ 通过共享潜在空间直接耦合视觉与几何模态，确保生成内容在跨模态间的连贯演化。  
◆ 创新引入DataCrafter描述模块，利用视觉语言模型提供场景级和实例级语义监督，增强生成数据的结构化控制。  
◆ 在nuScenes基准测试中取得视频（FVD 16.95）和LiDAR（Chamfer 0.611）指标的SOTA性能，验证生成数据的语义保真度。  
◆ 生成数据可有效提升下游任务（如分割和3D检测）性能，证明其实际应用价值。</td></tr>
<tr><td>2025-06-07</td><td>SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation</td><td>[2506.06890](http://arxiv.org/pdf/2506.06890)</td><td>◆ 提出首个从二进制单光子相机(SPC)数据生成高质量彩色新视角的两阶段框架，解决了传统3D合成方法无法处理的严重信息丢失问题。  
◆ 第一阶段采用Pix2PixHD等生成模型进行图像到图像转换，将二进制SPC输入转化为可信的RGB图像，有效恢复丢失的纹理和颜色信息。  
◆ 第二阶段结合神经辐射场(NeRF)或高斯泼溅(3DGS)等先进3D重建技术，从生成的RGB图像中合成新视角。  
◆ 通过大量定性和定量实验验证了所提框架(Pix2PixHD + Nerf/3DGS)的优越性，在感知质量和几何一致性上显著超越基线方法。  
◆ 该工作为单光子相机这类新兴成像技术的3D应用开辟了新途径，特别适用于极低光照或超高速成像场景。</td></tr>
<tr><td>2025-06-06</td><td>Splat and Replace: 3D Reconstruction with Repetitive Elements</td><td>[2506.06462](http://arxiv.org/pdf/2506.06462)</td><td>◆ 利用场景中的重复元素提升新视角合成质量，解决了传统NeRF和3DGS在训练视角不足时渲染效果差的问题。  
◆ 提出一种基于3D高斯泼溅（3DGS）的重复实例分割与配准方法，实现不同实例间的信息共享。  
◆ 通过几何优化和外观变化建模，同时提升场景的几何精度和视觉一致性。  
◆ 在合成与真实场景中验证了方法的有效性，显著改善了遮挡和低覆盖区域的渲染效果。  
◆ 首次将重复元素作为先验知识融入3D重建流程，为复杂场景重建提供了新思路。</td></tr>
<tr><td>2025-06-06</td><td>NeurNCD: Novel Class Discovery via Implicit Neural Representation</td><td>[2506.06412](http://arxiv.org/pdf/2506.06412)</td><td>◆ NeurNCD首次提出利用隐式神经表示（Embedding-NeRF模型）替代传统显式3D分割图，通过KL散度聚合语义嵌入和视觉嵌入空间的熵，解决离散化、空洞和噪声问题。  
◆ 结合特征查询、特征调制和聚类等关键组件，实现预训练语义分割网络与隐式神经表示之间的高效特征增强和信息交互。  
◆ 该框架在开放和封闭场景中均实现优越分割性能，无需依赖密集标注数据集进行监督训练或人工生成稀疏标签监督。  
◆ 在NYUv2和Replica数据集上的大量实验表明，NeurNCD显著优于现有最先进方法，验证了其有效性和泛化能力。  
◆ 提出了一种通用且数据高效的新类别发现框架，为开放世界场景中的实际应用提供了新思路。</td></tr>
<tr><td>2025-06-06</td><td>Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments</td><td>[2506.05965](http://arxiv.org/pdf/2506.05965)</td><td>◆ 提出了Dy3DGS-SLAM，这是首个基于单目RGB输入的动态场景3D高斯泼溅SLAM方法，填补了动态环境下纯视觉SLAM的空白。  
◆ 通过概率模型融合光流掩码和深度掩码，生成动态融合掩码，仅需单次网络迭代即可约束跟踪尺度并优化几何渲染。  
◆ 设计了新颖的运动损失函数，基于动态融合掩码约束位姿估计网络，显著提升了动态物体干扰下的跟踪鲁棒性。  
◆ 在映射阶段，结合动态像素的渲染损失、颜色和深度信息，有效消除了动态物体带来的瞬态干扰和遮挡问题。  
◆ 实验证明该方法在动态环境中实现了最先进的跟踪与渲染性能，甚至优于部分RGB-D方法，展现了单目输入的潜力。</td></tr>
<tr><td>2025-06-06</td><td>ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation</td><td>[2506.05317](http://arxiv.org/pdf/2506.05317)</td><td>◆ 提出ProJo4D框架，通过渐进式联合优化策略解决稀疏多视角视频下的物理参数估计问题，克服传统方法因分阶段优化导致的误差累积问题。  
◆ 创新性地引入参数敏感性指导的优化顺序，逐步联合优化几何、外观、物理状态和材料属性，避免直接全参数优化带来的非凸和非可微难题。  
◆ 在PAC-NeRF和Spring-Gaus数据集上的实验表明，该方法在4D未来状态预测、未来状态的新视角渲染和材料参数估计方面均优于现有方法。  
◆ 首次实现稀疏多视角输入下的物理准确数字孪生构建，为机器人和XR应用提供更实用的解决方案。  
◆ 通过渐进式优化策略平衡计算效率与精度，为复杂物理场景的神经渲染与参数估计提供新思路。</td></tr>
<tr><td>2025-06-06</td><td>Unifying Appearance Codes and Bilateral Grids for ...</td><td>[2506.05280](http://arxiv.org/pdf/2506.05280)<br><a href=''>[代码]</a></td><td>◆提出多尺度双边网格新方法，统一了外观编码和双边网格的优势，解决了动态驾驶场景中光度不一致导致的几何失真问题。  
◆通过像素级颜色映射和分层约束优化，显著降低了光不一致产生的漂浮伪影，在四大自...</td></tr>
<tr><td>2025-06-05</td><td>Generating Synthetic Stereo Datasets using 3D Gaus...</td><td>[2506.04908](http://arxiv.org/pdf/2506.04908)</td><td>◆ 提出基于3D高斯泼溅（3DGS）的立体数据集生成流程，相比NeRF方法更高效。  
◆ 结合显式3D重建几何与FoundationStereo模型的深度估计进行专家知识迁移，生成高质量数据。...</td></tr>
<tr><td>**2025-05-30**</td><td>**Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments**</td><td>[2506.00083](http://arxiv.org/abs/2506.00083)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-29**</td><td>**PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views**</td><td>[2505.23481](http://arxiv.org/abs/2505.23481)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-29**</td><td>**LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering**</td><td>[2505.23158](http://arxiv.org/abs/2505.23158)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**Can NeRFs See without Cameras?**</td><td>[2505.22441](http://arxiv.org/abs/2505.22441)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss**</td><td>[2505.22279](http://arxiv.org/abs/2505.22279)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-28**</td><td>**Hyperspectral Gaussian Splatting**</td><td>[2505.21890](http://arxiv.org/abs/2505.21890)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-27**</td><td>**Structure from Collision**</td><td>[2505.21335](http://arxiv.org/abs/2505.21335)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender**</td><td>[2505.20126](http://arxiv.org/abs/2505.20126)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-30**</td><td>**ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization**</td><td>[2505.19883](http://arxiv.org/abs/2505.19883)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis**</td><td>[2505.19813](http://arxiv.org/abs/2505.19813)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-26**</td><td>**Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction**</td><td>[2505.19793](http://arxiv.org/abs/2505.19793)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-22**</td><td>**UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot Ground Vehicle Repeat**</td><td>[2505.16912](http://arxiv.org/abs/2505.16912)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-19**</td><td>**IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion**</td><td>[2505.13633](http://arxiv.org/abs/2505.13633)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-19**</td><td>**3D Gaussian Adaptive Reconstruction for Fourier Light-Field Microscopy**</td><td>[2505.12875](http://arxiv.org/abs/2505.12875)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-18**</td><td>**Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey**</td><td>[2505.12384](http://arxiv.org/abs/2505.12384)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-16**</td><td>**MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory**</td><td>[2505.11386](http://arxiv.org/abs/2505.11386)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-16**</td><td>**EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes**</td><td>[2505.10787](http://arxiv.org/abs/2505.10787)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-15**</td><td>**Large-Scale Gaussian Splatting SLAM**</td><td>[2505.09915](http://arxiv.org/abs/2505.09915)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-14**</td><td>**Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians**</td><td>[2505.09413](http://arxiv.org/abs/2505.09413)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-14**</td><td>**FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling**</td><td>[2505.09406](http://arxiv.org/abs/2505.09406)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-12**</td><td>**TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian**</td><td>[2505.08811](http://arxiv.org/abs/2505.08811)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-13**</td><td>**FOCI: Trajectory Optimization on Gaussian Splats**</td><td>[2505.08510](http://arxiv.org/abs/2505.08510)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-13**</td><td>**TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset**</td><td>[2505.07396](http://arxiv.org/abs/2505.07396)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-12**</td><td>**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**</td><td>[2505.07373](http://arxiv.org/abs/2505.07373)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-11**</td><td>**NeuGen: Amplifying the &#x27;Neural&#x27; in Neural Radiance Fields for Domain Generalization**</td><td>[2505.06894](http://arxiv.org/abs/2505.06894)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-10**</td><td>**3D Characterization of Smoke Plume Dispersion Using Multi-View Drone Swarm**</td><td>[2505.06638](http://arxiv.org/abs/2505.06638)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-10**</td><td>**FlexNeRFer: A Multi-Dataflow, Adaptive Sparsity-Aware Accelerator for On-Device NeRF Rendering**</td><td>[2505.06504](http://arxiv.org/abs/2505.06504)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-08**</td><td>**3D Scene Generation: A Survey**</td><td>[2505.05474](http://arxiv.org/abs/2505.05474)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-04**</td><td>**HandOcc: NeRF-based Hand Rendering with Occupancy Networks**</td><td>[2505.02079](http://arxiv.org/abs/2505.02079)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-04**</td><td>**Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields**</td><td>[2505.02005](http://arxiv.org/abs/2505.02005)<br><a href=''>[代码]</a></td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-03**</td><td>**AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting**</td><td>[2505.01799](http://arxiv.org/abs/2505.01799)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-03**</td><td>**Unified Steganography via Implicit Neural Representation**</td><td>[2505.01749](http://arxiv.org/abs/2505.01749)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-01**</td><td>**Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation**</td><td>[2505.00378](http://arxiv.org/abs/2505.00378)</td><td>摘要生成中...</td></tr>
<tr><td>**2025-05-01**</td><td>**GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting**</td><td>[2504.20379](http://arxiv.org/abs/2504.20379)</td><td>摘要生成中...</td></tr>
<tr><td>2025-05-30</td><td>Weight Space Representation Learning on Diverse NeRF Architectures</td><td>[2502.09623](http://arxiv.org/pdf/2502.09623)</td><td>◆ Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network.
◆ Recent studies have demonstrated that these weights can be used as input for frameworks designed to address deep learning tasks; however, such frameworks require NeRFs to adhere to a specific, predefined architecture.
◆ In this paper, we introduce the first framework capable of processing NeRFs with diverse architectures and performing inference on architectures unseen at training time.</td></tr>
</tbody>
</table>
</div>

<div align='right'><a href='#top'>↑ 返回顶部</a></div>

---
> 本列表自动生成 | [反馈问题](https://github.com/your-repo/issues)
> 更新于: 2026.02.12
