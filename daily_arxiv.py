import os
import re
import json
import arxiv
import yaml
import logging
import argparse
import datetime
import requests
import time
import html

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"

# DeepSeek API配置
DEEPSEEK_API_KEY = "sk-179d350b272b4b4da85b426b6271c7b5"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

def load_config(config_file:str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''
    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = '' # NO-USE
        OR = ' OR ' # TODO
        def parse_filters(filters:list):
            ret = ''
            for idx in range(0,len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)
                else:
                    ret += (QUOTA + filter + QUOTA)
                if idx != len(filters) - 1:
                    ret += OR
            return ret
        for k,v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords
    with open(config_file,'r') as f:
        config = yaml.load(f,Loader=yaml.FullLoader)
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config

def get_authors(authors, first_author = False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0]
    return output

def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output

def get_code_link(qword:str) -> str:
    """
    This short function was auto-generated by ChatGPT.
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    query = f"{qword}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc"
    }
    r = requests.get(github_url, params=params)
    results = r.json()
    code_link = None
    if results["total_count"] > 0:
        code_link = results["items"][0]["html_url"]
    return code_link

def get_paper_summary(paper_title: str, paper_abstract: str) -> str:
    """
    使用DeepSeek API总结论文内容和创新点
    @param paper_title: 论文标题
    @param paper_abstract: 论文摘要
    @return: 格式化的总结内容
    """
    # 检查API密钥是否配置
    if not DEEPSEEK_API_KEY or DEEPSEEK_API_KEY == "your_api_key_here":
        logging.warning("DeepSeek API key not configured. Skipping summary generation.")
        return "API key missing"
    
    try:
        # 构建API请求
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # 创建更有效的提示词
        prompt = (
            f"请用1-2句话总结以下论文的核心贡献，并列出1-2个关键创新点:\n"
            f"标题: {paper_title}\n"
            f"摘要: {paper_abstract}\n\n"
            f"输出格式要求:\n"
            f"- 核心贡献不超过20字\n"
            f"- 创新点用'◆'符号开头\n"
            f"- 总字数不超过50字\n"
            f"- 使用中文"
        )
        
        data = {
            "model": "deepseek-reasoner",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 150,
            "stream": False
        }
        
        # 调用DeepSeek API
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=20)
        response.raise_for_status()
        
        # 解析响应
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        
        # 记录API调用详情
        logging.debug(f"DeepSeek API request successful. Response: {content}")
        
        # 清理内容
        content = re.sub(r"^.*?(核心贡献|创新点)[：:\s]*", "", content)
        content = content.replace('**', '').replace('###', '').strip()
        
        # 截断超过50字的内容
        if len(content) > 50:
            content = content[:47] + "..."
            
        return content
    
    except requests.exceptions.RequestException as e:
        logging.error(f"DeepSeek API请求失败: {e}")
        return "API请求失败"
    except (KeyError, IndexError) as e:
        logging.error(f"DeepSeek API响应解析失败: {e}")
        return "响应解析失败"
    except Exception as e:
        logging.error(f"DeepSeek API未知错误: {e}")
        return "未知错误"

def generate_fallback_summary(paper_abstract: str) -> str:
    """生成备用摘要（当API失败时使用）"""
    sentences = re.split(r'(?<=[.!?])\s+', paper_abstract)
    if len(sentences) >= 3:
        return sentences[0] + " " + sentences[1] + "..."
    elif sentences:
        return sentences[0][:100] + ("..." if len(sentences[0]) > 100 else "")
    return "无可用摘要"

def get_daily_papers(topic, query="slam", max_results=2):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """
    content = dict()
    content_to_web = dict()
    search_engine = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )

    for result in search_engine.results():
        paper_id = result.get_short_id()
        paper_title = result.title
        paper_url = result.entry_id
        code_url = base_url + paper_id
        paper_abstract = result.summary.replace("\n", " ")
        paper_authors = get_authors(result.authors)
        paper_first_author = get_authors(result.authors, first_author=True)
        primary_category = result.primary_category
        publish_time = result.published.date()
        update_time = result.updated.date()
        comments = result.comment

        logging.info(f"Processing: {paper_title} by {paper_first_author}")

        # 生成论文总结
        try:
            paper_summary = get_paper_summary(paper_title, paper_abstract)
            # 如果API返回错误，使用备用摘要
            if "失败" in paper_summary or "错误" in paper_summary:
                paper_summary = generate_fallback_summary(paper_abstract)
        except Exception as e:
            logging.error(f"摘要生成失败: {e}")
            paper_summary = generate_fallback_summary(paper_abstract)
        
        # 添加延迟以避免API速率限制
        time.sleep(1.5)

        # 处理arxiv ID
        ver_pos = paper_id.find('v')
        paper_key = paper_id[0:ver_pos] if ver_pos != -1 else paper_id
        paper_url = arxiv_url + 'abs/' + paper_key

        try:
            # 获取源码链接
            r = requests.get(code_url, timeout=10).json()
            repo_url = None
            if "official" in r and r["official"]:
                repo_url = r["official"]["url"]
            else:
                # 尝试GitHub搜索
                repo_url = get_code_link(paper_title)
            
            # 准备表格内容
            title_short = paper_title[:50] + ("..." if len(paper_title) > 50 else "")
            author_short = paper_first_author[:20] + ("..." if len(paper_first_author) > 20 else "")
            
            if repo_url:
                content[paper_key] = f"|{update_time}|{title_short}|{author_short}|[{paper_key}]({paper_url})|[code]({repo_url})|{paper_summary}|\n"
                content_to_web[paper_key] = f"- {update_time}, **{paper_title}**, {paper_first_author} et al., Paper: [{paper_key}]({paper_url}), Code: [link]({repo_url}), Summary: {paper_summary}"
            else:
                content[paper_key] = f"|{update_time}|{title_short}|{author_short}|[{paper_key}]({paper_url})|null|{paper_summary}|\n"
                content_to_web[paper_key] = f"- {update_time}, **{paper_title}**, {paper_first_author} et al., Paper: [{paper_key}]({paper_url}), Summary: {paper_summary}"

            # 添加注释信息
            if comments:
                content_to_web[paper_key] += f", {comments}\n"
            else:
                content_to_web[paper_key] += "\n"

        except Exception as e:
            logging.error(f"处理论文时出错 {paper_key}: {e}")
            # 创建没有代码链接的条目
            content[paper_key] = f"|{update_time}|{paper_title[:50]}|{paper_first_author[:20]}|[{paper_key}]({paper_url})|null|{paper_summary}|\n"
            content_to_web[paper_key] = f"- {update_time}, {paper_title}, {paper_first_author} et al., Paper: [{paper_key}]({paper_url}), Summary: {paper_summary}\n"

    data = {topic: content}
    data_web = {topic: content_to_web}
    return data, data_web

def update_paper_links(filename):
    '''每周更新JSON文件中的论文链接'''
    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date, title, authors, arxiv_id, code

    with open(filename, "r") as f:
        content = f.read()
        m = json.loads(content) if content else {}

        json_data = m.copy()

        for keywords, v in json_data.items():
            logging.info(f'Updating keyword: {keywords}')
            for paper_id, contents in v.items():
                contents = str(contents)
                try:
                    update_time, paper_title, paper_first_author, paper_url, code_url = parse_arxiv_string(contents)
                    contents = f"|{update_time}|{paper_title}|{paper_first_author}|{paper_url}|{code_url}|\n"
                    json_data[keywords][paper_id] = str(contents)
                    
                    if '|null|' in contents:
                        try:
                            code_url = base_url + paper_id
                            r = requests.get(code_url, timeout=10).json()
                            if "official" in r and r["official"]:
                                repo_url = r["official"]["url"]
                                if repo_url:
                                    new_cont = contents.replace('|null|', f'|[link]({repo_url})|')
                                    json_data[keywords][paper_id] = str(new_cont)
                                    logging.info(f'Updated code link for {paper_id}')
                        except Exception as e:
                            logging.error(f"更新链接时出错 {paper_id}: {e}")
                except Exception as e:
                    logging.error(f"解析论文条目时出错: {e}")
        
        with open(filename, "w") as f:
            json.dump(json_data, f)

def update_json_file(filename, data_dict):
    '''使用数据字典每日更新JSON文件'''
    with open(filename, "r") as f:
        content = f.read()
        m = json.loads(content) if content else {}

    json_data = m.copy()

    for data in data_dict:
        for keyword, papers in data.items():
            if keyword in json_data:
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename, "w") as f:
        json.dump(json_data, f)

def json_to_md(filename, md_filename,
               task='',
               to_web=False,
               use_title=True,
               use_tc=True,
               show_badge=True,
               use_b2t=True):
    """将JSON文件转换为Markdown格式"""
    DateNow = datetime.date.today().strftime('%Y.%m.%d')
    
    with open(filename, "r") as f:
        content = f.read()
        data = json.loads(content) if content else {}

    # 创建或清空Markdown文件
    with open(md_filename, "w+") as f:
        f.write(f"# 论文速递 ({DateNow})\n\n")
        f.write("> 每日更新计算机视觉领域的最新论文\n\n")
        f.write("> 使用说明: [点击查看](./docs/README.md#usage)\n\n")
        
        # 添加CSS样式
        f.write("""<style>
table {
  width: 100%;
  font-size: 0.85em;
  border-collapse: collapse;
}
th, td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: left;
}
th {
  background-color: #f2f2f2;
  font-weight: bold;
}
tr:nth-child(even) {
  background-color: #f9f9f9;
}
.paper-title {
  max-width: 250px;
}
.paper-summary {
  max-width: 350px;
  word-wrap: break-word;
}
</style>\n\n""")
        
        # 添加目录
        if use_tc:
            f.write("<details>\n<summary>目录</summary>\n<ol>\n")
            for keyword in data.keys():
                if data[keyword]:
                    kw_slug = re.sub(r'\W+', '-', keyword.lower())
                    f.write(f"<li><a href='#{kw_slug}'>{keyword}</a></li>\n")
            f.write("</ol>\n</details>\n\n")
        
        # 添加各个主题部分
        for keyword, papers in data.items():
            if not papers:
                continue
                
            kw_slug = re.sub(r'\W+', '-', keyword.lower())
            f.write(f"<h2 id='{kw_slug}'>{keyword}</h2>\n\n")
            
            # 表格标题
            f.write("| 日期 | 标题 | 作者 | 论文 | 代码 | 摘要 |\n")
            f.write("|:----|:-----|:-----|:-----|:-----|:------|\n")
            
            # 排序论文
            sorted_papers = sorted(papers.items(), key=lambda x: x[0], reverse=True)
            
            # 添加论文条目
            for paper_id, paper_entry in sorted_papers:
                # 清理条目中的特殊字符
                cleaned_entry = html.unescape(paper_entry)
                f.write(cleaned_entry)
            
            f.write("\n")
            
            # 返回顶部链接
            if use_b2t:
                f.write(f"<div align='right'><a href='#top'>↑ 返回顶部</a></div>\n\n")
        
        # 添加页脚
        f.write("---\n")
        f.write("> 本列表自动生成 | [反馈问题](https://github.com/your-repo/issues)\n")

    logging.info(f"{task} 已完成")

def demo(**config):
    data_collector = []
    data_collector_web = []
    
    keywords = config['kv']
    max_results = config['max_results']
    publish_readme = config['publish_readme']
    publish_gitpage = config['publish_gitpage']
    publish_wechat = config['publish_wechat']
    show_badge = config['show_badge']
    b_update = config['update_paper_links']
    
    logging.info(f'更新论文链接: {b_update}')
    if not b_update:
        logging.info("开始获取每日论文")
        for topic, keyword in keywords.items():
            logging.info(f"关键词: {topic}")
            data, data_web = get_daily_papers(topic, query=keyword, max_results=max_results)
            data_collector.append(data)
            data_collector_web.append(data_web)
        logging.info("获取每日论文完成")
    
    # 更新README.md
    if publish_readme:
        json_file = config['json_readme_path']
        md_file = config['md_readme_path']
        if b_update:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector)
        json_to_md(json_file, md_file, task='更新README', show_badge=show_badge)
    
    # 更新GitHub Pages
    if publish_gitpage:
        json_file = config['json_gitpage_path']
        md_file = config['md_gitpage_path']
        if b_update:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector)
        json_to_md(json_file, md_file, task='更新GitPage', to_web=True, 
                  show_badge=show_badge, use_tc=False, use_b2t=False)
    
    # 更新微信文档
    if publish_wechat:
        json_file = config['json_wechat_path']
        md_file = config['md_wechat_path']
        if b_update:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector_web)
        json_to_md(json_file, md_file, task='更新微信', to_web=False, 
                  use_title=False, show_badge=show_badge)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path', type=str, default='config.yaml',
                       help='配置文件路径')
    parser.add_argument('--update_paper_links', action='store_true',
                       help='是否更新论文链接')
    args = parser.parse_args()
    
    # 设置更详细的日志
    logging.getLogger().setLevel(logging.INFO)
    logging.info("启动论文速递更新")
    
    config = load_config(args.config_path)
    config['update_paper_links'] = args.update_paper_links
    demo(**config)
    
    logging.info("更新完成")