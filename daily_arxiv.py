import os
import re
import json
import arxiv
import yaml
import logging
import argparse
import datetime
import requests
import time
import html
import feedparser  # 添加备用解析库

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"

# DeepSeek API配置
DEEPSEEK_API_KEY = "sk-179d350b272b4b4da85b426b6271c7b5"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

def load_config(config_file:str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''
    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = '' # NO-USE
        OR = ' OR ' # TODO
        def parse_filters(filters:list):
            ret = ''
            for idx in range(0,len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)
                else:
                    ret += (QUOTA + filter + QUOTA)
                if idx != len(filters) - 1:
                    ret += OR
            return ret
        for k,v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords
    with open(config_file,'r') as f:
        config = yaml.load(f,Loader=yaml.FullLoader)
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config

def get_authors(authors, first_author = False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0] + "等" if len(authors) > 1 else authors[0]
    return output

def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output

def get_code_link(qword:str) -> str:
    """
    This short function was auto-generated by ChatGPT.
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    query = f"{qword}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc"
    }
    try:
        r = requests.get(github_url, params=params, timeout=15)
        results = r.json()
        code_link = None
        if results["total_count"] > 0:
            code_link = results["items"][0]["html_url"]
        return code_link
    except Exception as e:
        logging.error(f"GitHub搜索失败: {e}")
        return None

def get_paper_summary(paper_title: str, paper_abstract: str) -> str:
    """
    使用DeepSeek API总结论文内容和创新点
    @param paper_title: 论文标题
    @param paper_abstract: 论文摘要
    @return: 格式化的总结内容
    """
    # 检查API密钥是否配置
    if not DEEPSEEK_API_KEY or DEEPSEEK_API_KEY == "your_api_key_here":
        logging.warning("DeepSeek API key not configured. Skipping summary generation.")
        return generate_fallback_summary(paper_abstract)
    
    try:
        # 构建API请求
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # 创建更有效的提示词
        prompt = (
            f"请用1-2句话总结以下论文的核心贡献，并列出1-2个关键创新点:\n"
            f"标题: {paper_title}\n"
            f"摘要: {paper_abstract}\n\n"
            f"输出格式要求:\n"
            f"- 核心贡献不超过20字\n"
            f"- 创新点用'◆'符号开头\n"
            f"- 总字数不超过50字\n"
            f"- 使用中文"
        )
        
        data = {
            "model": "deepseek-reasoner",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 150,
            "stream": False
        }
        
        # 调用DeepSeek API
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=20)
        response.raise_for_status()
        
        # 解析响应
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        
        # 记录API调用详情
        logging.debug(f"DeepSeek API request successful. Response: {content}")
        
        # 清理内容
        content = re.sub(r"^.*?(核心贡献|创新点)[：:\s]*", "", content)
        content = content.replace('**', '').replace('###', '').strip()
        
        # 截断超过50字的内容
        if len(content) > 50:
            content = content[:47] + "..."
            
        return content
    
    except requests.exceptions.RequestException as e:
        logging.error(f"DeepSeek API请求失败: {e}")
        return generate_fallback_summary(paper_abstract)
    except (KeyError, IndexError) as e:
        logging.error(f"DeepSeek API响应解析失败: {e}")
        return generate_fallback_summary(paper_abstract)
    except Exception as e:
        logging.error(f"DeepSeek API未知错误: {e}")
        return generate_fallback_summary(paper_abstract)

def generate_fallback_summary(paper_abstract: str) -> str:
    """生成备用摘要（当API失败时使用）"""
    sentences = re.split(r'(?<=[.!?])\s+', paper_abstract)
    if len(sentences) >= 2:
        return sentences[0] + " " + sentences[1][:50] + "..."
    elif sentences:
        return sentences[0][:70] + ("..." if len(sentences[0]) > 70 else "")
    return "无可用摘要"

def fetch_arxiv_results(query, max_results=10):
    """获取arXiv结果（带重试机制）"""
    max_retries = 5
    for attempt in range(max_retries):
        try:
            # 使用arxiv库获取结果
            client = arxiv.Client(num_retries=3)
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            return list(client.results(search))
        except Exception as e:
            logging.warning(f"arXiv API请求失败 (尝试 {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                wait_time = (2 ** attempt) + 1  # 指数退避
                logging.info(f"等待 {wait_time} 秒后重试...")
                time.sleep(wait_time)
    
    # 如果所有重试都失败，尝试直接API调用
    logging.warning("arxiv.py库请求失败，尝试直接调用arXiv API")
    try:
        url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": query,
            "start": 0,
            "max_results": max_results,
            "sortBy": "submittedDate",
            "sortOrder": "descending"
        }
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        
        results = []
        for entry in feed.entries:
            # 解析arXiv返回的Atom格式数据
            result = arxiv.Result(
                entry_id=entry.id,
                updated=datetime.datetime.strptime(entry.updated, "%Y-%m-%dT%H:%M:%SZ"),
                published=datetime.datetime.strptime(entry.published, "%Y-%m-%dT%H:%M:%SZ"),
                title=entry.title,
                authors=[arxiv.Author(name=a.name) for a in entry.authors],
                summary=entry.summary,
                comment=entry.get("arxiv_comment", ""),
                journal_ref=entry.get("arxiv_journal_ref", ""),
                doi=entry.get("arxiv_doi", ""),
                primary_category=entry.get("arxiv_primary_category", {}).get("term", ""),
                categories=[t.term for t in entry.tags if t.scheme == "http://arxiv.org/schemas/atom"],
                links=[arxiv.Link(href=l.href, title=l.title, rel=l.rel) for l in entry.links],
                pdf_url=next((l.href for l in entry.links if l.title == "pdf"), None)
            )
            results.append(result)
        return results
    except Exception as e:
        logging.error(f"直接arXiv API请求失败: {e}")
        return []

def get_daily_papers(topic, query="slam", max_results=10):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """
    content = dict()
    content_to_web = dict()
    
    logging.info(f"获取主题 '{topic}' 的论文")
    results = fetch_arxiv_results(query, max_results)
    
    if not results:
        logging.error(f"无法获取主题 '{topic}' 的论文")
        return {}, {}

    for result in results:
        paper_id = result.get_short_id()
        paper_title = result.title
        paper_url = result.entry_id
        code_url = base_url + paper_id
        paper_abstract = result.summary.replace("\n", " ")
        paper_authors = get_authors(result.authors)
        paper_first_author = get_authors(result.authors, first_author=True)
        publish_time = result.published.date()
        update_time = result.updated.date()
        comments = result.comment

        logging.info(f"处理: {paper_title} by {paper_first_author}")

        # 生成论文总结
        try:
            paper_summary = get_paper_summary(paper_title, paper_abstract)
        except Exception as e:
            logging.error(f"摘要生成失败: {e}")
            paper_summary = generate_fallback_summary(paper_abstract)
        
        # 添加延迟以避免API速率限制
        time.sleep(1.5)

        # 处理arxiv ID
        ver_pos = paper_id.find('v')
        paper_key = paper_id[0:ver_pos] if ver_pos != -1 else paper_id
        paper_url = arxiv_url + 'abs/' + paper_key

        try:
            # 获取源码链接
            repo_url = None
            try:
                r = requests.get(code_url, timeout=15).json()
                if "official" in r and r["official"]:
                    repo_url = r["official"]["url"]
            except Exception as e:
                logging.warning(f"paperswithcode API请求失败: {e}")
            
            # 如果未找到代码，尝试GitHub搜索
            if not repo_url:
                repo_url = get_code_link(paper_title)
            
            # 准备表格内容
            title_short = paper_title[:50] + ("..." if len(paper_title) > 50 else "")
            author_short = paper_first_author[:20] + ("..." if len(paper_first_author) > 20 else "")
            summary_short = paper_summary[:100] + ("..." if len(paper_summary) > 100 else "")
            
            if repo_url:
                content[paper_key] = f"|{update_time}|{title_short}|{author_short}|[{paper_key}]({paper_url})|[code]({repo_url})|{summary_short}|\n"
                content_to_web[paper_key] = f"- {update_time}, **{paper_title}**, {paper_first_author} et al., Paper: [{paper_key}]({paper_url}), Code: [link]({repo_url}), Summary: {paper_summary}"
            else:
                content[paper_key] = f"|{update_time}|{title_short}|{author_short}|[{paper_key}]({paper_url})|无|{summary_short}|\n"
                content_to_web[paper_key] = f"- {update_time}, **{paper_title}**, {paper_first_author} et al., Paper: [{paper_key}]({paper_url}), Summary: {paper_summary}"

            # 添加注释信息
            if comments:
                content_to_web[paper_key] += f", {comments}\n"
            else:
                content_to_web[paper_key] += "\n"

        except Exception as e:
            logging.error(f"处理论文时出错 {paper_key}: {e}")
            # 创建没有代码链接的条目
            content[paper_key] = f"|{update_time}|{paper_title[:50]}|{paper_first_author[:20]}|[{paper_key}]({paper_url})|无|{paper_summary[:100]}|\n"
            content_to_web[paper_key] = f"- {update_time}, {paper_title}, {paper_first_author} et al., Paper: [{paper_key}]({paper_url}), Summary: {paper_summary}\n"

    data = {topic: content}
    data_web = {topic: content_to_web}
    return data, data_web

def update_paper_links(filename):
    '''每周更新JSON文件中的论文链接'''
    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date, title, authors, arxiv_id, code

    with open(filename, "r") as f:
        content = f.read()
        m = json.loads(content) if content else {}

        json_data = m.copy()

        for keywords, v in json_data.items():
            logging.info(f'更新关键词: {keywords}')
            for paper_id, contents in v.items():
                contents = str(contents)
                try:
                    update_time, paper_title, paper_first_author, paper_url, code_url = parse_arxiv_string(contents)
                    contents = f"|{update_time}|{paper_title}|{paper_first_author}|{paper_url}|{code_url}|\n"
                    json_data[keywords][paper_id] = str(contents)
                    
                    if '|无|' in contents or '|null|' in contents:
                        try:
                            code_url = base_url + paper_id
                            r = requests.get(code_url, timeout=15).json()
                            if "official" in r and r["official"]:
                                repo_url = r["official"]["url"]
                                if repo_url:
                                    new_cont = contents.replace('|无|', f'|[code]({repo_url})|').replace('|null|', f'|[code]({repo_url})|')
                                    json_data[keywords][paper_id] = str(new_cont)
                                    logging.info(f'为 {paper_id} 更新代码链接')
                        except Exception as e:
                            logging.error(f"更新链接时出错 {paper_id}: {e}")
                except Exception as e:
                    logging.error(f"解析论文条目时出错: {e}")
        
        with open(filename, "w") as f:
            json.dump(json_data, f)

def update_json_file(filename, data_dict):
    '''使用数据字典每日更新JSON文件'''
    with open(filename, "r") as f:
        content = f.read()
        m = json.loads(content) if content else {}

    json_data = m.copy()

    for data in data_dict:
        for keyword, papers in data.items():
            if keyword in json_data:
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename, "w") as f:
        json.dump(json_data, f)

def json_to_md(filename, md_filename,
               task='',
               to_web=False,
               use_title=True,
               use_tc=True,
               show_badge=True,
               use_b2t=True):
    """将JSON文件转换为Markdown格式"""
    DateNow = datetime.date.today().strftime('%Y.%m.%d')
    
    with open(filename, "r") as f:
        content = f.read()
        data = json.loads(content) if content else {}

    # 创建或清空Markdown文件
    with open(md_filename, "w+", encoding="utf-8") as f:
        f.write(f"# 计算机视觉领域最新论文 ({DateNow})\n\n")
        f.write("> 每日自动更新计算机视觉领域的最新arXiv论文\n\n")
        f.write("> 使用说明: [点击查看](./docs/README.md#usage)\n\n")
        
        # 添加CSS样式
        f.write("""<style>
table {
  width: 100%;
  font-size: 0.9em;
  border-collapse: collapse;
  margin-bottom: 15px;
}
th, td {
  border: 1px solid #ddd;
  padding: 10px;
  text-align: left;
}
th {
  background-color: #f8f9fa;
  font-weight: bold;
}
.paper-title {
  max-width: 250px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}
.paper-summary {
  max-width: 350px;
  word-wrap: break-word;
  font-size: 0.9em;
  color: #555;
}
</style>\n\n""")
        
        # 添加目录
        if use_tc:
            f.write("<details>\n<summary>目录</summary>\n<ol>\n")
            for keyword in data.keys():
                if data[keyword]:
                    kw_slug = re.sub(r'\W+', '-', keyword.lower())
                    f.write(f"<li><a href='#{kw_slug}'>{keyword}</a></li>\n")
            f.write("</ol>\n</details>\n\n")
        
        # 添加各个主题部分
        for keyword, papers in data.items():
            if not papers:
                continue
                
            kw_slug = re.sub(r'\W+', '-', keyword.lower())
            f.write(f"<h2 id='{kw_slug}'>{keyword}</h2>\n\n")
            
            # 表格标题
            f.write("| 日期 | 标题 | 作者 | 论文 | 代码 | 摘要 |\n")
            f.write("|:----|:-----|:-----|:-----|:-----|:------|\n")
            
            # 排序论文
            sorted_papers = sorted(papers.items(), key=lambda x: x[0], reverse=True)
            
            # 添加论文条目
            for paper_id, paper_entry in sorted_papers:
                # 清理条目中的特殊字符
                cleaned_entry = html.unescape(paper_entry)
                f.write(cleaned_entry)
            
            f.write("\n")
            
            # 返回顶部链接
            if use_b2t:
                f.write(f"<div align='right'><a href='#top'>↑ 返回顶部</a></div>\n\n")
        
        # 添加页脚
        f.write("---\n")
        f.write("> 本列表自动生成 | [反馈问题](https://github.com/your-repo/issues)\n")
        f.write("> 更新于: " + DateNow + "\n")

    logging.info(f"{task} 已完成")

def demo(**config):
    data_collector = []
    data_collector_web = []
    
    keywords = config['kv']
    max_results = config['max_results']
    publish_readme = config['publish_readme']
    publish_gitpage = config['publish_gitpage']
    publish_wechat = config['publish_wechat']
    show_badge = config['show_badge']
    b_update = config['update_paper_links']
    
    logging.info(f'更新论文链接: {b_update}')
    if not b_update:
        logging.info("开始获取每日论文")
        for topic, keyword in keywords.items():
            logging.info(f"关键词: {topic}")
            data, data_web = get_daily_papers(topic, query=keyword, max_results=max_results)
            data_collector.append(data)
            data_collector_web.append(data_web)
        logging.info("获取每日论文完成")
    
    # 更新README.md
    if publish_readme:
        json_file = config['json_readme_path']
        md_file = config['md_readme_path']
        if b_update:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector)
        json_to_md(json_file, md_file, task='更新README', show_badge=show_badge)
    
    # 更新GitHub Pages
    if publish_gitpage:
        json_file = config['json_gitpage_path']
        md_file = config['md_gitpage_path']
        if b_update:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector)
        json_to_md(json_file, md_file, task='更新GitPage', to_web=True, 
                  show_badge=show_badge, use_tc=False, use_b2t=False)
    
    # 更新微信文档
    if publish_wechat:
        json_file = config['json_wechat_path']
        md_file = config['md_wechat_path']
        if b_update:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector_web)
        json_to_md(json_file, md_file, task='更新微信', to_web=False, 
                  use_title=False, show_badge=show_badge)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path', type=str, default='config.yaml',
                       help='配置文件路径')
    parser.add_argument('--update_paper_links', action='store_true',
                       help='是否更新论文链接')
    args = parser.parse_args()
    
    # 设置更详细的日志
    logging.getLogger().setLevel(logging.INFO)
    logging.info("启动论文速递更新")
    
    config = load_config(args.config_path)
    config['update_paper_links'] = args.update_paper_links
    demo(**config)
    
    logging.info("更新完成")